,ID,TITLE,ABSTRACT,Computer Science,Physics,Mathematics,Statistics,Quantitative Biology,Quantitative Finance
837,838,Multi-view Low-rank Sparse Subspace Clustering,"  Most existing approaches address multi-view subspace clustering problem by
constructing the affinity matrix on each view separately and afterwards propose
how to extend spectral clustering algorithm to handle multi-view data. This
paper presents an approach to multi-view subspace clustering that learns a
joint subspace representation by constructing affinity matrix shared among all
views. Relying on the importance of both low-rank and sparsity constraints in
the construction of the affinity matrix, we introduce the objective that
balances between the agreement across different views, while at the same time
encourages sparsity and low-rankness of the solution. Related low-rank and
sparsity constrained optimization problem is for each view solved using the
alternating direction method of multipliers. Furthermore, we extend our
approach to cluster data drawn from nonlinear subspaces by solving the
corresponding problem in a reproducing kernel Hilbert space. The proposed
algorithm outperforms state-of-the-art multi-view subspace clustering
algorithms on one synthetic and four real-world datasets.
",1,0,0,1,0,0
9414,9415,Lipschitz perturbations of Morse-Smale semigroups,"  In this paper we will deal with Lipschitz continuous perturbations of
Morse-Smale semigroups with only equilibrium points as critical elements. We
study the behavior of the structure of equilibrium points and their connections
when subjected to non-differentiable perturbations. To this end we define more
general notions of \emph{hyperbolicity} and \emph{transversality}, which do not
require differentiability.
",0,0,1,0,0,0
5823,5824,Named Entity Evolution Recognition on the Blogosphere,"  Advancements in technology and culture lead to changes in our language. These
changes create a gap between the language known by users and the language
stored in digital archives. It affects user's possibility to firstly find
content and secondly interpret that content. In previous work we introduced our
approach for Named Entity Evolution Recognition~(NEER) in newspaper
collections. Lately, increasing efforts in Web preservation lead to increased
availability of Web archives covering longer time spans. However, language on
the Web is more dynamic than in traditional media and many of the basic
assumptions from the newspaper domain do not hold for Web data. In this paper
we discuss the limitations of existing methodology for NEER. We approach these
by adapting an existing NEER method to work on noisy data like the Web and the
Blogosphere in particular. We develop novel filters that reduce the noise and
make use of Semantic Web resources to obtain more information about terms. Our
evaluation shows the potentials of the proposed approach.
",1,0,0,0,0,0
3624,3625,Linear magnetoresistance in the charge density wave state of quasi-two-dimensional rare-earth tritellurides,"  We report measurements of the magnetoresistance in the charge density wave
(CDW) state of rare-earth tritellurides, namely TbTe$_3$ and HoTe$_3$. The
magnetic field dependence of magnetoresistance exhibits a temperature dependent
crossover between a conventional quadratic law at high $T$ and low $B$ and an
unusual linear dependence at low $T$ and high $B$. We present a quite general
model to explain the linear magnetoresistance taking into account the strong
scattering of quasiparticles on CDW fluctuations in the vicinity of ""hot spots""
of the Fermi surface (FS) where the FS reconstruction is the strongest.
",0,1,0,0,0,0
2266,2267,Satellite altimetry reveals spatial patterns of variations in the Baltic Sea wave climate,"  The main properties of the climate of waves in the seasonally ice-covered
Baltic Sea and its decadal changes since 1990 are estimated from satellite
altimetry data. The data set of significant wave heights (SWH) from all
existing nine satellites, cleaned and cross-validated against in situ
measurements, shows overall a very consistent picture. A comparison with visual
observations shows a good correspondence with correlation coefficients of
0.6-0.8. The annual mean SWH reveals a tentative increase of 0.005 m yr-1, but
higher quantiles behave in a cyclic manner with a timescale of 10-15 yr.
Changes in the basin-wide average SWH have a strong meridional pattern: an
increase in the central and western parts of the sea and decrease in the east.
This pattern is likely caused by a rotation of wind directions rather than by
an increase in the wind speed.
",0,1,0,0,0,0
985,986,A Galactic Cosmic Ray Electron Intensity Increase of a factor of up to 100 At Energies between 3 and 50 MeV in the Heliosheath between the Termination Shock and the Heliopause Due to Solar Modulation As Measured by Voyager 1,"  We have derived background corrected intensities of 3-50 MeV galactic
electrons observed by Voyager 1 as it passes through the heliosheath from 95 to
122 AU. The overall intensity change of the background corrected data from the
inner to the outer boundary of the heliosheath is a maximum of a factor ~100 at
15 MeV. At lower energies this fractional change becomes less and the corrected
electron spectra in the heliosheath becomes progressively steeper, reaching
values ~ -2.5 for the spectral index just outside of the termination shock. At
higher energies the spectra of electrons has an exponent changing from the
negative LIS spectral index of -1.3 to values approaching zero in the
heliosheath as a result of the solar modulation of the galactic electron
component. The large modulation effects observed below ~100 MV are possible
evidence for enhanced diffusion as part of the modulation process for electrons
in the heliosheath.
",0,1,0,0,0,0
3644,3645,Identifiability and Estimation of Structural Vector Autoregressive Models for Subsampled and Mixed Frequency Time Series,"  Causal inference in multivariate time series is challenging due to the fact
that the sampling rate may not be as fast as the timescale of the causal
interactions. In this context, we can view our observed series as a subsampled
version of the desired series. Furthermore, due to technological and other
limitations, series may be observed at different sampling rates, representing a
mixed frequency setting. To determine instantaneous and lagged effects between
time series at the true causal scale, we take a model-based approach based on
structural vector autoregressive (SVAR) models. In this context, we present a
unifying framework for parameter identifiability and estimation under both
subsampling and mixed frequencies when the noise, or shocks, are non-Gaussian.
Importantly, by studying the SVAR case, we are able to both provide
identifiability and estimation methods for the causal structure of both lagged
and instantaneous effects at the desired time scale. We further derive an exact
EM algorithm for inference in both subsampled and mixed frequency settings. We
validate our approach in simulated scenarios and on two real world data sets.
",0,0,0,1,0,0
1928,1929,Majorana bound states in hybrid 2D Josephson junctions with ferromagnetic insulators,"  We consider a Josephson junction consisting of superconductor/ferromagnetic
insulator (S/FI) bilayers as electrodes which proximizes a nearby 2D electron
gas. By starting from a generic Josephson hybrid planar setup we present an
exhaustive analysis of the the interplay between the superconducting and
magnetic proximity effects and the conditions under which the structure
undergoes transitions to a non-trivial topological phase. We address the 2D
bound state problem using a general transfer matrix approach that reduces the
problem to an effective 1D Hamiltonian. This allows for straightforward study
of topological properties in different symmetry classes. As an example we
consider a narrow channel coupled with multiple ferromagnetic superconducting
fingers, and discuss how the Majorana bound states can be spatially controlled
by tuning the superconducting phases. Following our approach we also show the
energy spectrum, the free energy and finally the multiterminal Josephson
current of the setup.
",0,1,0,0,0,0
1024,1025,Interactions between Health Searchers and Search Engines,"  The Web is an important resource for understanding and diagnosing medical
conditions. Based on exposure to online content, people may develop undue
health concerns, believing that common and benign symptoms are explained by
serious illnesses. In this paper, we investigate potential strategies to mine
queries and searcher histories for clues that could help search engines choose
the most appropriate information to present in response to exploratory medical
queries. To do this, we performed a longitudinal study of health search
behavior using the logs of a popular search engine. We found that query
variations which might appear innocuous (e.g. ""bad headache"" vs ""severe
headache"") may hold valuable information about the searcher which could be used
by search engines to improve performance. Furthermore, we investigated how
medically concerned users respond differently to search engine result pages
(SERPs) and find that their disposition for clicking on concerning pages is
pronounced, potentially leading to a self-reinforcement of concern. Finally, we
studied to which degree variations in the SERP impact future search and
real-world health-seeking behavior and obtained some surprising results (e.g.,
viewing concerning pages may lead to a short-term reduction of real-world
health seeking).
",1,0,0,0,0,0
14523,14524,Optimal Weighting for Exam Composition,"  A problem faced by many instructors is that of designing exams that
accurately assess the abilities of the students. Typically these exams are
prepared several days in advance, and generic question scores are used based on
rough approximation of the question difficulty and length. For example, for a
recent class taught by the author, there were 30 multiple choice questions
worth 3 points, 15 true/false with explanation questions worth 4 points, and 5
analytical exercises worth 10 points. We describe a novel framework where
algorithms from machine learning are used to modify the exam question weights
in order to optimize the exam scores, using the overall class grade as a proxy
for a student's true ability. We show that significant error reduction can be
obtained by our approach over standard weighting schemes, and we make several
new observations regarding the properties of the ""good"" and ""bad"" exam
questions that can have impact on the design of improved future evaluation
methods.
",0,0,0,1,0,0
19119,19120,Can Adversarial Networks Hallucinate Occluded People With a Plausible Aspect?,"  When you see a person in a crowd, occluded by other persons, you miss visual
information that can be used to recognize, re-identify or simply classify him
or her. You can imagine its appearance given your experience, nothing more.
Similarly, AI solutions can try to hallucinate missing information with
specific deep learning architectures, suitably trained with people with and
without occlusions. The goal of this work is to generate a complete image of a
person, given an occluded version in input, that should be a) without occlusion
b) similar at pixel level to a completely visible people shape c) capable to
conserve similar visual attributes (e.g. male/female) of the original one. For
the purpose, we propose a new approach by integrating the state-of-the-art of
neural network architectures, namely U-nets and GANs, as well as discriminative
attribute classification nets, with an architecture specifically designed to
de-occlude people shapes. The network is trained to optimize a Loss function
which could take into account the aforementioned objectives. As well we propose
two datasets for testing our solution: the first one, occluded RAP, created
automatically by occluding real shapes of the RAP dataset (which collects also
attributes of the people aspect); the second is a large synthetic dataset, AiC,
generated in computer graphics with data extracted from the GTA video game,
that contains 3D data of occluded objects by construction. Results are
impressive and outperform any other previous proposal. This result could be an
initial step to many further researches to recognize people and their behavior
in an open crowded world.
",1,0,0,0,0,0
5617,5618,Review of flexible and transparent thin-film transistors based on zinc oxide and related materials,"  Flexible and transparent electronics presents a new era of electronic
technologies. Ubiquitous applications involve wearable electronics, biosensors,
flexible transparent displays, radio-frequency identifications (RFIDs),
etc.Zinc oxide (ZnO) and related materials are the most commonly used inorganic
semiconductors in flexible and transparent devices, owing to their high
electrical performance, together with low processing temperature and good
optical transparency.In this paper, we review recent advances in flexible and
transparent thin-film transistors (TFTs) based on ZnO and related
materials.After a brief introduction, the main progresses on the preparation of
each component (substrate, electrodes, channel and dielectrics) are summarized
and discussed. Then, the effect of mechanical bending on electrical performance
was highlighted. Finally, we suggest the challenges and opportunities in future
investigations.
",0,1,0,0,0,0
5792,5793,"Portable, high-performance containers for HPC","  Building and deploying software on high-end computing systems is a
challenging task. High performance applications have to reliably run across
multiple platforms and environments, and make use of site-specific resources
while resolving complicated software-stack dependencies. Containers are a type
of lightweight virtualization technology that attempt to solve this problem by
packaging applications and their environments into standard units of software
that are: portable, easy to build and deploy, have a small footprint, and low
runtime overhead. In this work we present an extension to the container runtime
of Shifter that provides containerized applications with a mechanism to access
GPU accelerators and specialized networking from the host system, effectively
enabling performance portability of containers across HPC resources. The
presented extension makes possible to rapidly deploy high-performance software
on supercomputers from containerized applications that have been developed,
built, and tested in non-HPC commodity hardware, e.g. the laptop or workstation
of a researcher.
",1,0,0,0,0,0
5755,5756,Joint Pose and Principal Curvature Refinement Using Quadrics,"  In this paper we present a novel joint approach for optimising surface
curvature and pose alignment. We present two implementations of this joint
optimisation strategy, including a fast implementation that uses two frames and
an offline multi-frame approach. We demonstrate an order of magnitude
improvement in simulation over state of the art dense relative point-to-plane
Iterative Closest Point (ICP) pose alignment using our dense joint
frame-to-frame approach and show comparable pose drift to dense point-to-plane
ICP bundle adjustment using low-cost depth sensors. Additionally our improved
joint quadric based approach can be used to more accurately estimate surface
curvature on noisy point clouds than previous approaches.
",1,0,0,0,0,0
19849,19850,Dirac and Chiral Quantum Spin Liquids on the Honeycomb Lattice in a Magnetic Field,"  Motivated by recent experimental observations in $\alpha$-RuCl$_3$, we study
the $K$-$\Gamma$ model on the honeycomb lattice in an external magnetic field.
By a slave-particle representation and Variational Monte Carlo calculations, we
reproduce the phase transition from zigzag magnetic order to a field-induced
disordered phase. The nature of this state depends crucially on the field
orientation. For particular field directions in the honeycomb plane, we find a
gapless Dirac spin liquid, in agreement with recent experiments on
$\alpha$-RuCl$_3$. For a range of out-of-plane fields, we predict the existence
of a Kalmeyer-Laughlin-type chiral spin liquid, which would show an
integer-quantized thermal Hall effect.
",0,1,0,0,0,0
20243,20244,A Memristor-Based Optimization Framework for AI Applications,"  Memristors have recently received significant attention as ubiquitous
device-level components for building a novel generation of computing systems.
These devices have many promising features, such as non-volatility, low power
consumption, high density, and excellent scalability. The ability to control
and modify biasing voltages at the two terminals of memristors make them
promising candidates to perform matrix-vector multiplications and solve systems
of linear equations. In this article, we discuss how networks of memristors
arranged in crossbar arrays can be used for efficiently solving optimization
and machine learning problems. We introduce a new memristor-based optimization
framework that combines the computational merit of memristor crossbars with the
advantages of an operator splitting method, alternating direction method of
multipliers (ADMM). Here, ADMM helps in splitting a complex optimization
problem into subproblems that involve the solution of systems of linear
equations. The capability of this framework is shown by applying it to linear
programming, quadratic programming, and sparse optimization. In addition to
ADMM, implementation of a customized power iteration (PI) method for
eigenvalue/eigenvector computation using memristor crossbars is discussed. The
memristor-based PI method can further be applied to principal component
analysis (PCA). The use of memristor crossbars yields a significant speed-up in
computation, and thus, we believe, has the potential to advance optimization
and machine learning research in artificial intelligence (AI).
",1,0,0,1,0,0
2207,2208,Hot Stuff for One Year (HSOY) - A 583 million star proper motion catalogue derived from Gaia DR1 and PPMXL,"  Recently, the first installment of data from ESA's Gaia astrometric satellite
mission (Gaia-DR1) was released, containing positions of more than 1 billion
stars with unprecedented precision, as well as only proper motions and
parallaxes, however only for a subset of 2 million objects. The second release,
due in late 2017 or early 2018, will include those quantities for most objects.
In order to provide a dataset that bridges the time gap between the Gaia-DR1
and Gaia-DR2 releases and partly remedies the lack of proper motions in the
former, HSOY (""Hot Stuff for One Year"") was created as a hybrid catalogue
between Gaia-DR1 and ground-based astrometry, featuring proper motions (but no
parallaxes) for a large fraction of the DR1 objects. While not attempting to
compete with future Gaia releases in terms of data quality or number of
objects, the aim of HSOY is to provide improved proper motions partly based on
Gaia data, allowing some studies to be carried out just now or as pilot studies
for later larger projects requiring higher-precision data. The HSOY catalogue
was compiled using the positions taken from Gaia-DR1 combined with the input
data from the PPMXL catalogue, employing the same weighted least-squares
technique that was used to assemble the PPMXL catalogue itself. Results. This
effort resulted in a four-parameter astrometric catalogue containing
583,000,000 objects, with Gaia-DR1 quality positions and proper motions with
precisions from significantly less than 1 mas/yr to 5 mas/yr, depending on the
object's brightness and location on the sky.
",0,1,0,0,0,0
20888,20889,Deep Health Care Text Classification,"  Health related social media mining is a valuable apparatus for the early
recognition of the diverse antagonistic medicinal conditions. Mostly, the
existing methods are based on machine learning with knowledge-based learning.
This working note presents the Recurrent neural network (RNN) and Long
short-term memory (LSTM) based embedding for automatic health text
classification in the social media mining. For each task, two systems are built
and that classify the tweet at the tweet level. RNN and LSTM are used for
extracting features and non-linear activation function at the last layer
facilitates to distinguish the tweets of different categories. The experiments
are conducted on 2nd Social Media Mining for Health Applications Shared Task at
AMIA 2017. The experiment results are considerable; however the proposed method
is appropriate for the health text classification. This is primarily due to the
reason that, it doesn't rely on any feature engineering mechanisms.
",1,0,0,0,0,0
20859,20860,The Effect of Population Control Policies on Societal Fragmentation,"  Population control policies are proposed and in some places employed as a
means towards curbing population growth. This paper is concerned with a
disturbing side-effect of such policies, namely, the potential risk of societal
fragmentation due to changes in the distribution of family sizes. This effect
is illustrated in some simple settings and demonstrated by simulation. In
addition, the dependence of societal fragmentation on family size distribution
is analyzed. In particular, it is shown that under the studied model, any
population control policy that disallows families of 3 or more children incurs
the possible risk of societal fragmentation.
",1,1,0,0,0,0
15275,15276,Strategyproof Mechanisms for Additively Separable Hedonic Games and Fractional Hedonic Games,"  Additively separable hedonic games and fractional hedonic games have received
considerable attention. They are coalition forming games of selfish agents
based on their mutual preferences. Most of the work in the literature
characterizes the existence and structure of stable outcomes (i.e., partitions
in coalitions), assuming that preferences are given. However, there is little
discussion on this assumption. In fact, agents receive different utilities if
they belong to different partitions, and thus it is natural for them to declare
their preferences strategically in order to maximize their benefit. In this
paper we consider strategyproof mechanisms for additively separable hedonic
games and fractional hedonic games, that is, partitioning methods without
payments such that utility maximizing agents have no incentive to lie about
their true preferences. We focus on social welfare maximization and provide
several lower and upper bounds on the performance achievable by strategyproof
mechanisms for general and specific additive functions. In most of the cases we
provide tight or asymptotically tight results. All our mechanisms are simple
and can be computed in polynomial time. Moreover, all the lower bounds are
unconditional, that is, they do not rely on any computational or complexity
assumptions.
",1,0,0,0,0,0
3759,3760,A Multi-Objective Deep Reinforcement Learning Framework,"  This paper presents a new multi-objective deep reinforcement learning (MODRL)
framework based on deep Q-networks. We propose the use of linear and non-linear
methods to develop the MODRL framework that includes both single-policy and
multi-policy strategies. The experimental results on two benchmark problems
including the two-objective deep sea treasure environment and the
three-objective mountain car problem indicate that the proposed framework is
able to converge to the optimal Pareto solutions effectively. The proposed
framework is generic, which allows implementation of different deep
reinforcement learning algorithms in different complex environments. This
therefore overcomes many difficulties involved with standard multi-objective
reinforcement learning (MORL) methods existing in the current literature. The
framework creates a platform as a testbed environment to develop methods for
solving various problems associated with the current MORL. Details of the
framework implementation can be referred to
this http URL.
",0,0,0,1,0,0
10482,10483,Copycat CNN: Stealing Knowledge by Persuading Confession with Random Non-Labeled Data,"  In the past few years, Convolutional Neural Networks (CNNs) have been
achieving state-of-the-art performance on a variety of problems. Many companies
employ resources and money to generate these models and provide them as an API,
therefore it is in their best interest to protect them, i.e., to avoid that
someone else copies them. Recent studies revealed that state-of-the-art CNNs
are vulnerable to adversarial examples attacks, and this weakness indicates
that CNNs do not need to operate in the problem domain (PD). Therefore, we
hypothesize that they also do not need to be trained with examples of the PD in
order to operate in it.
Given these facts, in this paper, we investigate if a target black-box CNN
can be copied by persuading it to confess its knowledge through random
non-labeled data. The copy is two-fold: i) the target network is queried with
random data and its predictions are used to create a fake dataset with the
knowledge of the network; and ii) a copycat network is trained with the fake
dataset and should be able to achieve similar performance as the target
network.
This hypothesis was evaluated locally in three problems (facial expression,
object, and crosswalk classification) and against a cloud-based API. In the
copy attacks, images from both non-problem domain and PD were used. All copycat
networks achieved at least 93.7% of the performance of the original models with
non-problem domain data, and at least 98.6% using additional data from the PD.
Additionally, the copycat CNN successfully copied at least 97.3% of the
performance of the Microsoft Azure Emotion API. Our results show that it is
possible to create a copycat CNN by simply querying a target network as
black-box with random non-labeled data.
",0,0,0,1,0,0
8343,8344,On The Inductive Bias of Words in Acoustics-to-Word Models,"  Acoustics-to-word models are end-to-end speech recognizers that use words as
targets without relying on pronunciation dictionaries or graphemes. These
models are notoriously difficult to train due to the lack of linguistic
knowledge. It is also unclear how the amount of training data impacts the
optimization and generalization of such models. In this work, we study the
optimization and generalization of acoustics-to-word models under different
amounts of training data. In addition, we study three types of inductive bias,
leveraging a pronunciation dictionary, word boundary annotations, and
constraints on word durations. We find that constraining word durations leads
to the most improvement. Finally, we analyze the word embedding space learned
by the model, and find that the space has a structure dominated by the
pronunciation of words. This suggests that the contexts of words, instead of
their phonetic structure, should be the future focus of inductive bias in
acoustics-to-word models.
",1,0,0,0,0,0
13163,13164,Steering Social Activity: A Stochastic Optimal Control Point Of View,"  User engagement in online social networking depends critically on the level
of social activity in the corresponding platform--the number of online actions,
such as posts, shares or replies, taken by their users. Can we design
data-driven algorithms to increase social activity? At a user level, such
algorithms may increase activity by helping users decide when to take an action
to be more likely to be noticed by their peers. At a network level, they may
increase activity by incentivizing a few influential users to take more
actions, which in turn will trigger additional actions by other users. In this
paper, we model social activity using the framework of marked temporal point
processes, derive an alternate representation of these processes using
stochastic differential equations (SDEs) with jumps and, exploiting this
alternate representation, develop two efficient online algorithms with provable
guarantees to steer social activity both at a user and at a network level. In
doing so, we establish a previously unexplored connection between optimal
control of jump SDEs and doubly stochastic marked temporal point processes,
which is of independent interest. Finally, we experiment both with synthetic
and real data gathered from Twitter and show that our algorithms consistently
steer social activity more effectively than the state of the art.
",1,0,0,1,0,0
2476,2477,"On Blocking Collisions between People, Objects and other Robots","  Intentional or unintentional contacts are bound to occur increasingly more
often due to the deployment of autonomous systems in human environments. In
this paper, we devise methods to computationally predict imminent collisions
between objects, robots and people, and use an upper-body humanoid robot to
block them if they are likely to happen. We employ statistical methods for
effective collision prediction followed by sensor-based trajectory generation
and real-time control to attempt to stop the likely collisions using the most
favorable part of the blocking robot. We thoroughly investigate collisions in
various types of experimental setups involving objects, robots, and people.
Overall, the main contribution of this paper is to devise sensor-based
prediction, trajectory generation and control processes for highly articulated
robots to prevent collisions against people, and conduct numerous experiments
to validate this approach.
",1,0,0,0,0,0
3460,3461,Reciprocal space engineering with hyperuniform gold metasurfaces,"  Hyperuniform geometries feature correlated disordered topologies which follow
from a tailored k-space design. Here we study gold plasmonic hyperuniform
metasurfaces and we report evidence of the effectiveness of k-space engineering
on both light scattering and light emission experiments. The metasurfaces
possess interesting directional emission properties which are revealed by
momentum spectroscopy as diffraction and fluorescence emission rings at
size-specific k-vectors. The opening of these rotational-symmetric patterns
scales with the hyperuniform correlation length parameter as predicted via the
spectral function method.
",0,1,0,0,0,0
18009,18010,TuckER: Tensor Factorization for Knowledge Graph Completion,"  Knowledge graphs are structured representations of real world facts. However,
they typically contain only a small subset of all possible facts. Link
prediction is a task of inferring missing facts based on existing ones. We
propose TuckER, a relatively simple but powerful linear model based on Tucker
decomposition of the binary tensor representation of knowledge graph triples.
TuckER outperforms all previous state-of-the-art models across standard link
prediction datasets. We prove that TuckER is a fully expressive model, deriving
the bound on its entity and relation embedding dimensionality for full
expressiveness which is several orders of magnitude smaller than the bound of
previous state-of-the-art models ComplEx and SimplE. We further show that
several previously introduced linear models can be viewed as special cases of
TuckER.
",1,0,0,1,0,0
637,638,Análise comparativa de pesquisas de origens e destinos: uma abordagem baseada em Redes Complexas,"  In this paper, a comparative study was conducted between complex networks
representing origin and destination survey data. Similarities were found
between the characteristics of the networks of Brazilian cities with networks
of foreign cities. Power laws were found in the distributions of edge weights
and this scale - free behavior can occur due to the economic characteristics of
the cities.
",1,0,0,0,0,0
43,44,Generalized Approximate Message-Passing Decoder for Universal Sparse Superposition Codes,"  Sparse superposition (SS) codes were originally proposed as a
capacity-achieving communication scheme over the additive white Gaussian noise
channel (AWGNC) [1]. Very recently, it was discovered that these codes are
universal, in the sense that they achieve capacity over any memoryless channel
under generalized approximate message-passing (GAMP) decoding [2], although
this decoder has never been stated for SS codes. In this contribution we
introduce the GAMP decoder for SS codes, we confirm empirically the
universality of this communication scheme through its study on various channels
and we provide the main analysis tools: state evolution and potential. We also
compare the performance of GAMP with the Bayes-optimal MMSE decoder. We
empirically illustrate that despite the presence of a phase transition
preventing GAMP to reach the optimal performance, spatial coupling allows to
boost the performance that eventually tends to capacity in a proper limit. We
also prove that, in contrast with the AWGNC case, SS codes for binary input
channels have a vanishing error floor in the limit of large codewords.
Moreover, the performance of Hadamard-based encoders is assessed for practical
implementations.
",1,0,1,0,0,0
7527,7528,Active Inductive Logic Programming for Code Search,"  Modern search techniques either cannot efficiently incorporate human feedback
to refine search results or to express structural or semantic properties of
desired code. The key insight of our interactive code search technique ALICE is
that user feedback could be actively incorporated to allow users to easily
express and refine search queries. We design a query language to model the
structure and semantics of code as logic facts. Given a code example with user
annotations, ALICE automatically extracts a logic query from features that are
tagged as important. Users can refine the search query by labeling one or more
examples as desired (positive) or irrelevant (negative). ALICE then infers a
new logic query that separates the positives from negative examples via active
inductive logic programming. Our comprehensive and systematic simulation
experiment shows that ALICE removes a large number of false positives quickly
by actively incorporating user feedback. Its search algorithm is also robust to
noise and user labeling mistakes. Our choice of leveraging both positive and
negative examples and the nested containment structure of selected code is
effective in refining search queries. Compared with an existing technique,
Critics, ALICE does not require a user to manually construct a search pattern
and yet achieves comparable precision and recall with fewer search iterations
on average. A case study with users shows that ALICE is easy to use and helps
express complex code patterns.
",1,0,0,0,0,0
16044,16045,Foundations of Complex Event Processing,"  Complex Event Processing (CEP) has emerged as the unifying field for
technologies that require processing and correlating distributed data sources
in real-time. CEP finds applications in diverse domains, which has resulted in
a large number of proposals for expressing and processing complex events.
However, existing CEP languages lack from a clear semantics, making them hard
to understand and generalize. Moreover, there are no general techniques for
evaluating CEP query languages with clear performance guarantees.
In this paper we embark on the task of giving a rigorous and efficient
framework to CEP. We propose a formal language for specifying complex events,
called CEL, that contains the main features used in the literature and has a
denotational and compositional semantics. We also formalize the so-called
selection strategies, which had only been presented as by-design extensions to
existing frameworks. With a well-defined semantics at hand, we study how to
efficiently evaluate CEL for processing complex events in the case of unary
filters. We start by studying the syntactical properties of CEL and propose
rewriting optimization techniques for simplifying the evaluation of formulas.
Then, we introduce a formal computational model for CEP, called complex event
automata (CEA), and study how to compile CEL formulas into CEA. Furthermore, we
provide efficient algorithms for evaluating CEA over event streams using
constant time per event followed by constant-delay enumeration of the results.
By gathering these results together, we propose a framework for efficiently
evaluating CEL with unary filters. Finally, we show experimentally that this
framework consistently outperforms the competition, and even over trivial
queries can be orders of magnitude more efficient.
",1,0,0,0,0,0
15281,15282,Modelling dependency completion in sentence comprehension as a Bayesian hierarchical mixture process: A case study involving Chinese relative clauses,"  We present a case-study demonstrating the usefulness of Bayesian hierarchical
mixture modelling for investigating cognitive processes. In sentence
comprehension, it is widely assumed that the distance between linguistic
co-dependents affects the latency of dependency resolution: the longer the
distance, the longer the retrieval time (the distance-based account). An
alternative theory, direct-access, assumes that retrieval times are a mixture
of two distributions: one distribution represents successful retrievals (these
are independent of dependency distance) and the other represents an initial
failure to retrieve the correct dependent, followed by a reanalysis that leads
to successful retrieval. We implement both models as Bayesian hierarchical
models and show that the direct-access model explains Chinese relative clause
reading time data better than the distance account.
",1,0,0,1,0,0
11953,11954,Tverberg type theorems for matroids,"  In this paper we show a variant of colorful Tverberg's theorem which is valid
in any matroid: Let $S$ be a sequence of non-loops in a matroid $M$ of finite
rank $m$ with closure operator cl. Suppose that $S$ is colored in such a way
that the first color does not appear more than $r$-times and each other color
appears at most $(r-1)$-times. Then $S$ can be partitioned into $r$ rainbow
subsequences $S_1,\ldots, S_r$ such that $cl\,\emptyset\subsetneq
cl\,S_1\subseteq cl\, S_2\subseteq \ldots \subseteq cl\,S_r$. In particular,
$\emptyset\neq \bigcap_{i=1}^r cl\,S_i$. A subsequence is called rainbow if it
contains each color at most once.
The conclusion of our theorem is weaker than the conclusion of the original
Tverberg's theorem in $\mathbb R^d$, which states that $\bigcap conv\,S_i\neq
\emptyset$, whereas we only claim that $\bigcap aff\,S_i\neq \emptyset$. On the
other hand, our theorem strengthens the Tverberg's theorem in several other
ways: 1) it is applicable to any matroid (whereas Tverberg's theorem can only
be used in $\mathbb R^d$), 2) instead of $\bigcap cl\,S_i\neq \emptyset$ we
have the stronger condition $cl\,\emptyset\subsetneq cl\,S_1\subseteq
cl\,S_2\subseteq \ldots \subseteq cl\,S_r$, and 3) we add a color constraints
that are even stronger than the color constraints in the colorful version of
Tverberg's theorem.
Recently, the author together with Goaoc, Mabillard, Patáková, Tancer and
Wagner used the first property and applied the non-colorful version of this
theorem to homology groups with $GF(p)$ coefficients to obtain several
non-embeddability results, for details we refer to arXiv:1610.09063.
",0,0,1,0,0,0
20662,20663,Minimax Optimal Estimators for Additive Scalar Functionals of Discrete Distributions,"  In this paper, we consider estimators for an additive functional of $\phi$,
which is defined as $\theta(P;\phi)=\sum_{i=1}^k\phi(p_i)$, from $n$ i.i.d.
random samples drawn from a discrete distribution $P=(p_1,...,p_k)$ with
alphabet size $k$. We propose a minimax optimal estimator for the estimation
problem of the additive functional. We reveal that the minimax optimal rate is
characterized by the divergence speed of the fourth derivative of $\phi$ if the
divergence speed is high. As a result, we show there is no consistent estimator
if the divergence speed of the fourth derivative of $\phi$ is larger than
$p^{-4}$. Furthermore, if the divergence speed of the fourth derivative of
$\phi$ is $p^{4-\alpha}$ for $\alpha \in (0,1)$, the minimax optimal rate is
obtained within a universal multiplicative constant as $\frac{k^2}{(n\ln
n)^{2\alpha}} + \frac{k^{2-2\alpha}}{n}$.
",1,0,1,1,0,0
1432,1433,About Synchronized Globular Cluster Formation over Supra-galactic Scales,"  Observational and theoretical arguments support the idea that violent events
connected with $AGN$ activity and/or intense star forming episodes have played
a significant role in the early phases of galaxy formation at high red shifts.
Being old stellar systems, globular clusters seem adequate candidates to search
for the eventual signatures that might have been left by those energetic
phenomena. The analysis of the colour distributions of several thousands of
globular clusters in the Virgo and Fornax galaxy clusters reveals the existence
of some interesting and previously undetected features. A simple pattern
recognition technique, indicates the presence of ""colour modulations"",
distinctive for each galaxy cluster. The results suggest that the globular
cluster formation process has not been completely stochastic but, rather,
included a significant fraction of globulars that formed in a synchronized way
and over supra-galactic spatial scales.
",0,1,0,0,0,0
7659,7660,Competitive Equilibrium For almost All Incomes,"  Competitive equilibrium from equal incomes (CEEI) is a well-known rule for
fair allocation of resources among agents with different preferences. It has
many advantages, among them is the fact that a CEEI allocation is both Pareto
efficient and envy-free. However, when the resources are indivisible, a CEEI
allocation might not exist even when there are two agents and a single item.
In contrast to this discouraging non-existence result, Babaioff, Nisan and
Talgam-Cohen (2017) recently suggested a new and more encouraging approach to
allocation of indivisible items: instead of insisting that the incomes be
equal, they suggest to look at the entire space of possible incomes, and check
whether there exists a competitive equilibrium for almost all income-vectors
(CEFAI) --- all income-space except a subset of measure zero. They show that a
CEFAI exists when there are at most 3 items, or when there are 4 items and two
agents. They also show that when there are 5 items and two agents there might
not exist a CEFAI. They leave open the cases of 4 items with three or four
agents.
This paper presents a new way to implement a CEFAI, as a subgame-perfect
equilibrium of a sequential game. This new implementation allows us both to
offer much simpler solutions to the known cases (at most 3 items, and 4 items
with two agents), and to prove that a CEFAI exists even in the much more
difficult case of 4 items and three agents. Moreover, we prove that a CEFAI
might not exist with 4 items and four agents. When the items to be divided are
bads (chores), CEFAI exists for two agents with at most 4 chores, but does not
exist for two agents with 5 chores or with three agents with 3 or more chores.
Thus, this paper completes the characterization of CEFAI existence for monotone
preferences.
",1,0,0,0,0,0
11419,11420,Fourier multiplier theorems for Triebel-Lizorkin spaces,"  In this paper we study sharp generalizations of $\dot{F}_p^{0,q}$ multiplier
theorem of Mikhlin-Hörmander type. The class of multipliers that we consider
involves Herz spaces $K_u^{s,t}$. Plancherel's theorem proves
$\widehat{L_s^2}=K_2^{s,2}$ and we study the optimal triple $(u,t,s)$ for which
$\sup_{k\in\mathbb{Z}}{\big\Vert \big(
m(2^k\cdot)\varphi\big)^{\vee}\big\Vert_{K_u^{s,t}}}<\infty$ implies
$\dot{F}_p^{0,q}$ boundedness of multiplier operator $T_m$ where $\varphi$ is a
cutoff function. Our result also covers the $BMO$-type space
$\dot{F}_{\infty}^{0,q}$.
",0,0,1,0,0,0
19263,19264,A Backward Simulation Method for Stochastic Optimal Control Problems,"  A number of optimal decision problems with uncertainty can be formulated into
a stochastic optimal control framework. The Least-Squares Monte Carlo (LSMC)
algorithm is a popular numerical method to approach solutions of such
stochastic control problems as analytical solutions are not tractable in
general. This paper generalizes the LSMC algorithm proposed in Shen and Weng
(2017) to solve a wide class of stochastic optimal control models. Our
algorithm has three pillars: a construction of auxiliary stochastic control
model, an artificial simulation of the post-action value of state process, and
a shape-preserving sieve estimation method which equip the algorithm with a
number of merits including bypassing forward simulation and control
randomization, evading extrapolating the value function, and alleviating
computational burden of the tuning parameter selection. The efficacy of the
algorithm is corroborated by an application to pricing equity-linked insurance
products.
",0,0,0,0,0,1
13106,13107,Shallow water modeling of rolling pad instability in liquid metal batteries,"  Magnetohydrodynamically induced interface instability in liquid metal
batteries is analyzed. The batteries are represented by a simplified system in
the form of a rectangular cell, in which strong vertical electric current flows
through three horizontal layers: the layer of a heavy metal at the bottom, the
layer of a light metal at the top, and the layer of electrolyte in the middle.
A new two-dimensional nonlinear model based on the conservative shallow water
approximation is derived and utilized in a numerical study. It is found that in
the case of small density difference between the electrolyte and one of the
metals, the instability closely resembles the rolling pad instability observed
earlier in the aluminum reduction cells. When the two electrolyte-metal density
differences are comparable, the dynamics of unstable systems is more complex
and characterized by interaction between two nearly symmetric or antisymmetric
interfacial waves.
",0,1,0,0,0,0
4228,4229,Predicting Expressive Speaking Style From Text In End-To-End Speech Synthesis,"  Global Style Tokens (GSTs) are a recently-proposed method to learn latent
disentangled representations of high-dimensional data. GSTs can be used within
Tacotron, a state-of-the-art end-to-end text-to-speech synthesis system, to
uncover expressive factors of variation in speaking style. In this work, we
introduce the Text-Predicted Global Style Token (TP-GST) architecture, which
treats GST combination weights or style embeddings as ""virtual"" speaking style
labels within Tacotron. TP-GST learns to predict stylistic renderings from text
alone, requiring neither explicit labels during training nor auxiliary inputs
for inference. We show that, when trained on a dataset of expressive speech,
our system generates audio with more pitch and energy variation than two
state-of-the-art baseline models. We further demonstrate that TP-GSTs can
synthesize speech with background noise removed, and corroborate these analyses
with positive results on human-rated listener preference audiobook tasks.
Finally, we demonstrate that multi-speaker TP-GST models successfully factorize
speaker identity and speaking style. We provide a website with audio samples
for each of our findings.
",1,0,0,1,0,0
1887,1888,A Tutorial on Kernel Density Estimation and Recent Advances,"  This tutorial provides a gentle introduction to kernel density estimation
(KDE) and recent advances regarding confidence bands and geometric/topological
features. We begin with a discussion of basic properties of KDE: the
convergence rate under various metrics, density derivative estimation, and
bandwidth selection. Then, we introduce common approaches to the construction
of confidence intervals/bands, and we discuss how to handle bias. Next, we talk
about recent advances in the inference of geometric and topological features of
a density function using KDE. Finally, we illustrate how one can use KDE to
estimate a cumulative distribution function and a receiver operating
characteristic curve. We provide R implementations related to this tutorial at
the end.
",0,0,0,1,0,0
1531,1532,Towards an Understanding of the Effects of Augmented Reality Games on Disaster Management,"  Location-based augmented reality games have entered the mainstream with the
nearly overnight success of Niantic's Pokémon Go. Unlike traditional video
games, the fact that players of such games carry out actions in the external,
physical world to accomplish in-game objectives means that the large-scale
adoption of such games motivate people, en masse, to do things and go places
they would not have otherwise done in unprecedented ways. The social
implications of such mass-mobilisation of individual players are, in general,
difficult to anticipate or characterise, even for the short-term. In this work,
we focus on disaster relief, and the short- and long-term implications that a
proliferation of AR games like Pokémon Go, may have in disaster-prone regions
of the world. We take a distributed cognition approach and focus on one natural
disaster-prone region of New Zealand, the city of Wellington.
",1,0,0,0,0,0
1571,1572,Probabilistic Surfel Fusion for Dense LiDAR Mapping,"  With the recent development of high-end LiDARs, more and more systems are
able to continuously map the environment while moving and producing spatially
redundant information. However, none of the previous approaches were able to
effectively exploit this redundancy in a dense LiDAR mapping problem. In this
paper, we present a new approach for dense LiDAR mapping using probabilistic
surfel fusion. The proposed system is capable of reconstructing a high-quality
dense surface element (surfel) map from spatially redundant multiple views.
This is achieved by a proposed probabilistic surfel fusion along with a
geometry considered data association. The proposed surfel data association
method considers surface resolution as well as high measurement uncertainty
along its beam direction which enables the mapping system to be able to control
surface resolution without introducing spatial digitization. The proposed
fusion method successfully suppresses the map noise level by considering
measurement noise caused by laser beam incident angle and depth distance in a
Bayesian filtering framework. Experimental results with simulated and real data
for the dense surfel mapping prove the ability of the proposed method to
accurately find the canonical form of the environment without further
post-processing.
",1,0,0,0,0,0
10010,10011,Computationally Inferred Genealogical Networks Uncover Long-Term Trends in Assortative Mating,"  Genealogical networks, also known as family trees or population pedigrees,
are commonly studied by genealogists wanting to know about their ancestry, but
they also provide a valuable resource for disciplines such as digital
demography, genetics, and computational social science. These networks are
typically constructed by hand through a very time-consuming process, which
requires comparing large numbers of historical records manually. We develop
computational methods for automatically inferring large-scale genealogical
networks. A comparison with human-constructed networks attests to the accuracy
of the proposed methods. To demonstrate the applicability of the inferred
large-scale genealogical networks, we present a longitudinal analysis on the
mating patterns observed in a network. This analysis shows a consistent
tendency of people choosing a spouse with a similar socioeconomic status, a
phenomenon known as assortative mating. Interestingly, we do not observe this
tendency to consistently decrease (nor increase) over our study period of 150
years.
",1,0,0,0,1,0
12308,12309,On Security and Sparsity of Linear Classifiers for Adversarial Settings,"  Machine-learning techniques are widely used in security-related applications,
like spam and malware detection. However, in such settings, they have been
shown to be vulnerable to adversarial attacks, including the deliberate
manipulation of data at test time to evade detection. In this work, we focus on
the vulnerability of linear classifiers to evasion attacks. This can be
considered a relevant problem, as linear classifiers have been increasingly
used in embedded systems and mobile devices for their low processing time and
memory requirements. We exploit recent findings in robust optimization to
investigate the link between regularization and security of linear classifiers,
depending on the type of attack. We also analyze the relationship between the
sparsity of feature weights, which is desirable for reducing processing cost,
and the security of linear classifiers. We further propose a novel octagonal
regularizer that allows us to achieve a proper trade-off between them. Finally,
we empirically show how this regularizer can improve classifier security and
sparsity in real-world application examples including spam and malware
detection.
",1,0,0,0,0,0
2527,2528,ClipAudit: A Simple Risk-Limiting Post-Election Audit,"  We propose a simple risk-limiting audit for elections, ClipAudit. To
determine whether candidate A (the reported winner) actually beat candidate B
in a plurality election, ClipAudit draws ballots at random, without
replacement, until either all cast ballots have been drawn, or until \[ a - b
\ge \beta \sqrt{a+b}
\] where $a$ is the number of ballots in the sample for the reported winner
A, and $b$ is the number of ballots in the sample for opponent B, and where
$\beta$ is a constant determined a priori as a function of the number $n$ of
ballots cast and the risk-limit $\alpha$. ClipAudit doesn't depend on the
unofficial margin (as does Bravo). We show how to extend ClipAudit to contests
with multiple winners or losers, or to multiple contests.
",1,0,0,1,0,0
13585,13586,Quasar Rain: the Broad Emission Line Region as Condensations in the Warm Accretion Disk Wind,"  The origin of the broad emission line region (BELR) in quasars and active
galactic nuclei is still unclear. I propose that condensations form in the
warm, radiation pressure driven, accretion disk wind of quasars creating the
BEL clouds and uniting them with the other two manifestations of cool, 10,000
K, gas in quasars, the low ionization phase of the warm absorbers (WAs) and the
clouds causing X-ray eclipses. The cool clouds will condense quickly (days to
years), before the WA outflows reach escape velocity (which takes months to
centuries). Cool clouds form in equilibrium with the warm phase of the wind
because the rapidly varying X-ray quasar continuum changes the force
multiplier, causing pressure waves to move gas into stable locations in
pressure-temperature space. The narrow range of 2-phase equilibrium densities
may explain the scaling of the BELR size with the square root of luminosity,
while the scaling of cloud formation timescales could produce the Baldwin
effect. These dense clouds have force multipliers of order unity and so cannot
be accelerated to escape velocity. They fall back on a dynamical timescale
(months to centuries), producing an inflow that rains down toward the central
black hole. As they soon move at Mach ~40 with respect to the WA outflow, these
'raindrops' will be rapidly destroyed within months. This rain of clouds may
produce the elliptical BELR orbits implied by velocity resolved reverberation
mapping in some objects, and can explain the opening angle and destruction
timescale of the narrow 'cometary' tails of the clouds seen in X-ray eclipse
observations. Some consequences and challenges of this 'quasar rain' model are
presented along with several avenues for theoretical investigation.
",0,1,0,0,0,0
16660,16661,Coalescence of Two Impurities in a Trapped One-dimensional Bose Gas,"  We study the ground state of a one-dimensional (1D) trapped Bose gas with two
mobile impurity particles. To investigate this set-up, we develop a variational
procedure in which the coordinates of the impurity particles are slow-like
variables. We validate our method using the exact results obtained for small
systems. Then, we discuss energies and pair densities for systems that contain
of the order of one hundred atoms. We show that bosonic non-interacting
impurities cluster. To explain this clustering, we calculate and discuss
induced impurity-impurity potentials in a harmonic trap. Further, we compute
the force between static impurities in a ring ({\it {à} la} the Casimir
force), and contrast the two effective potentials: the one obtained from the
mean-field approximation, and the one due to the one-phonon exchange. Our
formalism and findings are important for understanding (beyond the polaron
model) the physics of modern 1D cold-atom systems with more than one impurity.
",0,1,0,0,0,0
1378,1379,A Riemannian gossip approach to subspace learning on Grassmann manifold,"  In this paper, we focus on subspace learning problems on the Grassmann
manifold. Interesting applications in this setting include low-rank matrix
completion and low-dimensional multivariate regression, among others. Motivated
by privacy concerns, we aim to solve such problems in a decentralized setting
where multiple agents have access to (and solve) only a part of the whole
optimization problem. The agents communicate with each other to arrive at a
consensus, i.e., agree on a common quantity, via the gossip protocol.
We propose a novel cost function for subspace learning on the Grassmann
manifold, which is a weighted sum of several sub-problems (each solved by an
agent) and the communication cost among the agents. The cost function has a
finite sum structure. In the proposed modeling approach, different agents learn
individual local subspace but they achieve asymptotic consensus on the global
learned subspace. The approach is scalable and parallelizable. Numerical
experiments show the efficacy of the proposed decentralized algorithms on
various matrix completion and multivariate regression benchmarks.
",1,0,1,0,0,0
19497,19498,Generalized Probabilistic Bisection for Stochastic Root-Finding,"  We consider numerical schemes for root finding of noisy responses through
generalizing the Probabilistic Bisection Algorithm (PBA) to the more practical
context where the sampling distribution is unknown and location-dependent. As
in standard PBA, we rely on a knowledge state for the approximate posterior of
the root location. To implement the corresponding Bayesian updating, we also
carry out inference of oracle accuracy, namely learning the probability of
correct response. To this end we utilize batched querying in combination with a
variety of frequentist and Bayesian estimators based on majority vote, as well
as the underlying functional responses, if available. For guiding sampling
selection we investigate both Information Directed sampling, as well as
Quantile sampling. Our numerical experiments show that these strategies perform
quite differently; in particular we demonstrate the efficiency of randomized
quantile sampling which is reminiscent of Thompson sampling. Our work is
motivated by the root-finding sub-routine in pricing of Bermudan financial
derivatives, illustrated in the last section of the paper.
",1,0,0,1,0,0
846,847,Smoothed Noise and Mexican Hat Coupling Produce Pattern in a Stochastic Neural Field,"  The formation of pattern in biological systems may be modeled by a set of
reaction-diffusion equations. A diffusion-type coupling operator biologically
significant in neuroscience is a difference of Gaussian functions (Mexican Hat
operator) used as a spatial-convolution kernel. We are interested in the
difference among behaviors of \emph{stochastic} neural field equations, namely
space-time stochastic differential-integral equations, and similar
deterministic ones. We explore, quantitatively, how the parameters of our model
that measure the shape of the coupling kernel, coupling strength, and aspects
of the spatially-smoothed space-time noise, control the pattern in the
resulting evolving random field. We find that a spatial pattern that is damped
in time in a deterministic system may be sustained and amplified by
stochasticity, most strikingly at an optimal spatio-temporal noise level. In
addition, we find that spatially-smoothed noise alone causes pattern formation
even without spatial coupling.
",0,0,0,0,1,0
10548,10549,Kinetics of the Phospholipid Multilayer Formation at the Surface of the Silica Sol,"  The ordering of a multilayer consisting of DSPC bilayers on a silica sol
substrate is studied within the model-independent approach to the
reconstruction of profiles of the electron density from X-ray reflectometry
data. It is found that the electroporation of bilayers in the field of anion
silica nanoparticles significantly accelerates the process of their saturation
with Na+ and H2O, which explains both a relatively small time of formation of
the structure of the multilayer of 10^5 - 7x10^5 s and ~13 % excess of the
electron density in it.
",0,1,0,0,0,0
6374,6375,On the Limitation of Convolutional Neural Networks in Recognizing Negative Images,"  Convolutional Neural Networks (CNNs) have achieved state-of-the-art
performance on a variety of computer vision tasks, particularly visual
classification problems, where new algorithms reported to achieve or even
surpass the human performance. In this paper, we examine whether CNNs are
capable of learning the semantics of training data. To this end, we evaluate
CNNs on negative images, since they share the same structure and semantics as
regular images and humans can classify them correctly. Our experimental results
indicate that when training on regular images and testing on negative images,
the model accuracy is significantly lower than when it is tested on regular
images. This leads us to the conjecture that current training methods do not
effectively train models to generalize the concepts. We then introduce the
notion of semantic adversarial examples - transformed inputs that semantically
represent the same objects, but the model does not classify them correctly -
and present negative images as one class of such inputs.
",1,0,0,1,0,0
3890,3891,The Word Problem of $\mathbb{Z}^n$ Is a Multiple Context-Free Language,"  The \emph{word problem} of a group $G = \langle \Sigma \rangle$ can be
defined as the set of formal words in $\Sigma^*$ that represent the identity in
$G$. When viewed as formal languages, this gives a strong connection between
classes of groups and classes of formal languages. For example, Anisimov showed
that a group is finite if and only if its word problem is a regular language,
and Muller and Schupp showed that a group is virtually-free if and only if its
word problem is a context-free language. Above this, not much was known, until
Salvati showed recently that the word problem of $\mathbb{Z}^2$ is a multiple
context-free language, giving first such example. We generalize Salvati's
result to show that the word problem of $\mathbb{Z}^n$ is a multiple
context-free language for any $n$.
",1,0,1,0,0,0
15539,15540,Pseudoconcavity of flag domains: The method of supporting cycles,"  A flag domain of a real from $G_0$ of a complex semismiple Lie group $G$ is
an open $G_0$-orbit $D$ in a (compact) $G$-flag manifold. In the usual way one
reduces to the case where $G_0$ is simple. It is known that if $D$ possesses
non-constant holomorphic functions, then it is the product of a compact flag
manifold and a Hermitian symmetric bounded domain. This pseudoconvex case is
rare in the geography of flag domains. Here it is shown that otherwise, i.e.,
when $\mathcal{O}(D)\cong\mathbb{C}$, the flag domain $D$ is pseudoconcave. In
a rather general setting the degree of the pseudoconcavity is estimated in
terms of root invariants. This estimate is explicitly computed for domains in
certain Grassmannians.
",0,0,1,0,0,0
7342,7343,Putting gravity in control,"  The aim of the present manuscript is to present a novel proposal in Geometric
Control Theory inspired in the principles of General Relativity and
energy-shaping control.
",0,0,1,0,0,0
17267,17268,Greedy Sparse Signal Reconstruction Using Matching Pursuit Based on Hope-tree,"  The reconstruction of sparse signals requires the solution of an
$\ell_0$-norm minimization problem in Compressed Sensing. Previous research has
focused on the investigation of a single candidate to identify the support
(index of nonzero elements) of a sparse signal. To ensure that the optimal
candidate can be obtained in each iteration, we propose here an iterative
greedy reconstruction algorithm (GSRA). First, the intersection of the support
sets estimated by the Orthogonal Matching Pursuit (OMP) and Subspace Pursuit
(SP) is set as the initial support set. Then, a hope-tree is built to expand
the set. Finally, a developed decreasing subspace pursuit method is used to
rectify the candidate set. Detailed simulation results demonstrate that GSRA is
more accurate than other typical methods in recovering Gaussian signals, 0--1
sparse signals, and synthetic signals.
",1,0,1,0,0,0
8957,8958,Representation of I(1) and I(2) autoregressive Hilbertian processes,"  We extend the Granger-Johansen representation theorems for I(1) and I(2)
vector autoregressive processes to accommodate processes that take values in an
arbitrary complex separable Hilbert space. This more general setting is of
central relevance for statistical applications involving functional time
series. We first obtain a range of necessary and sufficient conditions for a
pole in the inverse of a holomorphic index-zero Fredholm operator pencil to be
of first or second order. Those conditions form the basis for our development
of I(1) and I(2) representations of autoregressive Hilbertian processes.
Cointegrating and attractor subspaces are characterized in terms of the
behavior of the autoregressive operator pencil in a neighborhood of one.
",0,0,1,1,0,0
15389,15390,Latent Room-Temperature T$_c$ in Cuprate Superconductors,"  The ancient phrase, ""All roads lead to Rome"" applies to Chemistry and
Physics. Both are highly evolved sciences, with their own history, traditions,
language, and approaches to problems. Despite all these differences, these two
roads generally lead to the same place. For high temperature cuprate
superconductors however, the Chemistry and Physics roads do not meet or even
come close to each other. In this paper, we analyze the physics and chemistry
approaches to the doped electronic structure of cuprates and find the chemistry
doped hole (out-of-the-CuO$\mathrm{_2}$-planes) leads to explanations of a vast
array of normal state cuprate phenomenology using simple counting arguments.
The chemistry picture suggests that phonons are responsible for
superconductivity in cuprates. We identify the important phonon modes, and show
that the observed T$\mathrm{_c} \sim 100$ K, the T$\mathrm{_c}$-dome as a
function of hole doping, the change in T$\mathrm{_c}$ as a function of the
number of CuO$\mathrm{_2}$ layers per unit cell, the lack of an isotope effect
at optimal T$\mathrm{_c}$ doping, and the D-wave symmetry of the
superconducting Cooper pair wavefunction are all explained by the chemistry
picture. Finally, we show that ""crowding"" the dopants in cuprates leads to a
pair wavefunction with S-wave symmetry and T$\mathrm{_c}\approx280-390$ K.
Hence, we believe there is enormous ""latent"" T$\mathrm{_c}$ remaining in the
cuprate class of superconductors.
",0,1,0,0,0,0
16371,16372,Trajectory Normalized Gradients for Distributed Optimization,"  Recently, researchers proposed various low-precision gradient compression,
for efficient communication in large-scale distributed optimization. Based on
these work, we try to reduce the communication complexity from a new direction.
We pursue an ideal bijective mapping between two spaces of gradient
distribution, so that the mapped gradient carries greater information entropy
after the compression. In our setting, all servers should share a reference
gradient in advance, and they communicate via the normalized gradients, which
are the subtraction or quotient, between current gradients and the reference.
To obtain a reference vector that yields a stronger signal-to-noise ratio,
dynamically in each iteration, we extract and fuse information from the past
trajectory in hindsight, and search for an optimal reference for compression.
We name this to be the trajectory-based normalized gradients (TNG). It bridges
the research from different societies, like coding, optimization, systems, and
learning. It is easy to implement and can universally combine with existing
algorithms. Our experiments on benchmarking hard non-convex functions, convex
problems like logistic regression demonstrate that TNG is more
compression-efficient for communication of distributed optimization of general
functions.
",1,0,0,1,0,0
17367,17368,Characterizing Dust Attenuation in Local Star-Forming Galaxies: Near-Infrared Reddening and Normalization,"  We characterize the near-infrared (NIR) dust attenuation for a sample of
~5500 local (z<0.1) star-forming galaxies and obtain an estimate of their
average total-to-selective attenuation $k(\lambda)$. We utilize data from the
United Kingdom Infrared Telescope (UKIRT) and the Two Micron All-Sky Survey
(2MASS), which is combined with previously measured UV-optical data for these
galaxies. The average attenuation curve is slightly lower in the far-UV than
local starburst galaxies, by roughly 15%, but appears similar at longer
wavelengths with a total-to-selective normalization at V-band of
$R_V=3.67\substack{+0.44 \\ -0.35}$. Under the assumption of energy balance,
the total attenuated energy inferred from this curve is found to be broadly
consistent with the observed infrared dust emission ($L_{\rm{TIR}}$) in a small
sample of local galaxies for which far-IR measurements are available. However,
the significant scatter in this quantity among the sample may reflect large
variations in the attenuation properties of individual galaxies. We also derive
the attenuation curve for sub-populations of the main sample, separated
according to mean stellar population age (via $D_n4000$), specific star
formation rate, stellar mass, and metallicity, and find that they show only
tentative trends with low significance, at least over the range which is probed
by our sample. These results indicate that a single curve is reasonable for
applications seeking to broadly characterize large samples of galaxies in the
local Universe, while applications to individual galaxies would yield large
uncertainties and is not recommended.
",0,1,0,0,0,0
10583,10584,Comparison of Gini index and Tamura coefficient for holographic autofocusing based on the edge sparsity of the complex optical wavefront,"  The Sparsity of the Gradient (SoG) is a robust autofocusing criterion for
holography, where the gradient modulus of the complex refocused hologram is
calculated, on which a sparsity metric is applied. Here, we compare two
different choices of sparsity metrics used in SoG, specifically, the Gini index
(GI) and the Tamura coefficient (TC), for holographic autofocusing on
dense/connected or sparse samples. We provide a theoretical analysis predicting
that for uniformly distributed image data, TC and GI exhibit similar behavior,
while for naturally sparse images containing few high-valued signal entries and
many low-valued noisy background pixels, TC is more sensitive to distribution
changes in the signal and more resistive to background noise. These predictions
are also confirmed by experimental results using SoG-based holographic
autofocusing on dense and connected samples (such as stained breast tissue
sections) as well as highly sparse samples (such as isolated Giardia lamblia
cysts). Through these experiments, we found that ToG and GoG offer almost
identical autofocusing performance on dense and connected samples, whereas for
naturally sparse samples, GoG should be calculated on a relatively small region
of interest (ROI) closely surrounding the object, while ToG offers more
flexibility in choosing a larger ROI containing more background pixels.
",0,1,0,0,0,0
13687,13688,Strong correlations between the exponent $α$ and the particle number for a Renyi-monoatomic gas in Gibbs' statistical mechanics,"  Appealing to the 1902 Gibbs' formalism for classical statistical mechanics
(SM), the first SM axiomatic theory ever that successfully explained
equilibrium thermodynamics, we will here show that already at the classical
level there is a strong correlation between the Renyi's exponent $\alpha$ and
the number of particles for very simple systems. No reference to heat baths is
needed for such a purpose.
",0,1,0,0,0,0
6390,6391,ECO-AMLP: A Decision Support System using an Enhanced Class Outlier with Automatic Multilayer Perceptron for Diabetes Prediction,"  With advanced data analytical techniques, efforts for more accurate decision
support systems for disease prediction are on rise. Surveys by World Health
Organization (WHO) indicate a great increase in number of diabetic patients and
related deaths each year. Early diagnosis of diabetes is a major concern among
researchers and practitioners. The paper presents an application of
\textit{Automatic Multilayer Perceptron }which\textit{ }is combined with an
outlier detection method \textit{Enhanced Class Outlier Detection using
distance based algorithm }to create a prediction framework named as Enhanced
Class Outlier with Automatic Multi layer Perceptron (ECO-AMLP). A series of
experiments are performed on publicly available Pima Indian Diabetes Dataset to
compare ECO-AMLP with other individual classifiers as well as ensemble based
methods. The outlier technique used in our framework gave better results as
compared to other pre-processing and classification techniques. Finally, the
results are compared with other state-of-the-art methods reported in literature
for diabetes prediction on PIDD and achieved accuracy of 88.7\% bests all other
reported studies.
",1,0,0,0,0,0
18103,18104,Shiba Bound States across the mobility edge in doped InAs nanowires,"  We present a study of Andreev Quantum Dots (QDots) fabricated with
small-diameter (30 nm) Si-doped InAs nanowires where the Fermi level can be
tuned across a mobility edge separating localized states from delocalized
states. The transition to the insulating phase is identified by a drop in the
amplitude and width of the excited levels and is found to have remarkable
consequences on the spectrum of superconducting SubGap Resonances (SGRs). While
at deeply localized levels, only quasiparticles co-tunneling is observed, for
slightly delocalized levels, Shiba bound states form and a parity changing
quantum phase transition is identified by a crossing of the bound states at
zero energy. Finally, in the metallic regime, single Andreev resonances are
observed.
",0,1,0,0,0,0
7861,7862,Source Forager: A Search Engine for Similar Source Code,"  Developers spend a significant amount of time searching for code: e.g., to
understand how to complete, correct, or adapt their own code for a new context.
Unfortunately, the state of the art in code search has not evolved much beyond
text search over tokenized source. Code has much richer structure and semantics
than normal text, and this property can be exploited to specialize the
code-search process for better querying, searching, and ranking of code-search
results.
We present a new code-search engine named Source Forager. Given a query in
the form of a C/C++ function, Source Forager searches a pre-populated code
database for similar C/C++ functions. Source Forager preprocesses the database
to extract a variety of simple code features that capture different aspects of
code. A search returns the $k$ functions in the database that are most similar
to the query, based on the various extracted code features.
We tested the usefulness of Source Forager using a variety of code-search
queries from two domains. Our experiments show that the ranked results returned
by Source Forager are accurate, and that query-relevant functions can be
reliably retrieved even when searching through a large code database that
contains very few query-relevant functions.
We believe that Source Forager is a first step towards much-needed tools that
provide a better code-search experience.
",1,0,0,0,0,0
7189,7190,Estimating the spectral gap of a trace-class Markov operator,"  The utility of a Markov chain Monte Carlo algorithm is, in large part,
determined by the size of the spectral gap of the corresponding Markov
operator. However, calculating (and even approximating) the spectral gaps of
practical Monte Carlo Markov chains in statistics has proven to be an extremely
difficult and often insurmountable task, especially when these chains move on
continuous state spaces. In this paper, a method for accurate estimation of the
spectral gap is developed for general state space Markov chains whose operators
are non-negative and trace-class. The method is based on the fact that the
second largest eigenvalue (and hence the spectral gap) of such operators can be
bounded above and below by simple functions of the power sums of the
eigenvalues. These power sums often have nice integral representations. A
classical Monte Carlo method is proposed to estimate these integrals, and a
simple sufficient condition for finite variance is provided. This leads to
asymptotically valid confidence intervals for the second largest eigenvalue
(and the spectral gap) of the Markov operator. The efficiency of the method is
studied. For illustration, the method is applied to Albert and Chib's (1993)
data augmentation (DA) algorithm for Bayesian probit regression, and also to a
DA algorithm for Bayesian linear regression with non-Gaussian errors (Liu,
1996).
",0,0,1,1,0,0
1608,1609,A surface-hopping method for semiclassical calculations of cross sections for radiative association with electronic transitions,"  A semicalssical method based on surface-hopping techniques is developed to
model the dynamics of radiative association with electronic transitions in
arbitrary polyatomic systems. It can be proven that our method is an extension
of the established semiclassical formula used in the characterization of
diatomic molecule- formation. Our model is tested for diatomic molecules. It
gives the same cross sections as the former semiclassical formula, but contrary
to the former method it allows us to follow the fate of the trajectories after
the emission of a photon. This means that we can characterize the rovibrational
states of the stabilized molecules: using semiclassial quantization we can
obtain quantum state resolved cross sections or emission spectra for the
radiative association process. The calculated semiclassical state resolved
spectra show good agreement with the result of quantum mechanical perturbation
theory. Furthermore our surface-hopping model is not only applicable for the
description of radiative association but it can be use for semiclassical
characterization of any molecular process where spontaneous emission occurs.
",0,1,0,0,0,0
9594,9595,Knockoffs for the mass: new feature importance statistics with false discovery guarantees,"  An important problem in machine learning and statistics is to identify
features that causally affect the outcome. This is often impossible to do from
purely observational data, and a natural relaxation is to identify features
that are correlated with the outcome even conditioned on all other observed
features. For example, we want to identify that smoking really is correlated
with cancer conditioned on demographics. The knockoff procedure is a recent
breakthrough in statistics that, in theory, can identify truly correlated
features while guaranteeing that the false discovery is limited. The idea is to
create synthetic data -knockoffs- that captures correlations amongst the
features. However there are substantial computational and practical challenges
to generating and using knockoffs. This paper makes several key advances that
enable knockoff application to be more efficient and powerful. We develop an
efficient algorithm to generate valid knockoffs from Bayesian Networks. Then we
systematically evaluate knockoff test statistics and develop new statistics
with improved power. The paper combines new mathematical guarantees with
systematic experiments on real and synthetic data.
",0,0,0,1,0,0
1259,1260,Lyapunov exponents for products of matrices,"  Let ${\bf M}=(M_1,\ldots, M_k)$ be a tuple of real $d\times d$ matrices.
Under certain irreducibility assumptions, we give checkable criteria for
deciding whether ${\bf M}$ possesses the following property: there exist two
constants $\lambda\in {\Bbb R}$ and $C>0$ such that for any $n\in {\Bbb N}$ and
any $i_1, \ldots, i_n \in \{1,\ldots, k\}$, either $M_{i_1} \cdots M_{i_n}={\bf
0}$ or $C^{-1} e^{\lambda n} \leq \| M_{i_1} \cdots M_{i_n} \| \leq C
e^{\lambda n}$, where $\|\cdot\|$ is a matrix norm. The proof is based on
symbolic dynamics and the thermodynamic formalism for matrix products. As
applications, we are able to check the absolute continuity of a class of
overlapping self-similar measures on ${\Bbb R}$, the absolute continuity of
certain self-affine measures in ${\Bbb R}^d$ and the dimensional regularity of
a class of sofic affine-invariant sets in the plane.
",0,0,1,0,0,0
18863,18864,Markov Chain Monte Carlo Methods for Bayesian Data Analysis in Astronomy,"  Markov Chain Monte Carlo based Bayesian data analysis has now become the
method of choice for analyzing and interpreting data in almost all disciplines
of science. In astronomy, over the last decade, we have also seen a steady
increase in the number of papers that employ Monte Carlo based Bayesian
analysis. New, efficient Monte Carlo based methods are continuously being
developed and explored. In this review, we first explain the basics of Bayesian
theory and discuss how to set up data analysis problems within this framework.
Next, we provide an overview of various Monte Carlo based methods for
performing Bayesian data analysis. Finally, we discuss advanced ideas that
enable us to tackle complex problems and thus hold great promise for the
future. We also distribute downloadable computer software (available at
this https URL ) that implements some of the algorithms and
examples discussed here.
",0,1,0,1,0,0
17101,17102,Spin Hall effect of gravitational waves,"  Gravitons possess a Berry curvature due to their helicity. We derive the
semiclassical equations of motion for gravitons taking into account the Berry
curvature. We show that this quantum correction leads to the splitting of the
trajectories of right- and left-handed gravitational waves in curved space, and
that this correction can be understood as a topological phenomenon. This is the
spin Hall effect (SHE) of gravitational waves. We find that the SHE of
gravitational waves is twice as large as that of light. Possible future
observations of the SHE of gravitational waves can potentially test the quantum
nature of gravitons beyond the classical general relativity.
",0,1,0,0,0,0
4091,4092,Autocorrelation and Lower Bound on the 2-Adic Complexity of LSB Sequence of $p$-ary $m$-Sequence,"  In modern stream cipher, there are many algorithms, such as ZUC, LTE
encryption algorithm and LTE integrity algorithm, using bit-component sequences
of $p$-ary $m$-sequences as the input of the algorithm. Therefore, analyzing
their statistical property (For example, autocorrelation, linear complexity and
2-adic complexity) of bit-component sequences of $p$-ary $m$-sequences is
becoming an important research topic. In this paper, we first derive some
autocorrelation properties of LSB (Least Significant Bit) sequences of $p$-ary
$m$-sequences, i.e., we convert the problem of computing autocorrelations of
LSB sequences of period $p^n-1$ for any positive $n\geq2$ to the problem of
determining autocorrelations of LSB sequence of period $p-1$. Then, based on
this property and computer calculation, we list some autocorrelation
distributions of LSB sequences of $p$-ary $m$-sequences with order $n$ for some
small primes $p$'s, such as $p=3,5,7,11,17,31$. Additionally, using their
autocorrelation distributions and the method inspired by Hu, we give the lower
bounds on the 2-adic complexities of these LSB sequences. Our results show that
the main parts of all the lower bounds on the 2-adic complexity of these LSB
sequencesare larger than $\frac{N}{2}$, where $N$ is the period of these
sequences. Therefor, these bounds are large enough to resist the analysis of
RAA (Rational Approximation Algorithm) for FCSR (Feedback with Carry Shift
Register). Especially, for a Mersenne prime $p=2^k-1$, since all its
bit-component sequences of a $p$-ary $m$-sequence are shift equivalent, our
results hold for all its bit-component sequences.
",1,0,0,0,0,0
16062,16063,Measuring Quantum Entropy,"  The entropy of a quantum system is a measure of its randomness, and has
applications in measuring quantum entanglement. We study the problem of
measuring the von Neumann entropy, $S(\rho)$, and Rényi entropy,
$S_\alpha(\rho)$ of an unknown mixed quantum state $\rho$ in $d$ dimensions,
given access to independent copies of $\rho$.
We provide an algorithm with copy complexity $O(d^{2/\alpha})$ for estimating
$S_\alpha(\rho)$ for $\alpha<1$, and copy complexity $O(d^{2})$ for estimating
$S(\rho)$, and $S_\alpha(\rho)$ for non-integral $\alpha>1$. These bounds are
at least quadratic in $d$, which is the order dependence on the number of
copies required for learning the entire state $\rho$. For integral $\alpha>1$,
on the other hand, we provide an algorithm for estimating $S_\alpha(\rho)$ with
a sub-quadratic copy complexity of $O(d^{2-2/\alpha})$. We characterize the
copy complexity for integral $\alpha>1$ up to constant factors by providing
matching lower bounds. For other values of $\alpha$, and the von Neumann
entropy, we show lower bounds on the algorithm that achieves the upper bound.
This shows that we either need new algorithms for better upper bounds, or
better lower bounds to tighten the results.
For non-integral $\alpha$, and the von Neumann entropy, we consider the well
known Empirical Young Diagram (EYD) algorithm, which is the analogue of
empirical plug-in estimator in classical distribution estimation. As a
corollary, we strengthen a lower bound on the copy complexity of the EYD
algorithm for learning the maximally mixed state by showing that the lower
bound holds with exponential probability (which was previously known to hold
with a constant probability). For integral $\alpha>1$, we provide new
concentration results of certain polynomials that arise in Kerov algebra of
Young diagrams.
",1,0,0,0,0,0
4943,4944,Meta-Learning MCMC Proposals,"  Effective implementations of sampling-based probabilistic inference often
require manually constructed, model-specific proposals. Inspired by recent
progresses in meta-learning for training learning agents that can generalize to
unseen environments, we propose a meta-learning approach to building effective
and generalizable MCMC proposals. We parametrize the proposal as a neural
network to provide fast approximations to block Gibbs conditionals. The learned
neural proposals generalize to occurrences of common structural motifs across
different models, allowing for the construction of a library of learned
inference primitives that can accelerate inference on unseen models with no
model-specific training required. We explore several applications including
open-universe Gaussian mixture models, in which our learned proposals
outperform a hand-tuned sampler, and a real-world named entity recognition
task, in which our sampler yields higher final F1 scores than classical
single-site Gibbs sampling.
",1,0,0,1,0,0
18555,18556,Simplifying branched covering surface-knots by chart moves involving black vertices,"  A branched covering surface-knot is a surface-knot in the form of a branched
covering over an oriented surface-knot $F$, where we include the case when the
covering has no branch points. A branched covering surface-knot is presented by
a graph called a chart on a surface diagram of $F$. We can simplify a branched
covering surface-knot by an addition of 1-handles with chart loops to a form
such that its chart is the union of free edges and 1-handles with chart loops.
We investigate properties of such simplifications for the case when branched
covering surface-knots have a non-zero number of branch points, using chart
moves involving black vertices.
",0,0,1,0,0,0
7987,7988,Environmental feedback drives cooperation in spatial social dilemmas,"  Exploiting others is beneficial individually but it could also be detrimental
globally. The reverse is also true: a higher cooperation level may change the
environment in a way that is beneficial for all competitors. To explore the
possible consequence of this feedback we consider a coevolutionary model where
the local cooperation level determines the payoff values of the applied
prisoner's dilemma game. We observe that the coevolutionary rule provides a
significantly higher cooperation level comparing to the traditional setup
independently of the topology of the applied interaction graph. Interestingly,
this cooperation supporting mechanism offers lonely defectors a high surviving
chance for a long period hence the relaxation to the final cooperating state
happens logarithmically slow. As a consequence, the extension of the
traditional evolutionary game by considering interactions with the environment
provides a good opportunity for cooperators, but their reward may arrive with
some delay.
",0,0,0,0,1,0
20604,20605,Long-Lived Ultracold Molecules with Electric and Magnetic Dipole Moments,"  We create fermionic dipolar $^{23}$Na$^6$Li molecules in their triplet ground
state from an ultracold mixture of $^{23}$Na and $^6$Li. Using
magneto-association across a narrow Feshbach resonance followed by a two-photon
STIRAP transfer to the triplet ground state, we produce $3\,{\times}\,10^4$
ground state molecules in a spin-polarized state. We observe a lifetime of
$4.6\,\text{s}$ in an isolated molecular sample, approaching the $p$-wave
universal rate limit. Electron spin resonance spectroscopy of the triplet state
was used to determine the hyperfine structure of this previously unobserved
molecular state.
",0,1,0,0,0,0
14434,14435,Intelligent Sensor Based Bayesian Neural Network for Combined Parameters and States Estimation of a Brushed DC Motor,"  The objective of this paper is to develop an Artificial Neural Network (ANN)
model to estimate simultaneously, parameters and state of a brushed DC machine.
The proposed ANN estimator is novel in the sense that his estimates
simultaneously temperature, speed and rotor resistance based only on the
measurement of the voltage and current inputs. Many types of ANN estimators
have been designed by a lot of researchers during the last two decades. Each
type is designed for a specific application. The thermal behavior of the motor
is very slow, which leads to large amounts of data sets. The standard ANN use
often Multi-Layer Perceptron (MLP) with Levenberg-Marquardt Backpropagation
(LMBP), among the limits of LMBP in the case of large number of data, so the
use of MLP based on LMBP is no longer valid in our case. As solution, we
propose the use of Cascade-Forward Neural Network (CFNN) based Bayesian
Regulation backpropagation (BRBP). To test our estimator robustness a random
white-Gaussian noise has been added to the sets. The proposed estimator is in
our viewpoint accurate and robust.
",1,0,0,0,0,0
5798,5799,On the treatment of $\ell$-changing proton-hydrogen Rydberg atom collisions,"  Energy-conserving, angular momentum-changing collisions between protons and
highly excited Rydberg hydrogen atoms are important for precise understanding
of atomic recombination at the photon decoupling era, and the elemental
abundance after primordial nucleosynthesis. Early approaches to $\ell$-changing
collisions used perturbation theory for only dipole-allowed ($\Delta \ell=\pm
1$) transitions. An exact non-perturbative quantum mechanical treatment is
possible, but it comes at computational cost for highly excited Rydberg states.
In this note we show how to obtain a semi-classical limit that is accurate and
simple, and develop further physical insights afforded by the non-perturbative
quantum mechanical treatment.
",0,1,0,0,0,0
1015,1016,Two-photon exchange correction to the hyperfine splitting in muonic hydrogen,"  We reevaluate the Zemach, recoil and polarizability corrections to the
hyperfine splitting in muonic hydrogen expressing them through the low-energy
proton structure constants and obtain the precise values of the Zemach radius
and two-photon exchange (TPE) contribution. The uncertainty of TPE correction
to S energy levels in muonic hydrogen of 105 ppm exceeds the ppm accuracy level
of the forthcoming 1S hyperfine splitting measurements at PSI, J-PARC and
RIKEN-RAL.
",0,1,0,0,0,0
16935,16936,Convergence Analysis of Deterministic Kernel-Based Quadrature Rules in Misspecified Settings,"  This paper presents a convergence analysis of kernel-based quadrature rules
in misspecified settings, focusing on deterministic quadrature in Sobolev
spaces. In particular, we deal with misspecified settings where a test
integrand is less smooth than a Sobolev RKHS based on which a quadrature rule
is constructed. We provide convergence guarantees based on two different
assumptions on a quadrature rule: one on quadrature weights, and the other on
design points. More precisely, we show that convergence rates can be derived
(i) if the sum of absolute weights remains constant (or does not increase
quickly), or (ii) if the minimum distance between design points does not
decrease very quickly. As a consequence of the latter result, we derive a rate
of convergence for Bayesian quadrature in misspecified settings. We reveal a
condition on design points to make Bayesian quadrature robust to
misspecification, and show that, under this condition, it may adaptively
achieve the optimal rate of convergence in the Sobolev space of a lesser order
(i.e., of the unknown smoothness of a test integrand), under a slightly
stronger regularity condition on the integrand.
",1,0,0,1,0,0
18880,18881,On Modules over a G-set,"  Let R be a commutative ring with unity, M a module over R and let S be a
G-set for a finite group G. We define a set MS to be the set of elements
expressed as the formal finite sum of the form similar to the elements of group
ring RG. The set MS is a module over the group ring RG under the addition and
the scalar multiplication similar to the RG-module MG. With this notion, we not
only generalize but also unify the theories of both of the group algebra and
the group module, and we also establish some significant properties of MS. In
particular, we describe a method for decomposing a given RG-module MS as a
direct sum of RG-submodules. Furthermore, we prove the semisimplicity problem
of MS with regard to the properties of M, S and G.
",0,0,1,0,0,0
14881,14882,Bright and Gap Solitons in Membrane-Type Acoustic Metamaterials,"  We study analytically and numerically envelope solitons (bright and gap
solitons) in a one-dimensional, nonlinear acoustic metamaterial, composed of an
air-filled waveguide periodically loaded by clamped elastic plates. Based on
the transmission line approach, we derive a nonlinear dynamical lattice model
which, in the continuum approximation, leads to a nonlinear, dispersive and
dissipative wave equation. Applying the multiple scales perturbation method, we
derive an effective lossy nonlinear Schrödinger equation and obtain
analytical expressions for bright and gap solitons. We also perform direct
numerical simulations to study the dissipation-induced dynamics of the bright
and gap solitons. Numerical and analytical results, relying on the analytical
approximations and perturbation theory for solions, are found to be in good
agreement.
",0,1,0,0,0,0
19483,19484,On density of subgraphs of Cartesian products,"  In this paper, we extend two classical results about the density of subgraphs
of hypercubes to subgraphs $G$ of Cartesian products $G_1\times\cdots\times
G_m$ of arbitrary connected graphs. Namely, we show that
$\frac{|E(G)|}{|V(G)|}\le \lceil 2\max\{
\text{dens}(G_1),\ldots,\text{dens}(G_m)\} \rceil\log|V(G)|$, where
$\text{dens}(H)$ is the maximum ratio $\frac{|E(H')|}{|V(H')|}$ over all
subgraphs $H'$ of $H$. We introduce the notions of VC-dimension
$\text{VC-dim}(G)$ and VC-density $\text{VC-dens}(G)$ of a subgraph $G$ of a
Cartesian product $G_1\times\cdots\times G_m$, generalizing the classical
Vapnik-Chervonenkis dimension of set-families (viewed as subgraphs of
hypercubes). We prove that if $G_1,\ldots,G_m$ belong to the class ${\mathcal
G}(H)$ of all finite connected graphs not containing a given graph $H$ as a
minor, then for any subgraph $G$ of $G_1\times\cdots\times G_m$ a sharper
inequality $\frac{|E(G)|}{|V(G)|}\le \text{VC-dim}(G)\alpha(H)$ holds, where
$\alpha(H)$ is the density of the graphs from ${\mathcal G}(H)$. We refine and
sharpen those two results to several specific graph classes. We also derive
upper bounds (some of them polylogarithmic) for the size of adjacency labeling
schemes of subgraphs of Cartesian products.
",1,0,0,0,0,0
8323,8324,Electrical Tuning of Polarizaion-state Using Graphene-Integrated Metasurfaces,"  Plasmonic metasurfaces have been employed for tuning and controlling light
enabling various novel applications. Their appeal is enhanced with the
incorporation of an active element with the metasurfaces paving the way for
dynamic control. In this letter, we realize a dynamic polarization state
generator using graphene-integrated anisotropic metasurface (GIAM), where a
linear incidence polarization is controllably converted into an elliptical one.
The anisotropic metasurface leads to an intrinsic polarization conversion when
illuminated with non-orthogonal incident polarization. Additionally, the
single-layer graphene allows us to tune the phase and intensity of the
reflected light on the application of a gate voltage, enabling dynamic
polarization control. The stokes polarization parameters of the reflected light
are measured using rotating polarizer method and it is demonstrated that a
large change in the ellipticity as well as orientation angle can be induced by
this device. We also provide experimental evidence that the titl angle can
change independent of the ellipticity going from positive values to nearly zero
to negative values while ellipticity is constant.
",0,1,0,0,0,0
5082,5083,Schur $Q$-functions and the Capelli eigenvalue problem for the Lie superalgebra $\mathfrak q(n)$,"  Let $\mathfrak l:= \mathfrak q(n)\times\mathfrak q(n)$, where $\mathfrak
q(n)$ denotes the queer Lie superalgebra. The associative superalgebra $V$ of
type $Q(n)$ has a left and right action of $\mathfrak q(n)$, and hence is
equipped with a canonical $\mathfrak l$-module structure. We consider a
distinguished basis $\{D_\lambda\}$ of the algebra of $\mathfrak l$-invariant
super-polynomial differential operators on $V$, which is indexed by strict
partitions of length at most $n$. We show that the spectrum of the operator
$D_\lambda$, when it acts on the algebra $\mathscr P(V)$ of super-polynomials
on $V$, is given by the factorial Schur $Q$-function of Okounkov and Ivanov.
This constitutes a refinement and a new proof of a result of Nazarov, who
computed the top-degree homogeneous part of the Harish-Chandra image of
$D_\lambda$. As a further application, we show that the radial projections of
the spherical super-polynomials corresponding to the diagonal symmetric pair
$(\mathfrak l,\mathfrak m)$, where $\mathfrak m:=\mathfrak q(n)$, of
irreducible $\mathfrak l$-submodules of $\mathscr P(V)$ are the classical Schur
$Q$-functions.
",0,0,1,0,0,0
7480,7481,Zero divisor and unit elements with support of size 4 in group algebras of torsion free groups,"  Kaplansky Zero Divisor Conjecture states that if $G $ is a torsion free group
and $ \mathbb{F} $ is a field, then the group ring $\mathbb{F}[G]$ contains no
zero divisor and Kaplansky Unit Conjecture states that if $G $ is a torsion
free group and $ \mathbb{F} $ is a field, then $\mathbb{F}[G]$ contains no
non-trivial units. The support of an element $ \alpha= \sum_{x\in G}\alpha_xx$
in $\mathbb{F}[G] $, denoted by $supp(\alpha)$, is the set $ \{x \in
G|\alpha_x\neq 0\} $. In this paper we study possible zero divisors and units
with supports of size $ 4 $ in $\mathbb{F}[G]$. We prove that if
$ \alpha, \beta $ are non-zero elements in $ \mathbb{F}[G] $ for a possible
torsion free group $ G $ and an arbitrary field $ \mathbb{F} $ such that $
|supp(\alpha)|=4 $ and $ \alpha\beta=0 $, then $|supp(\beta)|\geq 7 $. In [J.
Group Theory, $16$ $ (2013),$ no. $5$, $667$-$693$], it is proved that if $
\mathbb{F}=\mathbb{F}_2 $ is the field with two elements, $ G $ is a torsion
free group and $ \alpha,\beta \in \mathbb{F}_2[G]\setminus \{0\}$ such that
$|supp(\alpha)|=4 $ and $ \alpha\beta =0 $, then $|supp(\beta)|\geq 8$. We
improve the latter result to $|supp(\beta)|\geq 9$. Also, concerning the Unit
Conjecture, we prove that if $\mathsf{a}\mathsf{b}=1$ for some
$\mathsf{a},\mathsf{b}\in \mathbb{F}[G]$ and $|supp(\mathsf{a})|=4$, then
$|supp(\mathsf{b})|\geq 6$.
",0,0,1,0,0,0
1724,1725,Perfect Edge Domination: Hard and Solvable Cases,"  Let $G$ be an undirected graph. An edge of $G$ dominates itself and all edges
adjacent to it. A subset $E'$ of edges of $G$ is an edge dominating set of $G$,
if every edge of the graph is dominated by some edge of $E'$. We say that $E'$
is a perfect edge dominating set of $G$, if every edge not in $E'$ is dominated
by exactly one edge of $E'$. The perfect edge dominating problem is to
determine a least cardinality perfect edge dominating set of $G$. For this
problem, we describe two NP-completeness proofs, for the classes of claw-free
graphs of degree at most 3, and for bounded degree graphs, of maximum degree at
most $d \geq 3$ and large girth. In contrast, we prove that the problem admits
an $O(n)$ time solution, for cubic claw-free graphs. In addition, we prove a
complexity dichotomy theorem for the perfect edge domination problem, based on
the results described in the paper. Finally, we describe a linear time
algorithm for finding a minimum weight perfect edge dominating set of a
$P_5$-free graph. The algorithm is robust, in the sense that, given an
arbitrary graph $G$, either it computes a minimum weight perfect edge
dominating set of $G$, or it exhibits an induced subgraph of $G$, isomorphic to
a $P_5$.
",1,0,0,0,0,0
6361,6362,Sparse Data Driven Mesh Deformation,"  Example-based mesh deformation methods are powerful tools for realistic shape
editing. However, existing techniques typically combine all the example
deformation modes, which can lead to overfitting, i.e. using a overly
complicated model to explain the user-specified deformation. This leads to
implausible or unstable deformation results, including unexpected global
changes outside the region of interest. To address this fundamental limitation,
we propose a sparse blending method that automatically selects a smaller number
of deformation modes to compactly describe the desired deformation. This along
with a suitably chosen deformation basis including spatially localized
deformation modes leads to significant advantages, including more meaningful,
reliable, and efficient deformations because fewer and localized deformation
modes are applied. To cope with large rotations, we develop a simple but
effective representation based on polar decomposition of deformation gradients,
which resolves the ambiguity of large global rotations using an
as-consistent-as-possible global optimization. This simple representation has a
closed form solution for derivatives, making it efficient for sparse localized
representation and thus ensuring interactive performance. Experimental results
show that our method outperforms state-of-the-art data-driven mesh deformation
methods, for both quality of results and efficiency.
",1,0,0,0,0,0
15022,15023,On the restricted Chebyshev-Boubaker polynomials,"  Using the language of Riordan arrays, we study a one-parameter family of
orthogonal polynomials that we call the restricted Chebyshev-Boubaker
polynomials. We characterize these polynomials in terms of the three term
recurrences that they satisfy, and we study certain central sequences defined
by their coefficient arrays. We give an integral representation for their
moments, and we show that the Hankel transforms of these moments have a simple
form. We show that the (sequence) Hankel transform of the row sums of the
corresponding moment matrix is defined by a family of polynomials closely
related to the Chebyshev polynomials of the second kind, and that these row
sums are in fact the moments of another family of orthogonal polynomials.
",0,0,1,0,0,0
1248,1249,"Short-time behavior of the heat kernel and Weyl's law on $RCD^*(K, N)$-spaces","  In this paper, we prove pointwise convergence of heat kernels for
mGH-convergent sequences of $RCD^*(K,N)$-spaces. We obtain as a corollary
results on the short-time behavior of the heat kernel in $RCD^*(K,N)$-spaces.
We use then these results to initiate the study of Weyl's law in the $RCD$
setting
",0,0,1,0,0,0
12611,12612,Fractional Brownian markets with time-varying volatility and high-frequency data,"  Diffusion processes driven by Fractional Brownian motion (FBM) have often
been considered in modeling stock price dynamics in order to capture the long
range dependence of stock price observed in reality. Option prices for such
models had been obtained by Necula (2002) under constant drift and volatility.
We obtain option prices under time varying volatility model. The expression
depends on volatility and the Hurst parameter in a complicated manner. We
derive a central limit theorem for the quadratic variation as an estimator for
volatility for both the cases, constant as well as time varying volatility.
That will help us to find estimators of the option prices and to find their
asymptotic distributions.
",0,0,1,1,0,0
5965,5966,A simple proof that the $(n^2-1)$-puzzle is hard,"  The 15 puzzle is a classic reconfiguration puzzle with fifteen uniquely
labeled unit squares within a $4 \times 4$ board in which the goal is to slide
the squares (without ever overlapping) into a target configuration. By
generalizing the puzzle to an $n \times n$ board with $n^2-1$ squares, we can
study the computational complexity of problems related to the puzzle; in
particular, we consider the problem of determining whether a given end
configuration can be reached from a given start configuration via at most a
given number of moves. This problem was shown NP-complete in Ratner and Warmuth
(1990). We provide an alternative simpler proof of this fact by reduction from
the rectilinear Steiner tree problem.
",1,0,0,0,0,0
4766,4767,Inferring Properties of the ISM from Supernova Remnant Size Distributions,"  We model the size distribution of supernova remnants to infer the surrounding
ISM density. Using simple, yet standard SNR evolution models, we find that the
distribution of ambient densities is remarkably narrow; either the standard
assumptions about SNR evolution are wrong, or observable SNRs are biased to a
narrow range of ambient densities. We show that the size distributions are
consistent with log-normal, which severely limits the number of model
parameters in any SNR population synthesis model. Simple Monte Carlo
simulations demonstrate that the size distribution is indistinguishable from
log-normal when the SNR sample size is less than 600. This implies that these
SNR distributions provide only information on the mean and variance, yielding
additional information only when the sample size grows larger than $\sim{600}$
SNRs. To infer the parameters of the ambient density, we use Bayesian
statistical inference under the assumption that SNR evolution is dominated by
the Sedov phase. In particular, we use the SNR sizes and explosion energies to
estimate the mean and variance of the ambient medium surrounding SNR
progenitors. We find that the mean ISM particle density around our sample of
SNRs is $\mu_{\log{n}} = -1.33$, in $\log_{10}$ of particles per cubic
centimeter, with variance $\sigma^2_{\log{n}} = 0.49$. If interpreted at face
value, this implies that most SNRs result from supernovae propagating in the
warm, ionized medium. However, it is also likely that either SNR evolution is
not dominated by the simple Sedov evolution or SNR samples are biased to the
warm, ionized medium (WIM).
",0,1,0,0,0,0
6633,6634,Programmable DNA-mediated decision maker,"  DNA-mediated computing is a novel technology that seeks to capitalize on the
enormous informational capacity of DNA and has tremendous computational ability
to compete with the current silicon-mediated computing, due to massive
parallelism and unique characteristics inherent in DNA interaction. In this
paper, the methodology of DNA-mediated computing is utilized to enrich decision
theory, by demonstrating how a novel programmable DNA-mediated normative
decision-making apparatus is able to capture rational choice under uncertainty.
",1,0,0,0,0,0
20472,20473,A new algorithm for irreducible decomposition of representations of finite groups,"  An algorithm for irreducible decomposition of representations of finite
groups over fields of characteristic zero is described. The algorithm uses the
fact that the decomposition induces a partition of the invariant inner product
into a complete set of mutually orthogonal projectors. By expressing the
projectors through the basis elements of the centralizer ring of the
representation, the problem is reduced to solving systems of quadratic
equations. The current implementation of the algorithm is able to split
representations of dimensions up to hundreds of thousands. Examples of
calculations are given.
",1,0,0,0,0,0
1337,1338,"p-FP: Extraction, Classification, and Prediction of Website Fingerprints with Deep Learning","  Recent advances in learning Deep Neural Network (DNN) architectures have
received a great deal of attention due to their ability to outperform
state-of-the-art classifiers across a wide range of applications, with little
or no feature engineering. In this paper, we broadly study the applicability of
deep learning to website fingerprinting. We show that unsupervised DNNs can be
used to extract low-dimensional feature vectors that improve the performance of
state-of-the-art website fingerprinting attacks. When used as classifiers, we
show that they can match or exceed performance of existing attacks across a
range of application scenarios, including fingerprinting Tor website traces,
fingerprinting search engine queries over Tor, defeating fingerprinting
defenses, and fingerprinting TLS-encrypted websites. Finally, we show that DNNs
can be used to predict the fingerprintability of a website based on its
contents, achieving 99% accuracy on a data set of 4500 website downloads.
",1,0,0,1,0,0
16783,16784,A New Classification of Technologies,"  This study here suggests a classification of technologies based on taxonomic
characteristics of interaction between technologies in complex systems that is
not a studied research field in economics of technical change. The proposed
taxonomy here categorizes technologies in four typologies, in a broad analogy
with the ecology: 1) technological parasitism is a relationship between two
technologies T1 and T2 in a complex system S where one technology T1 benefits
from the interaction with T2, whereas T2 has a negative side from interaction
with T1; 2) technological commensalism is a relationship between two
technologies in S where one technology benefits from the other without
affecting it; 3) technological mutualism is a relationship in which each
technology benefits from the activity of the other within complex systems; 4)
technological symbiosis is a long-term interaction between two (or more)
technologies that evolve together in complex systems. This taxonomy
systematizes the typologies of interactive technologies within complex systems
and predicts their evolutionary pathways that generate stepwise coevolutionary
processes of complex systems of technology. This study here begins the process
of generalizing, as far as possible, critical typologies of interactive
technologies that explain the long-run evolution of technology. The theoretical
framework developed here opens the black box of the interaction between
technologies that affects, with different types of technologies, the
evolutionary pathways of complex systems of technology over time and space.
Overall, then, this new theoretical framework may be useful for bringing a new
perspective to categorize the gradient of benefit to technologies from
interaction with other technologies that can be a ground work for development
of more sophisticated concepts to clarify technological and economic change in
human society.
",1,0,0,0,0,0
2165,2166,Mass transfer in asymptotic-giant-branch binary systems,"  Binary stars can interact via mass transfer when one member (the primary)
ascends onto a giant branch. The amount of gas ejected by the binary and the
amount of gas accreted by the secondary over the lifetime of the primary
influence the subsequent binary phenomenology. Some of the gas ejected by the
binary will remain gravitationally bound and its distribution will be closely
related to the formation of planetary nebulae. We investigate the nature of
mass transfer in binary systems containing an AGB star by adding radiative
transfer to the AstroBEAR AMR Hydro/MHD code.
",0,1,0,0,0,0
3369,3370,Numerical Simulations of Regolith Sampling Processes,"  We present recent improvements in the simulation of regolith sampling
processes in microgravity using the numerical particle method smooth particle
hydrodynamics (SPH). We use an elastic-plastic soil constitutive model for
large deformation and failure flows for dynamical behaviour of regolith. In the
context of projected small body (asteroid or small moons) sample return
missions, we investigate the efficiency and feasibility of a particular
material sampling method: Brushes sweep material from the asteroid's surface
into a collecting tray. We analyze the influence of different material
parameters of regolith such as cohesion and angle of internal friction on the
sampling rate. Furthermore, we study the sampling process in two environments
by varying the surface gravity (Earth's and Phobos') and we apply different
rotation rates for the brushes. We find good agreement of our sampling
simulations on Earth with experiments and provide estimations for the influence
of the material properties on the collecting rate.
",0,1,0,0,0,0
1433,1434,Geometric counting on wavefront real spherical spaces,"  We provide $L^p$-versus $L^\infty$-bounds for eigenfunctions on a real
spherical space $Z$ of wavefront type. It is shown that these bounds imply a
non-trivial error term estimate for lattice counting on $Z$. The paper also
serves as an introduction to geometric counting on spaces of the mentioned
type. Section 7 on higher rank is new and extends the result from v1 to higher
rank. Final version. To appear in Acta Math. Sinica.
",0,0,1,0,0,0
11993,11994,Automatic Question-Answering Using A Deep Similarity Neural Network,"  Automatic question-answering is a classical problem in natural language
processing, which aims at designing systems that can automatically answer a
question, in the same way as human does. In this work, we propose a deep
learning based model for automatic question-answering. First the questions and
answers are embedded using neural probabilistic modeling. Then a deep
similarity neural network is trained to find the similarity score of a pair of
answer and question. Then for each question, the best answer is found as the
one with the highest similarity score. We first train this model on a
large-scale public question-answering database, and then fine-tune it to
transfer to the customer-care chat data. We have also tested our framework on a
public question-answering database and achieved very good performance.
",1,0,0,0,0,0
19293,19294,Online learning with graph-structured feedback against adaptive adversaries,"  We derive upper and lower bounds for the policy regret of $T$-round online
learning problems with graph-structured feedback, where the adversary is
nonoblivious but assumed to have a bounded memory. We obtain upper bounds of
$\widetilde O(T^{2/3})$ and $\widetilde O(T^{3/4})$ for strongly-observable and
weakly-observable graphs, respectively, based on analyzing a variant of the
Exp3 algorithm. When the adversary is allowed a bounded memory of size 1, we
show that a matching lower bound of $\widetilde\Omega(T^{2/3})$ is achieved in
the case of full-information feedback. We also study the particular loss
structure of an oblivious adversary with switching costs, and show that in such
a setting, non-revealing strongly-observable feedback graphs achieve a lower
bound of $\widetilde\Omega(T^{2/3})$, as well.
",0,0,0,1,0,0
556,557,Adaptive local surface refinement based on LR NURBS and its application to contact,"  A novel adaptive local surface refinement technique based on Locally Refined
Non-Uniform Rational B-Splines (LR NURBS) is presented. LR NURBS can model
complex geometries exactly and are the rational extension of LR B-splines. The
local representation of the parameter space overcomes the drawback of
non-existent local refinement in standard NURBS-based isogeometric analysis.
For a convenient embedding into general finite element code, the Bézier
extraction operator for LR NURBS is formulated. An automatic remeshing
technique is presented that allows adaptive local refinement and coarsening of
LR NURBS. In this work, LR NURBS are applied to contact computations of 3D
solids and membranes. For solids, LR NURBS-enriched finite elements are used to
discretize the contact surfaces with LR NURBS finite elements, while the rest
of the body is discretized by linear Lagrange finite elements. For membranes,
the entire surface is discretized by LR NURBS. Various numerical examples are
shown, and they demonstrate the benefit of using LR NURBS: Compared to uniform
refinement, LR NURBS can achieve high accuracy at lower computational cost.
",1,0,0,0,0,0
12234,12235,Discovery of statistical equivalence classes using computer algebra,"  Discrete statistical models supported on labelled event trees can be
specified using so-called interpolating polynomials which are generalizations
of generating functions. These admit a nested representation. A new algorithm
exploits the primary decomposition of monomial ideals associated with an
interpolating polynomial to quickly compute all nested representations of that
polynomial. It hereby determines an important subclass of all trees
representing the same statistical model. To illustrate this method we analyze
the full polynomial equivalence class of a staged tree representing the best
fitting model inferred from a real-world dataset.
",0,0,1,1,0,0
13208,13209,The imprints of bars on the vertical stellar population gradients of galactic bulges,"  This is the second paper of a series aimed to study the stellar kinematics
and population properties of bulges in highly-inclined barred galaxies. In this
work, we carry out a detailed analysis of the stellar age, metallicity and
[Mg/Fe] of 28 highly-inclined ($i > 65^{o}$) disc galaxies, from S0 to S(B)c,
observed with the SAURON integral-field spectrograph. The sample is divided
into two clean samples of barred and unbarred galaxies, on the basis of the
correlation between the stellar velocity and h$_3$ profiles, as well as the
level of cylindrical rotation within the bulge region. We find that while the
mean stellar age, metallicity and [Mg/Fe] in the bulges of barred and unbarred
galaxies are not statistically distinct, the [Mg/Fe] gradients along the minor
axis (away from the disc) of barred galaxies are significantly different than
those without bars. For barred galaxies, stars that are vertically further away
from the midplane are in general more [Mg/Fe]--enhanced and thus the vertical
gradients in [Mg/Fe] for barred galaxies are mostly positive, while for
unbarred bulges the [Mg/Fe] profiles are typically negative or flat. This
result, together with the old populations observed in the barred sample,
indicates that bars are long-lasting structures, and therefore are not easily
destroyed. The marked [Mg/Fe] differences with the bulges of unbarred galaxies
indicate that different formation/evolution scenarios are required to explain
their build-up, and emphasizes the role of bars in redistributing stellar
material in the bulge dominated regions.
",0,1,0,0,0,0
17510,17511,Ancestral inference from haplotypes and mutations,"  We consider inference about the history of a sample of DNA sequences,
conditional upon the haplotype counts and the number of segregating sites
observed at the present time. After deriving some theoretical results in the
coalescent setting, we implement rejection sampling and importance sampling
schemes to perform the inference. The importance sampling scheme addresses an
extension of the Ewens Sampling Formula for a configuration of haplotypes and
the number of segregating sites in the sample. The implementations include both
constant and variable population size models. The methods are illustrated by
two human Y chromosome data sets.
",0,0,1,1,0,0
2644,2645,Decomposing the Quantile Ratio Index with applications to Australian income and wealth data,"  The quantile ratio index introduced by Prendergast and Staudte 2017 is a
simple and effective measure of relative inequality for income data that is
resistant to outliers. It measures the average relative distance of a randomly
chosen income from its symmetric quantile. Another useful property of this
index is investigated here: given a partition of the income distribution into a
union of sets of symmetric quantiles, one can find the conditional inequality
for each set as measured by the quantile ratio index and readily combine them
in a weighted average to obtain the index for the entire population. When
applied to data for various years, one can track how these contributions to
inequality vary over time, as illustrated here for Australian Bureau of
Statistics income and wealth data.
",0,0,0,1,0,0
16650,16651,Pili mediated intercellular forces shape heterogeneous bacterial microcolonies prior to multicellular differentiation,"  Microcolonies are aggregates of a few dozen to a few thousand cells exhibited
by many bacteria. The formation of microcolonies is a crucial step towards the
formation of more mature bacterial communities known as biofilms, but also
marks a significant change in bacterial physiology. Within a microcolony,
bacteria forgo a single cell lifestyle for a communal lifestyle hallmarked by
high cell density and physical interactions between cells potentially altering
their behaviour. It is thus crucial to understand how initially identical
single cells start to behave differently while assembling in these tight
communities. Here we show that cells in the microcolonies formed by the human
pathogen Neisseria gonorrhoeae (Ng) present differential motility behaviors
within an hour upon colony formation. Observation of merging microcolonies and
tracking of single cells within microcolonies reveal a heterogeneous motility
behavior: cells close to the surface of the microcolony exhibit a much higher
motility compared to cells towards the center. Numerical simulations of a
biophysical model for the microcolonies at the single cell level suggest that
the emergence of differential behavior within a multicellular microcolony of
otherwise identical cells is of mechanical origin. It could suggest a route
toward further bacterial differentiation and ultimately mature biofilms.
",0,1,0,0,0,0
5576,5577,Sparse-View X-Ray CT Reconstruction Using $\ell_1$ Prior with Learned Transform,"  A major challenge in X-ray computed tomography (CT) is reducing radiation
dose while maintaining high quality of reconstructed images. To reduce the
radiation dose, one can reduce the number of projection views (sparse-view CT);
however, it becomes difficult to achieve high quality image reconstruction as
the number of projection views decreases. Researchers have applied the concept
of learning sparse representations from (high-quality) CT image dataset to the
sparse-view CT reconstruction. We propose a new statistical CT reconstruction
model that combines penalized weighted-least squares (PWLS) and $\ell_1$
regularization with learned sparsifying transform (PWLS-ST-$\ell_1$), and an
algorithm for PWLS-ST-$\ell_1$. Numerical experiments for sparse-view 2D
fan-beam CT and 3D axial cone-beam CT show that the $\ell_1$ regularizer
significantly improves the sharpness of edges of reconstructed images compared
to the CT reconstruction methods using edge-preserving regularizer and $\ell_2$
regularization with learned ST.
",1,1,0,1,0,0
11116,11117,Don't Jump Through Hoops and Remove Those Loops: SVRG and Katyusha are Better Without the Outer Loop,"  The stochastic variance-reduced gradient method (SVRG) and its accelerated
variant (Katyusha) have attracted enormous attention in the machine learning
community in the last few years due to their superior theoretical properties
and empirical behaviour on training supervised machine learning models via the
empirical risk minimization paradigm. A key structural element in both of these
methods is the inclusion of an outer loop at the beginning of which a full pass
over the training data is made in order to compute the exact gradient, which is
then used to construct a variance-reduced estimator of the gradient. In this
work we design {\em loopless variants} of both of these methods. In particular,
we remove the outer loop and replace its function by a coin flip performed in
each iteration designed to trigger, with a small probability, the computation
of the gradient. We prove that the new methods enjoy the same superior
theoretical convergence properties as the original methods. However, we
demonstrate through numerical experiments that our methods have substantially
superior practical behavior.
",1,0,0,1,0,0
15714,15715,A note on surjectivity of piecewise affine mappings,"  A standard theorem in nonsmooth analysis states that a piecewise affine
function $F:\mathbb R^n\rightarrow\mathbb R^n$ is surjective if it is
coherently oriented in that the linear parts of its selection functions all
have the same nonzero determinant sign. In this note we prove that surjectivity
already follows from coherent orientation of the selection functions which are
active on the unbounded sets of a polyhedral subdivision of the domain
corresponding to $F$. A side bonus of the argumentation is a short proof of the
classical statement that an injective piecewise affine function is coherently
oriented.
",0,0,1,0,0,0
2505,2506,Almost complex structures on connected sums of complex projective spaces,"  We show that the m-fold connected sum $m\#\mathbb{C}\mathbb{P}^{2n}$ admits
an almost complex structure if and only if m is odd.
",0,0,1,0,0,0
9030,9031,Infinitesimal perturbation analysis for risk measures based on the Smith max-stable random field,"  When using risk or dependence measures based on a given underlying model, it
is essential to be able to quantify the sensitivity or robustness of these
measures with respect to the model parameters. In this paper, we consider an
underlying model which is very popular in spatial extremes, the Smith
max-stable random field. We study the sensitivity properties of risk or
dependence measures based on the values of this field at a finite number of
locations. Max-stable fields play a key role, e.g., in the modelling of natural
disasters. As their multivariate density is generally not available for more
than three locations, the Likelihood Ratio Method cannot be used to estimate
the derivatives of the risk measures with respect to the model parameters.
Thus, we focus on a pathwise method, the Infinitesimal Perturbation Analysis
(IPA). We provide a convenient and tractable sufficient condition for
performing IPA, which is intricate to obtain because of the very structure of
max-stable fields involving pointwise maxima over an infinite number of random
functions. IPA enables the consistent estimation of the considered measures'
derivatives with respect to the parameters characterizing the spatial
dependence. We carry out a simulation study which shows that the approach
performs well in various configurations.
",0,0,0,0,0,1
7463,7464,Persistent Monitoring of Stochastic Spatio-temporal Phenomena with a Small Team of Robots,"  This paper presents a solution for persistent monitoring of real-world
stochastic phenomena, where the underlying covariance structure changes sharply
across time, using a small number of mobile robot sensors. We propose an
adaptive solution for the problem where stochastic real-world dynamics are
modeled as a Gaussian Process (GP). The belief on the underlying covariance
structure is learned from recently observed dynamics as a Gaussian Mixture (GM)
in the low-dimensional hyper-parameters space of the GP and adapted across time
using Sequential Monte Carlo methods. Each robot samples a belief point from
the GM and locally optimizes a set of informative regions by greedy
maximization of the submodular entropy function. The key contributions of this
paper are threefold: adapting the belief on the covariance using Markov Chain
Monte Carlo (MCMC) sampling such that particles survive even under sharp
covariance changes across time; exploiting the belief to transform the problem
of entropy maximization into a decentralized one; and developing an
approximation algorithm to maximize entropy on a set of informative regions in
the continuous space. We illustrate the application of the proposed solution
through extensive simulations using an artificial dataset and multiple real
datasets from fixed sensor deployments, and compare it to three competing
state-of-the-art approaches.
",1,0,0,1,0,0
8810,8811,"Split, Send, Reassemble: A Formal Specification of a CAN Bus Protocol Stack","  We present a formal model for a fragmentation and a reassembly protocol
running on top of the standardised CAN bus, which is widely used in automotive
and aerospace applications. Although the CAN bus comes with an in-built
mechanism for prioritisation, we argue that this is not sufficient and provide
another protocol to overcome this shortcoming.
",1,0,0,0,0,0
15036,15037,Electron conduction in solid state via time varying wavevectors,"  In this paper, we study electron wavepacket dynamics in electric and magnetic
fields. We rigorously derive the semiclassical equations of electron dynamics
in electric and magnetic fields. We do it both for free electron and electron
in a periodic potential. We do this by introducing time varying wavevectors
$k(t)$. In the presence of magnetic field, our wavepacket reproduces the
classical cyclotron orbits once the origin of the Schröedinger equation is
correctly chosen to be center of cyclotron orbit. In the presence of both
electric and magnetic fields, our equations for wavepacket dynamics differ from
classical Lorentz force equations. We show that in a periodic potential, on
application of electric field, the electron wave function adiabatically follows
the wavefunction of a time varying Bloch wavevector $k(t)$, with its energies
suitably shifted with time. We derive the effective mass equation and discuss
conduction in conductors and insulators.
",0,1,0,0,0,0
934,935,Approches d'analyse distributionnelle pour améliorer la désambiguïsation sémantique,"  Word sense disambiguation (WSD) improves many Natural Language Processing
(NLP) applications such as Information Retrieval, Machine Translation or
Lexical Simplification. WSD is the ability of determining a word sense among
different ones within a polysemic lexical unit taking into account the context.
The most straightforward approach uses a semantic proximity measure between the
word sense candidates of the target word and those of its context. Such a
method very easily entails a combinatorial explosion. In this paper, we propose
two methods based on distributional analysis which enable to reduce the
exponential complexity without losing the coherence. We present a comparison
between the selection of distributional neighbors and the linearly nearest
neighbors. The figures obtained show that selecting distributional neighbors
leads to better results.
",1,0,0,0,0,0
2483,2484,Fixing an error in Caponnetto and de Vito (2007),"  The seminal paper of Caponnetto and de Vito (2007) provides minimax-optimal
rates for kernel ridge regression in a very general setting. Its proof,
however, contains an error in its bound on the effective dimensionality. In
this note, we explain the mistake, provide a correct bound, and show that the
main theorem remains true.
",0,0,1,1,0,0
17056,17057,DeepFace: Face Generation using Deep Learning,"  We use CNNs to build a system that both classifies images of faces based on a
variety of different facial attributes and generates new faces given a set of
desired facial characteristics. After introducing the problem and providing
context in the first section, we discuss recent work related to image
generation in Section 2. In Section 3, we describe the methods used to
fine-tune our CNN and generate new images using a novel approach inspired by a
Gaussian mixture model. In Section 4, we discuss our working dataset and
describe our preprocessing steps and handling of facial attributes. Finally, in
Sections 5, 6 and 7, we explain our experiments and results and conclude in the
following section. Our classification system has 82\% test accuracy.
Furthermore, our generation pipeline successfully creates well-formed faces.
",1,0,0,0,0,0
8769,8770,Robust and structural ergodicity analysis of stochastic biomolecular networks involving synthetic antithetic integral controllers,"  Ergodicity and output controllability have been shown to be fundamental
concepts for the analysis and synthetic design of closed-loop stochastic
reaction networks, as exemplified by the use of antithetic integral feedback
controllers. In [Gupta, Briat & Khammash, PLoS Comput. Biol., 2014], some
ergodicity and output controllability conditions for unimolecular and certain
classes of bimolecular reaction networks were obtained and formulated through
linear programs. To account for context dependence, these conditions were later
extended in [Briat & Khammash, CDC, 2016] to reaction networks with uncertain
rate parameters using simple and tractable, yet potentially conservative,
methods. Here we develop some exact theoretical methods for verifying, in a
robust setting, the original ergodicity and output controllability conditions
based on algebraic and polynomial techniques. Some examples are given for
illustration.
",1,0,1,0,0,0
7725,7726,A Topological proof that $O_2$ is $2$-MCFL,"  We give a new proof of Salvati's theorem that the group language $O_2$ is $2$
multiple context free. Unlike Salvati's proof, our arguments do not use any
idea specific to two-dimensions. This raises the possibility that the argument
might generalize to $O_n$.
",1,0,1,0,0,0
581,582,Stability of casein micelles cross-linked with genipin: a physicochemical study as a function of pH,"  Chemical or enzymatic cross-linking of casein micelles (CMs) increases their
stability against dissociating agents. In this paper, a comparative study of
stability between native CMs and CMs cross-linked with genipin (CMs-GP) as a
function of pH is described. Stability to temperature and ethanol were
investigated in the pH range 2.0-7.0. The size and the charge
($\zeta$-potential) of the particles were determined by dynamic light
scattering. Native CMs precipitated below pH 5.5, CMs-GP precipitated from pH
3.5 to 4.5, whereas no precipitation was observed at pH 2.0-3.0 or pH 4.5-7.0.
The isoelectric point of CMs-GP was determined to be pH 3.7. Highest stability
against heat and ethanol was observed for CMs-GP at pH 2, where visible
coagulation was determined only after 800 s at 140 $^\circ$C or 87.5% (v/v) of
ethanol. These results confirmed the hypothesis that cross-linking by GP
increased the stability of CMs.
",0,1,0,0,0,0
10224,10225,The absolutely Koszul property of Veronese subrings and Segre products,"  Absolutely Koszul algebras are a class of rings over which any finite graded
module has a rational Poincaré series. We provide a criterion to detect
non-absolutely Koszul rings. Combining the criterion with machine computations,
we identify large families of Veronese subrings and Segre products of
polynomial rings which are not absolutely Koszul. In particular, we classify
completely the absolutely Koszul algebras among Segre products of polynomial
rings, at least in characteristic $0$.
",0,0,1,0,0,0
9600,9601,Morphology of PbTe crystal surface sputtered by argon plasma under Secondary Neutral Mass Spectrometry conditions,"  We have investigated morphology of the lateral surfaces of PbTe crystal
samples grown from melt by the Bridgman method sputtered by Ar+ plasma with ion
energy of 50-550 eV for 5-50 minutes under Secondary Neutral Mass Spectrometry
(SNMS) conditions. The sputtered PbTe crystal surface was found to be
simultaneously both the source of sputtered material and the efficient
substrate for re-deposition of the sputtered material during the depth
profiling. During sputtering PbTe crystal surface is forming the dimple relief.
To be redeposited the sputtered Pb and Te form arrays of the microscopic
surface structures in the shapes of hillocks, pyramids, cones and others on the
PbTe crystal sputtered surface. Correlation between the density of re-deposited
microscopic surface structures, their shape, and average size, on the one hand,
and the energy and duration of sputtering, on the other, is revealed.
",0,1,0,0,0,0
13584,13585,Energy-efficient Hybrid CMOS-NEMS LIF Neuron Circuit in 28 nm CMOS Process,"  Designing analog sub-threshold neuromorphic circuits in deep sub-micron
technologies e.g. 28 nm can be a daunting task due to the problem of excessive
leakage current. We propose novel energy-efficient hybrid CMOS-nano
electro-mechanical switches (NEMS) Leaky Integrate and Fire (LIF) neuron and
synapse circuits and investigate the impact of NEM switches on the leakage
power and overall energy consumption. We analyze the performance of
biologically-inspired neuron circuit in terms of leakage power consumption and
present new energy-efficient neural circuits that operate with biologically
plausible firing rates. Our results show the proposed CMOS-NEMS neuron circuit
is, on average, 35% more energy-efficient than its CMOS counterpart with same
complexity in 28 nm process. Moreover, we discuss how NEM switches can be
utilized to further improve the scalability of mixed-signal neuromorphic
circuits.
",1,0,0,0,0,0
12215,12216,Inference via low-dimensional couplings,"  We investigate the low-dimensional structure of deterministic transformations
between random variables, i.e., transport maps between probability measures. In
the context of statistics and machine learning, these transformations can be
used to couple a tractable ""reference"" measure (e.g., a standard Gaussian) with
a target measure of interest. Direct simulation from the desired measure can
then be achieved by pushing forward reference samples through the map. Yet
characterizing such a map---e.g., representing and evaluating it---grows
challenging in high dimensions. The central contribution of this paper is to
establish a link between the Markov properties of the target measure and the
existence of low-dimensional couplings, induced by transport maps that are
sparse and/or decomposable. Our analysis not only facilitates the construction
of transformations in high-dimensional settings, but also suggests new
inference methodologies for continuous non-Gaussian graphical models. For
instance, in the context of nonlinear state-space models, we describe new
variational algorithms for filtering, smoothing, and sequential parameter
inference. These algorithms can be understood as the natural
generalization---to the non-Gaussian case---of the square-root
Rauch-Tung-Striebel Gaussian smoother.
",0,0,0,1,0,0
6772,6773,Limits of Yang-Mills α-connections,"  In the spirit of recent work of Lamm, Malchiodi and Micallef in the setting
of harmonic maps, we identify Yang-Mills connections obtained by approximations
with respect to the Yang-Mills {\alpha}-energy. More specifically, we show that
for the SU(2) Hopf fibration over the four sphere, for sufficiently small
{\alpha} values the SO(4) invariant ADHM instanton is the unique
{\alpha}-critical point which has Yang-Mills {\alpha}-energy lower than a
specific threshold.
",0,0,1,0,0,0
478,479,Dimension-free Wasserstein contraction of nonlinear filters,"  For a class of partially observed diffusions, sufficient conditions are given
for the map from initial condition of the signal to filtering distribution to
be contractive with respect to Wasserstein distances, with rate which has no
dependence on the dimension of the state-space and is stable under tensor
products of the model. The main assumptions are that the signal has affine
drift and constant diffusion coefficient, and that the likelihood functions are
log-concave. Contraction estimates are obtained from an $h$-process
representation of the transition probabilities of the signal reweighted so as
to condition on the observations.
",0,0,1,1,0,0
6206,6207,Special tilting modules for algebras with positive dominant dimension,"  We study a set of uniquely determined tilting and cotilting modules for an
algebra with positive dominant dimension, with the property that they are
generated or cogenerated (and usually both) by projective-injectives. These
modules have various interesting properties, for example that their
endomorphism algebras always have global dimension at most that of the original
algebra. We characterise d-Auslander-Gorenstein algebras and d-Auslander
algebras via the property that the relevant tilting and cotilting modules
coincide. By the Morita-Tachikawa correspondence, any algebra of dominant
dimension at least 2 may be expressed (essentially uniquely) as the
endomorphism algebra of a generator-cogenerator for another algebra, and we
also study our special tilting and cotilting modules from this point of view,
via the theory of recollements and intermediate extension functors.
",0,0,1,0,0,0
12269,12270,A Semi-Supervised and Inductive Embedding Model for Churn Prediction of Large-Scale Mobile Games,"  Mobile gaming has emerged as a promising market with billion-dollar revenues.
A variety of mobile game platforms and services have been developed around the
world. One critical challenge for these platforms and services is to understand
user churn behavior in mobile games. Accurate churn prediction will benefit
many stakeholders such as game developers, advertisers, and platform operators.
In this paper, we present the first large-scale churn prediction solution for
mobile games. In view of the common limitations of the state-of-the-art methods
built upon traditional machine learning models, we devise a novel
semi-supervised and inductive embedding model that jointly learns the
prediction function and the embedding function for user-app relationships. We
model these two functions by deep neural networks with a unique edge embedding
technique that is able to capture both contextual information and relationship
dynamics. We also design a novel attributed random walk technique that takes
into consideration both topological adjacency and attribute similarities. To
evaluate the performance of our solution, we collect real-world data from the
Samsung Game Launcher platform that includes tens of thousands of games and
hundreds of millions of user-app interactions. The experimental results with
this data demonstrate the superiority of our proposed model against existing
state-of-the-art methods.
",0,0,0,1,0,0
19870,19871,Introduction to the Special Issue on Digital Signal Processing in Radio Astronomy,"  Advances in astronomy are intimately linked to advances in digital signal
processing (DSP). This special issue is focused upon advances in DSP within
radio astronomy. The trend within that community is to use off-the-shelf
digital hardware where possible and leverage advances in high performance
computing. In particular, graphics processing units (GPUs) and field
programmable gate arrays (FPGAs) are being used in place of
application-specific circuits (ASICs); high-speed Ethernet and Infiniband are
being used for interconnect in place of custom backplanes. Further, to lower
hurdles in digital engineering, communities have designed and released
general-purpose FPGA-based DSP systems, such as the CASPER ROACH board, ASTRON
Uniboard and CSIRO Redback board. In this introductory article, we give a brief
historical overview, a summary of recent trends, and provide an outlook on
future directions.
",0,1,0,0,0,0
84,85,"Constraints, Lazy Constraints, or Propagators in ASP Solving: An Empirical Analysis","  Answer Set Programming (ASP) is a well-established declarative paradigm. One
of the successes of ASP is the availability of efficient systems.
State-of-the-art systems are based on the ground+solve approach. In some
applications this approach is infeasible because the grounding of one or few
constraints is expensive. In this paper, we systematically compare alternative
strategies to avoid the instantiation of problematic constraints, that are
based on custom extensions of the solver. Results on real and synthetic
benchmarks highlight some strengths and weaknesses of the different strategies.
(Under consideration for acceptance in TPLP, ICLP 2017 Special Issue.)
",1,0,0,0,0,0
14941,14942,Remark on arithmetic topology,"  We formalize the arithmetic topology, i.e. a relationship between knots and
primes. Namely, using the notion of a cluster C*-algebra we construct a functor
from the category of 3-dimensional manifolds M to a category of algebraic
number fields K, such that the prime ideals (ideals, resp.) in the ring of
integers of K correspond to knots (links, resp.) in M. It is proved that the
functor realizes all axioms of the arithmetic topology conjectured in the
1960's by Manin, Mazur and Mumford.
",0,0,1,0,0,0
15524,15525,The Frobenius number for sequences of triangular and tetrahedral numbers,"  We compute the Frobenius number for sequences of triangular and tetrahedral
numbers. In addition, we study some properties of the numerical semigroups
associated to those sequences.
",0,0,1,0,0,0
16431,16432,Trace Properties from Separation Logic Specifications,"  We propose a formal approach for relating abstract separation logic library
specifications with the trace properties they enforce on interactions between a
client and a library. Separation logic with abstract predicates enforces a
resource discipline that constrains when and how calls may be made between a
client and a library. Intuitively, this can enforce a protocol on the
interaction trace. This intuition is broadly used in the separation logic
community but has not previously been formalised. We provide just such a
formalisation. Our approach is based on using wrappers which instrument library
code to induce execution traces for the properties under examination. By
considering a separation logic extended with trace resources, we prove that
when a library satisfies its separation logic specification then its wrapped
version satisfies the same specification and, moreover, maintains the trace
properties as an invariant. Consequently, any client and library implementation
that are correct with respect to the separation logic specification will
satisfy the trace properties.
",1,0,0,0,0,0
19230,19231,Robust estimators for generalized linear models with a dispersion parameter,"  Highly robust and efficient estimators for the generalized linear model with
a dispersion parameter are proposed. The estimators are based on three steps.
In the first step the maximum rank correlation estimator is used to
consistently estimate the slopes up to a scale factor. In the second step, the
scale factor, the intercept, and the dispersion parameter are consistently
estimated using a MT-estimator of a simple regression model. The combined
estimator is highly robust but inefficient. Then, randomized quantile residuals
based on the initial estimators are used to detect outliers to be rejected and
to define a set S of observations to be retained. Finally, a conditional
maximum likelihood (CML) estimator given the observations in S is computed. We
show that, under the model, S tends to the complete sample for increasing
sample size. Therefore, the CML tends to the unconditional maximum likelihood
estimator. It is therefore highly efficient, while maintaining the high degree
of robustness of the initial estimator. The case of the negative binomial
regression model is studied in detail.
",0,0,0,1,0,0
8691,8692,Serial Correlations in Single-Subject fMRI with Sub-Second TR,"  When performing statistical analysis of single-subject fMRI data, serial
correlations need to be taken into account to allow for valid inference.
Otherwise, the variability in the parameter estimates might be under-estimated
resulting in increased false-positive rates. Serial correlations in fMRI data
are commonly characterized in terms of a first-order autoregressive (AR)
process and then removed via pre-whitening. The required noise model for the
pre-whitening depends on a number of parameters, particularly the repetition
time (TR). Here we investigate how the sub-second temporal resolution provided
by simultaneous multislice (SMS) imaging changes the noise structure in fMRI
time series. We fit a higher-order AR model and then estimate the optimal AR
model order for a sequence with a TR of less than 600 ms providing whole brain
coverage. We show that physiological noise modelling successfully reduces the
required AR model order, but remaining serial correlations necessitate an
advanced noise model. We conclude that commonly used noise models, such as the
AR(1) model, are inadequate for modelling serial correlations in fMRI using
sub-second TRs. Rather, physiological noise modelling in combination with
advanced pre-whitening schemes enable valid inference in single-subject
analysis using fast fMRI sequences.
",0,0,0,1,0,0
19002,19003,Precise but Natural Specification for Robot Tasks,"  We present Flipper, a natural language interface for describing high-level
task specifications for robots that are compiled into robot actions. Flipper
starts with a formal core language for task planning that allows expressing
rich temporal specifications and uses a semantic parser to provide a natural
language interface. Flipper provides immediate visual feedback by executing an
automatically constructed plan of the task in a graphical user interface. This
allows the user to resolve potentially ambiguous interpretations. Flipper
extends itself via naturalization: its users can add definitions for
utterances, from which Flipper induces new rules and adds them to the core
language, gradually growing a more and more natural task specification
language. Flipper improves the naturalization by generalizing the definition
provided by users. Unlike other task-specification systems, Flipper enables
natural language interactions while maintaining the expressive power and formal
precision of a programming language. We show through an initial user study that
natural language interactions and generalization can considerably ease the
description of tasks. Moreover, over time, users employ more and more concepts
outside of the initial core language. Such extensions are available to the
Flipper community, and users can use concepts that others have defined.
",1,0,0,0,0,0
4006,4007,Out-of-focus: Learning Depth from Image Bokeh for Robotic Perception,"  In this project, we propose a novel approach for estimating depth from RGB
images. Traditionally, most work uses a single RGB image to estimate depth,
which is inherently difficult and generally results in poor performance, even
with thousands of data examples. In this work, we alternatively use multiple
RGB images that were captured while changing the focus of the camera's lens.
This method leverages the natural depth information correlated to the different
patterns of clarity/blur in the sequence of focal images, which helps
distinguish objects at different depths. Since no such data set exists for
learning this mapping, we collect our own data set using customized hardware.
We then use a convolutional neural network for learning the depth from the
stacked focal images. Comparative studies were conducted on both a standard
RGBD data set and our own data set (learning from both single and multiple
images), and results verified that stacked focal images yield better depth
estimation than using just single RGB image.
",1,0,0,0,0,0
19662,19663,Tunneling of Glashow-Weinberg-Salam model particles from Black Hole Solutions in Rastall Theory,"  Using the semiclassical WKB approximation and Hamilton-Jacobi method, we
solve an equation of motion for the Glashow-Weinberg-Salam model, which is
important for understanding the unified gauge-theory of weak and
electromagnetic interactions. We calculate the tunneling rate of the massive
charged W-bosons in a background of electromagnetic field to investigate the
Hawking temperature of black holes surrounded by perfect fluid in Rastall
theory. Then, we study the quantum gravity effects on the generalized Proca
equation with generalized uncertainty principle (GUP) on this background. We
show that quantum gravity effects leave the remnants on the Hawking temperature
and the Hawking radiation becomes nonthermal.
",0,1,0,0,0,0
10528,10529,Transiently enhanced interlayer tunneling in optically driven high $T_c$ superconductors,"  Recent pump-probe experiments reported an enhancement of superconducting
transport along the $c$-axis of underdoped YBa$_2$Cu$_3$O$_{6+\delta}$ (YBCO),
induced by a mid-infrared optical pump pulse tuned to a specific lattice
vibration. To understand this transient non-equilibrium state, we develop a
pump-probe formalism for a stack of Josephson junctions, and we consider the
tunneling strengths in presence of modulation with an ultrashort optical pulse.
We demonstrate that a transient enhancement of the Josephson coupling can be
obtained for pulsed excitation and that this can be even larger than in a
continuously driven steady-state. Especially interesting is the conclusion that
the effect is largest when the material is parametrically driven at a frequency
immediately above the plasma frequency, in agreement with what is found
experimentally. For bilayer Josephson junctions, an enhancement similar to that
experimentally is predicted below the critical temperature $T_c$. This model
reproduces the essential features of the enhancement measured below $T_c$. To
reproduce the experimental results above $T_c$, we will explore extensions of
this model, such as in-plane and amplitude fluctuations, elsewhere.
",0,1,0,0,0,0
10749,10750,Theory of $L$-edge spectroscopy of strongly correlated systems,"  X-ray absorption spectroscopy measured at the $L$-edge of transition metals
(TMs) is a powerful element-selective tool providing direct information about
the correlation effects in the $3d$ states. The theoretical modeling of the
$2p\rightarrow3d$ excitation processes remains to be challenging for
contemporary \textit{ab initio} electronic structure techniques, due to strong
core-hole and multiplet effects influencing the spectra. In this work we
present a realization of the method combining the density-functional theory
with multiplet ligand field theory, proposed in Haverkort et al.
(this https URL), Phys. Rev. B 85, 165113
(2012). In this approach a single-impurity Anderson model (SIAM) is
constructed, with almost all parameters obtained from first principles, and
then solved to obtain the spectra. In our implementation we adopt the language
of the dynamical mean-field theory and utilize the local density of states and
the hybridization function, projected onto TM $3d$ states, in order to
construct the SIAM. The developed computational scheme is applied to calculate
the $L$-edge spectra for several TM monoxides. A very good agreement between
the theory and experiment is found for all studied systems. The effect of
core-hole relaxation, hybridization discretization, possible extensions of the
method as well as its limitations are discussed.
",0,1,0,0,0,0
8524,8525,A Hardware Platform for Efficient Multi-Modal Sensing with Adaptive Approximation,"  We present Warp, a hardware platform to support research in approximate
computing, sensor energy optimization, and energy-scavenged systems. Warp
incorporates 11 state-of-the-art sensor integrated circuits, computation, and
an energy-scavenged power supply, all within a miniature system that is just
3.6 cm x 3.3 cm x 0.5 cm. Warp's sensor integrated circuits together contain a
total of 21 sensors with a range of precisions and accuracies for measuring
eight sensing modalities of acceleration, angular rate, magnetic flux density
(compass heading), humidity, atmospheric pressure (elevation), infrared
radiation, ambient temperature, and color. Warp uses a combination of analog
circuits and digital control to facilitate further tradeoffs between sensor and
communication accuracy, energy efficiency, and performance. This article
presents the design of Warp and presents an evaluation of our hardware
implementation. The results show how Warp's design enables performance and
energy efficiency versus ac- curacy tradeoffs.
",1,0,0,0,0,0
20043,20044,Gravitational octree code performance evaluation on Volta GPU,"  In this study, the gravitational octree code originally optimized for the
Fermi, Kepler, and Maxwell GPU architectures is adapted to the Volta
architecture. The Volta architecture introduces independent thread scheduling
requiring either the insertion of the explicit synchronizations at appropriate
locations or the enforcement of the same implicit synchronizations as do the
Pascal or earlier architectures by specifying \texttt{-gencode
arch=compute\_60,code=sm\_70}. The performance measurements on Tesla V100, the
current flagship GPU by NVIDIA, revealed that the $N$-body simulations of the
Andromeda galaxy model with $2^{23} = 8388608$ particles took $3.8 \times
10^{-2}$~s or $3.3 \times 10^{-2}$~s per step for each case. Tesla V100
achieves a 1.4 to 2.2-fold acceleration in comparison with Tesla P100, the
flagship GPU in the previous generation. The observed speed-up of 2.2 is
greater than 1.5, which is the ratio of the theoretical peak performance of the
two GPUs. The independence of the units for integer operations from those for
floating-point number operations enables the overlapped execution of integer
and floating-point number operations. It hides the execution time of the
integer operations leading to the speed-up rate above the theoretical peak
performance ratio. Tesla V100 can execute $N$-body simulation with up to $25
\times 2^{20} = 26214400$ particles, and it took $2.0 \times 10^{-1}$~s per
step. It corresponds to $3.5$~TFlop/s, which is 22\% of the single-precision
theoretical peak performance.
",1,0,0,0,0,0
8219,8220,Transversality for local Morse homology with symmetries and applications,"  We prove the transversality result necessary for defining local Morse chain
complexes with finite cyclic group symmetry. Our arguments use special
regularized distance functions constructed using classical covering lemmas, and
an inductive perturbation process indexed by the strata of the isotropy set. A
global existence theorem for symmetric Morse-Smale pairs is also proved.
Regarding applications, we focus on Hamiltonian dynamics and rigorously
establish a local contact homology package based on discrete action
functionals. We prove a persistence theorem, analogous to the classical
shifting lemma for geodesics, asserting that the iteration map is an
isomorphism for good and admissible iterations. We also consider a
Chas-Sullivan product on non-invariant local Morse homology, which plays the
role of pair-of-pants product, and study its relationship to symplectically
degenerate maxima. Finally, we explore how our invariants can be used to study
bifurcation of critical points (and periodic points) under additional
symmetries.
",0,0,1,0,0,0
6028,6029,"To prune, or not to prune: exploring the efficacy of pruning for model compression","  Model pruning seeks to induce sparsity in a deep neural network's various
connection matrices, thereby reducing the number of nonzero-valued parameters
in the model. Recent reports (Han et al., 2015; Narang et al., 2017) prune deep
networks at the cost of only a marginal loss in accuracy and achieve a sizable
reduction in model size. This hints at the possibility that the baseline models
in these experiments are perhaps severely over-parameterized at the outset and
a viable alternative for model compression might be to simply reduce the number
of hidden units while maintaining the model's dense connection structure,
exposing a similar trade-off in model size and accuracy. We investigate these
two distinct paths for model compression within the context of energy-efficient
inference in resource-constrained environments and propose a new gradual
pruning technique that is simple and straightforward to apply across a variety
of models/datasets with minimal tuning and can be seamlessly incorporated
within the training process. We compare the accuracy of large, but pruned
models (large-sparse) and their smaller, but dense (small-dense) counterparts
with identical memory footprint. Across a broad range of neural network
architectures (deep CNNs, stacked LSTM, and seq2seq LSTM models), we find
large-sparse models to consistently outperform small-dense models and achieve
up to 10x reduction in number of non-zero parameters with minimal loss in
accuracy.
",1,0,0,1,0,0
12565,12566,Code Reuse With Transformation Objects,"  We present an approach for a lightweight datatype-generic programming in
Objective Caml programming language aimed at better code reuse. We show, that a
large class of transformations usually expressed via recursive functions with
pattern matching can be implemented using the single per-type traversal
function and the set of object-encoded transformations, which we call
transformation objects. Object encoding allows transformations to be modified,
inherited and extended in a conventional object-oriented manner. However, the
data representation is kept untouched which preserves the ability to construct
and pattern-match it in the usual way. Our approach equally works for regular
and polymorphic variant types which makes it possible to combine data types and
their transformations from statically typed and separately compiled components.
We also present an implementation which allows us to automatically derive most
functionality from a slightly augmented type descriptions.
",1,0,0,0,0,0
20443,20444,A Semantic Loss Function for Deep Learning with Symbolic Knowledge,"  This paper develops a novel methodology for using symbolic knowledge in deep
learning. From first principles, we derive a semantic loss function that
bridges between neural output vectors and logical constraints. This loss
function captures how close the neural network is to satisfying the constraints
on its output. An experimental evaluation shows that it effectively guides the
learner to achieve (near-)state-of-the-art results on semi-supervised
multi-class classification. Moreover, it significantly increases the ability of
the neural network to predict structured objects, such as rankings and paths.
These discrete concepts are tremendously difficult to learn, and benefit from a
tight integration of deep learning and symbolic reasoning methods.
",1,0,0,1,0,0
18482,18483,A finite element method framework for modeling rotating machines with superconducting windings,"  Electrical machines employing superconductors are attractive solutions in a
variety of application domains. Numerical models are powerful and necessary
tools to optimize their design and predict their performance. The
electromagnetic modeling of superconductors by finite-element method (FEM) is
usually based on a power-law resistivity for their electrical behavior. The
implementation of such constitutive law in conventional models of electrical
machines is quite problematic: the magnetic vector potential directly gives the
electric field and requires using a power-law depending on it. This power-law
is a non-bounded function that can generate enormous uneven values in low
electric field regions that can destroy the reliability of solutions. The
method proposed here consists in separating the model of an electrical machine
in two parts, where the magnetic field is calculated with the most appropriate
formulation: the H-formulation in the part containing the superconductors and
the A-formulation in the part containing conventional conductors (and possibly
permanent magnets). The main goal of this work is to determine and to correctly
apply the continuity conditions on the boundary separating the two regions.
Depending on the location of such boundary -- in the fixed or rotating part of
the machine -- the conditions that one needs to apply are different. In
addition, the application of those conditions requires the use of Lagrange
multipliers satisfying the field transforms of the electromagnetic quantities
in the two reference systems, the fixed and the rotating one. In this article,
several exemplary cases for the possible configurations are presented. In order
to emphasize and capture the essential point of this modeling strategy, the
discussed examples are rather simple. Nevertheless, they constitute a solid
starting point for modeling more complex and realistic devices.
",0,1,0,0,0,0
3070,3071,Assessing the state of e-Readiness for Small and Medium Companies in Mexico: a Proposed Taxonomy and Adoption Model,"  Emerging economies frequently show a large component of their Gross Domestic
Product to be dependant on the economic activity of small and medium
enterprises. Nevertheless, e-business solutions are more likely designed for
large companies. SMEs seem to follow a classical family-based management, used
to traditional activities, rather than seeking new ways of adding value to
their business strategy. Thus, a large portion of a nations economy may be at
disadvantage for competition. This paper aims at assessing the state of
e-business readiness of Mexican SMEs based on already published e-business
evolution models and by means of a survey research design. Data is being
collected in three cities with differing sizes and infrastructure conditions.
Statistical results are expected to be presented. A second part of this
research aims at applying classical adoption models to suggest potential causal
relationships, as well as more suitable recommendations for development.
",0,0,0,0,0,1
4120,4121,Dark trions and biexcitons in WS2 and WSe2 made bright by e-e scattering,"  The direct band gap character and large spin-orbit splitting of the valence
band edges (at the K and K' valleys) in monolayer transition metal
dichalcogenides have put these two-dimensional materials under the spot-light
of intense experimental and theoretical studies. In particular, for Tungsten
dichalcogenides it has been found that the sign of spin splitting of conduction
band edges makes ground state excitons radiatively inactive (dark) due to spin
and momentum mismatch between the constituent electron and hole. One might
similarly assume that the ground states of charged excitons and biexcitons in
these monolayers are also dark. Here, we show that the intervalley
K$\leftrightarrows$K' electron-electron scattering mixes bright and dark states
of these complexes, and estimate the radiative lifetimes in the ground states
of these ""semi-dark"" trions and biexcitons to be ~ 10ps, and analyse how these
complexes appear in the temperature-dependent photoluminescence spectra of WS2
and WSe2 monolayers.
",0,1,0,0,0,0
11709,11710,"Online characterization of planetary surfaces: PlanetServer, an open-source analysis and visualization tool","  The lack of open-source tools for hyperspectral data visualization and
analysiscreates a demand for new tools. In this paper we present the new
PlanetServer,a set of tools comprising a web Geographic Information System
(GIS) and arecently developed Python Application Programming Interface (API)
capableof visualizing and analyzing a wide variety of hyperspectral data from
differentplanetary bodies. Current WebGIS open-source tools are evaluated in
orderto give an overview and contextualize how PlanetServer can help in this
mat-ters. The web client is thoroughly described as well as the datasets
availablein PlanetServer. Also, the Python API is described and exposed the
reason ofits development. Two different examples of mineral characterization of
differenthydrosilicates such as chlorites, prehnites and kaolinites in the Nili
Fossae areaon Mars are presented. As the obtained results show positive outcome
in hyper-spectral analysis and visualization compared to previous literature,
we suggestusing the PlanetServer approach for such investigations.
",1,1,0,0,0,0
7405,7406,Loop-augmented forests and a variant of the Foulkes' conjecture,"  A loop-augmented forest is a labeled rooted forest with loops on some of its
roots. By exploiting an interplay between nilpotent partial functions and
labeled rooted forests, we investigate the permutation action of the symmetric
group on loop-augmented forests. Furthermore, we describe an extension of the
Foulkes' conjecture and prove a special case. Among other important outcomes of
our analysis are a complete description of the stabilizer subgroup of an
idempotent in the semigroup of partial transformations and a generalization of
the (Knuth-Sagan) hook length formula.
",0,0,1,0,0,0
8825,8826,P4K: A Formal Semantics of P4 and Applications,"  Programmable packet processors and P4 as a programming language for such
devices have gained significant interest, because their flexibility enables
rapid development of a diverse set of applications that work at line rate.
However, this flexibility, combined with the complexity of devices and
networks, increases the chance of introducing subtle bugs that are hard to
discover manually. Worse, this is a domain where bugs can have catastrophic
consequences, yet formal analysis tools for P4 programs / networks are missing.
We argue that formal analysis tools must be based on a formal semantics of
the target language, rather than on its informal specification. To this end, we
provide an executable formal semantics of the P4 language in the K framework.
Based on this semantics, K provides an interpreter and various analysis tools
including a symbolic model checker and a deductive program verifier for P4.
This paper overviews our formal K semantics of P4, as well as several P4
language design issues that we found during our formalization process. We also
discuss some applications resulting from the tools provided by K for P4
programmers and network administrators as well as language designers and
compiler developers, such as detection of unportable code, state space
exploration of P4 programs and of networks, bug finding using symbolic
execution, data plane verification, program verification, and translation
validation.
",1,0,0,0,0,0
12142,12143,Quantum Blockchain using entanglement in time,"  A conceptual design for a quantum blockchain is proposed. Our method involves
encoding the blockchain into a temporal GHZ (Greenberger-Horne-Zeilinger) state
of photons that do not simultaneously coexist. It is shown that the
entanglement in time, as opposed to an entanglement in space, provides the
crucial quantum advantage. All the subcomponents of this system have already
been shown to be experimentally realized. Perhaps more shockingly, our encoding
procedure can be interpreted as non-classically influencing the past; hence
this decentralized quantum blockchain can be viewed as a quantum networked time
machine.
",0,0,0,0,0,1
20746,20747,Miura transformations for discrete Painlevé equations coming from the affine E$_8$ Weyl group,"  We derive integrable equations starting from autonomous mappings with a
general form inspired by the additive systems associated to the affine Weyl
group E$_8^{(1)}$. By deautonomisation we obtain two hitherto unknown systems,
one of which turns out to be a linearisable one, and we show that both these
systems arise from the deautonomisation of a non-QRT mapping. In order to
unambiguously prove the integrability of these nonautonomous systems, we
introduce a series of Miura transformations which allows us to prove that one
of these systems is indeed a discrete Painlevé equation, related to the
affine Weyl group E$_7^{(1)}$, and to cast it in canonical form. A similar
sequence of Miura transformations allows us to effectively linearise the second
system we obtain. An interesting off-shoot of our calculations is that the
series of Miura transformations, when applied at the autonomous limit, allows
one to transform a non-QRT invariant into a QRT one.
",0,1,1,0,0,0
13879,13880,Accurately and Efficiently Interpreting Human-Robot Instructions of Varying Granularities,"  Humans can ground natural language commands to tasks at both abstract and
fine-grained levels of specificity. For instance, a human forklift operator can
be instructed to perform a high-level action, like ""grab a pallet"" or a
low-level action like ""tilt back a little bit."" While robots are also capable
of grounding language commands to tasks, previous methods implicitly assume
that all commands and tasks reside at a single, fixed level of abstraction.
Additionally, methods that do not use multiple levels of abstraction encounter
inefficient planning and execution times as they solve tasks at a single level
of abstraction with large, intractable state-action spaces closely resembling
real world complexity. In this work, by grounding commands to all the tasks or
subtasks available in a hierarchical planning framework, we arrive at a model
capable of interpreting language at multiple levels of specificity ranging from
coarse to more granular. We show that the accuracy of the grounding procedure
is improved when simultaneously inferring the degree of abstraction in language
used to communicate the task. Leveraging hierarchy also improves efficiency:
our proposed approach enables a robot to respond to a command within one second
on 90% of our tasks, while baselines take over twenty seconds on half the
tasks. Finally, we demonstrate that a real, physical robot can ground commands
at multiple levels of abstraction allowing it to efficiently plan different
subtasks within the same planning hierarchy.
",1,0,0,0,0,0
2142,2143,Meta-Learning for Contextual Bandit Exploration,"  We describe MELEE, a meta-learning algorithm for learning a good exploration
policy in the interactive contextual bandit setting. Here, an algorithm must
take actions based on contexts, and learn based only on a reward signal from
the action taken, thereby generating an exploration/exploitation trade-off.
MELEE addresses this trade-off by learning a good exploration strategy for
offline tasks based on synthetic data, on which it can simulate the contextual
bandit setting. Based on these simulations, MELEE uses an imitation learning
strategy to learn a good exploration policy that can then be applied to true
contextual bandit tasks at test time. We compare MELEE to seven strong baseline
contextual bandit algorithms on a set of three hundred real-world datasets, on
which it outperforms alternatives in most settings, especially when differences
in rewards are large. Finally, we demonstrate the importance of having a rich
feature representation for learning how to explore.
",1,0,0,1,0,0
4031,4032,Learning Context-Sensitive Convolutional Filters for Text Processing,"  Convolutional neural networks (CNNs) have recently emerged as a popular
building block for natural language processing (NLP). Despite their success,
most existing CNN models employed in NLP share the same learned (and static)
set of filters for all input sentences. In this paper, we consider an approach
of using a small meta network to learn context-sensitive convolutional filters
for text processing. The role of meta network is to abstract the contextual
information of a sentence or document into a set of input-aware filters. We
further generalize this framework to model sentence pairs, where a
bidirectional filter generation mechanism is introduced to encapsulate
co-dependent sentence representations. In our benchmarks on four different
tasks, including ontology classification, sentiment analysis, answer sentence
selection, and paraphrase identification, our proposed model, a modified CNN
with context-sensitive filters, consistently outperforms the standard CNN and
attention-based CNN baselines. By visualizing the learned context-sensitive
filters, we further validate and rationalize the effectiveness of proposed
framework.
",1,0,0,1,0,0
9003,9004,Comments on avalanche flow models based on the concept of random kinetic energy,"  In a series of papers, Bartelt and co-workers developed novel snow-avalanche
models in which \emph{random kinetic energy} $R_K$ (a.k.a.\ granular
temperature) is a key concept. The earliest models were for a single, constant
density layer, using a Voellmy model but with $R_K$-dependent friction
parameters. This was then extended to variable density, and finally a
suspension layer (powder-snow cloud) was added. The physical basis and
mathematical formulation of these models is critically reviewed here, with the
following main findings: (i) Key assumptions in the original RKE model differ
substantially from established results on dense granular flows; in particular,
the effective friction coefficient decreases to zero with velocity in the RKE
model. (ii) In the variable-density model, non-canonical interpretation of the
energy balance leads to a third-order evolution equation for the flow depth or
density, whereas the stated assumptions imply a first-order equation. (iii) The
model for the suspension layer neglects gravity and disregards well established
theoretical and experimental results on particulate gravity currents. Some
options for improving these aspects are discussed.
",0,1,0,0,0,0
11887,11888,Magnetic control of Goos-Hanchen shifts in a yttrium-iron-garnet film,"  We investigate the Goos-Hanchen (G-H) shifts reflected and transmitted by a
yttrium-iron-garnet (YIG) film for both normal and oblique incidence. It is
found that the nonreciprocity effect of the MO material does not only result in
a nonvanishing reflected shift at normal incidence, but also leads to a
slab-thickness-independent term which breaks the symmetry between the reflected
and transmitted shifts at oblique incidence. The asymptotic behaviors of the
normal-incidence reflected shift are obtained in the vicinity of two
characteristic frequencies corresponding to a minimum reflectivity and a total
reflection, respectively. Moreover, the coexistence of two types of
negative-reflected-shift (NRS) at oblique incidence is discussed. We show that
the reversal of the shifts from positive to negative values can be realized by
tuning the magnitude of applied magnetic field, the frequency of incident wave
and the slab thickness as well as the incident angle. In addition, we further
investigate two special cases for practical purposes: the reflected shift with
a total reflection and the transmitted shift with a total transmission.
Numerical simulations are also performed to verify our analytical results.
",0,1,0,0,0,0
8159,8160,Searching for axion stars and Q-balls with a terrestrial magnetometer network,"  Light (pseudo-)scalar fields are promising candidates to be the dark matter
in the Universe. Under certain initial conditions in the early Universe and/or
with certain types of self-interactions, they can form compact dark-matter
objects such as axion stars or Q-balls. Direct encounters with such objects can
be searched for by using a global network of atomic magnetometers. It is shown
that for a range of masses and radii not ruled out by existing observations,
the terrestrial encounter rate with axion stars or Q-balls can be sufficiently
high (at least once per year) for a detection. Furthermore, it is shown that a
global network of atomic magnetometers is sufficiently sensitive to
pseudoscalar couplings to atomic spins so that a transit through an axion star
or Q-ball could be detected over a broad range of unexplored parameter space.
",0,1,0,0,0,0
13771,13772,An informative path planning framework for UAV-based terrain monitoring,"  Unmanned aerial vehicles (UAVs) represent a new frontier in a wide range of
monitoring and research applications. To fully leverage their potential, a key
challenge is planning missions for efficient data acquisition in complex
environments. To address this issue, this article introduces a general
informative path planning (IPP) framework for monitoring scenarios using an
aerial robot. The approach is capable of mapping either discrete or continuous
target variables on a terrain using variable-resolution data received from
probabilistic sensors. During a mission, the terrain maps built online are used
to plan information-rich trajectories in continuous 3-D space by optimizing
initial solutions obtained by a course grid search. Extensive simulations show
that our approach is more efficient than existing methods. We also demonstrate
its real-time application on a photorealistic mapping scenario using a publicly
available dataset.
",1,0,0,0,0,0
14957,14958,A Robust Utility Learning Framework via Inverse Optimization,"  In many smart infrastructure applications flexibility in achieving
sustainability goals can be gained by engaging end-users. However, these users
often have heterogeneous preferences that are unknown to the decision-maker
tasked with improving operational efficiency. Modeling user interaction as a
continuous game between non-cooperative players, we propose a robust parametric
utility learning framework that employs constrained feasible generalized least
squares estimation with heteroskedastic inference. To improve forecasting
performance, we extend the robust utility learning scheme by employing
bootstrapping with bagging, bumping, and gradient boosting ensemble methods.
Moreover, we estimate the noise covariance which provides approximated
correlations between players which we leverage to develop a novel correlated
utility learning framework. We apply the proposed methods both to a toy example
arising from Bertrand-Nash competition between two firms as well as to data
from a social game experiment designed to encourage energy efficient behavior
amongst smart building occupants. Using occupant voting data for shared
resources such as lighting, we simulate the game defined by the estimated
utility functions to demonstrate the performance of the proposed methods.
",1,0,1,0,0,0
2638,2639,Mixtures of Skewed Matrix Variate Bilinear Factor Analyzers,"  Clustering is the process of finding and analyzing underlying group structure
in data. In recent years, data as become increasingly higher dimensional and,
therefore, an increased need has arisen for dimension reduction techniques for
clustering. Although such techniques are firmly established in the literature
for multivariate data, there is a relative paucity in the area of matrix
variate or three way data. Furthermore, the few methods that are available all
assume matrix variate normality, which is not always sensible if cluster
skewness or excess kurtosis is present. Mixtures of bilinear factor analyzers
models using skewed matrix variate distributions are proposed. In all, four
such mixture models are presented, based on matrix variate skew-t, generalized
hyperbolic, variance gamma and normal inverse Gaussian distributions,
respectively.
",0,0,0,1,0,0
15991,15992,Microscopic Conductivity of Lattice Fermions at Equilibrium - Part II: Interacting Particles,"  We apply Lieb-Robinson bounds for multi-commutators we recently derived to
study the (possibly non-linear) response of interacting fermions at thermal
equilibrium to perturbations of the external electromagnetic field. This
analysis leads to an extension of the results for quasi-free fermions of
\cite{OhmI,OhmII} to fermion systems on the lattice with short-range
interactions. More precisely, we investigate entropy production and charge
transport properties of non-autonomous $C^{\ast }$-dynamical systems associated
with interacting lattice fermions within bounded static potentials and in
presence of an electric field that is time- and space-dependent. We verify the
1st law of thermodynamics for the heat production of the system under
consideration. In linear response theory, the latter is related with Ohm and
Joule's laws. These laws are proven here to hold at the microscopic scale,
uniformly with respect to the size of the (microscopic) region where the
electric field is applied. An important outcome is the extension of the notion
of conductivity measures to interacting fermions.
",0,0,1,0,0,0
6057,6058,Deep Learning Methods for Efficient Large Scale Video Labeling,"  We present a solution to ""Google Cloud and YouTube-8M Video Understanding
Challenge"" that ranked 5th place. The proposed model is an ensemble of three
model families, two frame level and one video level. The training was performed
on augmented dataset, with cross validation.
",1,0,0,1,0,0
2351,2352,Modification of low-temperature silicon dioxide films under the influence of technology factors,"  The structure, composition and electrophysical characteristics of
low-temperature silicon dioxide films under influence of various technology
factors, such as ion implantation, laser irradiation, thermal and photonic
annealing, have been studied. Silicon dioxide films have been obtained by
monosilane oxidation using plasma chemical method, reactive cathode sputtering,
and tetraethoxysilane pyrolysis. In the capacity of substrates, germanium,
silicon, gallium arsenide and gallium nitride were used. Structure and
composition of the dielectric films were analyzed by methods of infrared
transmission spectroscopy and frustrated internal reflectance spectroscopy.
Analysis of modification efficiency of low-temperature silicon dioxide films
has been made depending on the substrate type, structure and properties of the
films, their moisture permeability, dielectric deposition technique, type and
dose of implantation ions, temperature and kind of annealing.
",0,1,0,0,0,0
10081,10082,"Energy transfer, pressure tensor and heating of kinetic plasma","  Kinetic plasma turbulence cascade spans multiple scales ranging from
macroscopic fluid flow to sub-electron scales. Mechanisms that dissipate large
scale energy, terminate the inertial range cascade and convert kinetic energy
into heat are hotly debated. Here we revisit these puzzles using fully kinetic
simulation. By performing scale-dependent spatial filtering on the Vlasov
equation, we extract information at prescribed scales and introduce several
energy transfer functions. This approach allows highly inhomogeneous energy
cascade to be quantified as it proceeds down to kinetic scales. The pressure
work, $-\left( \boldsymbol{P} \cdot \nabla \right) \cdot \boldsymbol{u}$, can
trigger a channel of the energy conversion between fluid flow and random
motions, which is a collision-free generalization of the viscous dissipation in
collisional fluid. Both the energy transfer and the pressure work are strongly
correlated with velocity gradients.
",0,1,0,0,0,0
9617,9618,Nonlinear Modulational Instability of Dispersive PDE Models,"  We prove nonlinear modulational instability for both periodic and localized
perturbations of periodic traveling waves for several dispersive PDEs,
including the KDV type equations (e.g. the Whitham equation, the generalized
KDV equation, the Benjamin-Ono equation), the nonlinear Schrödinger equation
and the BBM equation. First, the semigroup estimates required for the nonlinear
proof are obtained by using the Hamiltonian structures of the linearized PDEs;
Second, for KDV type equations the loss of derivative in the nonlinear term is
overcome in two complementary cases: (1) for smooth nonlinear terms and general
dispersive operators, we construct higher order approximation solutions and
then use energy type estimates; (2) for nonlinear terms of low regularity, with
some additional assumption on the dispersive operator, we use a bootstrap
argument to overcome the loss of derivative.
",0,0,1,0,0,0
14940,14941,Recovering sparse graphs,"  We construct a fixed parameter algorithm parameterized by d and k that takes
as an input a graph G' obtained from a d-degenerate graph G by complementing on
at most k arbitrary subsets of the vertex set of G and outputs a graph H such
that G and H agree on all but f(d,k) vertices.
Our work is motivated by the first order model checking in graph classes that
are first order interpretable in classes of sparse graphs. We derive as a
corollary that if G_0 is a graph class with bounded expansion, then the first
order model checking is fixed parameter tractable in the class of all graphs
that can obtained from a graph G from G_0 by complementing on at most k
arbitrary subsets of the vertex set of G; this implies an earlier result that
the first order model checking is fixed parameter tractable in graph classes
interpretable in classes of graphs with bounded maximum degree.
",1,0,0,0,0,0
606,607,Boosting the Actor with Dual Critic,"  This paper proposes a new actor-critic-style algorithm called Dual
Actor-Critic or Dual-AC. It is derived in a principled way from the Lagrangian
dual form of the Bellman optimality equation, which can be viewed as a
two-player game between the actor and a critic-like function, which is named as
dual critic. Compared to its actor-critic relatives, Dual-AC has the desired
property that the actor and dual critic are updated cooperatively to optimize
the same objective function, providing a more transparent way for learning the
critic that is directly related to the objective function of the actor. We then
provide a concrete algorithm that can effectively solve the minimax
optimization problem, using techniques of multi-step bootstrapping, path
regularization, and stochastic dual ascent algorithm. We demonstrate that the
proposed algorithm achieves the state-of-the-art performances across several
benchmarks.
",1,0,0,0,0,0
1252,1253,Using a Predator-Prey Model to Explain Variations of Cloud Spot Price,"  The spot pricing scheme has been considered to be resource-efficient for
providers and cost-effective for consumers in the Cloud market. Nevertheless,
unlike the static and straightforward strategies of trading on-demand and
reserved Cloud services, the market-driven mechanism for trading spot service
would be complicated for both implementation and understanding. The largely
invisible market activities and their complex interactions could especially
make Cloud consumers hesitate to enter the spot market. To reduce the
complexity in understanding the Cloud spot market, we decided to reveal the
backend information behind spot price variations. Inspired by the methodology
of reverse engineering, we developed a Predator-Prey model that can simulate
the interactions between demand and resource based on the visible spot price
traces. The simulation results have shown some basic regular patterns of market
activities with respect to Amazon's spot instance type m3.large. Although the
findings of this study need further validation by using practical data, our
work essentially suggests a promising approach (i.e.~using a Predator-Prey
model) to investigate spot market activities.
",1,0,0,0,0,0
16232,16233,The Persistent Homotopy Type Distance,"  We introduce the persistent homotopy type distance dHT to compare real valued
functions defined on possibly different homotopy equivalent topological spaces.
The underlying idea in the definition of dHT is to measure the minimal shift
that is necessary to apply to one of the two functions in order that the
sublevel sets of the two functions become homotopically equivalent. This
distance is interesting in connection with persistent homology. Indeed, our
main result states that dHT still provides an upper bound for the bottleneck
distance between the persistence diagrams of the intervening functions.
Moreover, because homotopy equivalences are weaker than homeomorphisms, this
implies a lifting of the standard stability results provided by the L-infty
distance and the natural pseudo-distance dNP. From a different standpoint, we
prove that dHT extends the L-infty distance and dNP in two ways. First, we show
that, appropriately restricting the category of objects to which dHT applies,
it can be made to coincide with the other two distances. Finally, we show that
dHT has an interpretation in terms of interleavings that naturally places it in
the family of distances used in persistence theory.
",1,0,1,0,0,0
19252,19253,The inverse hull of 0-left cancellative semigroups,"  Given a semigroup S with zero, which is left-cancellative in the sense that
st=sr \neq 0 implies that t=r, we construct an inverse semigroup called the
inverse hull of S, denoted H(S). When S admits least common multiples, in a
precise sense defined below, we study the idempotent semilattice of H(S), with
a focus on its spectrum. When S arises as the language semigroup for a subsift
X on a finite alphabet, we discuss the relationship between H(S) and several
C*-algebras associated to X appearing in the literature.
",0,0,1,0,0,0
2930,2931,When Work Matters: Transforming Classical Network Structures to Graph CNN,"  Numerous pattern recognition applications can be formed as learning from
graph-structured data, including social network, protein-interaction network,
the world wide web data, knowledge graph, etc. While convolutional neural
network (CNN) facilitates great advances in gridded image/video understanding
tasks, very limited attention has been devoted to transform these successful
network structures (including Inception net, Residual net, Dense net, etc.) to
establish convolutional networks on graph, due to its irregularity and
complexity geometric topologies (unordered vertices, unfixed number of adjacent
edges/vertices). In this paper, we aim to give a comprehensive analysis of when
work matters by transforming different classical network structures to graph
CNN, particularly in the basic graph recognition problem. Specifically, we
firstly review the general graph CNN methods, especially in its spectral
filtering operation on the irregular graph data. We then introduce the basic
structures of ResNet, Inception and DenseNet into graph CNN and construct these
network structures on graph, named as G_ResNet, G_Inception, G_DenseNet. In
particular, it seeks to help graph CNNs by shedding light on how these
classical network structures work and providing guidelines for choosing
appropriate graph network frameworks. Finally, we comprehensively evaluate the
performance of these different network structures on several public graph
datasets (including social networks and bioinformatic datasets), and
demonstrate how different network structures work on graph CNN in the graph
recognition task.
",0,0,0,1,0,0
3980,3981,The norm residue symbol for higher local fields,"  Since the development of higher local class field theory, several explicit
reciprocity laws have been constructed. In particular, there are formulas
describing the higher-dimensional Hilbert symbol given, among others, by M.
Kurihara, A. Zinoviev and S. Vostokov. K. Kato also has explicit formulas for
the higher-dimensional Kummer pairing associated to certain (one-dimensional)
$p$-divisible groups.
In this paper we construct an explicit reciprocity law describing the Kummer
pairing associated to any (one-dimensional) formal group. The formulas are a
generalization to higher-dimensional local fields of Kolyvagin's reciprocity
laws. The formulas obtained describe the values of the pairing in terms of
multidimensional $p$-adic differentiation, the logarithm of the formal group,
the generalized trace and the norm on Milnor K-groups.
In the second part of this paper, we will apply the results obtained here to
give explicit formulas for the generalized Hilbert symbol and the Kummer
pairing associated to a Lubin-Tate formal group. The results obtained in the
second paper constitute a generalization to higher local fields, of the
formulas of Artin-Hasse, K. Iwasawa and A. Wiles.
",0,0,1,0,0,0
19731,19732,Localic completion of uniform spaces,"  We extend the notion of localic completion of generalised metric spaces by
Steven Vickers to the setting of generalised uniform spaces. A generalised
uniform space (gus) is a set X equipped with a family of generalised metrics on
X, where a generalised metric on X is a map from the product of X to the upper
reals satisfying zero self-distance law and triangle inequality.
For a symmetric generalised uniform space, the localic completion lifts its
generalised uniform structure to a point-free generalised uniform structure.
This point-free structure induces a complete generalised uniform structure on
the set of formal points of the localic completion that gives the standard
completion of the original gus with Cauchy filters.
We extend the localic completion to a full and faithful functor from the
category of locally compact uniform spaces into that of overt locally compact
completely regular formal topologies. Moreover, we give an elementary
characterisation of the cover of the localic completion of a locally compact
uniform space that simplifies the existing characterisation for metric spaces.
These results generalise the corresponding results for metric spaces by Erik
Palmgren.
Furthermore, we show that the localic completion of a symmetric gus is
equivalent to the point-free completion of the uniform formal topology
associated with the gus.
We work in Aczel's constructive set theory CZF with the Regular Extension
Axiom. Some of our results also require Countable Choice.
",1,0,1,0,0,0
16328,16329,Connectivity jamming game for physical layer attack in peer to peer networks,"  Because of the open access nature of wireless communications, wireless
networks can suffer from malicious activity, such as jamming attacks, aimed at
undermining the network's ability to sustain communication links and acceptable
throughput. One important consideration when designing networks is to
appropriately tune the network topology and its connectivity so as to support
the communication needs of those participating in the network. This paper
examines the problem of interference attacks that are intended to harm
connectivity and throughput, and illustrates the method of mapping network
performance parameters into the metric of topographic connectivity.
Specifically, this paper arrives at anti-jamming strategies aimed at coping
with interference attacks through a unified stochastic game. In such a
framework, an entity trying to protect a network faces a dilemma: (i) the
underlying motivations for the adversary can be quite varied, which depends
largely on the network's characteristics such as power and distance; (ii) the
metrics for such an attack can be incomparable (e.g., network connectivity and
total throughput). To deal with the problem of such incomparable metrics, this
paper proposes using the attack's expected duration as a unifying metric to
compare distinct attack metrics because a longer-duration of unsuccessful
attack assumes a higher cost. Based on this common metric, a mechanism of
maxmin selection for an attack prevention strategy is suggested.
",1,0,0,0,0,0
18023,18024,Online Learning to Rank in Stochastic Click Models,"  Online learning to rank is a core problem in information retrieval and
machine learning. Many provably efficient algorithms have been recently
proposed for this problem in specific click models. The click model is a model
of how the user interacts with a list of documents. Though these results are
significant, their impact on practice is limited, because all proposed
algorithms are designed for specific click models and lack convergence
guarantees in other models. In this work, we propose BatchRank, the first
online learning to rank algorithm for a broad class of click models. The class
encompasses two most fundamental click models, the cascade and position-based
models. We derive a gap-dependent upper bound on the $T$-step regret of
BatchRank and evaluate it on a range of web search queries. We observe that
BatchRank outperforms ranked bandits and is more robust than CascadeKL-UCB, an
existing algorithm for the cascade model.
",1,0,0,1,0,0
19334,19335,Learning at the Ends: From Hand to Tool Affordances in Humanoid Robots,"  One of the open challenges in designing robots that operate successfully in
the unpredictable human environment is how to make them able to predict what
actions they can perform on objects, and what their effects will be, i.e., the
ability to perceive object affordances. Since modeling all the possible world
interactions is unfeasible, learning from experience is required, posing the
challenge of collecting a large amount of experiences (i.e., training data).
Typically, a manipulative robot operates on external objects by using its own
hands (or similar end-effectors), but in some cases the use of tools may be
desirable, nevertheless, it is reasonable to assume that while a robot can
collect many sensorimotor experiences using its own hands, this cannot happen
for all possible human-made tools.
Therefore, in this paper we investigate the developmental transition from
hand to tool affordances: what sensorimotor skills that a robot has acquired
with its bare hands can be employed for tool use? By employing a visual and
motor imagination mechanism to represent different hand postures compactly, we
propose a probabilistic model to learn hand affordances, and we show how this
model can generalize to estimate the affordances of previously unseen tools,
ultimately supporting planning, decision-making and tool selection tasks in
humanoid robots. We present experimental results with the iCub humanoid robot,
and we publicly release the collected sensorimotor data in the form of a hand
posture affordances dataset.
",1,0,0,1,0,0
5466,5467,Two-term spectral asymptotics for the Dirichlet pseudo-relativistic kinetic energy operator on a bounded domain,"  Continuing the series of works following Weyl's one-term asymptotic formula
for the counting function $N(\lambda)=\sum_{n=1}^\infty(\lambda_n{-}\lambda)_-$
of the eigenvalues of the Dirichlet Laplacian and the much later found two-term
expansion on domains with highly regular boundary by Ivrii and Melrose, we
prove a two-term asymptotic expansion of the $N$-th Cesàro mean of the
eigenvalues of $\sqrt{-\Delta + m^2} - m$ for $m>0$ with Dirichlet boundary
condition on a bounded domain $\Omega\subset\mathbb R^d$ for $d\geq 2$,
extending a result by Frank and Geisinger for the fractional Laplacian ($m=0$)
and improving upon the small-time asymptotics of the heat trace $Z(t) =
\sum_{n=1}^\infty e^{-t \lambda_n}$ by Bañuelos et al. and Park and Song.
",0,0,1,0,0,0
19092,19093,Evidence for the formation of comet 67P/Churyumov-Gerasimenko through gravitational collapse of a bound clump of pebbles,"  The processes that led to the formation of the planetary bodies in the Solar
System are still not fully understood. Using the results obtained with the
comprehensive suite of instruments on-board ESA's Rosetta mission, we present
evidence that comet 67P/Churyumov-Gerasimenko likely formed through the gentle
gravitational collapse of a bound clump of mm-sized dust aggregates
(""pebbles""), intermixed with microscopic ice particles. This formation scenario
leads to a cometary make-up that is simultaneously compatible with the global
porosity, homogeneity, tensile strength, thermal inertia, vertical temperature
profiles, sizes and porosities of emitted dust, and the steep increase in
water-vapour production rate with decreasing heliocentric distance, measured by
the instruments on-board the Rosetta spacecraft and the Philae lander. Our
findings suggest that the pebbles observed to be abundant in protoplanetary
discs around young stars provide the building material for comets and other
minor bodies.
",0,1,0,0,0,0
20901,20902,Universal Constraints on the Location of Extrema of Eigenfunctions of Non-Local Schrödinger Operators,"  We derive a lower bound on the location of global extrema of eigenfunctions
for a large class of non-local Schrödinger operators in convex domains under
Dirichlet exterior conditions, featuring the symbol of the kinetic term, the
strength of the potential, and the corresponding eigenvalue, and involving a
new universal constant. We show a number of probabilistic and spectral
geometric implications, and derive a Faber-Krahn type inequality for non-local
operators. Our study also extends to potentials with compact support, and we
establish bounds on the location of extrema relative to the boundary edge of
the support or level sets around minima of the potential.
",0,0,1,0,0,0
6553,6554,Resonant Scattering Characteristics of Homogeneous Dielectric Sphere,"  In the present article the classical problem of electromagnetic scattering by
a single homogeneous sphere is revisited. Main focus is the study of the
scattering behavior as a function of the material contrast and the size
parameters for all electric and magnetic resonances of a dielectric sphere.
Specifically, the Padé approximants are introduced and utilized as an
alternative system expansion of the Mie coefficients. Low order Padé
approximants can give compact and physically insightful expressions for the
scattering system and the enabled dynamic mechanisms. Higher order approximants
are used for predicting accurately the resonant pole spectrum. These results
are summarized into general pole formulae, covering up to fifth order magnetic
and forth order electric resonances of a small dielectric sphere. Additionally,
the connection between the radiative damping process and the resonant linewidth
is investigated. The results obtained reveal the fundamental connection of the
radiative damping mechanism with the maximum width occurring for each
resonance. Finally, the suggested system ansatz is used for studying the
resonant absorption maximum through a circuit-inspired perspective.
",0,1,0,0,0,0
2040,2041,Towards Noncommutative Topological Quantum Field Theory: New invariants for 3-manifolds,"  We define some new invariants for 3-manifolds using the space of taut codim-1
foliations along with various techniques from noncommutative geometry. These
invariants originate from our attempt to generalise Topological Quantum Field
Theories in the Noncommutative geometry / topology realm.
",0,0,1,0,0,0
10004,10005,Localization of hidden Chua attractors by the describing function method,"  In this paper the Chua circuit with five linear elements and saturation
non-linearity is studied. Numerical localization of self-excited attractor in
the Chua circuit model can be done by computation of trajectory with initial
data in a vicinity of an unstable equilibrium. For a hidden attractor its basin
of attraction does not overlap with a small vicinity of equilibria, so it is
difficult to find the corresponding initial data for localization. This survey
is devoted to the application of describing function method for localization of
hidden periodic and chaotic attractors in the Chua model. We use a rigorous
justification of the describing function method, based on the method of small
parameter, to get the initial data for the visualization of the hidden
attractors. A new configuration of hidden Chua attractors is presented.
",0,1,0,0,0,0
2553,2554,Future Energy Consumption Prediction Based on Grey Forecast Model,"  We use grey forecast model to predict the future energy consumption of four
states in the U.S, and make some improvments to the model.
",0,0,0,1,0,0
17891,17892,Active Learning for Regression Using Greedy Sampling,"  Regression problems are pervasive in real-world applications. Generally a
substantial amount of labeled samples are needed to build a regression model
with good generalization ability. However, many times it is relatively easy to
collect a large number of unlabeled samples, but time-consuming or expensive to
label them. Active learning for regression (ALR) is a methodology to reduce the
number of labeled samples, by selecting the most beneficial ones to label,
instead of random selection. This paper proposes two new ALR approaches based
on greedy sampling (GS). The first approach (GSy) selects new samples to
increase the diversity in the output space, and the second (iGS) selects new
samples to increase the diversity in both input and output spaces. Extensive
experiments on 12 UCI and CMU StatLib datasets from various domains, and on 15
subjects on EEG-based driver drowsiness estimation, verified their
effectiveness and robustness.
",0,0,0,1,0,0
14016,14017,"Comment on ""Spin-Orbit Coupling Induced Gap in Graphene on Pt(111) with Intercalated Pb Monolayer""","  Recently a paper of Klimovskikh et al. was published presenting experimental
and theoretical analysis of the graphene/Pb/Pt(111) system. The authors
investigate the crystallographic and electronic structure of this
graphene-based system by means of LEED, ARPES, and spin-resolved PES of the
graphene $\pi$ states in the vicinity of the Dirac point of graphene. The
authors of this paper demonstrate that an energy gap of approx. 200 meV is
opened in the spectral function of graphene directly at the Dirac point of
graphene and spin-splitting of 100 meV is detected for the upper part of the
Dirac cone. On the basis of the spin-resolved photoelectron spectroscopy
measurements of the region around the gap the authors claim that these
splittings are of a spin-orbit nature and that the observed spin structure
confirms the observation of the quantum spin Hall state in graphene, proposed
in earlier theoretical works. Here we will show that careful systematic
analysis of the experimental data presented in this manuscript is needed and
their interpretation require more critical consideration for making such
conclusions. Our analysis demonstrates that the proposed effects and
interpretations are questionable and require further more careful experiments.
",0,1,0,0,0,0
8904,8905,Half-quadratic transportation problems,"  We present a primal--dual memory efficient algorithm for solving a relaxed
version of the general transportation problem. Our approach approximates the
original cost function with a differentiable one that is solved as a sequence
of weighted quadratic transportation problems. The new formulation allows us to
solve differentiable, non-- convex transportation problems.
",1,0,1,0,0,0
17700,17701,Direct frequency-comb spectroscopy of $6S_{1/2}$-$8S_{1/2}$ transitions of atomic cesium,"  Direct frequency-comb spectroscopy is used to probe the absolute frequencies
of $6S_{1/2}$-$8S_{1/2}$ two-photon transitions of atomic cesium in hot vapor
environment. By utilizing the coherent control method of temporally splitting
the laser spectrum above and below the two-photon resonance frequency,
Doppler-free absorption is built in two spatially distinct locations and imaged
for high-precision spectroscopy. Theoretical analysis finds that these
transition lines are measured with uncertainty below $5\times10^{-10}$, mainly
contributed from laser-induced AC Stark shift.
",0,1,0,0,0,0
6130,6131,A Proof of the Conjecture of Lehmer and of the Conjecture of Schinzel-Zassenhaus,"  The conjecture of Lehmer is proved to be true. The proof mainly relies upon:
(i) the properties of the Parry Upper functions $f_{house(\alpha)}(z)$
associated with the dynamical zeta functions $\zeta_{house(\alpha)}(z)$ of the
Rényi--Parry arithmetical dynamical systems, for $\alpha$ an algebraic
integer $\alpha$ of house ""$house(\alpha)$"" greater than 1, (ii) the discovery
of lenticuli of poles of $\zeta_{house(\alpha)}(z)$ which uniformly
equidistribute at the limit on a limit ""lenticular"" arc of the unit circle,
when $house(\alpha)$ tends to $1^+$, giving rise to a continuous lenticular
minorant ${\rm M}_{r}(house(\alpha))$ of the Mahler measure ${\rm M}(\alpha)$,
(iii) the Poincaré asymptotic expansions of these poles and of this minorant
${\rm M}_{r}(house(\alpha))$ as a function of the dynamical degree. With the
same arguments the conjecture of Schinzel-Zassenhaus is proved to be true. An
inequality improving those of Dobrowolski and Voutier ones is obtained. The set
of Salem numbers is shown to be bounded from below by the Perron number
$\theta_{31}^{-1} = 1.08545\ldots$, dominant root of the trinomial $-1 - z^{30}
+ z^{31}$. Whether Lehmer's number is the smallest Salem number remains open. A
lower bound for the Weil height of nonzero totally real algebraic numbers,
$\neq \pm 1$, is obtained (Bogomolov property). For sequences of algebraic
integers of Mahler measure smaller than the smallest Pisot number, whose houses
have a dynamical degree tending to infinity, the Galois orbit measures of
conjugates are proved to converge towards the Haar measure on $|z|=1$ (limit
equidistribution).
",0,0,1,0,0,0
11299,11300,Mosquito detection with low-cost smartphones: data acquisition for malaria research,"  Mosquitoes are a major vector for malaria, causing hundreds of thousands of
deaths in the developing world each year. Not only is the prevention of
mosquito bites of paramount importance to the reduction of malaria transmission
cases, but understanding in more forensic detail the interplay between malaria,
mosquito vectors, vegetation, standing water and human populations is crucial
to the deployment of more effective interventions. Typically the presence and
detection of malaria-vectoring mosquitoes is only quantified by hand-operated
insect traps or signified by the diagnosis of malaria. If we are to gather
timely, large-scale data to improve this situation, we need to automate the
process of mosquito detection and classification as much as possible. In this
paper, we present a candidate mobile sensing system that acts as both a
portable early warning device and an automatic acoustic data acquisition
pipeline to help fuel scientific inquiry and policy. The machine learning
algorithm that powers the mobile system achieves excellent off-line
multi-species detection performance while remaining computationally efficient.
Further, we have conducted preliminary live mosquito detection tests using
low-cost mobile phones and achieved promising results. The deployment of this
system for field usage in Southeast Asia and Africa is planned in the near
future. In order to accelerate processing of field recordings and labelling of
collected data, we employ a citizen science platform in conjunction with
automated methods, the former implemented using the Zooniverse platform,
allowing crowdsourcing on a grand scale.
",1,0,0,1,0,0
1729,1730,A Correspondence Between Random Neural Networks and Statistical Field Theory,"  A number of recent papers have provided evidence that practical design
questions about neural networks may be tackled theoretically by studying the
behavior of random networks. However, until now the tools available for
analyzing random neural networks have been relatively ad-hoc. In this work, we
show that the distribution of pre-activations in random neural networks can be
exactly mapped onto lattice models in statistical physics. We argue that
several previous investigations of stochastic networks actually studied a
particular factorial approximation to the full lattice model. For random linear
networks and random rectified linear networks we show that the corresponding
lattice models in the wide network limit may be systematically approximated by
a Gaussian distribution with covariance between the layers of the network. In
each case, the approximate distribution can be diagonalized by Fourier
transformation. We show that this approximation accurately describes the
results of numerical simulations of wide random neural networks. Finally, we
demonstrate that in each case the large scale behavior of the random networks
can be approximated by an effective field theory.
",1,1,0,1,0,0
18043,18044,The effect of the choice of neural network depth and breadth on the size of its hypothesis space,"  We show that the number of unique function mappings in a neural network
hypothesis space is inversely proportional to $\prod_lU_l!$, where $U_{l}$ is
the number of neurons in the hidden layer $l$.
",0,0,0,1,0,0
2616,2617,Global existence in the 1D quasilinear parabolic-elliptic chemotaxis system with critical nonlinearity,"  The paper should be viewed as complement of an earlier result in [8]. In the
paper just mentioned it is shown that 1d case of a quasilinear
parabolic-elliptic Keller-Segel system is very special. Namely, unlike in
higher dimensions, there is no critical nonlinearity. Indeed, for the nonlinear
diffusion of the form 1/u all the solutions, independently on the magnitude of
initial mass, stay bounded. However, the argument presented in [8] deals with
the Jager-Luckhaus type system. And is very sensitive to this restriction.
Namely, the change of variables introduced in [8], being a main step of the
method, works only for the Jager-Luckhaus modification. It does not seem to be
applicable in the usual version of the parabolic-elliptic Keller-Segel system.
The present paper fulfils this gap and deals with the case of the usual
parabolic-elliptic version. To handle it we establish a new Lyapunov-like
functional (it is related to what was done in [8]), which leads to global
existence of the initial-boundary value problem for any initial mass.
",0,0,1,0,0,0
5128,5129,Robust estimation of tree structured Gaussian Graphical Model,"  Consider jointly Gaussian random variables whose conditional independence
structure is specified by a graphical model. If we observe realizations of the
variables, we can compute the covariance matrix, and it is well known that the
support of the inverse covariance matrix corresponds to the edges of the
graphical model. Instead, suppose we only have noisy observations. If the noise
at each node is independent, we can compute the sum of the covariance matrix
and an unknown diagonal. The inverse of this sum is (in general) dense. We ask:
can the original independence structure be recovered? We address this question
for tree structured graphical models. We prove that this problem is
unidentifiable, but show that this unidentifiability is limited to a small
class of candidate trees. We further present additional constraints under which
the problem is identifiable. Finally, we provide an O(n^3) algorithm to find
this equivalence class of trees.
",1,0,0,1,0,0
18359,18360,Learning to Avoid Errors in GANs by Manipulating Input Spaces,"  Despite recent advances, large scale visual artifacts are still a common
occurrence in images generated by GANs. Previous work has focused on improving
the generator's capability to accurately imitate the data distribution
$p_{data}$. In this paper, we instead explore methods that enable GANs to
actively avoid errors by manipulating the input space. The core idea is to
apply small changes to each noise vector in order to shift them away from areas
in the input space that tend to result in errors. We derive three different
architectures from that idea. The main one of these consists of a simple
residual module that leads to significantly less visual artifacts, while only
slightly decreasing diversity. The module is trivial to add to existing GANs
and costs almost zero computation and memory.
",1,0,0,1,0,0
14871,14872,Vector bundles over classifying spaces of p-local finite groups and Benson-Carlson duality,"  In this paper we obtain a description of the Grothendieck group of complex
vector bundles over the classifying space of a p-local finite group in terms of
representation rings of subgroups of its Sylow. We also prove a stable elements
formula for generalized cohomological invariants of p-local finite groups,
which is used to show the existence of unitary embeddings of p-local finite
groups. Finally, we show that the augmentation map for the cochains of the
classifying space of a p-local finite group is Gorenstein in the sense of
Dwyer-Greenlees-Iyengar and obtain some consequences about the cohomology ring
of these classifying spaces.
",0,0,1,0,0,0
6102,6103,Defect entropies and enthalpies in Barium Fluoride,"  Various experimental techniques, have revealed that the predominant intrinsic
point defects in BaF$_2$ are anion Frenkel defects. Their formation enthalpy
and entropy as well as the corresponding parameters for the fluorine vacancy
and fluorine interstitial motion have been determined. In addition, low
temperature dielectric relaxation measurements in BaF$_2$ doped with uranium
leads to the parameters {\tau}$_0$, E in the Arrhenius relation
{\tau}={\tau}$_0$exp(E/kBT) for the relaxation time {\tau}. For the relaxation
peak associated with a single tetravalent uranium, the migration entropy
deduced from the pre-exponential factor {\tau}$_0$, is smaller than the anion
Frenkel defect formation entropy by almost two orders of magnitude. We show
that, despite their great variation, the defect entropies and enthalpies are
interconnected through a model based on anharmonic properties of the bulk
material that have been recently studied by employing density-functional theory
and density-functional perturbation theory.
",0,1,0,0,0,0
9868,9869,Activation Ensembles for Deep Neural Networks,"  Many activation functions have been proposed in the past, but selecting an
adequate one requires trial and error. We propose a new methodology of
designing activation functions within a neural network at each layer. We call
this technique an ""activation ensemble"" because it allows the use of multiple
activation functions at each layer. This is done by introducing additional
variables, $\alpha$, at each activation layer of a network to allow for
multiple activation functions to be active at each neuron. By design,
activations with larger $\alpha$ values at a neuron is equivalent to having the
largest magnitude. Hence, those higher magnitude activations are ""chosen"" by
the network. We implement the activation ensembles on a variety of datasets
using an array of Feed Forward and Convolutional Neural Networks. By using the
activation ensemble, we achieve superior results compared to traditional
techniques. In addition, because of the flexibility of this methodology, we
more deeply explore activation functions and the features that they capture.
",0,0,0,1,0,0
20890,20891,Predicting Native Language from Gaze,"  A fundamental question in language learning concerns the role of a speaker's
first language in second language acquisition. We present a novel methodology
for studying this question: analysis of eye-movement patterns in second
language reading of free-form text. Using this methodology, we demonstrate for
the first time that the native language of English learners can be predicted
from their gaze fixations when reading English. We provide analysis of
classifier uncertainty and learned features, which indicates that differences
in English reading are likely to be rooted in linguistic divergences across
native languages. The presented framework complements production studies and
offers new ground for advancing research on multilingualism.
",1,0,0,0,0,0
17498,17499,Enabling large-scale viscoelastic calculations via neural network acceleration,"  One of the most significant challenges involved in efforts to understand the
effects of repeated earthquake cycle activity are the computational costs of
large-scale viscoelastic earthquake cycle models. Computationally intensive
viscoelastic codes must be evaluated thousands of times and locations, and as a
result, studies tend to adopt a few fixed rheological structures and model
geometries, and examine the predicted time-dependent deformation over short
(<10 yr) time periods at a given depth after a large earthquake. Training a
deep neural network to learn a computationally efficient representation of
viscoelastic solutions, at any time, location, and for a large range of
rheological structures, allows these calculations to be done quickly and
reliably, with high spatial and temporal resolution. We demonstrate that this
machine learning approach accelerates viscoelastic calculations by more than
50,000%. This magnitude of acceleration will enable the modeling of
geometrically complex faults over thousands of earthquake cycles across wider
ranges of model parameters and at larger spatial and temporal scales than have
been previously possible.
",0,1,0,0,0,0
6814,6815,Towards quantitative methods to assess network generative models,"  Assessing generative models is not an easy task. Generative models should
synthesize graphs which are not replicates of real networks but show
topological features similar to real graphs. We introduce an approach for
assessing graph generative models using graph classifiers. The inability of an
established graph classifier for distinguishing real and synthesized graphs
could be considered as a performance measurement for graph generators.
",1,0,0,0,0,0
2273,2274,ParaGraphE: A Library for Parallel Knowledge Graph Embedding,"  Knowledge graph embedding aims at translating the knowledge graph into
numerical representations by transforming the entities and relations into
continuous low-dimensional vectors. Recently, many methods [1, 5, 3, 2, 6] have
been proposed to deal with this problem, but existing single-thread
implementations of them are time-consuming for large-scale knowledge graphs.
Here, we design a unified parallel framework to parallelize these methods,
which achieves a significant time reduction without influencing the accuracy.
We name our framework as ParaGraphE, which provides a library for parallel
knowledge graph embedding. The source code can be downloaded from
this https URL .
",1,0,0,0,0,0
1238,1239,Recognizing Objects In-the-wild: Where Do We Stand?,"  The ability to recognize objects is an essential skill for a robotic system
acting in human-populated environments. Despite decades of effort from the
robotic and vision research communities, robots are still missing good visual
perceptual systems, preventing the use of autonomous agents for real-world
applications. The progress is slowed down by the lack of a testbed able to
accurately represent the world perceived by the robot in-the-wild. In order to
fill this gap, we introduce a large-scale, multi-view object dataset collected
with an RGB-D camera mounted on a mobile robot. The dataset embeds the
challenges faced by a robot in a real-life application and provides a useful
tool for validating object recognition algorithms. Besides describing the
characteristics of the dataset, the paper evaluates the performance of a
collection of well-established deep convolutional networks on the new dataset
and analyzes the transferability of deep representations from Web images to
robotic data. Despite the promising results obtained with such representations,
the experiments demonstrate that object classification with real-life robotic
data is far from being solved. Finally, we provide a comparative study to
analyze and highlight the open challenges in robot vision, explaining the
discrepancies in the performance.
",1,0,0,0,0,0
8777,8778,Geometry-Oblivious FMM for Compressing Dense SPD Matrices,"  We present GOFMM (geometry-oblivious FMM), a novel method that creates a
hierarchical low-rank approximation, ""compression,"" of an arbitrary dense
symmetric positive definite (SPD) matrix. For many applications, GOFMM enables
an approximate matrix-vector multiplication in $N \log N$ or even $N$ time,
where $N$ is the matrix size. Compression requires $N \log N$ storage and work.
In general, our scheme belongs to the family of hierarchical matrix
approximation methods. In particular, it generalizes the fast multipole method
(FMM) to a purely algebraic setting by only requiring the ability to sample
matrix entries. Neither geometric information (i.e., point coordinates) nor
knowledge of how the matrix entries have been generated is required, thus the
term ""geometry-oblivious."" Also, we introduce a shared-memory parallel scheme
for hierarchical matrix computations that reduces synchronization barriers. We
present results on the Intel Knights Landing and Haswell architectures, and on
the NVIDIA Pascal architecture for a variety of matrices.
",1,0,0,0,0,0
15205,15206,Markov-Modulated Linear Regression,"  Classical linear regression is considered for a case when regression
parameters depend on the external random environment. The last is described as
a continuous time Markov chain with finite state space. Here the expected
sojourn times in various states are additional regressors. Necessary formulas
for an estimation of regression parameters have been derived. The numerical
example illustrates the results obtained.
",0,0,0,1,0,0
6595,6596,Twistor theory at fifty: from contour integrals to twistor strings,"  We review aspects of twistor theory, its aims and achievements spanning
thelast five decades. In the twistor approach, space--time is secondary with
events being derived objects that correspond to compact holomorphic curves in a
complex three--fold -- the twistor space. After giving an elementary
construction of this space we demonstrate how solutions to linear and nonlinear
equations of mathematical physics: anti-self-duality (ASD) equations on
Yang--Mills, or conformal curvature can be encoded into twistor cohomology.
These twistor correspondences yield explicit examples of Yang--Mills, and
gravitational instantons which we review. They also underlie the twistor
approach to integrability: the solitonic systems arise as symmetry reductions
of ASD Yang--Mills equations, and Einstein--Weyl dispersionless systems are
reductions of ASD conformal equations.
We then review the holomorphic string theories in twistor and ambitwistor
spaces, and explain how these theories give rise to remarkable new formulae for
the computation of quantum scattering amplitudes. Finally we discuss the
Newtonian limit of twistor theory, and its possible role in Penrose's proposal
for a role of gravity in quantum collapse of a wave function.
",0,1,1,0,0,0
15478,15479,Counting $G$-Extensions by Discriminant,"  The problem of analyzing the number of number field extensions $L/K$ with
bounded (relative) discriminant has been the subject of renewed interest in
recent years, with significant advances made by Schmidt, Ellenberg-Venkatesh,
Bhargava, Bhargava-Shankar-Wang, and others. In this paper, we use the geometry
of numbers and invariant theory of finite groups, in a manner similar to
Ellenberg and Venkatesh, to give an upper bound on the number of extensions
$L/K$ with fixed degree, bounded relative discriminant, and specified Galois
closure.
",0,0,1,0,0,0
18729,18730,Half-Duplex Base Station with Adaptive Scheduling of the in-Band Uplink-Receptions and Downlink-Transmissions,"  In this paper, we propose a novel reception/transmission scheme for
half-duplex base stations (BSs). In particular, we propose a half-duplex BS
that employes in-band uplink-receptions from user 1 and downlink-transmissions
to user 2, which occur in different time slots. Furthermore, we propose optimal
adaptive scheduling of the in-band uplink-receptions and downlink-transmissions
of the BS such that the uplink-downlink rate/throughput region is maximized and
the outage probabilities of the uplink and downlink channels are minimized.
Practically, this results in selecting whether in a given time slot the BS
should receive from user 1 or transmit to user 2 based on the qualities of the
in-band uplink-reception and downlink-transmission channels. Compared to the
performance achieved with a conventional full-duplex division (FDD) base
station, two main gains can be highlighted: 1) Increased uplink-downlink
rate/throughput region; 2) Doubling of the diversity gain of both the uplink
and downlink channels.
",1,0,0,0,0,0
207,208,Weak Form of Stokes-Dirac Structures and Geometric Discretization of Port-Hamiltonian Systems,"  We present the mixed Galerkin discretization of distributed parameter
port-Hamiltonian systems. On the prototypical example of hyperbolic systems of
two conservation laws in arbitrary spatial dimension, we derive the main
contributions: (i) A weak formulation of the underlying geometric
(Stokes-Dirac) structure with a segmented boundary according to the causality
of the boundary ports. (ii) The geometric approximation of the Stokes-Dirac
structure by a finite-dimensional Dirac structure is realized using a mixed
Galerkin approach and power-preserving linear maps, which define minimal
discrete power variables. (iii) With a consistent approximation of the
Hamiltonian, we obtain finite-dimensional port-Hamiltonian state space models.
By the degrees of freedom in the power-preserving maps, the resulting family of
structure-preserving schemes allows for trade-offs between centered
approximations and upwinding. We illustrate the method on the example of
Whitney finite elements on a 2D simplicial triangulation and compare the
eigenvalue approximation in 1D with a related approach.
",1,0,0,0,0,0
7601,7602,Bayesian Coreset Construction via Greedy Iterative Geodesic Ascent,"  Coherent uncertainty quantification is a key strength of Bayesian methods.
But modern algorithms for approximate Bayesian posterior inference often
sacrifice accurate posterior uncertainty estimation in the pursuit of
scalability. This work shows that previous Bayesian coreset construction
algorithms---which build a small, weighted subset of the data that approximates
the full dataset---are no exception. We demonstrate that these algorithms scale
the coreset log-likelihood suboptimally, resulting in underestimated posterior
uncertainty. To address this shortcoming, we develop greedy iterative geodesic
ascent (GIGA), a novel algorithm for Bayesian coreset construction that scales
the coreset log-likelihood optimally. GIGA provides geometric decay in
posterior approximation error as a function of coreset size, and maintains the
fast running time of its predecessors. The paper concludes with validation of
GIGA on both synthetic and real datasets, demonstrating that it reduces
posterior approximation error by orders of magnitude compared with previous
coreset constructions.
",0,0,0,1,0,0
18904,18905,"Inter-Operator Resource Management for Millimeter Wave, Multi-Hop Backhaul Networks","  In this paper, a novel framework is proposed for optimizing the operation and
performance of a large-scale, multi-hop millimeter wave (mmW) backhaul within a
wireless small cell network (SCN) that encompasses multiple mobile network
operators (MNOs). The proposed framework enables the small base stations (SBSs)
to jointly decide on forming the multi-hop, mmW links over backhaul
infrastructure that belongs to multiple, independent MNOs, while properly
allocating resources across those links. In this regard, the problem is
addressed using a novel framework based on matching theory that is composed to
two, highly inter-related stages: a multi-hop network formation stage and a
resource management stage. One unique feature of this framework is that it
jointly accounts for both wireless channel characteristics and economic factors
during both network formation and resource management. The multi-hop network
formation stage is formulated as a one-to-many matching game which is solved
using a novel algorithm, that builds on the so-called deferred acceptance
algorithm and is shown to yield a stable and Pareto optimal multi-hop mmW
backhaul network. Then, a one-to-many matching game is formulated to enable
proper resource allocation across the formed multi-hop network. This game is
then shown to exhibit peer effects and, as such, a novel algorithm is developed
to find a stable and optimal resource management solution that can properly
cope with these peer effects. Simulation results show that the proposed
framework yields substantial gains, in terms of the average sum rate, reaching
up to 27% and 54%, respectively, compared to a non-cooperative scheme in which
inter-operator sharing is not allowed and a random allocation approach. The
results also show that our framework provides insights on how to manage pricing
and the cost of the cooperative mmW backhaul network for the MNOs.
",1,0,0,0,0,0
8470,8471,Tracking Emerges by Colorizing Videos,"  We use large amounts of unlabeled video to learn models for visual tracking
without manual human supervision. We leverage the natural temporal coherency of
color to create a model that learns to colorize gray-scale videos by copying
colors from a reference frame. Quantitative and qualitative experiments suggest
that this task causes the model to automatically learn to track visual regions.
Although the model is trained without any ground-truth labels, our method
learns to track well enough to outperform the latest methods based on optical
flow. Moreover, our results suggest that failures to track are correlated with
failures to colorize, indicating that advancing video colorization may further
improve self-supervised visual tracking.
",1,0,0,0,0,0
11896,11897,The Accuracy of Confidence Intervals for Field Normalised Indicators,"  When comparing the average citation impact of research groups, universities
and countries, field normalisation reduces the influence of discipline and
time. Confidence intervals for these indicators can help with attempts to infer
whether differences between sets of publications are due to chance factors.
Although both bootstrapping and formulae have been proposed for these, their
accuracy is unknown. In response, this article uses simulated data to
systematically compare the accuracy of confidence limits in the simplest
possible case, a single field and year. The results suggest that the MNLCS
(Mean Normalised Log-transformed Citation Score) confidence interval formula is
conservative for large groups but almost always safe, whereas bootstrap MNLCS
confidence intervals tend to be accurate but can be unsafe for smaller world or
group sample sizes. In contrast, bootstrap MNCS (Mean Normalised Citation
Score) confidence intervals can be very unsafe, although their accuracy
increases with sample sizes.
",1,0,0,0,0,0
5361,5362,Instantons for 4-manifolds with periodic ends and an obstruction to embeddings of 3-manifolds,"  We construct an obstruction for the existence of embeddings of homology
$3$-sphere into homology $S^3\times S^1$ under some cohomological condition.
The obstruction is defined as an element in the filtered version of the
instanton Floer cohomology due to R.Fintushel-R.Stern. We make use of the
$\mathbb{Z}$-fold covering space of homology $S^3\times S^1$ and the instantons
on it.
",0,0,1,0,0,0
5738,5739,A recommender system to restore images with impulse noise,"  We build a collaborative filtering recommender system to restore images with
impulse noise for which the noisy pixels have been previously identified. We
define this recommender system in terms of a new color image representation
using three matrices that depend on the noise-free pixels of the image to
restore, and two parameters: $k$, the number of features; and $\lambda$, the
regularization factor. We perform experiments on a well known image database to
test our algorithm and we provide image quality statistics for the results
obtained. We discuss the roles of bias and variance in the performance of our
algorithm as determined by the values of $k$ and $\lambda$, and provide
guidance on how to choose the values of these parameters. Finally, we discuss
the possibility of using our collaborative filtering recommender system to
perform image inpainting and super-resolution.
",1,0,0,1,0,0
6623,6624,Geometrical morphology,"  We explore inflectional morphology as an example of the relationship of the
discrete and the continuous in linguistics. The grammar requests a form of a
lexeme by specifying a set of feature values, which corresponds to a corner M
of a hypercube in feature value space. The morphology responds to that request
by providing a morpheme, or a set of morphemes, whose vector sum is
geometrically closest to the corner M. In short, the chosen morpheme $\mu$ is
the morpheme (or set of morphemes) that maximizes the inner product of $\mu$
and M.
",1,0,0,0,0,0
49,50,Large Scale Automated Forecasting for Monitoring Network Safety and Security,"  Real time large scale streaming data pose major challenges to forecasting, in
particular defying the presence of human experts to perform the corresponding
analysis. We present here a class of models and methods used to develop an
automated, scalable and versatile system for large scale forecasting oriented
towards safety and security monitoring. Our system provides short and long term
forecasts and uses them to detect safety and security issues in relation with
multiple internet connected devices well in advance they might take place.
",0,0,0,1,0,0
15527,15528,Tilings of the plane with unit area triangles of bounded diameter,"  There exist tilings of the plane with pairwise noncongruent triangles of
equal area and bounded perimeter. Analogously, there exist tilings with
triangles of equal perimeter, the areas of which are bounded from below by a
positive constant. This solves a problem of Nandakumar.
",1,0,1,0,0,0
19185,19186,"Atomic Data Revisions for Transitions Relevant to Observations of Interstellar, Circumgalactic, and Intergalactic Matter","  Measurements of element abundances in galaxies from astrophysical
spectroscopy depend sensitively on the atomic data used. With the goal of
making the latest atomic data accessible to the community, we present a
compilation of selected atomic data for resonant absorption lines at
wavelengths longward of 911.753 {\AA} (the \ion{H}{1} Lyman limit), for key
heavy elements (heavier than atomic number 5) of astrophysical interest. In
particular, we focus on the transitions of those ions that have been observed
in the Milky Way interstellar medium (ISM), the circumgalactic medium (CGM) of
the Milky Way and/or other galaxies, and the intergalactic medium (IGM).
We provide wavelengths, oscillator strengths, associated accuracy grades, and
references to the oscillator strength determinations. We also attempt to
compare and assess the recent oscillator strength determinations. For about
22\% of the lines that have updated oscillator strength values, the differences
between the former values and the updated ones are $\gtrsim$~0.1 dex.
Our compilation will be a useful resource for absorption line studies of the
ISM, as well as studies of the CGM and IGM traced by sight lines to quasars and
gamma-ray bursts. Studies (including those enabled by future generations of
extremely large telescopes) of absorption by galaxies against the light of
background galaxies will also benefit from our compilation.
",0,1,0,0,0,0
15658,15659,Concept Drift and Anomaly Detection in Graph Streams,"  Graph representations offer powerful and intuitive ways to describe data in a
multitude of application domains. Here, we consider stochastic processes
generating graphs and propose a methodology for detecting changes in
stationarity of such processes. The methodology is general and considers a
process generating attributed graphs with a variable number of vertices/edges,
without the need to assume one-to-one correspondence between vertices at
different time steps. The methodology acts by embedding every graph of the
stream into a vector domain, where a conventional multivariate change detection
procedure can be easily applied. We ground the soundness of our proposal by
proving several theoretical results. In addition, we provide a specific
implementation of the methodology and evaluate its effectiveness on several
detection problems involving attributed graphs representing biological
molecules and drawings. Experimental results are contrasted with respect to
suitable baseline methods, demonstrating the effectiveness of our approach.
",1,0,0,1,0,0
9291,9292,Learning to Embed Words in Context for Syntactic Tasks,"  We present models for embedding words in the context of surrounding words.
Such models, which we refer to as token embeddings, represent the
characteristics of a word that are specific to a given context, such as word
sense, syntactic category, and semantic role. We explore simple, efficient
token embedding models based on standard neural network architectures. We learn
token embeddings on a large amount of unannotated text and evaluate them as
features for part-of-speech taggers and dependency parsers trained on much
smaller amounts of annotated data. We find that predictors endowed with token
embeddings consistently outperform baseline predictors across a range of
context window and training set sizes.
",1,0,0,0,0,0
2984,2985,"Semi-parametric Dynamic Asymmetric Laplace Models for Tail Risk Forecasting, Incorporating Realized Measures","  The joint Value at Risk (VaR) and expected shortfall (ES) quantile regression
model of Taylor (2017) is extended via incorporating a realized measure, to
drive the tail risk dynamics, as a potentially more efficient driver than daily
returns. Both a maximum likelihood and an adaptive Bayesian Markov Chain Monte
Carlo method are employed for estimation, whose properties are assessed and
compared via a simulation study; results favour the Bayesian approach, which is
subsequently employed in a forecasting study of seven market indices and two
individual assets. The proposed models are compared to a range of parametric,
non-parametric and semi-parametric models, including GARCH, Realized-GARCH and
the joint VaR and ES quantile regression models in Taylor (2017). The
comparison is in terms of accuracy of one-day-ahead Value-at-Risk and Expected
Shortfall forecasts, over a long forecast sample period that includes the
global financial crisis in 2007-2008. The results favor the proposed models
incorporating a realized measure, especially when employing the sub-sampled
Realized Variance and the sub-sampled Realized Range.
",0,0,0,0,0,1
1739,1740,Dynamic Safe Interruptibility for Decentralized Multi-Agent Reinforcement Learning,"  In reinforcement learning, agents learn by performing actions and observing
their outcomes. Sometimes, it is desirable for a human operator to
\textit{interrupt} an agent in order to prevent dangerous situations from
happening. Yet, as part of their learning process, agents may link these
interruptions, that impact their reward, to specific states and deliberately
avoid them. The situation is particularly challenging in a multi-agent context
because agents might not only learn from their own past interruptions, but also
from those of other agents. Orseau and Armstrong defined \emph{safe
interruptibility} for one learner, but their work does not naturally extend to
multi-agent systems. This paper introduces \textit{dynamic safe
interruptibility}, an alternative definition more suited to decentralized
learning problems, and studies this notion in two learning frameworks:
\textit{joint action learners} and \textit{independent learners}. We give
realistic sufficient conditions on the learning algorithm to enable dynamic
safe interruptibility in the case of joint action learners, yet show that these
conditions are not sufficient for independent learners. We show however that if
agents can detect interruptions, it is possible to prune the observations to
ensure dynamic safe interruptibility even for independent learners.
",1,0,0,1,0,0
15526,15527,The photon identification loophole in EPRB experiments: computer models with single-wing selection,"  Recent Einstein-Podolsky-Rosen-Bohm experiments [M. Giustina et al. Phys.
Rev. Lett. 115, 250401 (2015); L. K. Shalm et al. Phys. Rev. Lett. 115, 250402
(2015)] that claim to be loophole free are scrutinized and are shown to suffer
a photon identification loophole. The combination of a digital computer and
discrete-event simulation is used to construct a minimal but faithful model of
the most perfected realization of these laboratory experiments. In contrast to
prior simulations, all photon selections are strictly made, as they are in the
actual experiments, at the local station and no other ""post-selection"" is
involved. The simulation results demonstrate that a manifestly non-quantum
model that identifies photons in the same local manner as in these experiments
can produce correlations that are in excellent agreement with those of the
quantum theoretical description of the corresponding thought experiment, in
conflict with Bell's theorem. The failure of Bell's theorem is possible because
of our recognition of the photon identification loophole. Such identification
measurement-procedures are necessarily included in all actual experiments but
are not included in the theory of Bell and his followers.
",0,1,0,0,0,0
13009,13010,Learning retrosynthetic planning through self-play,"  The problem of retrosynthetic planning can be framed as one player game, in
which the chemist (or a computer program) works backwards from a molecular
target to simpler starting materials though a series of choices regarding which
reactions to perform. This game is challenging as the combinatorial space of
possible choices is astronomical, and the value of each choice remains
uncertain until the synthesis plan is completed and its cost evaluated. Here,
we address this problem using deep reinforcement learning to identify policies
that make (near) optimal reaction choices during each step of retrosynthetic
planning. Using simulated experience or self-play, we train neural networks to
estimate the expected synthesis cost or value of any given molecule based on a
representation of its molecular structure. We show that learned policies based
on this value network outperform heuristic approaches in synthesizing
unfamiliar molecules from available starting materials using the fewest number
of reactions. We discuss how the learned policies described here can be
incorporated into existing synthesis planning tools and how they can be adapted
to changes in the synthesis cost objective or material availability.
",1,0,0,1,0,0
18448,18449,Discretization of SU(2) and the Orthogonal Group Using Icosahedral Symmetries and the Golden Numbers,"  The vertices of the four dimensional $120$-cell form a non-crystallographic
root system whose corresponding symmetry group is the Coxeter group $H_{4}$.
There are two special coordinate representations of this root system in which
they and their corresponding Coxeter groups involve only rational numbers and
the golden ratio $\tau$. The two are related by the conjugation $\tau
\mapsto\tau' = -1/\tau$. This paper investigates what happens when the two root
systems are combined and the group generated by both versions of $H_{4}$ is
allowed to operate on them. The result is a new, but infinite, `root system'
$\Sigma$ which itself turns out to have a natural structure of the unitary
group $SU(2,\mathcal R)$ over the ring $\mathcal R = \mathbb
Z[\frac{1}{2},\tau]$ (called here golden numbers). Acting upon it is the
naturally associated infinite reflection group $H^{\infty}$, which we prove is
of index $2$ in the orthogonal group $O(4,\mathcal R)$. The paper makes
extensive use of the quaternions over $\mathcal R$ and leads to highly
structured discretized filtration of $SU(2)$. We use this to offer a simple and
effective way to approximate any element of $SU(2)$ to any degree of accuracy
required using the repeated actions of just five fixed reflections, a process
that may find application in computational methods in quantum mechanics.
",0,0,1,0,0,0
12292,12293,Ermakov-Painlevé II Symmetry Reduction of a Korteweg Capillarity System,"  A class of nonlinear Schrödinger equations involving a triad of power law
terms together with a de Broglie-Bohm potential is shown to admit symmetry
reduction to a hybrid Ermakov-Painlevé II equation which is linked, in turn,
to the integrable Painlevé XXXIV equation. A nonlinear Schrödinger
encapsulation of a Korteweg-type capillary system is thereby used in the
isolation of such a Ermakov-Painlevé II reduction valid for a multi-parameter
class of free energy functions. Iterated application of a Bäcklund
transformation then allows the construction of novel classes of exact solutions
of the nonlinear capillarity system in terms of Yablonskii-Vorob'ev polynomials
or classical Airy functions. A Painlevé XXXIV equation is derived for the
density in the capillarity system and seen to correspond to the symmetry
reduction of its Bernoulli integral of motion.
",0,1,1,0,0,0
3882,3883,Generalized Sheet Transition Conditions (GSTCs) for a Metascreen -- A Fishnet Metasurface,"  We used a multiple-scale homogenization method to derive generalized sheet
transition conditions (GSTCs) for electromagnetic fields at the surface of a
metascreen---a metasurface with a ""fishnet"" structure. These surfaces are
characterized by periodically-spaced arbitrary-shaped apertures in an otherwise
relatively impenetrable surface. The parameters in these GSTCs are interpreted
as effective surface susceptibilities and surface porosities, which are related
to the geometry of the apertures that constitute the metascreen. Finally, we
emphasize the subtle but important difference between the GSTCs required for
metascreens and those required for metafilms (a metasurface with a ""cermet""
structure, i.e., an array of isolated (non-touching) scatterers).
",0,1,0,0,0,0
13151,13152,Graph Model Selection via Random Walks,"  In this paper, we present a novel approach based on the random walk process
for finding meaningful representations of a graph model. Our approach leverages
the transient behavior of many short random walks with novel initialization
mechanisms to generate model discriminative features. These features are able
to capture a more comprehensive structural signature of the underlying graph
model. The resulting representation is invariant to both node permutation and
the size of the graph, allowing direct comparison between large classes of
graphs. We test our approach on two challenging model selection problems: the
discrimination in the sparse regime of an Erdös-Renyi model from a
stochastic block model and the planted clique problem. Our representation
approach achieves performance that closely matches known theoretical limits in
addition to being computationally simple and scalable to large graphs.
",1,0,0,0,0,0
6487,6488,"Responses of Pre-transitional Materials with Stress-Generating Defects to External Stimuli: Superelasticity, Supermagnetostriction, Invar and Elinvar Effects","  We considered a generic case of pre-transitional materials with static
stress-generating defects, dislocations and coherent nano-precipitates, at
temperatures close but above the starting temperature of martensitic
transformation, Ms. Using the Phase Field Microelasticity theory and 3D
simulation, we demonstrated that the local stress generated by these defects
produces equilibrium nano-size martensitic embryos (MEs) in pre-transitional
state, these embryos being orientation variants of martensite. This is a new
type of equilibrium: the thermoelastic equilibrium between the MEs and parent
phase in which the total volume of MEs and their size are equilibrium internal
thermodynamic parameters. This thermoelastic equilibrium exists only in
presence of the stress-generating defects. Cooling the pre-transitional state
towards Ms or applying the external stimuli, stress or magnetic field, results
in a shift of the thermoelastic equilibrium provided by a reversible
anhysteretic growth of MEs that results in a giant ME-generated macroscopic
strain. In particular, this effect can be associated with the diffuse phase
transformations observed in some ferroelectrics above the Curie point. It is
shown that the ME-generated strain is giant and describes a superelasticity if
the applied field is stress. It describes a super magnetostriction if the
martensite (or austenite) are ferromagnetic and the applied field is a magnetic
field. In general, the material with defects can be a multiferroic with a giant
multiferroic response if the parent and martensitic phase have different
ferroic properties. Finally the ME-generated strain may explain or, at least,
contribute to the Invar and Elinvar effects that are typically observed in
pre-transitional austenite. The thermoelastic equilibrium and all these effects
exist only if the interaction between the defects and MEs is infinite-range.
",0,1,0,0,0,0
8317,8318,Deep generative models of genetic variation capture mutation effects,"  The functions of proteins and RNAs are determined by a myriad of interactions
between their constituent residues, but most quantitative models of how
molecular phenotype depends on genotype must approximate this by simple
additive effects. While recent models have relaxed this constraint to also
account for pairwise interactions, these approaches do not provide a tractable
path towards modeling higher-order dependencies. Here, we show how latent
variable models with nonlinear dependencies can be applied to capture
beyond-pairwise constraints in biomolecules. We present a new probabilistic
model for sequence families, DeepSequence, that can predict the effects of
mutations across a variety of deep mutational scanning experiments
significantly better than site independent or pairwise models that are based on
the same evolutionary data. The model, learned in an unsupervised manner solely
from sequence information, is grounded with biologically motivated priors,
reveals latent organization of sequence families, and can be used to
extrapolate to new parts of sequence space
",0,1,0,1,0,0
11880,11881,A Simple Solution for Maximum Range Flight,"  Within the standard framework of quasi-steady flight, this paper derives a
speed that realizes the maximal obtainable range per unit of fuel. If this
speed is chosen at each instant of a flight plan $h(x)$ giving altitude $h$ as
a function of distance $x$, a variational problem for finding an optimal $h(x)$
can be formulated and solved. It yields flight plans with maximal range, and
these turn out to consist of mainly three phases using the optimal speed:
starting with a climb at maximal continuous admissible thrust, ending with a
continuous descent at idle thrust, and in between with a transition based on a
solution of the Euler-Lagrange equation for the variational problem. A similar
variational problem is derived and solved for speed-restricted flights, e.g. at
250 KIAS below 10000 ft. In contrast to the literature, the approach of this
paper does not need more than standard ordinary differential equations solving
variational problems to derive range-optimal trajectories. Various numerical
examplesbased on a Standard Business Jet are added for illustration.
",0,0,1,0,0,0
19785,19786,Data Augmentation for Low-Resource Neural Machine Translation,"  The quality of a Neural Machine Translation system depends substantially on
the availability of sizable parallel corpora. For low-resource language pairs
this is not the case, resulting in poor translation quality. Inspired by work
in computer vision, we propose a novel data augmentation approach that targets
low-frequency words by generating new sentence pairs containing rare words in
new, synthetically created contexts. Experimental results on simulated
low-resource settings show that our method improves translation quality by up
to 2.9 BLEU points over the baseline and up to 3.2 BLEU over back-translation.
",1,0,0,0,0,0
8937,8938,Mechanism Deduction from Noisy Chemical Reaction Networks,"  We introduce KiNetX, a fully automated meta-algorithm for the kinetic
analysis of complex chemical reaction networks derived from semi-accurate but
efficient electronic structure calculations. It is designed to (i) accelerate
the automated exploration of such networks, and (ii) cope with model-inherent
errors in electronic structure calculations on elementary reaction steps. We
developed and implemented KiNetX to possess three features. First, KiNetX
evaluates the kinetic relevance of every species in a (yet incomplete) reaction
network to confine the search for new elementary reaction steps only to those
species that are considered possibly relevant. Second, KiNetX identifies and
eliminates all kinetically irrelevant species and elementary reactions to
reduce a complex network graph to a comprehensible mechanism. Third, KiNetX
estimates the sensitivity of species concentrations toward changes in
individual rate constants (derived from relative free energies), which allows
us to systematically select the most efficient electronic structure model for
each elementary reaction given a predefined accuracy. The novelty of KiNetX
consists in the rigorous propagation of correlated free-energy uncertainty
through all steps of our kinetic analyis. To examine the performance of KiNetX,
we developed AutoNetGen. It semirandomly generates chemistry-mimicking reaction
networks by encoding chemical logic into their underlying graph structure.
AutoNetGen allows us to consider a vast number of distinct chemistry-like
scenarios and, hence, to discuss assess the importance of rigorous uncertainty
propagation in a statistical context. Our results reveal that KiNetX reliably
supports the deduction of product ratios, dominant reaction pathways, and
possibly other network properties from semi-accurate electronic structure data.
",0,0,0,0,1,0
12051,12052,Hidden area and mechanical nonlinearities in freestanding graphene,"  We investigated the effect of out-of-plane crumpling on the mechanical
response of graphene membranes. In our experiments, stress was applied to
graphene membranes using pressurized gas while the strain state was monitored
through two complementary techniques: interferometric profilometry and Raman
spectroscopy. By comparing the data obtained through these two techniques, we
determined the geometric hidden area which quantifies the crumpling strength.
While the devices with hidden area $\sim0~\%$ obeyed linear mechanics with
biaxial stiffness $428\pm10$ N/m, specimens with hidden area in the range
$0.5-1.0~\%$ were found to obey an anomalous Hooke's law with an exponent
$\sim0.1$.
",0,1,0,0,0,0
4373,4374,Training of Deep Neural Networks based on Distance Measures using RMSProp,"  The vanishing gradient problem was a major obstacle for the success of deep
learning. In recent years it was gradually alleviated through multiple
different techniques. However the problem was not really overcome in a
fundamental way, since it is inherent to neural networks with activation
functions based on dot products. In a series of papers, we are going to analyze
alternative neural network structures which are not based on dot products. In
this first paper, we revisit neural networks built up of layers based on
distance measures and Gaussian activation functions. These kinds of networks
were only sparsely used in the past since they are hard to train when using
plain stochastic gradient descent methods. We show that by using Root Mean
Square Propagation (RMSProp) it is possible to efficiently learn multi-layer
neural networks. Furthermore we show that when appropriately initialized these
kinds of neural networks suffer much less from the vanishing and exploding
gradient problem than traditional neural networks even for deep networks.
",1,0,0,1,0,0
1903,1904,Fundamental solutions for Schrodinger operators with general inverse square potentials,"  In this paper, we classify the fundamental solutions for a class of
Schrodinger operators.
",0,0,1,0,0,0
10449,10450,On the reducibility of induced representations for classical p-adic groups and related affine Hecke algebras,"  Let $\pi $ be an irreducible smooth complex representation of a general
linear $p$-adic group and let $\sigma $ be an irreducible complex supercuspidal
representation of a classical $p$-adic group of a given type, so that
$\pi\otimes\sigma $ is a representation of a standard Levi subgroup of a
$p$-adic classical group of higher rank. We show that the reducibility of the
representation of the appropriate $p$-adic classical group obtained by
(normalized) parabolic induction from $\pi\otimes\sigma $ does not depend on
$\sigma $, if $\sigma $ is ""separated"" from the supercuspidal support of $\pi
$. (Here, ""separated"" means that, for each factor $\rho $ of a representation
in the supercuspidal support of $\pi $, the representation parabolically
induced from $\rho\otimes\sigma $ is irreducible.) This was conjectured by E.
Lapid and M. Tadić. (In addition, they proved, using results of C. Jantzen,
that this induced representation is always reducible if the supercuspidal
support is not separated.)
More generally, we study, for a given set $I$ of inertial orbits of
supercuspidal representations of $p$-adic general linear groups, the category
$\CC _{I,\sigma}$ of smooth complex finitely generated representations of
classical $p$-adic groups of fixed type, but arbitrary rank, and supercuspidal
support given by $\sigma $ and $I$, show that this category is equivalent to a
category of finitely generated right modules over a direct sum of tensor
products of extended affine Hecke algebras of type $A$, $B$ and $D$ and
establish functoriality properties, relating categories with disjoint $I$'s. In
this way, we extend results of C. Jantzen who proved a bijection between
irreducible representations corresponding to these categories. The proof of the
above reducibility result is then based on Hecke algebra arguments, using
Kato's exotic geometry.
",0,0,1,0,0,0
3734,3735,On the Complexity of the Weighted Fused Lasso,"  The solution path of the 1D fused lasso for an $n$-dimensional input is
piecewise linear with $\mathcal{O}(n)$ segments (Hoefling et al. 2010 and
Tibshirani et al 2011). However, existing proofs of this bound do not hold for
the weighted fused lasso. At the same time, results for the generalized lasso,
of which the weighted fused lasso is a special case, allow $\Omega(3^n)$
segments (Mairal et al. 2012). In this paper, we prove that the number of
segments in the solution path of the weighted fused lasso is
$\mathcal{O}(n^2)$, and that, for some instances, it is $\Omega(n^2)$. We also
give a new, very simple, proof of the $\mathcal{O}(n)$ bound for the fused
lasso.
",0,0,0,1,0,0
8789,8790,Proof of a conjecture of Abdollahi-Akbari-Maimani concerning the non-commutative graph of finite groups,"  The non--commuting graph $\Gamma(G)$ of a non--abelian group $G$ is defined
as follows. The vertex set $V(\Gamma(G))$ of $\Gamma(G)$ is $G\setminus Z(G)$
where $Z(G)$ denotes the center of $G$ and two vertices $x$ and $y$ are
adjacent if and only if $xy\neq yx$. For non--abelian finite groups $G$ and $H$
it is conjectured that if $\Gamma(G) \cong \Gamma(H)$, then $|G|=|H|$. We prove
the conjecture.
",0,0,1,0,0,0
13242,13243,Thermal conductivity changes across a structural phase transition: the case of high-pressure silica,"  By means of first-principles calculations, we investigate the thermal
properties of silica as it evolves, under hydrostatic compression, from a
stishovite phase into a CaCl$_2$-type structure. We compute the thermal
conductivity tensor by solving the linearized Boltzmann transport equation
iteratively in a wide temperature range, using for this the pressure-dependent
harmonic and anharmonic interatomic couplings obtained from first principles.
Most remarkably, we find that, at low temperatures, SiO$_2$ displays a large
peak in the in-plane thermal conductivity and a highly anisotropic behavior
close to the structural transformation. We trace back the origin of these
features by analyzing the phonon contributions to the conductivity. We discuss
the implications of our results in the general context of continuous structural
transformations in solids, as well as the potential geological interest of our
results for silica.
",0,1,0,0,0,0
9951,9952,Time evolution of the Luttinger model with nonuniform temperature profile,"  We study the time evolution of a one-dimensional interacting fermion system
described by the Luttinger model starting from a nonequilibrium state defined
by a smooth temperature profile $T(x)$. As a specific example we consider the
case when $T(x)$ is equal to $T_L$ ($T_R$) far to the left (right). Using a
series expansion in $\epsilon = 2(T_{R} - T_{L})/(T_{L}+T_{R})$, we compute the
energy density, the heat current density, and the fermion two-point correlation
function for all times $t \geq 0$. For local (delta-function) interactions, the
first two are computed to all orders, giving simple exact expressions involving
the Schwarzian derivative of the integral of $T(x)$. For nonlocal interactions,
breaking scale invariance, we compute the nonequilibrium steady state (NESS) to
all orders and the evolution to first order in $\epsilon$. The heat current in
the NESS is universal even when conformal invariance is broken by the
interactions, and its dependence on $T_{L,R}$ agrees with numerical results for
the $XXZ$ spin chain. Moreover, our analytical formulas predict peaks at short
times in the transition region between different temperatures and show
dispersion effects that, even if nonuniversal, are qualitatively similar to
ones observed in numerical simulations for related models, such as spin chains
and interacting lattice fermions.
",0,1,1,0,0,0
18691,18692,A simple mathematical model for unemployment: a case study in Portugal with optimal control,"  We propose a simple mathematical model for unemployment. Despite its
simpleness, we claim that the model is more realistic and useful than recent
models available in the literature. A case study with real data from Portugal
supports our claim. An optimal control problem is formulated and solved, which
provides some non-trivial and interesting conclusions.
",0,0,0,0,0,1
3641,3642,CMS-HF Calorimeter Upgrade for Run II,"  CMS-HF Calorimeters have been undergoing a major upgrade for the last couple
of years to alleviate the problems encountered during Run I, especially in the
PMT and the readout systems. In this poster, the problems caused by the old
PMTs installed in the detectors and their solutions will be explained.
Initially, regular PMTs with thicker windows, causing large Cherenkov
radiation, were used. Instead of the light coming through the fibers from the
detector, stray muons passing through the PMT itself produce Cherenkov
radiation in the PMT window, resulting in erroneously large signals. Usually,
large signals are the result of very high-energy particles in the calorimeter
and are tagged as important. As a result, these so-called window events
generate false triggers. Four-anode PMTs with thinner windows were selected to
reduce these window events. Additional channels also help eliminate such
remaining events through algorithms comparing the output of different PMT
channels. During the EYETS 16/17 period in the LHC operations, the final
components of the modifications to the readout system, namely the two-channel
front-end electronics cards, are installed. Complete upgrade of the HF
Calorimeter, including the preparations for the Run II will be discussed in
this poster, with possible effects on the eventual data taking.
",0,1,0,0,0,0
5775,5776,Directed negative-weight percolation,"  We consider a directed variant of the negative-weight percolation model in a
two-dimensional, periodic, square lattice. The problem exhibits edge weights
which are taken from a distribution that allows for both positive and negative
values. Additionally, in this model variant all edges are directed. For a given
realization of the disorder, a minimally weighted loop/path configuration is
determined by performing a non-trivial transformation of the original lattice
into a minimum weight perfect matching problem. For this problem, fast
polynomial-time algorithms are available, thus we could study large systems
with high accuracy. Depending on the fraction of negatively and positively
weighted edges in the lattice, a continuous phase transition can be identified,
whose characterizing critical exponents we have estimated by a finite-size
scaling analyses of the numerically obtained data. We observe a strong change
of the universality class with respect to standard directed percolation, as
well as with respect to undirected negative-weight percolation. Furthermore,
the relation to directed polymers in random media is illustrated.
",0,1,0,0,0,0
12012,12013,Lower bounds for weak approximation errors for spatial spectral Galerkin approximations of stochastic wave equations,"  Although for a number of semilinear stochastic wave equations existence and
uniqueness results for corresponding solution processes are known from the
literature, these solution processes are typically not explicitly known and
numerical approximation methods are needed in order for mathematical modelling
with stochastic wave equations to become relevant for real world applications.
This, in turn, requires the numerical analysis of convergence rates for such
numerical approximation processes. A recent article by the authors proves upper
bounds for weak errors for spatial spectral Galerkin approximations of a class
of semilinear stochastic wave equations. The findings there are complemented by
the main result of this work, that provides lower bounds for weak errors which
show that in the general framework considered the established upper bounds can
essentially not be improved.
",0,0,1,0,0,0
8677,8678,Causal Inference with Two Versions of Treatment,"  Causal effects are commonly defined as comparisons of the potential outcomes
under treatment and control, but this definition is threatened by the
possibility that the treatment or control condition is not well-defined,
existing instead in more than one version. A simple, widely applicable analysis
is proposed to address the possibility that the treatment or control condition
exists in two versions with two different treatment effects. This analysis
loses no power in the main comparison of treatment and control, provides
additional information about version effects, and controls the family-wise
error rate in several comparisons. The method is motivated and illustrated
using an on-going study of the possibility that repeated head trauma in high
school football causes an increase in risk of early on-set dementia.
",0,0,0,1,0,0
10776,10777,Constraining the Dynamics of Deep Probabilistic Models,"  We introduce a novel generative formulation of deep probabilistic models
implementing ""soft"" constraints on their function dynamics. In particular, we
develop a flexible methodological framework where the modeled functions and
derivatives of a given order are subject to inequality or equality constraints.
We then characterize the posterior distribution over model and constraint
parameters through stochastic variational inference. As a result, the proposed
approach allows for accurate and scalable uncertainty quantification on the
predictions and on all parameters. We demonstrate the application of equality
constraints in the challenging problem of parameter inference in ordinary
differential equation models, while we showcase the application of inequality
constraints on the problem of monotonic regression of count data. The proposed
approach is extensively tested in several experimental settings, leading to
highly competitive results in challenging modeling applications, while offering
high expressiveness, flexibility and scalability.
",0,0,0,1,0,0
3601,3602,Discovery of Shifting Patterns in Sequence Classification,"  In this paper, we investigate the multi-variate sequence classification
problem from a multi-instance learning perspective. Real-world sequential data
commonly show discriminative patterns only at specific time periods. For
instance, we can identify a cropland during its growing season, but it looks
similar to a barren land after harvest or before planting. Besides, even within
the same class, the discriminative patterns can appear in different periods of
sequential data. Due to such property, these discriminative patterns are also
referred to as shifting patterns. The shifting patterns in sequential data
severely degrade the performance of traditional classification methods without
sufficient training data.
We propose a novel sequence classification method by automatically mining
shifting patterns from multi-variate sequence. The method employs a
multi-instance learning approach to detect shifting patterns while also
modeling temporal relationships within each multi-instance bag by an LSTM model
to further improve the classification performance. We extensively evaluate our
method on two real-world applications - cropland mapping and affective state
recognition. The experiments demonstrate the superiority of our proposed method
in sequence classification performance and in detecting discriminative shifting
patterns.
",1,0,0,1,0,0
52,53,"Parallelism, Concurrency and Distribution in Constraint Handling Rules: A Survey","  Constraint Handling Rules is an effective concurrent declarative programming
language and a versatile computational logic formalism. CHR programs consist of
guarded reactive rules that transform multisets of constraints. One of the main
features of CHR is its inherent concurrency. Intuitively, rules can be applied
to parts of a multiset in parallel. In this comprehensive survey, we give an
overview of concurrent and parallel as well as distributed CHR semantics,
standard and more exotic, that have been proposed over the years at various
levels of refinement. These semantics range from the abstract to the concrete.
They are related by formal soundness results. Their correctness is established
as correspondence between parallel and sequential computations. We present
common concise sample CHR programs that have been widely used in experiments
and benchmarks. We review parallel CHR implementations in software and
hardware. The experimental results obtained show a consistent parallel speedup.
Most implementations are available online. The CHR formalism can also be used
to implement and reason with models for concurrency. To this end, the Software
Transaction Model, the Actor Model, Colored Petri Nets and the Join-Calculus
have been faithfully encoded in CHR. Under consideration in Theory and Practice
of Logic Programming (TPLP).
",1,0,0,0,0,0
9969,9970,Universal Planning Networks,"  A key challenge in complex visuomotor control is learning abstract
representations that are effective for specifying goals, planning, and
generalization. To this end, we introduce universal planning networks (UPN).
UPNs embed differentiable planning within a goal-directed policy. This planning
computation unrolls a forward model in a latent space and infers an optimal
action plan through gradient descent trajectory optimization. The
plan-by-gradient-descent process and its underlying representations are learned
end-to-end to directly optimize a supervised imitation learning objective. We
find that the representations learned are not only effective for goal-directed
visual imitation via gradient-based trajectory optimization, but can also
provide a metric for specifying goals using images. The learned representations
can be leveraged to specify distance-based rewards to reach new target states
for model-free reinforcement learning, resulting in substantially more
effective learning when solving new tasks described via image-based goals. We
were able to achieve successful transfer of visuomotor planning strategies
across robots with significantly different morphologies and actuation
capabilities.
",1,0,0,1,0,0
11804,11805,Imbedding results in Musielak-Orlicz spaces with an application to anisotropic nonlinear Neumann problems,"  We prove a continuous embedding that allows us to obtain a boundary trace
imbedding result for anisotropic Musielak-Orlicz spaces, which we then apply to
obtain an existence result for Neumann problems with nonlinearities on the
boundary associated to some anisotropic nonlinear elliptic equations in
Musielak-Orlicz spaces constructed from Musielak-Orlicz functions on which and
on their conjugates we do not assume the $\Delta_2$-condition. The uniqueness
is also studied.
",0,0,1,0,0,0
17091,17092,Algorithmic Bio-surveillance For Precise Spatio-temporal Prediction of Zoonotic Emergence,"  Viral zoonoses have emerged as the key drivers of recent pandemics. Human
infection by zoonotic viruses are either spillover events -- isolated
infections that fail to cause a widespread contagion -- or species jumps, where
successful adaptation to the new host leads to a pandemic. Despite expensive
bio-surveillance efforts, historically emergence response has been reactive,
and post-hoc. Here we use machine inference to demonstrate a high accuracy
predictive bio-surveillance capability, designed to pro-actively localize an
impending species jump via automated interrogation of massive sequence
databases of viral proteins. Our results suggest that a jump might not purely
be the result of an isolated unfortunate cross-infection localized in space and
time; there are subtle yet detectable patterns of genotypic changes
accumulating in the global viral population leading up to emergence. Using tens
of thousands of protein sequences simultaneously, we train models that track
maximum achievable accuracy for disambiguating host tropism from the primary
structure of surface proteins, and show that the inverse classification
accuracy is a quantitative indicator of jump risk. We validate our claim in the
context of the 2009 swine flu outbreak, and the 2004 emergence of H5N1
subspecies of Influenza A from avian reservoirs; illustrating that
interrogation of the global viral population can unambiguously track a near
monotonic risk elevation over several preceding years leading to eventual
emergence.
",0,0,0,1,1,0
5767,5768,Spin tracking of polarized protons in the Main Injector at Fermilab,"  The Main Injector (MI) at Fermilab currently produces high-intensity beams of
protons at energies of 120 GeV for a variety of physics experiments.
Acceleration of polarized protons in the MI would provide opportunities for a
rich spin physics program at Fermilab. To achieve polarized proton beams in the
Fermilab accelerator complex, detailed spin tracking simulations with realistic
parameters based on the existing facility are required. This report presents
studies at the MI using a single 4-twist Siberian snake to determine the
depolarizing spin resonances for the relevant synchrotrons. Results will be
presented first for a perfect MI lattice, followed by a lattice that includes
the real MI imperfections, such as the measured magnet field errors and
quadrupole misalignments. The tolerances of each of these factors in
maintaining polarization in the Main Injector will be discussed.
",0,1,0,0,0,0
17314,17315,Divide-and-Conquer Checkpointing for Arbitrary Programs with No User Annotation,"  Classical reverse-mode automatic differentiation (AD) imposes only a small
constant-factor overhead in operation count over the original computation, but
has storage requirements that grow, in the worst case, in proportion to the
time consumed by the original computation. This storage blowup can be
ameliorated by checkpointing, a process that reorders application of classical
reverse-mode AD over an execution interval to tradeoff space \vs\ time.
Application of checkpointing in a divide-and-conquer fashion to strategically
chosen nested execution intervals can break classical reverse-mode AD into
stages which can reduce the worst-case growth in storage from linear to
sublinear. Doing this has been fully automated only for computations of
particularly simple form, with checkpoints spanning execution intervals
resulting from a limited set of program constructs. Here we show how the
technique can be automated for arbitrary computations. The essential innovation
is to apply the technique at the level of the language implementation itself,
thus allowing checkpoints to span any execution interval.
",1,0,0,0,0,0
5832,5833,Extreme values of the Riemann zeta function on the 1-line,"  We prove that there are arbitrarily large values of $t$ such that
$|\zeta(1+it)| \geq e^{\gamma} (\log_2 t + \log_3 t) + \mathcal{O}(1)$. This
essentially matches the prediction for the optimal lower bound in a conjecture
of Granville and Soundararajan. Our proof uses a new variant of the ""long
resonator"" method. While earlier implementations of this method crucially
relied on a ""sparsification"" technique to control the mean-square of the
resonator function, in the present paper we exploit certain self-similarity
properties of a specially designed resonator function.
",0,0,1,0,0,0
2793,2794,Goldstone-like phonon modes in a (111)-strained perovskite,"  Goldstone modes are massless particles resulting from spontaneous symmetry
breaking. Although such modes are found in elementary particle physics as well
as in condensed matter systems like superfluid helium, superconductors and
magnons - structural Goldstone modes are rare. Epitaxial strain in thin films
can induce structures and properties not accessible in bulk and has been
intensively studied for (001)-oriented perovskite oxides. Here we predict
Goldstone-like phonon modes in (111)-strained SrMnO3 by first-principles
calculations. Under compressive strain the coupling between two in-plane
rotational instabilities give rise to a Mexican hat shaped energy surface
characteristic of a Goldstone mode. Conversely, large tensile strain induces
in-plane polar instabilities with no directional preference, giving rise to a
continuous polar ground state. Such phonon modes with U(1) symmetry could
emulate structural condensed matter Higgs modes. The mass of this Higgs boson,
given by the shape of the Mexican hat energy surface, can be tuned by strain
through proper choice of substrate.
",0,1,0,0,0,0
11866,11867,Blood-based metabolic signatures in Alzheimer's disease,"  Introduction: Identification of blood-based metabolic changes might provide
early and easy-to-obtain biomarkers.
Methods: We included 127 AD patients and 121 controls with
CSF-biomarker-confirmed diagnosis (cut-off tau/A$\beta_{42}$: 0.52). Mass
spectrometry platforms determined the concentrations of 53 amine, 22 organic
acid, 120 lipid, and 40 oxidative stress compounds. Multiple signatures were
assessed: differential expression (nested linear models), classification
(logistic regression), and regulatory (network extraction).
Results: Twenty-six metabolites were differentially expressed. Metabolites
improved the classification performance of clinical variables from 74% to 79%.
Network models identified 5 hubs of metabolic dysregulation: Tyrosine,
glycylglycine, glutamine, lysophosphatic acid C18:2 and platelet activating
factor C16:0. The metabolite network for APOE $\epsilon$4 negative AD patients
was less cohesive compared to the network for APOE $\epsilon$4 positive AD
patients.
Discussion: Multiple signatures point to various promising peripheral markers
for further validation. The network differences in AD patients according to
APOE genotype may reflect different pathways to AD.
",0,0,0,1,0,0
13623,13624,Critical magnetic fields in a superconductor coupled to a superfluid,"  We study a superconductor that is coupled to a superfluid via density and
derivative couplings. Starting from a Lagrangian for two complex scalar fields,
we derive a temperature-dependent Ginzburg-Landau potential, which is then used
to compute the phase diagram at nonzero temperature and external magnetic
field. This includes the calculation of the critical magnetic fields for the
transition to an array of magnetic flux tubes, based on an approximation for
the interaction between the flux tubes. We find that the transition region
between type-I and type-II superconductivity changes qualitatively due to the
presence of the superfluid: the phase transitions at the upper and lower
critical fields in the type-II regime become first order, opening the
possibility of clustered flux tube phases. These flux tube clusters may be
realized in the core of neutron stars, where superconducting protons are
expected to be coupled to superfluid neutrons.
",0,1,0,0,0,0
16520,16521,Arbitrary Beam Synthesis of Different Hybrid Beamforming Systems,"  For future mmWave mobile communication systems the use of analog/hybrid
beamforming is envisioned be a key as- pect. The synthesis of beams is a key
technology of enable the best possible operation during beamsearch, data
transmission and MU MIMO operation. The developed method for synthesizing beams
is based on previous work in radar technology considering only phase array
antennas. With this technique it is possible to generate a desired beam of any
shape with the constraints of the desired target transceiver antenna frontend.
It is not constraint to a certain antenna array geometry, but can handle 1d, 2d
and even 3d antenna array geometries like cylindric arrays. The numerical
examples show that the method can synthesize beams by considering a user
defined tradeoff between gain, transition width and passband ripples.
",1,0,0,0,0,0
1250,1251,Cross-stream migration of a surfactant-laden deformable droplet in a Poiseuille flow,"  The motion of a viscous deformable droplet suspended in an unbounded
Poiseuille flow in the presence of bulk-insoluble surfactants is studied
analytically. Assuming the convective transport of fluid and heat to be
negligible, we perform a small-deformation perturbation analysis to obtain the
droplet migration velocity. The droplet dynamics strongly depends on the
distribution of surfactants along the droplet interface, which is governed by
the relative strength of convective transport of surfactants as compared with
the diffusive transport of surfactants. The present study is focused on the
following two limits: (i) when the surfactant transport is dominated by surface
diffusion, and (ii) when the surfactant transport is dominated by surface
convection. In the first limiting case, it is seen that the axial velocity of
the droplet decreases with increase in the advection of the surfactants along
the surface. The variation of cross-stream migration velocity, on the other
hand, is analyzed over three different regimes based on the ratio of the
viscosity of the droplet phase to that of the carrier phase. In the first
regime the migration velocity decreases with increase in surface advection of
the surfactants although there is no change in direction of droplet migration.
For the second regime, the direction of the cross-stream migration of the
droplet changes depending on different parameters. In the third regime, the
migration velocity is merely affected by any change in the surfactant
distribution. For the other limit of higher surface advection in comparison to
surface diffusion of the surfactants, the axial velocity of the droplet is
found to be independent of the surfactant distribution. However, the
cross-stream velocity is found to decrease with increase in non-uniformity in
surfactant distribution.
",0,1,0,0,0,0
3340,3341,Saxion Cosmology for Thermalized Gravitino Dark Matter,"  In all supersymmetric theories, gravitinos, with mass suppressed by the
Planck scale, are an obvious candidate for dark matter; but if gravitinos ever
reached thermal equilibrium, such dark matter is apparently either too abundant
or too hot, and is excluded. However, in theories with an axion, a saxion
condensate is generated during an early era of cosmological history and its
late decay dilutes dark matter. We show that such dilution allows previously
thermalized gravitinos to account for the observed dark matter over very wide
ranges of gravitino mass, keV < $m_{3/2}$ < TeV, axion decay constant, $10^9$
GeV < $f_a$ < $10^{16}$ GeV, and saxion mass, 10 MeV < $m_s$ < 100 TeV.
Constraints on this parameter space are studied from BBN, supersymmetry
breaking, gravitino and axino production from freeze-in and saxion decay, and
from axion production from both misalignment and parametric resonance
mechanisms. Large allowed regions of $(m_{3/2}, f_a, m_s)$ remain, but differ
for DFSZ and KSVZ theories. Superpartner production at colliders may lead to
events with displaced vertices and kinks, and may contain saxions decaying to
$(WW,ZZ,hh), gg, \gamma \gamma$ or a pair of Standard Model fermions. Freeze-in
may lead to a sub-dominant warm component of gravitino dark matter, and saxion
decay to axions may lead to dark radiation.
",0,1,0,0,0,0
19408,19409,Gravity Formality,"  We show that Willwacher's cyclic formality theorem can be extended to
preserve natural Gravity operations on cyclic multivector fields and cyclic
multidifferential operators. We express this in terms of a homotopy Gravity
quasi-isomorphism with explicit local formulas. For this, we develop operadic
tools related to mixed complexes and cyclic homology and prove that the operad
$\mathsf M_\circlearrowleft$ of natural operations on cyclic operators is
formal and hence quasi-isomorphic to the Gravity operad.
",0,0,1,0,0,0
943,944,Neutronic Analysis on Potential Accident Tolerant Fuel-Cladding Combination U$_3$Si$_2$-FeCrAl,"  Neutronic performance is investigated for a potential accident tolerant fuel
(ATF),which consists of U$_3$Si$_2$ fuel and FeCrAl cladding. In comparison
with current UO$_2$-Zr system, FeCrAl has a better oxidation resistance but a
larger thermal neutron absorption cross section. U$_3$Si$_2$ has a higher
thermal conductivity and a higher uranium density, which can compensate the
reactivity suppressed by FeCrAl. Based on neutronic investigations, a possible
U$_3$Si$_2$-FeCrAl fuel-cladding systemis taken into consideration. Fundamental
properties of the suggested fuel-cladding combination are investigated in a
fuel assembly.These properties include moderator and fuel temperature
coefficients, control rods worth, radial power distribution (in a fuel rod),
and different void reactivity coefficients. The present work proves that the
new combination has less reactivity variation during its service lifetime.
Although, compared with the current system, it has a little larger deviation on
power distribution and a little less negative temperature coefficient and void
reactivity coefficient and its control rods worth is less important, variations
of these parameters are less important during the service lifetime of fuel.
Hence, U$_3$Si$_2$-FeCrAl system is a potential ATF candidate from a neutronic
view.
",0,1,0,0,0,0
325,326,Geometrically stopped Markovian random growth processes and Pareto tails,"  Many empirical studies document power law behavior in size distributions of
economic interest such as cities, firms, income, and wealth. One mechanism for
generating such behavior combines independent and identically distributed
Gaussian additive shocks to log-size with a geometric age distribution. We
generalize this mechanism by allowing the shocks to be non-Gaussian (but
light-tailed) and dependent upon a Markov state variable. Our main results
provide sharp bounds on tail probabilities and simple formulas for Pareto
exponents. We present two applications: (i) we show that the tails of the
wealth distribution in a heterogeneous-agent dynamic general equilibrium model
with idiosyncratic endowment risk decay exponentially, unlike models with
investment risk where the tails may be Paretian, and (ii) we show that a random
growth model for the population dynamics of Japanese prefectures is consistent
with the observed Pareto exponent but only after allowing for Markovian
dynamics.
",0,0,1,0,0,0
20685,20686,Discovery of Latent 3D Keypoints via End-to-end Geometric Reasoning,"  This paper presents KeypointNet, an end-to-end geometric reasoning framework
to learn an optimal set of category-specific 3D keypoints, along with their
detectors. Given a single image, KeypointNet extracts 3D keypoints that are
optimized for a downstream task. We demonstrate this framework on 3D pose
estimation by proposing a differentiable objective that seeks the optimal set
of keypoints for recovering the relative pose between two views of an object.
Our model discovers geometrically and semantically consistent keypoints across
viewing angles and instances of an object category. Importantly, we find that
our end-to-end framework using no ground-truth keypoint annotations outperforms
a fully supervised baseline using the same neural network architecture on the
task of pose estimation. The discovered 3D keypoints on the car, chair, and
plane categories of ShapeNet are visualized at this http URL.
",0,0,0,1,0,0
10262,10263,Urban Delay Tolerant Network Simulator (UDTNSim v0.1),"  Delay Tolerant Networking (DTN) is an approach to networking which handles
network disruptions and high delays that may occur in many kinds of
communication networks. The major reasons for high delay include partial
connectivity of networks as can be seen in many types of ad hoc wireless
networks with frequent network partitions, long propagation time as experienced
in inter-planetary and deep space networks, and frequent link disruptions due
to the mobility of nodes as observed in terrestrial wireless network
environments. Experimenting network architectures, protocols, and mobility
models in such real-world scenarios is difficult due to the complexities
involved in the network environment. Therefore, in this document, we present
the documentation of an Urban Delay Tolerant Network Simulator (UDTNSim)
version 0.1, capable of simulating urban road network environments with DTN
characteristics including mobility models and routing protocols. The mobility
models included in this version of UDTNSim are (i) Stationary Movement, (ii)
Simple Random Movement, (iii) Path Type Based Movememt, (iv) Path Memory Based
Movement, (v) Path Type with Restricted Movement, and (vi) Path Type with Wait
Movement. In addition to mobility models, we also provide three routing and
data hand-off protocols: (i) Epidemic Routing, (ii) Superior Only Handoff, and
(iii) Superior Peer Handoff. UDTNSim v0.1 is designed using object-oriented
programming approach in order to provide flexibility in addition of new
features to the DTN environment. UDTNSim v0.1 is distributed as an open source
simulator for the use of the research community.
",1,0,0,0,0,0
6920,6921,On the scaling of entropy viscosity in high order methods,"  In this work, we outline the entropy viscosity method and discuss how the
choice of scaling influences the size of viscosity for a simple shock problem.
We present examples to illustrate the performance of the entropy viscosity
method under two distinct scalings.
",0,0,1,0,0,0
7198,7199,A Utility-Driven Multi-Queue Admission Control Solution for Network Slicing,"  The combination of recent emerging technologies such as network function
virtualization (NFV) and network programmability (SDN) gave birth to the
Network Slicing revolution. 5G networks consist of multi-tenant infrastructures
capable of offering leased network ""slices"" to new customers (e.g., vertical
industries) enabling a new telecom business model: Slice-as-aService (SlaaS).
In this paper, we aim i ) to study the slicing admission control problem by
means of a multi-queuing system for heterogeneous tenant requests, ii ) to
derive its statistical behavior model, and iii ) to provide a utility-based
admission control optimization. Our results analyze the capability of the
proposed SlaaS system to be approximately Markovian and evaluate its
performance as compared to legacy solutions.
",1,0,0,0,0,0
20675,20676,Social Media Analysis For Organizations: Us Northeastern Public And State Libraries Case Study,"  Social networking sites such as Twitter have provided a great opportunity for
organizations such as public libraries to disseminate information for public
relations purposes. However, there is a need to analyze vast amounts of social
media data. This study presents a computational approach to explore the content
of tweets posted by nine public libraries in the northeastern United States of
America. In December 2017, this study extracted more than 19,000 tweets from
the Twitter accounts of seven state libraries and two urban public libraries.
Computational methods were applied to collect the tweets and discover
meaningful themes. This paper shows how the libraries have used Twitter to
represent their services and provides a starting point for different
organizations to evaluate the themes of their public tweets.
",1,0,0,1,0,0
19835,19836,The role of local-geometrical-orders on the growth of dynamic-length-scales in glass-forming liquids,"  The precise nature of complex structural relaxation as well as an explanation
for the precipitous growth of relaxation time in cooling glass-forming liquids
are essential to the understanding of vitrification of liquids. The dramatic
increase of relaxation time is believed to be caused by the growth of one or
more correlation lengths, which has received much attention recently. Here, we
report a direct link between the growth of a specific local-geometrical-order
and an increase of dynamic-length-scale as the atomic dynamics in metallic
glass-forming liquids slow down. Although several types of local
geometrical-orders are present in these metallic liquids, the growth of
icosahedral ordering is found to be directly related to the increase of the
dynamic-length-scale. This finding suggests an intriguing scenario that the
transient icosahedral ordering could be the origin of the dynamic-length-scale
in metallic glass-forming liquids.
",0,1,0,0,0,0
16180,16181,An evolutionary game model for behavioral gambit of loyalists: Global awareness and risk-aversion,"  We study the phase diagram of a minority game where three classes of agents
are present. Two types of agents play a risk-loving game that we model by the
standard Snowdrift Game. The behaviour of the third type of agents is coded by
{\em indifference} w.r.t. the game at all: their dynamics is designed to
account for risk-aversion as an innovative behavioral gambit. From this point
of view, the choice of this solitary strategy is enhanced when innovation
starts, while is depressed when it becomes the majority option. This implies
that the payoff matrix of the game becomes dependent on the global awareness of
the agents measured by the relevance of the population of the indifferent
players. The resulting dynamics is non-trivial with different kinds of phase
transition depending on a few model parameters. The phase diagram is studied on
regular as well as complex networks.
",0,0,0,0,1,0
17938,17939,Detection of irregular QRS complexes using Hermite Transform and Support Vector Machine,"  Computer based recognition and detection of abnormalities in ECG signals is
proposed. For this purpose, the Support Vector Machines (SVM) are combined with
the advantages of Hermite transform representation. SVM represent a special
type of classification techniques commonly used in medical applications.
Automatic classification of ECG could make the work of cardiologic departments
faster and more efficient. It would also reduce the number of false diagnosis
and, as a result, save lives. The working principle of the SVM is based on
translating the data into a high dimensional feature space and separating it
using a linear classificator. In order to provide an optimal representation for
SVM application, the Hermite transform domain is used. This domain is proved to
be suitable because of the similarity of the QRS complex with Hermite basis
functions. The maximal signal information is obtained using a small set of
features that are used for detection of irregular QRS complexes. The aim of the
paper is to show that these features can be employed for automatic ECG signal
analysis.
",1,0,0,0,0,0
15818,15819,A Survey of Question Answering for Math and Science Problem,"  Turing test was long considered the measure for artificial intelligence. But
with the advances in AI, it has proved to be insufficient measure. We can now
aim to mea- sure machine intelligence like we measure human intelligence. One
of the widely accepted measure of intelligence is standardized math and science
test. In this paper, we explore the progress we have made towards the goal of
making a machine smart enough to pass the standardized test. We see the
challenges and opportunities posed by the domain, and note that we are quite
some ways from actually making a system as smart as a even a middle school
scholar.
",1,0,0,0,0,0
20798,20799,Dropout as a Low-Rank Regularizer for Matrix Factorization,"  Regularization for matrix factorization (MF) and approximation problems has
been carried out in many different ways. Due to its popularity in deep
learning, dropout has been applied also for this class of problems. Despite its
solid empirical performance, the theoretical properties of dropout as a
regularizer remain quite elusive for this class of problems. In this paper, we
present a theoretical analysis of dropout for MF, where Bernoulli random
variables are used to drop columns of the factors. We demonstrate the
equivalence between dropout and a fully deterministic model for MF in which the
factors are regularized by the sum of the product of squared Euclidean norms of
the columns. Additionally, we inspect the case of a variable sized
factorization and we prove that dropout achieves the global minimum of a convex
approximation problem with (squared) nuclear norm regularization. As a result,
we conclude that dropout can be used as a low-rank regularizer with data
dependent singular-value thresholding.
",1,0,0,1,0,0
17043,17044,Multilevel preconditioner of Polynomial Chaos Method for quantifying uncertainties in a blood pump,"  More than 23 million people are suffered by Heart failure worldwide. Despite
the modern transplant operation is well established, the lack of heart
donations becomes a big restriction on transplantation frequency. With respect
to this matter, ventricular assist devices (VADs) can play an important role in
supporting patients during waiting period and after the surgery. Moreover, it
has been shown that VADs by means of blood pump have advantages for working
under different conditions. While a lot of work has been done on modeling the
functionality of the blood pump, but quantifying uncertainties in a numerical
model is a challenging task. We consider the Polynomial Chaos (PC) method,
which is introduced by Wiener for modeling stochastic process with Gaussian
distribution. The Galerkin projection, the intrusive version of the generalized
Polynomial Chaos (gPC), has been densely studied and applied for various
problems. The intrusive Galerkin approach could represent stochastic process
directly at once with Polynomial Chaos series expansions, it would therefore
optimize the total computing effort comparing with classical non-intrusive
methods. We compared different preconditioning techniques for a steady state
simulation of a blood pump configuration in our previous work, the comparison
shows that an inexact multilevel preconditioner has a promising performance. In
this work, we show an instationary blood flow through a FDA blood pump
configuration with Galerkin Projection method, which is implemented in our open
source Finite Element library Hiflow3. Three uncertainty sources are
considered: inflow boundary condition, rotor angular speed and dynamic
viscosity, the numerical results are demonstrated with more than 30 Million
degrees of freedom by using supercomputer.
",0,1,0,0,0,0
3113,3114,Towards Neural Co-Processors for the Brain: Combining Decoding and Encoding in Brain-Computer Interfaces,"  The field of brain-computer interfaces is poised to advance from the
traditional goal of controlling prosthetic devices using brain signals to
combining neural decoding and encoding within a single neuroprosthetic device.
Such a device acts as a ""co-processor"" for the brain, with applications ranging
from inducing Hebbian plasticity for rehabilitation after brain injury to
reanimating paralyzed limbs and enhancing memory. We review recent progress in
simultaneous decoding and encoding for closed-loop control and plasticity
induction. To address the challenge of multi-channel decoding and encoding, we
introduce a unifying framework for developing brain co-processors based on
artificial neural networks and deep learning. These ""neural co-processors"" can
be used to jointly optimize cost functions with the nervous system to achieve
desired behaviors ranging from targeted neuro-rehabilitation to augmentation of
brain function.
",0,0,0,0,1,0
6780,6781,Autocommuting probability of a finite group,"  Let $G$ be a finite group and $\Aut(G)$ the automorphism group of $G$. The
autocommuting probability of $G$, denoted by $\Pr(G, \Aut(G))$, is the
probability that a randomly chosen automorphism of $G$ fixes a randomly chosen
element of $G$. In this paper, we study $\Pr(G, \Aut(G))$ through a
generalization. We obtain a computing formula, several bounds and
characterizations of $G$ through $\Pr(G, \Aut(G))$. We conclude the paper by
showing that the generalized autocommuting probability of $G$ remains unchanged
under autoisoclinism.
",0,0,1,0,0,0
8925,8926,Quantifying the Contributions of Training Data and Algorithm Logic to the Performance of Automated Cause-assignment Algorithms for Verbal Autopsy,"  A verbal autopsy (VA) consists of a survey with a relative or close contact
of a person who has recently died. VA surveys are commonly used to infer likely
causes of death for individuals when deaths happen outside of hospitals or
healthcare facilities. Several statistical and algorithmic methods are
available to assign cause of death using VA surveys. Each of these methods
require as inputs some information about the joint distribution of symptoms and
causes. In this note, we examine the generalizability of this symptom-cause
information by comparing different automated coding methods using various
combinations of inputs and evaluation data. VA algorithm performance is
affected by both the specific SCI themselves and the logic of a given
algorithm. Using a variety of performance metrics for all existing VA
algorithms, we demonstrate that in general the adequacy of the information
about the joint distribution between symptoms and cause affects performance at
least as much or more than algorithm logic.
",0,0,0,1,0,0
4433,4434,Iron Intercalated Covalent-Organic Frameworks: First Crystalline Porous Thermoelectric Materials,"  Covalent-organic frameworks (COFs) are intriguing platforms for designing
functional molecular materials. Here, we present a computational study based on
van der Waals dispersion-corrected hybrid density functional theory
calculations to analyze the material properties of boroxine-linked and
triazine-linked intercalated-COFs. The effect of Fe atoms on the electronic
band structures near the Fermi energy level of the intercalated-COFs have been
investigated. The density of states (DOSs) computations have been performed to
analyze the material properties of these kind of intercalated-COFs. We predict
that COFs's electronic properties can be fine tuned by adding Fe atoms between
two organic layers in their structures. The new COFs are predicted to be
thermoelectric materials. These intercalated-COFs provide a new strategy to
create thermoelectric materials within a rigid porous network in a highly
controlled and predictable manner.
",0,1,0,0,0,0
20806,20807,A Causal And-Or Graph Model for Visibility Fluent Reasoning in Tracking Interacting Objects,"  Tracking humans that are interacting with the other subjects or environment
remains unsolved in visual tracking, because the visibility of the human of
interests in videos is unknown and might vary over time. In particular, it is
still difficult for state-of-the-art human trackers to recover complete human
trajectories in crowded scenes with frequent human interactions. In this work,
we consider the visibility status of a subject as a fluent variable, whose
change is mostly attributed to the subject's interaction with the surrounding,
e.g., crossing behind another object, entering a building, or getting into a
vehicle, etc. We introduce a Causal And-Or Graph (C-AOG) to represent the
causal-effect relations between an object's visibility fluent and its
activities, and develop a probabilistic graph model to jointly reason the
visibility fluent change (e.g., from visible to invisible) and track humans in
videos. We formulate this joint task as an iterative search of a feasible
causal graph structure that enables fast search algorithm, e.g., dynamic
programming method. We apply the proposed method on challenging video sequences
to evaluate its capabilities of estimating visibility fluent changes of
subjects and tracking subjects of interests over time. Results with comparisons
demonstrate that our method outperforms the alternative trackers and can
recover complete trajectories of humans in complicated scenarios with frequent
human interactions.
",1,0,0,0,0,0
8486,8487,On the local view of atmospheric available potential energy,"  The possibility of constructing Lorenz's concept of available potential
energy (APE) from a local principle has been known for some time, but has
received very little attention so far. Yet, the local APE framework offers the
advantage of providing a positive definite local form of potential energy,
which like kinetic energy can be transported, converted, and created/dissipated
locally. In contrast to Lorenz's definition, which relies on the exact from of
potential energy, the local APE theory uses the particular form of potential
energy appropriate to the approximations considered. In this paper, this idea
is illustrated for the dry hydrostatic primitive equations, whose relevant form
of potential energy is the specific enthalpy. The local APE density is
non-quadratic in general, but can nevertheless be partitioned exactly into mean
and eddy components regardless of the Reynolds averaging operator used.
This paper introduces a new form of the local APE that is easily computable
from atmospheric datasets. The advantages of using the local APE over the
classical Lorenz APE are highlighted. The paper also presents the first
calculation of the three-dimensional local APE in observation-based atmospheric
data. Finally, it illustrates how the eddy and mean components of the local APE
can be used to study regional and temporal variability in the large-scale
circulation. It is revealed that advection from high latitudes is necessary to
supply APE into the storm track regions, and that Greenland and Ross Sea, which
have suffered from rapid land ice and sea ice loss in recent decades, are
particularly susceptible to APE variability.
",0,1,0,0,0,0
15592,15593,Particlelike scattering states in a microwave cavity,"  We realize scattering states in a lossy and chaotic two-dimensional microwave
cavity which follow bundles of classical particle trajectories. To generate
such particlelike scattering states we measure the system's transmission matrix
and apply an adapted Wigner-Smith time-delay formalism to it. The necessary
shaping of the incident wave is achieved in situ using phase and amplitude
regulated microwave antennas. Our experimental findings pave the way for
establishing spatially confined communication channels that avoid possible
intruders or obstacles in wave-based communication systems.
",0,1,0,0,0,0
10355,10356,Almost Boltzmann Exploration,"  Boltzmann exploration is widely used in reinforcement learning to provide a
trade-off between exploration and exploitation. Recently, in (Cesa-Bianchi et
al., 2017) it has been shown that pure Boltzmann exploration does not perform
well from a regret perspective, even in the simplest setting of stochastic
multi-armed bandit (MAB) problems. In this paper, we show that a simple
modification to Boltzmann exploration, motivated by a variation of the standard
doubling trick, achieves $O(K\log^{1+\alpha} T)$ regret for a stochastic MAB
problem with $K$ arms, where $\alpha>0$ is a parameter of the algorithm. This
improves on the result in (Cesa-Bianchi et al., 2017), where an algorithm
inspired by the Gumbel-softmax trick achieves $O(K\log^2 T)$ regret. We also
show that our algorithm achieves $O(\beta(G) \log^{1+\alpha} T)$ regret in
stochastic MAB problems with graph-structured feedback, without knowledge of
the graph structure, where $\beta(G)$ is the independence number of the
feedback graph. Additionally, we present extensive experimental results on real
datasets and applications for multi-armed bandits with both traditional bandit
feedback and graph-structured feedback. In all cases, our algorithm performs as
well or better than the state-of-the-art.
",1,0,0,1,0,0
2669,2670,Path Planning for Multiple Heterogeneous Unmanned Vehicles with Uncertain Service Times,"  This article presents a framework and develops a formulation to solve a path
planning problem for multiple heterogeneous Unmanned Vehicles (UVs) with
uncertain service times for each vehicle--target pair. The vehicles incur a
penalty proportional to the duration of their total service time in excess of a
preset constant. The vehicles differ in their motion constraints and are
located at distinct depots at the start of the mission. The vehicles may also
be equipped with disparate sensors. The objective is to find a tour for each
vehicle that starts and ends at its respective depot such that every target is
visited and serviced by some vehicle while minimizing the sum of the total
travel distance and the expected penalty incurred by all the vehicles. We
formulate the problem as a two-stage stochastic program with recourse, present
the theoretical properties of the formulation and advantages of using such a
formulation, as opposed to a deterministic expected value formulation, to solve
the problem. Extensive numerical simulations also corroborate the effectiveness
of the proposed approach.
",1,0,1,0,0,0
17068,17069,HARPS-N high spectral resolution observations of Cepheids I. The Baade-Wesselink projection factor of δ Cep revisited,"  The projection factor p is the key quantity used in the Baade-Wesselink (BW)
method for distance determination; it converts radial velocities into pulsation
velocities. Several methods are used to determine p, such as geometrical and
hydrodynamical models or the inverse BW approach when the distance is known. We
analyze new HARPS-N spectra of delta Cep to measure its cycle-averaged
atmospheric velocity gradient in order to better constrain the projection
factor. We first apply the inverse BW method to derive p directly from
observations. The projection factor can be divided into three subconcepts: (1)
a geometrical effect (p0); (2) the velocity gradient within the atmosphere
(fgrad); and (3) the relative motion of the optical pulsating photosphere with
respect to the corresponding mass elements (fo-g). We then measure the fgrad
value of delta Cep for the first time. When the HARPS-N mean cross-correlated
line-profiles are fitted with a Gaussian profile, the projection factor is
pcc-g = 1.239 +/- 0.034(stat) +/- 0.023(syst). When we consider the different
amplitudes of the radial velocity curves that are associated with 17 selected
spectral lines, we measure projection factors ranging from 1.273 to 1.329. We
find a relation between fgrad and the line depth measured when the Cepheid is
at minimum radius. This relation is consistent with that obtained from our best
hydrodynamical model of delta Cep and with our projection factor decomposition.
Using the observational values of p and fgrad found for the 17 spectral lines,
we derive a semi-theoretical value of fo-g. We alternatively obtain fo-g =
0.975+/-0.002 or 1.006+/-0.002 assuming models using radiative transfer in
plane-parallel or spherically symmetric geometries, respectively. The new
HARPS-N observations of delta Cep are consistent with our decomposition of the
projection factor.
",0,1,0,0,0,0
8693,8694,Cloaking using complementary media for electromagnetic waves,"  Negative index materials are artificial structures whose refractive index has
negative value over some frequency range. The study of these materials has
attracted a lot of attention in the scientific community not only because of
their many potential interesting applications but also because of challenges in
understanding their intriguing properties due to the sign-changing coefficients
in equations describing their properties. In this paper, we establish cloaking
using complementary media for electromagnetic waves. This confirms and extends
the suggestions in two dimensions of Lai et al. for the full Maxwell equations.
The analysis is based on the reflecting and removing localized singularity
techniques, three-sphere inequalities, and the fact that the Maxwell equations
can be reduced to a weakly coupled second order elliptic equations.
",0,0,1,0,0,0
16173,16174,Semantic Similarity from Natural Language and Ontology Analysis,"  Artificial Intelligence federates numerous scientific fields in the aim of
developing machines able to assist human operators performing complex
treatments -- most of which demand high cognitive skills (e.g. learning or
decision processes). Central to this quest is to give machines the ability to
estimate the likeness or similarity between things in the way human beings
estimate the similarity between stimuli.
In this context, this book focuses on semantic measures: approaches designed
for comparing semantic entities such as units of language, e.g. words,
sentences, or concepts and instances defined into knowledge bases. The aim of
these measures is to assess the similarity or relatedness of such semantic
entities by taking into account their semantics, i.e. their meaning --
intuitively, the words tea and coffee, which both refer to stimulating
beverage, will be estimated to be more semantically similar than the words
toffee (confection) and coffee, despite that the last pair has a higher
syntactic similarity. The two state-of-the-art approaches for estimating and
quantifying semantic similarities/relatedness of semantic entities are
presented in detail: the first one relies on corpora analysis and is based on
Natural Language Processing techniques and semantic models while the second is
based on more or less formal, computer-readable and workable forms of knowledge
such as semantic networks, thesaurus or ontologies. (...) Beyond a simple
inventory and categorization of existing measures, the aim of this monograph is
to convey novices as well as researchers of these domains towards a better
understanding of semantic similarity estimation and more generally semantic
measures.
",1,0,0,0,0,0
7331,7332,Smooth positon solutions of the focusing modified Korteweg-de Vries equation,"  The $n$-fold Darboux transformation $T_{n}$ of the focusing real mo\-di\-fied
Kor\-te\-weg-de Vries (mKdV) equation is expressed in terms of the determinant
representation. Using this representation, the $n$-soliton solutions of the
mKdV equation are also expressed by determinants whose elements consist of the
eigenvalues $\lambda_{j}$ and the corresponding eigenfunctions of the
associated Lax equation. The nonsingular $n$-positon solutions of the focusing
mKdV equation are obtained in the special limit
$\lambda_{j}\rightarrow\lambda_{1}$, from the corresponding $n$-soliton
solutions and by using the associated higher-order Taylor expansion.
Furthermore, the decomposition method of the $n$-positon solution into $n$
single-soliton solutions, the trajectories, and the corresponding ""phase
shifts"" of the multi-positons are also investigated.
",0,1,0,0,0,0
19343,19344,A Dynamically Reconfigurable Terahertz Array Antenna for Near-field Imaging Applications,"  A proof of concept for high speed near-field imaging with sub-wavelength
resolution using SLM is presented. An 8 channel THz detector array antenna with
an electrode gap of 100 um and length of 5 mm is fabricated using the
commercially available GaAs semiconductor substrate. Each array antenna can be
excited simultaneously by spatially reconfiguring the optical probe beam and
the THz electric field can be recorded using 8 channel lock-in amplifiers. By
scanning the probe beam along the length of the array antenna, a 2D image can
be obtained with amplitude, phase and frequency information.
",0,1,0,0,0,0
7620,7621,Ultrashort dark solitons interactions and nonlinear tunneling in the modified nonlinear Schrödinger equation with variable coefficients,"  We present the study of the dark soliton dynamics in an inhomogenous fiber by
means of a variable coefficient modified nonlinear Schrödinger equation
(Vc-MNLSE) with distributed dispersion, self-phase modulation, self-steepening
and linear gain/loss. The ultrashort dark soliton pulse evolution and
interaction is studied by using the Hirota bilinear (HB) method. In particular,
we give much insight into the effect of self-steepening (SS) on the dark
soliton dynamics. The study reveals a shock wave formation, as a major effect
of SS. Numerically, we study the dark soliton propagation in the continuous
wave background, and the stability of the soliton solution is tested in the
presence of photon noise. The elastic collision behaviors of the dark solitons
are discussed by the asymptotic analysis. On the other hand, considering the
nonlinear tunneling of dark soliton through barrier/well, we find that the
tunneling of the dark soliton depends on the height of the barrier and the
amplitude of the soliton. The intensity of the tunneling soliton either forms a
peak or valley and retains its shape after the tunneling. For the case of
exponential background, the soliton tends to compress after tunneling through
the barrier/well.
",0,1,0,0,0,0
14722,14723,"Open data, open review and open dialogue in making social sciences plausible","  Nowadays, protecting trust in social sciences also means engaging in open
community dialogue, which helps to safeguard robustness and improve efficiency
of research methods. The combination of open data, open review and open
dialogue may sound simple but implementation in the real world will not be
straightforward. However, in view of Begley and Ellis's (2012) statement that,
""the scientific process demands the highest standards of quality, ethics and
rigour,"" they are worth implementing. More importantly, they are feasible to
work on and likely will help to restore plausibility to social sciences
research. Therefore, I feel it likely that the triplet of open data, open
review and open dialogue will gradually emerge to become policy requirements
regardless of the research funding source.
",0,0,0,1,0,0
5605,5606,On stably trivial spin torsors over low-dimensional schemes,"  The paper discusses stably trivial torsors for spin and orthogonal groups
over smooth affine schemes over infinite perfect fields of characteristic
unequal to 2. We give a complete description of all the invariants relevant for
the classification of such objects over schemes of dimension at most $3$, along
with many examples. The results are based on the
$\mathbb{A}^1$-representability theorem for torsors and transfer of known
computations of $\mathbb{A}^1$-homotopy sheaves along the sporadic isomorphisms
to spin groups.
",0,0,1,0,0,0
5140,5141,Extensions of the Benson-Solomon fusion systems,"  The Benson-Solomon systems comprise the only known family of simple saturated
fusion systems at the prime two that do not arise as the fusion system of any
finite group. We determine the automorphism groups and the possible almost
simple extensions of these systems and of their centric linking systems.
",0,0,1,0,0,0
14293,14294,Blind Spots for Direct Detection with Simplified DM Models and the LHC,"  Using the existing simplified model framework, we build several dark matter
models which have suppressed spin-independent scattering cross section. We show
that the scattering cross section can vanish due to interference effects with
models obtained by simple combinations of simplified models. For weakly
interacting massive particle (WIMP) masses $\gtrsim$10 GeV, collider limits are
usually much weaker than the direct detection limits coming from LUX or
XENON100. However, for our model combinations, LHC analyses are more
competitive for some parts of the parameter space. The regions with direct
detection blind spots can be strongly constrained from the complementary use of
several Large Hadron Collider (LHC) searches like mono-jet, jets + missing
transverse energy, heavy vector resonance searches, etc. We evaluate the
strongest limits for combinations of scalar + vector, ""squark"" + vector, and
scalar + ""squark"" mediator, and present the LHC 14 TeV projections.
",0,1,0,0,0,0
8254,8255,A Nonparametric Bayesian Approach to Copula Estimation,"  We propose a novel Dirichlet-based Pólya tree (D-P tree) prior on the
copula and based on the D-P tree prior, a nonparametric Bayesian inference
procedure. Through theoretical analysis and simulations, we are able to show
that the flexibility of the D-P tree prior ensures its consistency in copula
estimation, thus able to detect more subtle and complex copula structures than
earlier nonparametric Bayesian models, such as a Gaussian copula mixture.
Further, the continuity of the imposed D-P tree prior leads to a more favorable
smoothing effect in copula estimation over classic frequentist methods,
especially with small sets of observations. We also apply our method to the
copula prediction between the S\&P 500 index and the IBM stock prices during
the 2007-08 financial crisis, finding that D-P tree-based methods enjoy strong
robustness and flexibility over classic methods under such irregular market
behaviors.
",0,0,0,1,0,0
6043,6044,Existence of Evolutionarily Stable Strategies Remains Hard to Decide for a Wide Range of Payoff Values,"  The concept of an evolutionarily stable strategy (ESS), introduced by Smith
and Price, is a refinement of Nash equilibrium in 2-player symmetric games in
order to explain counter-intuitive natural phenomena, whose existence is not
guaranteed in every game. The problem of deciding whether a game possesses an
ESS has been shown to be $\Sigma_{2}^{P}$-complete by Conitzer using the
preceding important work by Etessami and Lochbihler. The latter, among other
results, proved that deciding the existence of ESS is both NP-hard and
coNP-hard. In this paper we introduce a ""reduction robustness"" notion and we
show that deciding the existence of an ESS remains coNP-hard for a wide range
of games even if we arbitrarily perturb within some intervals the payoff values
of the game under consideration. In contrast, ESS exist almost surely for large
games with random and independent payoffs chosen from the same distribution.
",1,0,0,0,0,0
3470,3471,Monte Carlo determination of the low-energy constants for a two-dimensional spin-1 Heisenberg model with spatial anisotropy,"  The low-energy constants, namely the spin stiffness $\rho_s$, the staggered
magnetization density ${\cal M}_s$ per area, and the spinwave velocity $c$ of
the two-dimensional (2D) spin-1 Heisenberg model on the square and rectangular
lattices are determined using the first principles Monte Carlo method. In
particular, the studied models have antiferromagnetic couplings $J_1$ and $J_2$
in the spatial 1- and 2-directions, respectively. For each considered
$J_2/J_1$, the aspect ratio of the corresponding linear box sizes $L_2/L_1$
used in the simulations is adjusted so that the squares of the two spatial
winding numbers take the same values. In addition, the relevant finite-volume
and -temperature predictions from magnon chiral perturbation theory are
employed in extracting the numerical values of these low-energy constants. Our
results of $\rho_{s1}$ are in quantitative agreement with those obtained by the
series expansion method over a broad range of $J_2/J_1$. This in turn provides
convincing numerical evidence for the quantitative correctness of our approach.
The ${\cal M}_s$ and $c$ presented here for the spatially anisotropic models
are new and can be used as benchmarks for future related studies.
",0,1,0,0,0,0
6796,6797,Time-Optimal Trajectories of Generic Control-Affine Systems Have at Worst Iterated Fuller Singularities,"  We consider in this paper the regularity problem for time-optimal
trajectories of a single-input control-affine system on a n-dimensional
manifold. We prove that, under generic conditions on the drift and the
controlled vector field, any control u associated with an optimal trajectory is
smooth out of a countable set of times. More precisely, there exists an integer
K, only depending on the dimension n, such that the non-smoothness set of u is
made of isolated points, accumulations of isolated points, and so on up to K-th
order iterated accumulations.
",0,0,1,0,0,0
16937,16938,Friendship Maintenance and Prediction in Multiple Social Networks,"  Due to the proliferation of online social networks (OSNs), users find
themselves participating in multiple OSNs. These users leave their activity
traces as they maintain friendships and interact with other users in these
OSNs. In this work, we analyze how users maintain friendship in multiple OSNs
by studying users who have accounts in both Twitter and Instagram.
Specifically, we study the similarity of a user's friendship and the evenness
of friendship distribution in multiple OSNs. Our study shows that most users in
Twitter and Instagram prefer to maintain different friendships in the two OSNs,
keeping only a small clique of common friends in across the OSNs. Based upon
our empirical study, we conduct link prediction experiments to predict missing
friendship links in multiple OSNs using the neighborhood features, neighborhood
friendship maintenance features and cross-link features. Our link prediction
experiments shows that un- supervised methods can yield good accuracy in
predicting links in one OSN using another OSN data and the link prediction
accuracy can be further improved using supervised method with friendship
maintenance and others measures as features.
",1,1,0,0,0,0
42,43,Probing valley filtering effect by Andreev reflection in zigzag graphene nanoribbon,"  Ballistic point contact (BPC) with zigzag edges in graphene is a main
candidate of a valley filter, in which the polarization of the valley degree of
freedom can be selected by using a local gate voltage. Here, we propose to
detect the valley filtering effect by Andreev reflection. Because electrons in
the lowest conduction band and the highest valence band of the BPC possess
opposite chirality, the inter-band Andreev reflection is strongly suppressed,
after multiple scattering and interference. We draw this conclusion by both the
scattering matrix analysis and the numerical simulation. The Andreev reflection
as a function of the incident energy of electrons and the local gate voltage at
the BPC is obtained, by which the parameter region for a perfect valley filter
and the direction of valley polarization can be determined. The Andreev
reflection exhibits an oscillatory decay with the length of the BPC, indicating
a negative correlation to valley polarization.
",0,1,0,0,0,0
2749,2750,Multiple Access Wiretap Channel with Noiseless Feedback,"  The physical layer security in the up-link of the wireless communication
systems is often modeled as the multiple access wiretap channel (MAC-WT), and
recently it has received a lot attention. In this paper, the MAC-WT has been
re-visited by considering the situation that the legitimate receiver feeds his
received channel output back to the transmitters via two noiseless channels,
respectively. This model is called the MAC-WT with noiseless feedback. Inner
and outer bounds on the secrecy capacity region of this feedback model are
provided. To be specific, we first present a decode-and-forward (DF) inner
bound on the secrecy capacity region of this feedback model, and this bound is
constructed by allowing each transmitter to decode the other one's transmitted
message from the feedback, and then each transmitter uses the decoded message
to re-encode his own messages, i.e., this DF inner bound allows the independent
transmitters to co-operate with each other. Then, we provide a hybrid inner
bound which is strictly larger than the DF inner bound, and it is constructed
by using the feedback as a tool not only to allow the independent transmitters
to co-operate with each other, but also to generate two secret keys
respectively shared between the legitimate receiver and the two transmitters.
Finally, we give a sato-type outer bound on the secrecy capacity region of this
feedback model. The results of this paper are further explained via a Gaussian
example.
",1,0,1,0,0,0
14710,14711,On compact packings of the plane with circles of three radii,"  A compact circle-packing $P$ of the Euclidean plane is a set of circles which
bound mutually disjoint open discs with the property that, for every circle
$S\in P$, there exists a maximal indexed set $\{A_{0},\ldots,A_{n-1}\}\subseteq
P$ so that, for every $i\in\{0,\ldots,n-1\}$, the circle $A_{i}$ is tangent to
both circles $S$ and $A_{i+1\mod n}$ .
We show that there exist at most $11462$ pairs $(r,s)$ with $0<s<r<1$ for
which there exist a compact circle-packing of the plane consisting of circles
with radii $s$, $r$ and $1$.
We discuss computing the exact values of such $0<s<r<1$ as roots of
polynomials and exhibit a selection of compact circle-packings consisting of
circles of three radii. We also discuss the apparent infeasibility of computing
all these values on contemporary consumer hardware.
",0,0,1,0,0,0
16737,16738,Multilingual Hierarchical Attention Networks for Document Classification,"  Hierarchical attention networks have recently achieved remarkable performance
for document classification in a given language. However, when multilingual
document collections are considered, training such models separately for each
language entails linear parameter growth and lack of cross-language transfer.
Learning a single multilingual model with fewer parameters is therefore a
challenging but potentially beneficial objective. To this end, we propose
multilingual hierarchical attention networks for learning document structures,
with shared encoders and/or shared attention mechanisms across languages, using
multi-task learning and an aligned semantic space as input. We evaluate the
proposed models on multilingual document classification with disjoint label
sets, on a large dataset which we provide, with 600k news documents in 8
languages, and 5k labels. The multilingual models outperform monolingual ones
in low-resource as well as full-resource settings, and use fewer parameters,
thus confirming their computational efficiency and the utility of
cross-language transfer.
",1,0,0,0,0,0
13909,13910,Field dependence of non-reciprocal magnons in chiral MnSi,"  Spin waves in chiral magnetic materials are strongly influenced by the
Dzyaloshinskii-Moriya interaction resulting in intriguing phenomena like
non-reciprocal magnon propagation and magnetochiral dichroism. Here, we study
the non-reciprocal magnon spectrum of the archetypical chiral magnet MnSi and
its evolution as a function of magnetic field covering the field-polarized and
conical helix phase. Using inelastic neutron scattering, the magnon energies
and their spectral weights are determined quantitatively after deconvolution
with the instrumental resolution. In the field-polarized phase the imaginary
part of the dynamical susceptibility $\chi''(\varepsilon, {\bf q})$ is shown to
be asymmetric with respect to wavevectors ${\bf q}$ longitudinal to the applied
magnetic field ${\bf H}$, which is a hallmark of chiral magnetism. In the
helimagnetic phase, $\chi''(\varepsilon, {\bf q})$ becomes increasingly
symmetric with decreasing ${\bf H}$ due to the formation of helimagnon bands
and the activation of additional spinflip and non-spinflip scattering channels.
The neutron spectra are in excellent quantitative agreement with the low-energy
theory of cubic chiral magnets with a single fitting parameter being the
damping rate of spin waves.
",0,1,0,0,0,0
10508,10509,"Flying, Hopping Pit-Bots for Cave and Lava Tube Exploration on the Moon and Mars","  Wheeled ground robots are limited from exploring extreme environments such as
caves, lava tubes and skylights. Small robots that utilize unconventional
mobility through hopping, flying and rolling can overcome many roughness
limitations and thus extend exploration sites of interest on Moon and Mars. In
this paper we introduce a network of 3 kg, 0.30 m diameter ball robots
(pit-bots) that can fly, hop and roll using an onboard miniature propulsion
system. These pit-bots can be deployed from a lander or large rover. Each robot
is equipped with a smartphone sized computer, stereo camera and laser
rangefinder to per-form navigation and mapping. The ball robot can carry a
payload of 1 kg or perform sample return. Our studies show a range of 5 km and
0.7 hours flight time on the Moon.
",1,1,0,0,0,0
13201,13202,Impact of Continuous Integration on Code Reviews,"  Peer code review and continuous integration often interleave with each other
in the modern software quality management. Although several studies investigate
how non-technical factors (e.g., reviewer workload), developer participation
and even patch size affect the code review process, the impact of continuous
integration on code reviews is not yet properly understood. In this paper, we
report an exploratory study using 578K automated build entries where we
investigate the impact of automated builds on the code reviews. Our
investigation suggests that successfully passed builds are more likely to
encourage new code review participation in a pull request. Frequently built
projects are found to be maintaining a steady level of reviewing activities
over the years, which was quite missing from the rarely built projects.
Experiments with 26,516 automated build entries reported that our proposed
model can identify 64% of the builds that triggered new code reviews later.
",1,0,0,0,0,0
2890,2891,Gaia and VLT astrometry of faint stars: Precision of Gaia DR1 positions and updated VLT parallaxes of ultracool dwarfs,"  We compared positions of the Gaia first data release (DR1) secondary data set
at its faint limit with CCD positions of stars in 20 fields observed with the
VLT/FORS2 camera. The FORS2 position uncertainties are smaller than one
milli-arcsecond (mas) and allowed us to perform an independent verification of
the DR1 astrometric precision. In the fields that we observed with FORS2, we
projected the Gaia DR1 positions into the CCD plane, performed a polynomial fit
between the two sets of matching stars, and carried out statistical analyses of
the residuals in positions. The residual RMS roughly matches the expectations
given by the Gaia DR1 uncertainties, where we identified three regimes in terms
of Gaia DR1 precision: for G = 17-20 stars we found that the formal DR1
position uncertainties of stars with DR1 precisions in the range of 0.5-5 mas
are underestimated by 63 +/- 5\%, whereas the DR1 uncertainties of stars in the
range 7-10 mas are overestimated by a factor of two. For the best-measured and
generally brighter G = 16-18 stars with DR1 positional uncertainties of <0.5
mas, we detected 0.44 +/- 0.13 mas excess noise in the residual RMS, whose
origin can be in both FORS2 and Gaia DR1. By adopting Gaia DR1 as the absolute
reference frame we refined the pixel scale determination of FORS2, leading to
minor updates to the parallaxes of 20 ultracool dwarfs that we published
previously. We also updated the FORS2 absolute parallax of the Luhman 16 binary
brown dwarf system to 501.42 +/- 0.11 mas
",0,1,0,0,0,0
20711,20712,Classification of Pressure Gradient of Human Common Carotid Artery and Ascending Aorta on the Basis of Age and Gender,"  The current work is done to see which artery has more chance of having
cardiovascular diseases by measuring value of pressure gradient in the common
carotid artery (CCA) and ascending aorta according to age and gender. Pressure
gradient is determined in the CCA and ascending aorta of presumed healthy
volunteers, having age between 10 and 60 years. A real 2D model of both aorta
and common carotid artery is constructed for different age groups using
computational fluid dynamics (CFD). Pressure gradient of both the arteries are
calculated and compared for different age groups and gender. It is found that
with increase in diameter of common carotid artery and ascending aorta with
advancing age pressure gradient decreases. The value of pressure gradient of
aorta is found less than common carotid artery in both cases of age and gender.
",0,1,0,0,0,0
20679,20680,An optimal transportation approach for assessing almost stochastic order,"  When stochastic dominance $F\leq_{st}G$ does not hold, we can improve
agreement to stochastic order by suitably trimming both distributions. In this
work we consider the $L_2-$Wasserstein distance, $\mathcal W_2$, to stochastic
order of these trimmed versions. Our characterization for that distance
naturally leads to consider a $\mathcal W_2$-based index of disagreement with
stochastic order, $\varepsilon_{\mathcal W_2}(F,G)$. We provide asymptotic
results allowing to test $H_0: \varepsilon_{\mathcal W_2}(F,G)\geq
\varepsilon_0$ vs $H_a: \varepsilon_{\mathcal W_2}(F,G)<\varepsilon_0$, that,
under rejection, would give statistical guarantee of almost stochastic
dominance. We include a simulation study showing a good performance of the
index under the normal model.
",0,0,0,1,0,0
5799,5800,Complex Economic Activities Concentrate in Large Cities,"  Why do some economic activities agglomerate more than others? And, why does
the agglomeration of some economic activities continue to increase despite
recent developments in communication and transportation technologies? In this
paper, we present evidence that complex economic activities concentrate more in
large cities. We find this to be true for technologies, scientific
publications, industries, and occupations. Using historical patent data, we
show that the urban concentration of complex economic activities has been
continuously increasing since 1850. These findings suggest that the increasing
urban concentration of jobs and innovation might be a consequence of the
growing complexity of the economy.
",1,0,0,0,0,0
14908,14909,Approximation of full-boundary data from partial-boundary electrode measurements,"  Measurements on a subset of the boundary are common in electrical impedance
tomography, especially any electrode model can be interpreted as a partial
boundary problem. The information obtained is different to full-boundary
measurements as modeled by the ideal continuum model. In this study we discuss
an approach to approximate full-boundary data from partial-boundary
measurements that is based on the knowledge of the involved projections. The
approximate full-boundary data can then be obtained as the solution of a
suitable optimization problem on the coefficients of the Neumann-to-Dirichlet
map. By this procedure we are able to improve the reconstruction quality of
continuum model based algorithms, in particular we present the effectiveness
with a D-bar method. Reconstructions are presented for noisy simulated and real
measurement data.
",0,0,1,0,0,0
15691,15692,Affect Estimation in 3D Space Using Multi-Task Active Learning for Regression,"  Acquisition of labeled training samples for affective computing is usually
costly and time-consuming, as affects are intrinsically subjective, subtle and
uncertain, and hence multiple human assessors are needed to evaluate each
affective sample. Particularly, for affect estimation in the 3D space of
valence, arousal and dominance, each assessor has to perform the evaluations in
three dimensions, which makes the labeling problem even more challenging. Many
sophisticated machine learning approaches have been proposed to reduce the data
labeling requirement in various other domains, but so far few have considered
affective computing. This paper proposes two multi-task active learning for
regression approaches, which select the most beneficial samples to label, by
considering the three affect primitives simultaneously. Experimental results on
the VAM corpus demonstrated that our optimal sample selection approaches can
result in better estimation performance than random selection and several
traditional single-task active learning approaches. Thus, they can help
alleviate the data labeling problem in affective computing, i.e., better
estimation performance can be obtained from fewer labeling queries.
",1,0,0,1,0,0
19650,19651,High-Performance Code Generation though Fusion and Vectorization,"  We present a technique for automatically transforming kernel-based
computations in disparate, nested loops into a fused, vectorized form that can
reduce intermediate storage needs and lead to improved performance on
contemporary hardware.
We introduce representations for the abstract relationships and data
dependencies of kernels in loop nests and algorithms for manipulating them into
more efficient form; we similarly introduce techniques for determining data
access patterns for stencil-like array accesses and show how this can be used
to elide storage and improve vectorization.
We discuss our prototype implementation of these ideas---named HFAV---and its
use of a declarative, inference-based front-end to drive transformations, and
we present results for some prominent codes in HPC.
",1,0,0,0,0,0
19514,19515,"Locally Repairable Codes with Multiple $(r_{i}, δ_{i})$-Localities","  In distributed storage systems, locally repairable codes (LRCs) are
introduced to realize low disk I/O and repair cost. In order to tolerate
multiple node failures, the LRCs with \emph{$(r, \delta)$-locality} are further
proposed. Since hot data is not uncommon in a distributed storage system, both
Zeh \emph{et al.} and Kadhe \emph{et al.} focus on the LRCs with \emph{multiple
localities or unequal localities} (ML-LRCs) recently, which said that the
localities among the code symbols can be different. ML-LRCs are attractive and
useful in reducing repair cost for hot data. In this paper, we generalize the
ML-LRCs to the $(r,\delta)$-locality case of multiple node failures, and define
an LRC with multiple $(r_{i}, \delta_{i})_{i\in [s]}$ localities ($s\ge 2$),
where $r_{1}\leq r_{2}\leq\dots\leq r_{s}$ and
$\delta_{1}\geq\delta_{2}\geq\dots\geq\delta_{s}\geq2$. Such codes ensure that
some hot data could be repaired more quickly and have better failure-tolerance
in certain cases because of relatively smaller $r_{i}$ and larger $\delta_{i}$.
Then, we derive a Singleton-like upper bound on the minimum distance for the
proposed LRCs by employing the regenerating-set technique. Finally, we obtain a
class of explicit and structured constructions of optimal ML-LRCs, and further
extend them to the cases of multiple $(r_{i}, \delta)_{i\in [s]}$ localities.
",1,0,0,0,0,0
17737,17738,Psychophysical laws as reflection of mental space properties,"  The paper is devoted to the relationship between psychophysics and physics of
mind. The basic trends in psychophysics development are briefly discussed with
special attention focused on Teghtsoonian's hypotheses. These hypotheses pose
the concept of the universality of inner psychophysics and enable to speak
about psychological space as an individual object with its own properties.
Turning to the two-component description of human behavior (I. Lubashevsky,
Physics of the Human Mind, Springer, 2017) the notion of mental space is
formulated and human perception of external stimuli is treated as the emergence
of the corresponding images in the mental space. On one hand, these images are
caused by external stimuli and their magnitude bears the information about the
intensity of the corresponding stimuli. On the other hand, the individual
structure of such images as well as their subsistence after emergence is
determined only by the properties of mental space on its own. Finally, the
mental operations of image comparison and their scaling are defined in a way
allowing for the bounded capacity of human cognition. As demonstrated, the
developed theory of stimulus perception is able to explain the basic
regularities of psychophysics, e.g., (i) the regression and range effects
leading to the overestimation of weak stimuli and the underestimation of strong
stimuli, (ii) scalar variability (Weber's and Ekman' laws), and (\textit{iii})
the sequential (memory) effects. As the final result, a solution to the
Fechner-Stevens dilemma is proposed. This solution posits that Fechner's
logarithmic law is not a consequences of Weber's law but stems from the
interplay of uncertainty in evaluating stimulus intensities and the multi-step
scaling required to overcome the stimulus incommensurability.
",0,0,0,0,1,0
7913,7914,On the uniqueness of complete biconservative surfaces in $\mathbb{R}^3$,"  We study the uniqueness of complete biconservative surfaces in the Euclidean
space $\mathbb{R}^3$, and prove that the only complete biconservative regular
surfaces in $\mathbb{R}^3$ are either $CMC$ or certain surfaces of revolution.
In particular, any compact biconservative regular surface in $\mathbb{R}^3$ is
a round sphere.
",0,0,1,0,0,0
1318,1319,Exploiting network topology for large-scale inference of nonlinear reaction models,"  The development of chemical reaction models aids understanding and prediction
in areas ranging from biology to electrochemistry and combustion. A systematic
approach to building reaction network models uses observational data not only
to estimate unknown parameters, but also to learn model structure. Bayesian
inference provides a natural approach to this data-driven construction of
models. Yet traditional Bayesian model inference methodologies that numerically
evaluate the evidence for each model are often infeasible for nonlinear
reaction network inference, as the number of plausible models can be
combinatorially large. Alternative approaches based on model-space sampling can
enable large-scale network inference, but their realization presents many
challenges. In this paper, we present new computational methods that make
large-scale nonlinear network inference tractable. First, we exploit the
topology of networks describing potential interactions among chemical species
to design improved ""between-model"" proposals for reversible-jump Markov chain
Monte Carlo. Second, we introduce a sensitivity-based determination of move
types which, when combined with network-aware proposals, yields significant
additional gains in sampling performance. These algorithms are demonstrated on
inference problems drawn from systems biology, with nonlinear differential
equation models of species interactions.
",1,0,0,1,0,0
9482,9483,A consistent measure of the merger histories of massive galaxies using close-pair statistics I: Major mergers at $z < 3.5$,"  We use a large sample of $\sim 350,000$ galaxies constructed by combining the
UKIDSS UDS, VIDEO/CFHT-LS, UltraVISTA/COSMOS and GAMA survey regions to probe
the major merging histories of massive galaxies ($>10^{10}\ \mathrm{M}_\odot$)
at $0.005 < z < 3.5$. We use a method adapted from that presented in
Lopez-Sanjuan et al. (2014) using the full photometric redshift probability
distributions, to measure pair $\textit{fractions}$ of flux-limited, stellar
mass selected galaxy samples using close-pair statistics. The pair fraction is
found to weakly evolve as $\propto (1+z)^{0.8}$ with no dependence on stellar
mass. We subsequently derive major merger $\textit{rates}$ for galaxies at $>
10^{10}\ \mathrm{M}_\odot$ and at a constant number density of $n > 10^{-4}$
Mpc$^{-3}$, and find rates a factor of 2-3 smaller than previous works,
although this depends strongly on the assumed merger timescale and likelihood
of a close-pair merging. Galaxies undergo approximately 0.5 major mergers at $z
< 3.5$, accruing an additional 1-4 $\times 10^{10}\ \mathrm{M}_\odot$ in the
process. Major merger accretion rate densities of $\sim 2 \times 10^{-4}$
$\mathrm{M}_\odot$ yr$^{-1}$ Mpc$^{-3}$ are found for number density selected
samples, indicating that direct progenitors of local massive
($>10^{11}\mathrm{M}_\odot$) galaxies have experienced a steady supply of
stellar mass via major mergers throughout their evolution. While pair fractions
are found to agree with those predicted by the Henriques et al. (2014)
semi-analytic model, the Illustris hydrodynamical simulation fails to
quantitatively reproduce derived merger rates. Furthermore, we find major
mergers become a comparable source of stellar mass growth compared to
star-formation at $z < 1$, but is 10-100 times smaller than the SFR density at
higher redshifts.
",0,1,0,0,0,0
10638,10639,On the restricted almost unbiased Liu estimator in the Logistic regression model,"  It is known that when the multicollinearity exists in the logistic regression
model, variance of maximum likelihood estimator is unstable. As a remedy, in
the context of biased shrinkage ridge estimation, Chang (2015) introduced an
almost unbiased Liu estimator in the logistic regression model. Making use of
his approach, when some prior knowledge in the form of linear restrictions are
also available, we introduce a restricted almost unbiased Liu estimator in the
logistic regression model. Statistical properties of this newly defined
estimator are derived and some comparison result are also provided in the form
of theorems. A Monte Carlo simulation study along with a real data example are
given to investigate the performance of this estimator.
",0,0,1,1,0,0
9754,9755,Faster and Simpler Distributed Algorithms for Testing and Correcting Graph Properties in the CONGEST-Model,"  In this paper we present distributed testing algorithms of graph properties
in the CONGEST-model [Censor-Hillel et al. 2016]. We present one-sided error
testing algorithms in the general graph model.
We first describe a general procedure for converting $\epsilon$-testers with
a number of rounds $f(D)$, where $D$ denotes the diameter of the graph, to
$O((\log n)/\epsilon)+f((\log n)/\epsilon)$ rounds, where $n$ is the number of
processors of the network. We then apply this procedure to obtain an optimal
tester, in terms of $n$, for testing bipartiteness, whose round complexity is
$O(\epsilon^{-1}\log n)$, which improves over the $poly(\epsilon^{-1} \log
n)$-round algorithm by Censor-Hillel et al. (DISC 2016). Moreover, for
cycle-freeness, we obtain a \emph{corrector} of the graph that locally corrects
the graph so that the corrected graph is acyclic. Note that, unlike a tester, a
corrector needs to mend the graph in many places in the case that the graph is
far from having the property.
In the second part of the paper we design algorithms for testing whether the
network is $H$-free for any connected $H$ of size up to four with round
complexity of $O(\epsilon^{-1})$. This improves over the
$O(\epsilon^{-2})$-round algorithms for testing triangle freeness by
Censor-Hillel et al. (DISC 2016) and for testing excluded graphs of size $4$ by
Fraigniaud et al. (DISC 2016).
In the last part we generalize the global tester by Iwama and Yoshida (ITCS
2014) of testing $k$-path freeness to testing the exclusion of any tree of
order $k$. We then show how to simulate this algorithm in the CONGEST-model in
$O(k^{k^2+1}\cdot\epsilon^{-k})$ rounds.
",1,0,0,0,0,0
17650,17651,Graded components of Local cohomology modules II,"  Let $A$ be a commutative Noetherian ring containing a field $K$ of
characteristic zero and let $R= A[X_1, \ldots, X_m]$. Consider $R$ as standard
graded with $°A=0$ and $°X_i=1$ for all $i$. We present a few results
about the behavior of the graded components of local cohomology modules
$H_I^i(R)$ where $I$ is an arbitrary homogeneous ideal in $R$. We mostly
restrict our attention to the Vanishing, Tameness and Rigidity problems.
",0,0,1,0,0,0
15959,15960,An information theoretic approach to the autoencoder,"  We present a variation of the Autoencoder (AE) that explicitly maximizes the
mutual information between the input data and the hidden representation. The
proposed model, the InfoMax Autoencoder (IMAE), by construction is able to
learn a robust representation and good prototypes of the data. IMAE is compared
both theoretically and then computationally with the state of the art models:
the Denoising and Contractive Autoencoders in the one-hidden layer setting and
the Variational Autoencoder in the multi-layer case. Computational experiments
are performed with the MNIST and Fashion-MNIST datasets and demonstrate
particularly the strong clusterization performance of IMAE.
",1,0,0,1,0,0
19891,19892,On the bottom of spectra under coverings,"  For a Riemannian covering $M_1\to M_0$ of complete Riemannian manifolds with
boundary (possibly empty) and respective fundamental groups
$\Gamma_1\subseteq\Gamma_0$, we show that the bottoms of the spectra of $M_0$
and $M_1$ coincide if the right action of $\Gamma_0$ on
$\Gamma_1\backslash\Gamma_0$ is amenable.
",0,0,1,0,0,0
14565,14566,Magic wavelengths of Ca$^{+}$ ion for linearly and circularly polarized light,"  The dynamic dipole polarizabilities of the low-lying states of Ca$^{+}$ for
linearly and circularly polarized light are calculated by using relativistic
configuration interaction plus core polarization (RCICP) approach. The magic
wavelengths, at which the two levels of the transitions have the same ac Stark
shifts, for $4s$-$4p_{j,m}$ and $4s$-$3d_{j,m}$ magnetic sublevels transitions
are determined. The present magic wavelengths for linearly polarized light
agree with the available results excellently. The polarizability for the
circularly polarized light has the scalar, vector and tensor components. The
dynamic polarizability is different for each of magnetic sublevels of the
atomic state. Additional magic wavelengths have been found for the circularly
polarized light. We recommend that the measurement of the magic wavelength near
850 nm for $4s-4p_{\frac32,m=\pm\frac32,\pm\frac12}$ could be able to determine
the oscillator strength ratio of $f_{4p_{\frac32} \to 3d_{\frac32}}$ and
$f_{4p_{\frac32} \to 3d_{\frac52}}$.
",0,1,0,0,0,0
9543,9544,Exploiting Friction in Torque Controlled Humanoid Robots,"  A common architecture for torque controlled humanoid robots consists in two
nested loops. The outer loop generates desired joint/motor torques, and the
inner loop stabilises these desired values. In doing so, the inner loop usually
compensates for joint friction phenomena, thus removing their inherent
stabilising property that may be also beneficial for high level control
objectives. This paper shows how to exploit friction for joint and task space
control of humanoid robots. Experiments are carried out using the humanoid
robot iCub.
",1,0,0,0,0,0
15163,15164,Thermal Characterization of Microscale Heat Convection under Rare Gas Condition by a Modified Hot Wire Method,"  As power electronics shrinks down to sub-micron scale, the thermal transport
from a solid surface to environment becomes significant. Under circumstances
when the device works in rare gas environment, the scale for thermal transport
is comparable to the mean free path of molecules, and is difficult to
characterize. In this work, we present an experimental study about thermal
transport around a microwire in rare gas environment by using a steady state
hot wire method. Unlike conventional hot wire technique of using transient heat
transfer process, this method considers both the heat conduction along the wire
and convection effect from wire surface to surroundings. Convection heat
transfer coefficient from a platinum wire in diameter 25 um to air is
characterized under different heating power and air pressures to comprehend the
effect of temperature and density of gas molecules. It is observed that
convection heat transfer coefficient varies from 14 Wm-2K-1 at 7 Pa to 629
Wm-2K-1 at atmosphere pressure. In free molecule regime, Nusselt number has a
linear relationship with inverse Knudsen number and the slope of 0.274 is
employed to determined equivalent thermal dissipation boundary as 7.03E10-4 m.
In transition regime, the equivalent thermal dissipation boundary is obtained
as 5.02E10-4 m. Under a constant pressure, convection heat transfer coefficient
decreases with increasing temperature, and this correlation is more sensitive
to larger pressure. This work provides a pathway for studying both heat
conduction and heat convection effect at micro/nanoscale under rare gas
environment, the knowledge of which is essential for regulating heat
dissipation in various industrial applications.
",0,1,0,0,0,0
8856,8857,Self-adjoint approximations of degenerate Schrodinger operator,"  The problem of construction a quantum mechanical evolution for the
Schrodinger equation with a degenerate Hamiltonian which is a symmetric
operator that does not have self-adjoint extensions is considered. Self-adjoint
regularization of the Hamiltonian does not lead to a preserving probability
limiting evolution for vectors from the Hilbert space but it is used to
construct a limiting evolution of states on a C*-algebra of compact operators
and on an abelian subalgebra of operators in the Hilbert space. The limiting
evolution of the states on the abelian algebra can be presented by the Kraus
decomposition with two terms. Both of this terms are corresponded to the
unitary and shift components of Wold's decomposition of isometric semigroup
generated by the degenerate Hamiltonian. Properties of the limiting evolution
of the states on the C*-algebras are investigated and it is shown that pure
states could evolve into mixed states.
",0,0,1,0,0,0
11287,11288,Dirac nodal lines and induced spin Hall effect in metallic rutile oxides,"  We have found Dirac nodal lines (DNLs) in the band structures of metallic
rutile oxides IrO$_2$, OsO$_2$, and RuO$_2$ and revealed a large spin Hall
conductivity contributed by these nodal lines, which explains a strong spin
Hall effect (SHE) of IrO$_2$ discovered recently. Two types of DNLs exist. The
first type forms DNL networks that extend in the whole Brillouin zone and
appears only in the absence of spin-orbit coupling (SOC), which induces surface
states on the boundary. Because of SOC-induced band anti-crossing, a large
intrinsic SHE can be realized in these compounds. The second type appears at
the Brillouin zone edges and is stable against SOC because of the protection of
nonsymmorphic symmetry. Besides reporting new DNL materials, our work reveals
the general relationship between DNLs and the SHE, indicating a way to apply
Dirac nodal materials for spintronics.
",0,1,0,0,0,0
5075,5076,Impact of the positive ion current on large size neutrino detectors and delayed photon emission,"  Given their small mobility coefficient in liquid argon with respect to the
electrons, the ions spend a considerably longer time in the active volume. We
studied the effects of the positive ion current in a liquid argon time
projection chamber, in the context of massive argon experiments for neutrino
physics. The constant recombination between free ions and electrons produces a
quenching of the charge signal and a constant emission of photons, uncorrelated
in time and space to the physical interactions. The predictions evidence some
potential concerns for multi-ton argon detectors, particularly when operated on
surface
",0,1,0,0,0,0
12594,12595,Talking Open Data,"  Enticing users into exploring Open Data remains an important challenge for
the whole Open Data paradigm. Standard stock interfaces often used by Open Data
portals are anything but inspiring even for tech-savvy users, let alone those
without an articulated interest in data science. To address a broader range of
citizens, we designed an open data search interface supporting natural language
interactions via popular platforms like Facebook and Skype. Our data-aware
chatbot answers search requests and suggests relevant open datasets, bringing
fun factor and a potential of viral dissemination into Open Data exploration.
The current system prototype is available for Facebook
(this https URL) and Skype
(this https URL) users.
",1,0,0,0,0,0
19716,19717,Unstable Footwear as a Speed-Dependent Noise-Based Training Gear to Exercise Inverted Pendulum Motion During Walking,"  Previous research on unstable footwear has suggested that it may induce
plantar mechanical noise during walking. The purpose of this study was to
explore whether unstable footwear could be considered as a noise-based training
gear to exercise body center of mass (CoM) motion during walking or not. Ground
reaction forces were collected among 24 healthy young women walking at speeds
between 3 and 6 km h-1 with control running shoes and unstable rocker-bottom
shoes. The external mechanical work, the recovery of mechanical energy of the
CoM during and within the step cycles, and the phase shift between potential
and kinetic energy curves of the CoM were computed. Our findings support the
idea that unstable rocker-bottom footwear could serve as a speed-dependent
noise- based training gear to exercise CoM motion during walking. At slow
speed, it acts as a stochastic resonance or facilitator, whereas at brisk speed
it acts as a constraint.
",0,1,0,0,0,0
2241,2242,Defining Equations of Nilpotent Orbits for Borel Subgroups of Modality Zero in Type $A_{n}$,"  Let $G$ be a quasi-simple algebraic group defined over an algebraically
closed field $k$ and $B$ a Borel subgroup of $G$ acting on the nilradical
$\mathfrak{n}$ of its Lie algebra $\mathfrak{b}$ via the Adjoint
representation. It is known that $B$ has only finitely many orbits in only five
cases: when $G$ is of type $A_{n}$ for $n \leq 4$, and when $G$ is type
$B_{2}$. In this paper, we elaborate on this work in the case when $G =SL_{n
+1}(k)$ (type $A_{n})$, for $n \leq 4$, by finding the polynomial defining
equations of each orbit. Consequences of these equations include the dimension
of the orbits and the closure ordering on the set of orbits, although these
facts are already known. The other case, when $G$ is type $B_{2}$, can be
approached the same way and is treated in a separate paper, where we believe
the determination of the closure order is new.
",0,0,1,0,0,0
1478,1479,GANs for Biological Image Synthesis,"  In this paper, we propose a novel application of Generative Adversarial
Networks (GAN) to the synthesis of cells imaged by fluorescence microscopy.
Compared to natural images, cells tend to have a simpler and more geometric
global structure that facilitates image generation. However, the correlation
between the spatial pattern of different fluorescent proteins reflects
important biological functions, and synthesized images have to capture these
relationships to be relevant for biological applications. We adapt GANs to the
task at hand and propose new models with casual dependencies between image
channels that can generate multi-channel images, which would be impossible to
obtain experimentally. We evaluate our approach using two independent
techniques and compare it against sensible baselines. Finally, we demonstrate
that by interpolating across the latent space we can mimic the known changes in
protein localization that occur through time during the cell cycle, allowing us
to predict temporal evolution from static images.
",1,0,0,1,0,0
3155,3156,Learning from various labeling strategies for suicide-related messages on social media: An experimental study,"  Suicide is an important but often misunderstood problem, one that researchers
are now seeking to better understand through social media. Due in large part to
the fuzzy nature of what constitutes suicidal risks, most supervised approaches
for learning to automatically detect suicide-related activity in social media
require a great deal of human labor to train. However, humans themselves have
diverse or conflicting views on what constitutes suicidal thoughts. So how to
obtain reliable gold standard labels is fundamentally challenging and, we
hypothesize, depends largely on what is asked of the annotators and what slice
of the data they label. We conducted multiple rounds of data labeling and
collected annotations from crowdsourcing workers and domain experts. We
aggregated the resulting labels in various ways to train a series of supervised
models. Our preliminary evaluations show that using unanimously agreed labels
from multiple annotators is helpful to achieve robust machine models.
",1,0,0,0,0,0
9623,9624,Frank-Wolfe Optimization for Symmetric-NMF under Simplicial Constraint,"  Symmetric nonnegative matrix factorization has found abundant applications in
various domains by providing a symmetric low-rank decomposition of nonnegative
matrices. In this paper we propose a Frank-Wolfe (FW) solver to optimize the
symmetric nonnegative matrix factorization problem under a simplicial
constraint, which has recently been proposed for probabilistic clustering.
Compared with existing solutions, this algorithm is simple to implement, and
has no hyperparameters to be tuned. Building on the recent advances of FW
algorithms in nonconvex optimization, we prove an $O(1/\varepsilon^2)$
convergence rate to $\varepsilon$-approximate KKT points, via a tight bound
$\Theta(n^2)$ on the curvature constant, which matches the best known result in
unconstrained nonconvex setting using gradient methods. Numerical results
demonstrate the effectiveness of our algorithm. As a side contribution, we
construct a simple nonsmooth convex problem where the FW algorithm fails to
converge to the optimum. This result raises an interesting question about
necessary conditions of the success of the FW algorithm on convex problems.
",1,0,1,1,0,0
9732,9733,Connectivity Properties of Factorization Posets in Generated Groups,"  We consider three notions of connectivity and their interactions in partially
ordered sets coming from reduced factorizations of an element in a generated
group. While one form of connectivity essentially reflects the connectivity of
the poset diagram, the other two are a bit more involved: Hurwitz-connectivity
has its origins in algebraic geometry, and shellability in topology. We propose
a framework to study these connectivity properties in a uniform way. Our main
tool is a certain total order of the generators that is compatible with the
chosen element.
",0,0,1,0,0,0
18688,18689,Machine learning quantum mechanics: solving quantum mechanics problems using radial basis function networks,"  Inspired by the recent work of Carleo and Troyer[1], we apply machine
learning methods to quantum mechanics in this article. The radial basis
function network in a discrete basis is used as the variational wavefunction
for the ground state of a quantum system. Variational Monte Carlo(VMC)
calculations are carried out for some simple Hamiltonians. The results are in
good agreements with theoretical values. The smallest eigenvalue of a Hermitian
matrix can also be acquired using VMC calculations. Our results demonstrate
that machine learning techniques are capable of solving quantum mechanical
problems.
",0,1,0,0,0,0
12906,12907,A Physics Tragedy,"  The measurement problem and three other vexing experiments in quantum physics
are described. It is shown how Quantum Field Theory, as formulated by Julian
Schwinger, provides simple solutions for all four experiments. It is also shown
how this theory resolves many other problems of Quantum Mechanics and
Relativity, including a new and simple derivation of E = mc2.
",0,1,0,0,0,0
62,63,Superconducting properties of Cu intercalated Bi$_2$Se$_3$ studied by Muon Spin Spectroscopy,"  We present muon spin rotation measurements on superconducting Cu intercalated
Bi$_2$Se$_3$, which was suggested as a realization of a topological
superconductor. We observe a clear evidence of the superconducting transition
below 4 K, where the width of magnetic field distribution increases as the
temperature is decreased. The measured broadening at mK temperatures suggests a
large London penetration depth in the $ab$ plane ($\lambda_{\mathrm{eff}}\sim
1.6$ $\mathrm{\mu}$m). We show that the temperature dependence of this
broadening follows the BCS prediction, but could be consistent with several gap
symmetries.
",0,1,0,0,0,0
15902,15903,SafetyNets: Verifiable Execution of Deep Neural Networks on an Untrusted Cloud,"  Inference using deep neural networks is often outsourced to the cloud since
it is a computationally demanding task. However, this raises a fundamental
issue of trust. How can a client be sure that the cloud has performed inference
correctly? A lazy cloud provider might use a simpler but less accurate model to
reduce its own computational load, or worse, maliciously modify the inference
results sent to the client. We propose SafetyNets, a framework that enables an
untrusted server (the cloud) to provide a client with a short mathematical
proof of the correctness of inference tasks that they perform on behalf of the
client. Specifically, SafetyNets develops and implements a specialized
interactive proof (IP) protocol for verifiable execution of a class of deep
neural networks, i.e., those that can be represented as arithmetic circuits.
Our empirical results on three- and four-layer deep neural networks demonstrate
the run-time costs of SafetyNets for both the client and server are low.
SafetyNets detects any incorrect computations of the neural network by the
untrusted server with high probability, while achieving state-of-the-art
accuracy on the MNIST digit recognition (99.4%) and TIMIT speech recognition
tasks (75.22%).
",1,0,0,0,0,0
12581,12582,A Language for Probabilistically Oblivious Computation,"  An oblivious computation is one that is free of direct and indirect
information leaks, e.g., due to observable differences in timing and memory
access patterns. This paper presents Lobliv, a core language whose type system
enforces obliviousness. Prior work on type-enforced oblivious computation has
focused on deterministic programs. Lobliv is new in its consideration of
programs that implement probabilistic algorithms, such as those involved in
cryptography. Lobliv employs a substructural type system and a novel notion of
probability region to ensure that information is not leaked via the
distribution of visible events. The use of regions was motivated by a source of
unsoundness that we discovered in the type system of ObliVM, a language for
implementing state of the art oblivious algorithms and data structures. We
prove that Lobliv's type system enforces obliviousness and show that it is
nevertheless powerful enough to check state-of-the-art, efficient oblivious
data structures, such as stacks and queues, and even tree-based oblivious RAMs.
",1,0,0,0,0,0
12856,12857,Defining and estimating stochastic rate change in a dynamic general insurance portfolio,"  Rate change calculations in the literature involve deterministic methods that
measure the change in premium for a given policy. The definition of rate change
as a statistical parameter is proposed to address the stochastic nature of the
premium charged for a policy. It promotes the idea that rate change is a
property of an asymptotic population to be estimated, not just a property to
measure or monitor in the sample of observed policies that are written. Various
models and techniques are given for estimating this stochastic rate change and
quantifying the uncertainty in the estimates. The use of matched sampling is
emphasized for rate change estimation, as it adjusts for changes in policy
characteristics by directly searching for similar policies across policy years.
This avoids any of the assumptions and recipes that are required to re-rate
policies in years where they were not written, as is common with deterministic
methods. Such procedures can be subjective or implausible if the structure of
rating algorithms change or there are complex and heterogeneous exposure bases
and coverages. The methods discussed are applied to a motor premium database.
The application includes the use of a genetic algorithm with parallel
computations to automatically optimize the matched sampling.
",0,0,0,0,0,1
16024,16025,Concentration of quantum states from quantum functional and transportation cost inequalities,"  Quantum functional inequalities (e.g. the logarithmic Sobolev- and Poincaré
inequalities) have found widespread application in the study of the behavior of
primitive quantum Markov semigroups. The classical counterparts of these
inequalities are related to each other via a so-called transportation cost
inequality of order 2 (TC2). The latter inequality relies on the notion of a
metric on the set of probability distributions called the Wasserstein distance
of order 2. (TC2) in turn implies a transportation cost inequality of order 1
(TC1). In this paper, we introduce quantum generalizations of the inequalities
(TC1) and (TC2), making use of appropriate quantum versions of the Wasserstein
distances, one recently defined by Carlen and Maas and the other defined by us.
We establish that these inequalities are related to each other, and to the
quantum modified logarithmic Sobolev- and Poincaré inequalities, as in the
classical case. We also show that these inequalities imply certain
concentration-type results for the invariant state of the underlying semigroup.
We consider the example of the depolarizing semigroup to derive concentration
inequalities for any finite dimensional full-rank quantum state. These
inequalities are then applied to derive upper bounds on the error probabilities
occurring in the setting of finite blocklength quantum parameter estimation.
",0,0,1,0,0,0
12950,12951,Scikit-Multiflow: A Multi-output Streaming Framework,"  Scikit-multiflow is a multi-output/multi-label and stream data mining
framework for the Python programming language. Conceived to serve as a platform
to encourage democratization of stream learning research, it provides multiple
state of the art methods for stream learning, stream generators and evaluators.
scikit-multiflow builds upon popular open source frameworks including
scikit-learn, MOA and MEKA. Development follows the FOSS principles and quality
is enforced by complying with PEP8 guidelines and using continuous integration
and automatic testing. The source code is publicly available at
this https URL.
",0,0,0,1,0,0
20725,20726,The Impact of Antenna Height Difference on the Performance of Downlink Cellular Networks,"  Capable of significantly reducing cell size and enhancing spatial reuse,
network densification is shown to be one of the most dominant approaches to
expand network capacity. Due to the scarcity of available spectrum resources,
nevertheless, the over-deployment of network infrastructures, e.g., cellular
base stations (BSs), would strengthen the inter-cell interference as well, thus
in turn deteriorating the system performance. On this account, we investigate
the performance of downlink cellular networks in terms of user coverage
probability (CP) and network spatial throughput (ST), aiming to shed light on
the limitation of network densification. Notably, it is shown that both CP and
ST would be degraded and even diminish to be zero when BS density is
sufficiently large, provided that practical antenna height difference (AHD)
between BSs and users is involved to characterize pathloss. Moreover, the
results also reveal that the increase of network ST is at the expense of the
degradation of CP. Therefore, to balance the tradeoff between user and network
performance, we further study the critical density, under which ST could be
maximized under the CP constraint. Through a special case study, it follows
that the critical density is inversely proportional to the square of AHD. The
results in this work could provide helpful guideline towards the application of
network densification in the next-generation wireless networks.
",1,0,0,0,0,0
9307,9308,Improving brain computer interface performance by data augmentation with conditional Deep Convolutional Generative Adversarial Networks,"  One of the big restrictions in brain computer interface field is the very
limited training samples, it is difficult to build a reliable and usable system
with such limited data. Inspired by generative adversarial networks, we propose
a conditional Deep Convolutional Generative Adversarial (cDCGAN) Networks
method to generate more artificial EEG signal automatically for data
augmentation to improve the performance of convolutional neural networks in
brain computer interface field and overcome the small training dataset
problems. We evaluate the proposed cDCGAN method on BCI competition dataset of
motor imagery. The results show that the generated artificial EEG data from
Gaussian noise can learn the features from raw EEG data and has no less than
the classification accuracy of raw EEG data in the testing dataset. Also by
using generated artificial data can effectively improve classification accuracy
at the same model with limited training data.
",0,0,0,1,1,0
19060,19061,Frequency truncated discrete-time system norm,"  Multirate digital signal processing and model reduction applications require
computation of the frequency truncated norm of a discrete-time system. This
paper explains how to compute the frequency truncated norm of a discrete-time
system. To this end, a much-generalized problem of integrating a transfer
function of a discrete-time system given in the descriptor form over an
interval of limited frequencies is also discussed along with its computation.
",1,0,0,0,0,0
18140,18141,Variation of field enhancement factor near the emitter tip,"  The field enhancement factor at the emitter tip and its variation in a close
neighbourhood determines the emitter current in a Fowler-Nordheim like
formulation. For an axially symmetric emitter with a smooth tip, it is shown
that the variation can be accounted by a $\cos{\tilde{\theta}}$ factor in
appropriately defined normalized co-ordinates. This is shown analytically for a
hemi-ellipsoidal emitter and confirmed numerically for other emitter shapes
with locally quadratic tips.
",0,1,0,0,0,0
19228,19229,Nonconvex One-bit Single-label Multi-label Learning,"  We study an extreme scenario in multi-label learning where each training
instance is endowed with a single one-bit label out of multiple labels. We
formulate this problem as a non-trivial special case of one-bit rank-one matrix
sensing and develop an efficient non-convex algorithm based on alternating
power iteration. The proposed algorithm is able to recover the underlying
low-rank matrix model with linear convergence. For a rank-$k$ model with $d_1$
features and $d_2$ classes, the proposed algorithm achieves $O(\epsilon)$
recovery error after retrieving $O(k^{1.5}d_1 d_2/\epsilon)$ one-bit labels
within $O(kd)$ memory. Our bound is nearly optimal in the order of
$O(1/\epsilon)$. This significantly improves the state-of-the-art sampling
complexity of one-bit multi-label learning. We perform experiments to verify
our theory and evaluate the performance of the proposed algorithm.
",1,0,0,1,0,0
8012,8013,Bio-Inspired Multi-Layer Spiking Neural Network Extracts Discriminative Features from Speech Signals,"  Spiking neural networks (SNNs) enable power-efficient implementations due to
their sparse, spike-based coding scheme. This paper develops a bio-inspired SNN
that uses unsupervised learning to extract discriminative features from speech
signals, which can subsequently be used in a classifier. The architecture
consists of a spiking convolutional/pooling layer followed by a fully connected
spiking layer for feature discovery. The convolutional layer of leaky,
integrate-and-fire (LIF) neurons represents primary acoustic features. The
fully connected layer is equipped with a probabilistic spike-timing-dependent
plasticity learning rule. This layer represents the discriminative features
through probabilistic, LIF neurons. To assess the discriminative power of the
learned features, they are used in a hidden Markov model (HMM) for spoken digit
recognition. The experimental results show performance above 96% that compares
favorably with popular statistical feature extraction methods. Our results
provide a novel demonstration of unsupervised feature acquisition in an SNN.
",1,0,0,0,0,0
11369,11370,How constant shifts affect the zeros of certain rational harmonic functions,"  We study the effect of constant shifts on the zeros of rational harmomic
functions $f(z) = r(z) - \conj{z}$. In particular, we characterize how shifting
through the caustics of $f$ changes the number of zeros and their respective
orientations. This also yields insight into the nature of the singular zeros of
$f$. Our results have applications in gravitational lensing theory, where
certain such functions $f$ represent gravitational point-mass lenses, and a
constant shift can be interpreted as the position of the light source of the
lens.
",0,1,1,0,0,0
8472,8473,Equilibrium distributions and discrete Schur-constant models,"  This paper introduces Schur-constant equilibrium distribution models of
dimension n for arithmetic non-negative random variables. Such a model is
defined through the (several orders) equilibrium distributions of a univariate
survival function. First, the bivariate case is considered and analyzed in
depth, stressing the main characteristics of the Poisson case. The analysis is
then extended to the multivariate case. Several properties are derived,
including the implicit correlation and the distribution of the sum.
",0,0,0,1,0,0
10260,10261,Thermal fracturing on comets. Applications to 67P/Churyumov-Gerasimenko,"  We simulate the stresses induced by temperature changes in a putative hard
layer near the surface of comet 67P/Churyumov--Gerasimenko with a
thermo-viscoelastic model. Such a layer could be formed by the recondensation
or sintering of water ice (and dust grains), as suggested by laboratory
experiments and computer simulations, and would explain the high compressive
strength encountered by experiments on board the Philae lander. Changes in
temperature from seasonal insolation variation penetrate into the comet's
surface to depths controlled by the thermal inertia, causing the material to
expand and contract. Modelling this with a Maxwellian viscoelastic response on
a spherical nucleus, we show that a hard, icy layer with similar properties to
Martian permafrost will experience high stresses: up to tens of MPa, which
exceed its material strength (a few MPa), down to depths of centimetres to a
metre. The stress distribution with latitude is confirmed qualitatively when
taking into account the comet's complex shape but neglecting thermal inertia.
Stress is found to be comparable to the material strength everywhere for
sufficient thermal inertia ($\gtrsim50$ J m$^{-2}$ K$^{-1}$ s$^{-1/2}$) and ice
content ($\gtrsim 45\%$ at the equator). In this case, stresses penetrate to a
typical depth of $\sim0.25$ m, consistent with the detection of metre-scale
thermal contraction crack polygons all over the comet. Thermal fracturing may
be an important erosion process on cometary surfaces which breaks down material
and weakens cliffs.
",0,1,0,0,0,0
18455,18456,Hyperfine Structure of the $B^3Π_1$ State and Predictions of Optical Cycling Behavior in the $X\rightarrow B$ transition of TlF,"  The rotational and hyperfine spectrum of the $X^1\Sigma^+ \rightarrow
B^3\Pi_1$ transition in TlF molecules was measured using laser-induced
fluorescence from both a thermal and a cryogenic molecular beam. Rotational and
hyperfine constants for the $B$ state are obtained. The large magnetic
hyperfine interaction of the Tl nuclear spin leads to significant mixing of the
lowest $B$ state rotational levels. Updated, more precise measurements of the
$B\rightarrow X$ vibrational branching fractions are also presented. The
combined rovibrational branching fractions allow for the prediction of the
number of photons that can be scattered in a given TlF optical cycling scheme.
",0,1,0,0,0,0
13653,13654,"Diffeological, Frölicher, and Differential Spaces","  Differential calculus on Euclidean spaces has many generalisations. In
particular, on a set $X$, a diffeological structure is given by maps from open
subsets of Euclidean spaces to $X$, a differential structure is given by maps
from $X$ to $\mathbb{R}$, and a Frölicher structure is given by maps from
$\mathbb{R}$ to $X$ as well as maps from $X$ to $\mathbb{R}$. We illustrate the
relations between these structures through examples.
",0,0,1,0,0,0
18367,18368,A one-dimensional mathematical model of collecting lymphatics coupled with an electro-fluid-mechanical contraction model and valve dynamics,"  We propose a one-dimensional model for collecting lymphatics coupled with a
novel Electro-Fluid-Mechanical Contraction (EFMC) model for dynamical
contractions, based on a modified FitzHugh-Nagumo model for action potentials.
The one-dimensional model for a compliant lymphatic vessel is a set of
hyperbolic Partial Differential Equations (PDEs). The EFMC model combines the
electrical activity of lymphangions (action potentials) with fluid-mechanical
feedback (stretch of the lymphatic wall and wall shear stress) and the
mechanical variation of the lymphatic wall properties (contractions). The EFMC
model is governed by four Ordinary Differential Equations (ODEs) and
phenomenologically relies on: (1) environmental calcium influx, (2)
stretch-activated calcium influx, and (3) contraction inhibitions induced by
wall shear stresses. We carried out a complete mathematical analysis of the
stability of the stationary state of the EFMC model. Overall, the EFMC model
allows imitating the influence of pressure and wall shear stress on the
frequency of contractions observed experimentally. Lymphatic valves are
modelled using a well-established lumped-parameter model which allows
simulating stenotic and regurgitant valves. We analysed several lymphodynamical
indexes of a single lymphangion for a wide range of upstream and downstream
pressure combinations. Stenotic and regurgitant valves were modelled, and their
effects are here quantified. Results for stenotic valves showed in the
downstream lymphangion that for low frequencies of contractions the Calculated
Pump Flow (CPF) index remained almost unaltered, while for high frequencies the
CPF dramatically decreased depending on the severity of the stenosis (up to 93%
for a severe stenosis). Results for incompetent valves showed that the net flow
during a lymphatic cycle tends to zero as the degree of incompetence increases.
",0,1,1,0,0,0
5836,5837,Normal forms of dispersive scalar Poisson brackets with two independent variables,"  We classify the dispersive Poisson brackets with one dependent variable and
two independent variables, with leading order of hydrodynamic type, up to Miura
transformations. We show that, in contrast to the case of a single independent
variable for which a well known triviality result exists, the Miura equivalence
classes are parametrised by an infinite number of constants, which we call
numerical invariants of the brackets. We obtain explicit formulas for the first
few numerical invariants.
",0,1,1,0,0,0
17026,17027,Indefinite boundary value problems on graphs,"  We consider the spectral structure of indefinite second order boundary-value
problems on graphs. A variational formulation for such boundary-value problems
on graphs is given and we obtain both full and half-range completeness results.
This leads to a max-min principle and as a consequence we can formulate an
analogue of Dirichlet-Neumann bracketing and this in turn gives rise to
asymptotic approximations for the eigenvalues.
",0,0,1,0,0,0
5917,5918,Families of Thue equations associated with a rank one subgroup of the unit group of a number field,"  Twisting a binary form $F_0(X,Y)\in{\mathbb{Z}}[X,Y]$ of degree $d\ge 3$ by
powers $\upsilon^a$ ($a\in{\mathbb{Z}}$) of an algebraic unit $\upsilon$ gives
rise to a binary form $F_a(X,Y)\in{\mathbb{Z}}[X,Y]$. More precisely, when $K$
is a number field of degree $d$, $\sigma_1,\sigma_2,\dots,\sigma_d$ the
embeddings of $K$ into $\mathbb{C}$, $\alpha$ a nonzero element in $K$,
$a_0\in{\mathbb{Z}}$, $a_0>0$ and $$ F_0(X,Y)=a_0\displaystyle\prod_{i=1}^d
(X-\sigma_i(\alpha) Y), $$ then for $a\in{\mathbb{Z}}$ we set $$
F_a(X,Y)=\displaystyle a_0\prod_{i=1}^d (X-\sigma_i(\alpha\upsilon^a) Y). $$
Given $m\ge 0$, our main result is an effective upper bound for the solutions
$(x,y,a)\in{\mathbb{Z}}^3$ of the Diophantine inequalities $$ 0<|F_a(x,y)|\le m
$$ for which $xy\not=0$ and ${\mathbb{Q}}(\alpha \upsilon^a)=K$. Our estimate
involves an effectively computable constant depending only on $d$; it is
explicit in terms of $m$, in terms of the heights of $F_0$ and of $\upsilon$,
and in terms of the regulator of the number field $K$.
",0,0,1,0,0,0
6859,6860,Second differentials in the Quillen spectral sequence,"  For an algebraic variety $X$ we introduce generalized first Chern classes,
which are defined for coherent sheaves on $X$ with support in codimension $p$
and take values in $CH^p(X)$. We use them to provide an explicit formula for
the differentials ${d_2^p: E_2^{p,-p-1} \to E_2^{p+2, -p-2} \cong CH^{p+2}(X)}$
in the Quillen spectral sequence.
",0,0,1,0,0,0
4047,4048,Pressure Drop and Flow development in the Entrance Region of Micro-Channels with Second Order Slip Boundary Conditions and the Requirement for Development Length,"  In the present investigation, the development of axial velocity profile, the
requirement for development length ($L^*_{fd}=L/D_{h}$) and the pressure drop
in the entrance region of circular and parallel plate micro-channels have been
critically analysed for a large range of operating conditions ($10^{-2}\le
Re\le 10^{4}$, $10^{-4}\le Kn\le 0.2$ and $0\le C_2\le 0.5$). For this purpose,
the conventional Navier-Stokes equations have been numerically solved using the
finite volume method on non-staggered grid, while employing the second-order
velocity slip condition at the wall with $C_1=1$. The results indicate that
although the magnitude of local velocity slip at the wall is always greater
than that for the fully-developed section, the local wall shear stress,
particularly for higher $Kn$ and $C_2$, could be considerably lower than its
fully-developed value. This effect, which is more prominent for lower $Re$,
significantly affects the local and the fully-developed incremental pressure
drop number $K(x)$ and $K_{fd}$, respectively. As a result, depending upon the
operating condition, $K_{fd}$, as well as $K(x)$, could assume negative values.
This never reported observation implies that in the presence of enhanced
velocity slip at the wall, the pressure gradient in the developing region could
even be less than that in the fully-developed section. From simulated data, it
has been observed that both $L^*_{fd}$ and $K_{fd}$ are characterised by the
low and the high $Re$ asymptotes, using which, extremely accurate correlations
for them have been proposed for both geometries. Although owing to the complex
nature, no correlation could be derived for $K(x)$ and an exact knowledge of
$K(x)$ is necessary for evaluating the actual pressure drop for a duct length
$L^*<L^*_{fd}$, a method has been proposed that provides a conservative
estimate of the pressure drop for both $K_{fd}>0$ and $K_{fd}\le0$.
",0,1,0,0,0,0
601,602,Subset Labeled LDA for Large-Scale Multi-Label Classification,"  Labeled Latent Dirichlet Allocation (LLDA) is an extension of the standard
unsupervised Latent Dirichlet Allocation (LDA) algorithm, to address
multi-label learning tasks. Previous work has shown it to perform in par with
other state-of-the-art multi-label methods. Nonetheless, with increasing label
sets sizes LLDA encounters scalability issues. In this work, we introduce
Subset LLDA, a simple variant of the standard LLDA algorithm, that not only can
effectively scale up to problems with hundreds of thousands of labels but also
improves over the LLDA state-of-the-art. We conduct extensive experiments on
eight data sets, with label sets sizes ranging from hundreds to hundreds of
thousands, comparing our proposed algorithm with the previously proposed LLDA
algorithms (Prior--LDA, Dep--LDA), as well as the state of the art in extreme
multi-label classification. The results show a steady advantage of our method
over the other LLDA algorithms and competitive results compared to the extreme
multi-label classification algorithms.
",0,0,0,1,0,0
18614,18615,Sharp off-diagonal weighted norm estimates for the Bergman projection,"  We prove that for $1<p\le q<\infty$, $qp\geq {p'}^2$ or $p'q'\geq q^2$,
$\frac{1}{p}+\frac{1}{p'}=\frac{1}{q}+\frac{1}{q'}=1$, $$\|\omega
P_\alpha(f)\|_{L^p(\mathcal{H},y^{\alpha+(2+\alpha)(\frac{q}{p}-1)}dxdy)}\le
C_{p,q,\alpha}[\omega]_{B_{p,q,\alpha}}^{(\frac{1}{p'}+\frac{1}{q})\max\{1,\frac{p'}{q}\}}\|\omega
f\|_{L^p(\mathcal{H},y^{\alpha}dxdy)}$$ where $P_\alpha$ is the weighted
Bergman projection of the upper-half plane $\mathcal{H}$, and
$$[\omega]_{B_{p,q,\alpha}}:=\sup_{I\subset
\mathbb{R}}\left(\frac{1}{|I|^{2+\alpha}}\int_{Q_I}\omega^{q}dV_\alpha\right)\left(\frac{1}{|I|^{2+\alpha}}\int_{Q_I}\omega^{-p'}dV_\alpha\right)^{\frac{q}{p'}},$$
with $Q_I=\{z=x+iy\in \mathbb{C}: x\in I, 0<y<|I|\}$.
",0,0,1,0,0,0
7040,7041,Multi-Dialect Speech Recognition With A Single Sequence-To-Sequence Model,"  Sequence-to-sequence models provide a simple and elegant solution for
building speech recognition systems by folding separate components of a typical
system, namely acoustic (AM), pronunciation (PM) and language (LM) models into
a single neural network. In this work, we look at one such sequence-to-sequence
model, namely listen, attend and spell (LAS), and explore the possibility of
training a single model to serve different English dialects, which simplifies
the process of training multi-dialect systems without the need for separate AM,
PM and LMs for each dialect. We show that simply pooling the data from all
dialects into one LAS model falls behind the performance of a model fine-tuned
on each dialect. We then look at incorporating dialect-specific information
into the model, both by modifying the training targets by inserting the dialect
symbol at the end of the original grapheme sequence and also feeding a 1-hot
representation of the dialect information into all layers of the model.
Experimental results on seven English dialects show that our proposed system is
effective in modeling dialect variations within a single LAS model,
outperforming a LAS model trained individually on each of the seven dialects by
3.1 ~ 16.5% relative.
",1,0,0,0,0,0
5297,5298,A New Compton-thick AGN in our Cosmic Backyard: Unveiling the Buried Nucleus in NGC 1448 with NuSTAR,"  NGC 1448 is one of the nearest luminous galaxies ($L_{8-1000\mu m} >$ 10$^{9}
L_{\odot}$) to ours ($z$ $=$ 0.00390), and yet the active galactic nucleus
(AGN) it hosts was only recently discovered, in 2009. In this paper, we present
an analysis of the nuclear source across three wavebands: mid-infrared (MIR)
continuum, optical, and X-rays. We observed the source with the Nuclear
Spectroscopic Telescope Array (NuSTAR), and combined this data with archival
Chandra data to perform broadband X-ray spectral fitting ($\approx$0.5-40 keV)
of the AGN for the first time. Our X-ray spectral analysis reveals that the AGN
is buried under a Compton-thick (CT) column of obscuring gas along our
line-of-sight, with a column density of $N_{\rm H}$(los) $\gtrsim$ 2.5 $\times$
10$^{24}$ cm$^{-2}$. The best-fitting torus models measured an intrinsic 2-10
keV luminosity of $L_{2-10\rm{,int}}$ $=$ (3.5-7.6) $\times$ 10$^{40}$ erg
s$^{-1}$, making NGC 1448 one of the lowest luminosity CTAGNs known. In
addition to the NuSTAR observation, we also performed optical spectroscopy for
the nucleus in this edge-on galaxy using the European Southern Observatory New
Technology Telescope. We re-classify the optical nuclear spectrum as a Seyfert
on the basis of the Baldwin-Philips-Terlevich diagnostic diagrams, thus
identifying the AGN at optical wavelengths for the first time. We also present
high spatial resolution MIR observations of NGC 1448 with Gemini/T-ReCS, in
which a compact nucleus is clearly detected. The absorption-corrected 2-10 keV
luminosity measured from our X-ray spectral analysis agrees with that predicted
from the optical [OIII]$\lambda$5007\AA\ emission line and the MIR 12$\mu$m
continuum, further supporting the CT nature of the AGN.
",0,1,0,0,0,0
7423,7424,LinXGBoost: Extension of XGBoost to Generalized Local Linear Models,"  XGBoost is often presented as the algorithm that wins every ML competition.
Surprisingly, this is true even though predictions are piecewise constant. This
might be justified in high dimensional input spaces, but when the number of
features is low, a piecewise linear model is likely to perform better. XGBoost
was extended into LinXGBoost that stores at each leaf a linear model. This
extension, equivalent to piecewise regularized least-squares, is particularly
attractive for regression of functions that exhibits jumps or discontinuities.
Those functions are notoriously hard to regress. Our extension is compared to
the vanilla XGBoost and Random Forest in experiments on both synthetic and
real-world data sets.
",1,0,0,1,0,0
16943,16944,Categorical Probabilistic Theories,"  We present a simple categorical framework for the treatment of probabilistic
theories, with the aim of reconciling the fields of Categorical Quantum
Mechanics (CQM) and Operational Probabilistic Theories (OPTs). In recent years,
both CQM and OPTs have found successful application to a number of areas in
quantum foundations and information theory: they present many similarities,
both in spirit and in formalism, but they remain separated by a number of
subtle yet important differences. We attempt to bridge this gap, by adopting a
minimal number of operationally motivated axioms which provide clean
categorical foundations, in the style of CQM, for the treatment of the problems
that OPTs are concerned with.
",0,0,1,0,0,0
10112,10113,Opinion formation in a locally interacting community with recommender,"  We present a user of model interaction based on the physics of kinetic
exchange, and extend it to individuals placed in a grid with local interaction.
We show with numerical analysis and partial analytical results that the
critical symmetry breaking transitions and percolation effects typical of the
full interaction model do not take place if the range of interaction is
limited, allowing for the co-existence of majorty and minority opinions in the
same community.
We then introduce a peer recommender system in the model, showing that, even
with very local iteraction and a small probability of appeal to the
recommender, its presence is sufficient to make both symmetry breaking and
percolation reappear. This seems to indicate that one effect of a
recommendation system is to uniform the opinions of a community, reducing
minority opinions or making them disappear. Although the recommender system
does uniform the community opinion, it doesn't constrain it, in the sense that
all opinions have the same probability of becoming the dominating one. We do a
partial study, however, that suggests that a ""mischievous"" recommender might be
able to bias a community so that one opinion will emerge over the opposite with
overwhelming probability.
",1,1,0,0,0,0
10299,10300,Machine learning for graph-based representations of three-dimensional discrete fracture networks,"  Structural and topological information play a key role in modeling flow and
transport through fractured rock in the subsurface. Discrete fracture network
(DFN) computational suites such as dfnWorks are designed to simulate flow and
transport in such porous media. Flow and transport calculations reveal that a
small backbone of fractures exists, where most flow and transport occurs.
Restricting the flowing fracture network to this backbone provides a
significant reduction in the network's effective size. However, the particle
tracking simulations needed to determine the reduction are computationally
intensive. Such methods may be impractical for large systems or for robust
uncertainty quantification of fracture networks, where thousands of forward
simulations are needed to bound system behavior.
In this paper, we develop an alternative network reduction approach to
characterizing transport in DFNs, by combining graph theoretical and machine
learning methods. We consider a graph representation where nodes signify
fractures and edges denote their intersections. Using random forest and support
vector machines, we rapidly identify a subnetwork that captures the flow
patterns of the full DFN, based primarily on node centrality features in the
graph. Our supervised learning techniques train on particle-tracking backbone
paths found by dfnWorks, but run in negligible time compared to those
simulations. We find that our predictions can reduce the network to
approximately 20% of its original size, while still generating breakthrough
curves consistent with those of the original network.
",1,1,0,1,0,0
10720,10721,Better Text Understanding Through Image-To-Text Transfer,"  Generic text embeddings are successfully used in a variety of tasks. However,
they are often learnt by capturing the co-occurrence structure from pure text
corpora, resulting in limitations of their ability to generalize. In this
paper, we explore models that incorporate visual information into the text
representation. Based on comprehensive ablation studies, we propose a
conceptually simple, yet well performing architecture. It outperforms previous
multimodal approaches on a set of well established benchmarks. We also improve
the state-of-the-art results for image-related text datasets, using orders of
magnitude less data.
",1,0,0,0,0,0
14357,14358,Redshift determination through weighted phase correlation: a linearithmic implementation,"  We present a new algorithm having a time complexity of O(N log N) and
designed to retrieve the phase at which an input signal and a set of not
necessarily orthogonal templates match best in a weighted chi-squared sense.
The proposed implementation is based on an orthogonalization algorithm and thus
also benefits from high numerical stability. We apply this method successfully
to the redshift determination of quasars from the twelfth Sloan Digital Sky
Survey (SDSS) quasar catalogue and derive the proper spectral reduction and
redshift selection methods. Derivations of the redshift uncertainty and the
associated confidence are also provided. The results of this application are
comparable to the performance of the SDSS pipeline, while not having a
quadratic time dependence.
",0,1,0,0,0,0
11955,11956,Mining Density Contrast Subgraphs,"  Dense subgraph discovery is a key primitive in many graph mining
applications, such as detecting communities in social networks and mining gene
correlation from biological data. Most studies on dense subgraph mining only
deal with one graph. However, in many applications, we have more than one graph
describing relations among a same group of entities. In this paper, given two
graphs sharing the same set of vertices, we investigate the problem of
detecting subgraphs that contrast the most with respect to density. We call
such subgraphs Density Contrast Subgraphs, or DCS in short. Two widely used
graph density measures, average degree and graph affinity, are considered. For
both density measures, mining DCS is equivalent to mining the densest subgraph
from a ""difference"" graph, which may have both positive and negative edge
weights. Due to the existence of negative edge weights, existing dense subgraph
detection algorithms cannot identify the subgraph we need. We prove the
computational hardness of mining DCS under the two graph density measures and
develop efficient algorithms to find DCS. We also conduct extensive experiments
on several real-world datasets to evaluate our algorithms. The experimental
results show that our algorithms are both effective and efficient.
",1,0,0,0,0,0
17105,17106,"FFT Convolutions are Faster than Winograd on Modern CPUs, Here is Why","  Winograd-based convolution has quickly gained traction as a preferred
approach to implement convolutional neural networks (ConvNet) on various
hardware platforms because it requires fewer floating point operations than
FFT-based or direct convolutions.
This paper compares three highly optimized implementations (regular FFT--,
Gauss--FFT--, and Winograd--based convolutions) on modern multi-- and
many--core CPUs. Although all three implementations employed the same
optimizations for modern CPUs, our experimental results with two popular
ConvNets (VGG and AlexNet) show that the FFT--based implementations generally
outperform the Winograd--based approach, contrary to the popular belief.
To understand the results, we use a Roofline performance model to analyze the
three implementations in detail, by looking at each of their computation phases
and by considering not only the number of floating point operations, but also
the memory bandwidth and the cache sizes. The performance analysis explains
why, and under what conditions, the FFT--based implementations outperform the
Winograd--based one, on modern CPUs.
",1,0,0,0,0,0
1246,1247,Optimal Transport: Fast Probabilistic Approximation with Exact Solvers,"  We propose a simple subsampling scheme for fast randomized approximate
computation of optimal transport distances. This scheme operates on a random
subset of the full data and can use any exact algorithm as a black-box
back-end, including state-of-the-art solvers and entropically penalized
versions. It is based on averaging the exact distances between empirical
measures generated from independent samples from the original measures and can
easily be tuned towards higher accuracy or shorter computation times. To this
end, we give non-asymptotic deviation bounds for its accuracy in the case of
discrete optimal transport problems. In particular, we show that in many
important instances, including images (2D-histograms), the approximation error
is independent of the size of the full problem. We present numerical
experiments that demonstrate that a very good approximation in typical
applications can be obtained in a computation time that is several orders of
magnitude smaller than what is required for exact computation of the full
problem.
",0,0,0,1,0,0
8444,8445,Opportunistic Downlink Interference Alignment for Multi-Cell MIMO Networks,"  In this paper, we propose an opportunistic downlink interference alignment
(ODIA) for interference-limited cellular downlink, which intelligently combines
user scheduling and downlink IA techniques. The proposed ODIA not only
efficiently reduces the effect of inter-cell interference from other-cell base
stations (BSs) but also eliminates intra-cell interference among spatial
streams in the same cell. We show that the minimum number of users required to
achieve a target degrees-of-freedom (DoF) can be fundamentally reduced, i.e.,
the fundamental user scaling law can be improved by using the ODIA, compared
with the existing downlink IA schemes. In addition, we adopt a limited feedback
strategy in the ODIA framework, and then analyze the number of feedback bits
required for the system with limited feedback to achieve the same user scaling
law of the ODIA as the system with perfect CSI. We also modify the original
ODIA in order to further improve sum-rate, which achieves the optimal multiuser
diversity gain, i.e., $\log\log N$, per spatial stream even in the presence of
downlink inter-cell interference, where $N$ denotes the number of users in a
cell. Simulation results show that the ODIA significantly outperforms existing
interference management techniques in terms of sum-rate in realistic cellular
environments. Note that the ODIA operates in a non-collaborative and decoupled
manner, i.e., it requires no information exchange among BSs and no iterative
beamformer optimization between BSs and users, thus leading to an easier
implementation.
",1,0,1,0,0,0
8464,8465,X-Shooter study of accretion in Chamaeleon I: II. A steeper increase of accretion with stellar mass for very low mass stars?,"  The dependence of the mass accretion rate on the stellar properties is a key
constraint for star formation and disk evolution studies. Here we present a
study of a sample of stars in the Chamaeleon I star forming region carried out
using the VLT/X-Shooter spectrograph. The sample is nearly complete down to
M~0.1Msun for the young stars still harboring a disk in this region. We derive
the stellar and accretion parameters using a self-consistent method to fit the
broad-band flux-calibrated medium resolution spectrum. The correlation between
the accretion luminosity to the stellar luminosity, and of the mass accretion
rate to the stellar mass in the logarithmic plane yields slopes of 1.9 and 2.3,
respectively. These slopes and the accretion rates are consistent with previous
results in various star forming regions and with different theoretical
frameworks. However, we find that a broken power-law fit, with a steeper slope
for stellar luminosity smaller than ~0.45 Lsun and for stellar masses smaller
than ~ 0.3 Msun, is slightly preferred according to different statistical
tests, but the single power-law model is not excluded. The steeper relation for
lower mass stars can be interpreted as a faster evolution in the past for
accretion in disks around these objects, or as different accretion regimes in
different stellar mass ranges. Finally, we find two regions on the mass
accretion versus stellar mass plane empty of objects. One at high mass
accretion rates and low stellar masses, which is related to the steeper
dependence of the two parameters we derived. The second one is just above the
observational limits imposed by chromospheric emission. This empty region is
located at M~0.3-0.4Msun, typical masses where photoevaporation is known to be
effective, and at mass accretion rates ~10^-10 Msun/yr, a value compatible with
the one expected for photoevaporation to rapidly dissipate the inner disk.
",0,1,0,0,0,0
6612,6613,Calibration for Weak Variance-Alpha-Gamma Processes,"  The weak variance-alpha-gamma process is a multivariate Lévy process
constructed by weakly subordinating Brownian motion, possibly with correlated
components with an alpha-gamma subordinator. It generalises the
variance-alpha-gamma process of Semeraro constructed by traditional
subordination. We compare three calibration methods for the weak
variance-alpha-gamma process, method of moments, maximum likelihood estimation
(MLE) and digital moment estimation (DME). We derive a condition for Fourier
invertibility needed to apply MLE and show in our simulations that MLE produces
a better fit when this condition holds, while DME produces a better fit when it
is violated. We also find that the weak variance-alpha-gamma process exhibits a
wider range of dependence and produces a significantly better fit than the
variance-alpha-gamma process on an S&P500-FTSE100 data set, and that DME
produces the best fit in this situation.
",0,0,0,0,0,1
12141,12142,"The Junk News Aggregator: Examining junk news posted on Facebook, starting with the 2018 US Midterm Elections","  In recent years, the phenomenon of online misinformation and junk news
circulating on social media has come to constitute an important and widespread
problem affecting public life online across the globe, particularly around
important political events such as elections. At the same time, there have been
calls for more transparency around misinformation on social media platforms, as
many of the most popular social media platforms function as ""walled gardens,""
where it is impossible for researchers and the public to readily examine the
scale and nature of misinformation activity as it is unfolding on the
platforms. In order to help address this, this paper, we present the Junk News
Aggregator, an interactive web tool made publicly available, which allows the
public to examine, in near real-time, all of the public content posted to
Facebook by important junk news sources in the US. It allows the public to gain
access to and examine the latest articles posted on Facebook (the most popular
social media platform in the US and one where content is not readily accessible
at scale from the open Web), as well as organise them by time, news publisher,
and keywords of interest, and sort them based on all eight engagement metrics
available on Facebook. Therefore, the Aggregator allows the public to gain
insights on the volume, content, key themes, and types and volumes of
engagement received by content posted by junk news publishers, in near real
time, hence opening up and offering transparency in these activities, at scale
across the top most popular junk news publishers and in near real time. In this
way, the Aggregator can help increase transparency around the nature, volume,
and engagement with junk news on social media, and serve as a media literacy
tool for the public.
",1,0,0,0,0,0
5868,5869,Stochastic Primal-Dual Hybrid Gradient Algorithm with Arbitrary Sampling and Imaging Applications,"  We propose a stochastic extension of the primal-dual hybrid gradient
algorithm studied by Chambolle and Pock in 2011 to solve saddle point problems
that are separable in the dual variable. The analysis is carried out for
general convex-concave saddle point problems and problems that are either
partially smooth / strongly convex or fully smooth / strongly convex. We
perform the analysis for arbitrary samplings of dual variables, and obtain
known deterministic results as a special case. Several variants of our
stochastic method significantly outperform the deterministic variant on a
variety of imaging tasks.
",1,0,1,0,0,0
6207,6208,The dynamical structure of political corruption networks,"  Corruptive behaviour in politics limits economic growth, embezzles public
funds, and promotes socio-economic inequality in modern democracies. We analyse
well-documented political corruption scandals in Brazil over the past 27 years,
focusing on the dynamical structure of networks where two individuals are
connected if they were involved in the same scandal. Our research reveals that
corruption runs in small groups that rarely comprise more than eight people, in
networks that have hubs and a modular structure that encompasses more than one
corruption scandal. We observe abrupt changes in the size of the largest
connected component and in the degree distribution, which are due to the
coalescence of different modules when new scandals come to light or when
governments change. We show further that the dynamical structure of political
corruption networks can be used for successfully predicting partners in future
scandals. We discuss the important role of network science in detecting and
mitigating political corruption.
",1,0,0,1,0,0
16237,16238,Critical factors and enablers of food quality and safety compliance risk management in the Vietnamese seafood supply chain,"  Recently, along with the emergence of food scandals, food supply chains have
to face with ever-increasing pressure from compliance with food quality and
safety regulations and standards. This paper aims to explore critical factors
of compliance risk in food supply chain with an illustrated case in Vietnamese
seafood industry. To this end, this study takes advantage of both primary and
secondary data sources through a comprehensive literature research of
industrial and scientific papers, combined with expert interview. Findings
showed that there are three main critical factor groups influencing on
compliance risk including challenges originating from Vietnamese food supply
chain itself, characteristics of regulation and standards, and business
environment. Furthermore, author proposed enablers to eliminate compliance
risks to food supply chain managers as well as recommendations to government
and other influencers and supporters.
",0,0,0,0,0,1
13363,13364,Implementing universal nonadiabatic holonomic quantum gates with transmons,"  Geometric phases are well known to be noise-resilient in quantum
evolutions/operations. Holonomic quantum gates provide us with a robust way
towards universal quantum computation, as these quantum gates are actually
induced by nonabelian geometric phases. Here we propose and elaborate how to
efficiently implement universal nonadiabatic holonomic quantum gates on simpler
superconducting circuits, with a single transmon serving as a qubit. In our
proposal, an arbitrary single-qubit holonomic gate can be realized in a
single-loop scenario, by varying the amplitudes and phase difference of two
microwave fields resonantly coupled to a transmon, while nontrivial two-qubit
holonomic gates may be generated with a transmission-line resonator being
simultaneously coupled to the two target transmons in an effective resonant
way. Moreover, our scenario may readily be scaled up to a two-dimensional
lattice configuration, which is able to support large scalable quantum
computation, paving the way for practically implementing universal nonadiabatic
holonomic quantum computation with superconducting circuits.
",0,1,0,0,0,0
10019,10020,F-TRIDYN: A Binary Collision Approximation Code for Simulating Ion Interactions with Rough Surfaces,"  Fractal TRIDYN (F-TRIDYN) is a modified version of the widely used Monte
Carlo, Binary Collision Approximation code TRIDYN that includes an explicit
model of surface roughness and additional output modes for coupling to plasma
edge and material codes. Surface roughness plays an important role in ion
irradiation processes such as sputtering; roughness can significantly increase
the angle of maximum sputtering and change the maximum observed sputtering
yield by a factor of 2 or more. The complete effect of surface roughness on
sputtering and other ion irradiation phenomena is not completely understood.
Many rough surfaces can be consistently and realistically modeled by fractals,
using the fractal dimension and fractal length scale as the sole input
parameters. F-TRIDYN includes a robust fractal surface algorithm that is more
computationally efficient than those in previous fractal codes and which
reproduces available experimental sputtering data from rough surfaces. Fractals
provide a compelling path toward a complete and concise understanding of the
effect that surface geometry plays on the behavior of plasma-facing materials.
F-TRIDYN is a flexible code for simulating ion-solid interactions and coupling
to plasma and material codes for multiscale modeling.
",0,1,0,0,0,0
6295,6296,An aptamer-biosensor for azole class antifungal drugs,"  This report describes the development of an aptamer for sensing azole
antifungal drugs for therapeutic drug monitoring. Modified Synthetic Evolution
of Ligands through Exponential Enrichment (SELEX) was used to discover a DNA
aptamer recognizing azole class antifungal drugs. This aptamer undergoes a
secondary structural change upon binding to its target molecule as shown
through fluorescence anisotropy-based binding measurements. Experiments using
circular dichroism spectroscopy, revealed a unique double G-quadruplex
structure that was essential and specific for binding to the azole antifungal
target. Aptamer-functionalized Graphene Field Effect Transistor (GFET) devices
were created and used to measure the binding of strength of azole antifungals
to this surface. In total this aptamer and the supporting sensing platform
could provide a valuable tool for improving the treatment of patients with
invasive fungal infections.
",0,1,0,0,0,0
10231,10232,Frobenius elements in Galois representations with SL_n image,"  Suppose we have a elliptic curve over a number field whose mod $l$
representation has image isomorphic to $SL_2(\mathbb{F}_l)$. We present a
method to determine Frobenius elements of the associated Galois group which
incorporates the linear structure available. We are able to distinguish
$SL_n(\mathbb{F}_l)$-conjugacy from $GL_n(\mathbb{F}_l)$-conjugacy; this can be
thought of as being analogous to a result which distinguishes $A_n$-conjugacy
from $S_n$-conjugacy when the Galois group is considered as a permutation
group.
",0,0,1,0,0,0
11188,11189,Statistical estimation of superhedging prices,"  We consider statistical estimation of superhedging prices using historical
stock returns in a frictionless market with d traded assets. We introduce a
simple plugin estimator based on empirical measures, show it is consistent but
lacks suitable robustness. This is addressed by our improved estimators which
use a larger set of martingale measures defined through a tradeoff between the
radius of Wasserstein balls around the empirical measure and the allowed norm
of martingale densities. We also study convergence rates, convergence of
superhedging strategies, and our study extends, in part, to the case of a
market with traded options and to a multiperiod setting.
",0,0,0,0,0,1
8536,8537,On tamed almost complex four manifolds,"  This paper proves that on any tamed closed almost complex four-manifold
$(M,J)$ whose dimension of $J$-anti-invariant cohomology is equal to self-dual
second Betti number minus one, there exists a new symplectic form compatible
with the given almost complex structure $J$. In particular, if the self-dual
second Betti number is one, we give an affirmative answer to Donaldson question
for tamed closed almost complex four-manifolds that is a conjecture in joint
paper of Tosatti, Weinkove and Yau. Our approach is along the lines used by
Buchdahl to give a unified proof of the Kodaira conjecture. Thus, our main
result gives an affirmative answer to the Kodaira conjecture in symplectic
version.
",0,0,1,0,0,0
3716,3717,The Autonomic Architecture of the Licas System,"  Licas (lightweight internet-based communication for autonomic services) is a
distributed framework for building service-based systems. The framework
provides a p2p server and more intelligent processing of information through
its AI algorithms. Distributed communication includes XML-RPC, REST, HTTP and
Web Services. It can now provide a robust platform for building different types
of system, where Microservices or SOA would be possible. However, the system
may be equally suited for the IoT, as it provides classes to connect with
external sources and has an optional Autonomic Manager with a MAPE control loop
integrated into the communication process. The system is also mobile-compatible
with Android. This paper focuses in particular on the autonomic setup and how
that might be used. A novel linking mechanism has been described previously [5]
that can be used to dynamically link sources and this is also considered, as
part of the autonomous framework.
",1,0,0,0,0,0
20465,20466,Thermoelectric phase diagram of the SrTiO3-SrNbO3 solid solution system,"  Thermoelectric energy conversion - the exploitation of the Seebeck effect to
convert waste heat into electricity - has attracted an increasing amount of
research attention for energy harvesting technology. Niobium-doped strontium
titanate (SrTi1-xNbxO3) is one of the most promising thermoelectric material
candidates, particularly as it poses a much lesser environmental risk in
comparison to materials based on heavy metal elements. Two-dimensional electron
confinement, e.g. through the formation of superlattices or two-dimensional
electron gases, is recognized as an effective strategy to improve the
thermoelectric performance of SrTi1-xNbxO3. Although electron confinement is
closely related to the electronic structure, the fundamental electronic phase
behavior of the SrTi1-xNbxO3 solid solution system has yet to be
comprehensively investigated. Here, we present a thermoelectric phase diagram
for the SrTi1-xNbxO3 (0.05 =< x =< 1) solid solution system, which we derived
from the characterization of epitaxial films. We observed two thermoelectric
phase boundaries in the system, which originate from the step-like decrease in
carrier effective mass at x ~ 0.3, and from a local minimum in carrier
relaxation time at x ~ 0.5. The origins of these phase boundaries are
considered to be related to isovalent/heterovalent B-site substitution:
parabolic Ti 3d orbitals dominate electron conduction for compositions with x <
0.3, whereas the Nb 4d orbital dominates when x > 0.3. At x ~ 0.5, a tetragonal
distortion of the lattice, in which the B-site is composed of Ti4+ and Nb4+
ions, leads to the formation of tail-like impurity bands, which maximizes the
electron scattering. These results provide a foundation for further research
into improving the thermoelectric performance of SrTi1-xNbxO3.
",0,1,0,0,0,0
7847,7848,A new Hysteretic Nonlinear Energy Sink (HNES),"  The behavior of a new Hysteretic Nonlinear Energy Sink (HNES) coupled to a
linear primary oscillator is investigated in shock mitigation. Apart from a
small mass and a nonlinear elastic spring of the Duffing oscillator, the HNES
is also comprised of a purely hysteretic and a linear elastic spring of
potentially negative stiffness, connected in parallel. The Bouc-Wen model is
used to describe the force produced by both the purely hysteretic and linear
elastic springs. Coupling the primary oscillator with the HNES three nonlinear
equations of motion are derived, in terms of the two displacements and the
dimensionless hysteretic variable, which are integrated numerically using the
analog equation method. The performance of the HNES is examined by quantifying
the percentage of the initially induced energy in the primary system that is
passively transferred and dissipated by the HNES. Remarkable results are
achieved for a wide range of initial input energies. The great performance of
the HNES is mostly evidenced when the linear spring stiffness takes on negative
values.
",0,1,0,0,0,0
3983,3984,"Intelligent Home Energy Management System for Distributed Renewable Generators, Dispatchable Residential Loads and Distributed Energy Storage Devices","  This paper presents an intelligent home energy management system integrated
with dispatchable loads (e.g., clothes washers and dryers), distributed
renewable generators (e.g., roof-top solar panels), and distributed energy
storage devices (e.g., plug-in electric vehicles). The overall goal is to
reduce the total operating costs and the carbon emissions for a future
residential house, while satisfying the end-users comfort levels. This paper
models a wide variety of home appliances and formulates the economic operation
problem using mixed integer linear programming. Case studies are performed to
validate and demonstrate the effectiveness of the proposed solution algorithm.
Simulation results also show the positive impact of dispatchable loads,
distributed renewable generators, and distributed energy storage devices on a
future residential house.
",0,0,1,0,0,0
4831,4832,Straggler Mitigation in Distributed Optimization Through Data Encoding,"  Slow running or straggler tasks can significantly reduce computation speed in
distributed computation. Recently, coding-theory-inspired approaches have been
applied to mitigate the effect of straggling, through embedding redundancy in
certain linear computational steps of the optimization algorithm, thus
completing the computation without waiting for the stragglers. In this paper,
we propose an alternate approach where we embed the redundancy directly in the
data itself, and allow the computation to proceed completely oblivious to
encoding. We propose several encoding schemes, and demonstrate that popular
batch algorithms, such as gradient descent and L-BFGS, applied in a
coding-oblivious manner, deterministically achieve sample path linear
convergence to an approximate solution of the original problem, using an
arbitrarily varying subset of the nodes at each iteration. Moreover, this
approximation can be controlled by the amount of redundancy and the number of
nodes used in each iteration. We provide experimental results demonstrating the
advantage of the approach over uncoded and data replication strategies.
",1,0,0,1,0,0
11736,11737,A Diophantine approximation problem with two primes and one $k$-th power of a prime,"  We refine a result of the last two Authors of [8] on a Diophantine
approximation problem with two primes and a $k$-th power of a prime which was
only proved to hold for $1<k<4/3$. We improve the $k$-range to $1<k\le 3$ by
combining Harman's technique on the minor arc with a suitable estimate for the
$L^4$-norm of the relevant exponential sum over primes $S_k$. In the common
range we also give a stronger bound for the approximation.
",0,0,1,0,0,0
13021,13022,A Driver-in-the Loop Fuel Economic Control Strategy for Connected Vehicles in Urban Roads,"  In this paper, we focus on developing driver-in-the loop fuel economic
control strategy for multiple connected vehicles. The control strategy is
considered to work in a driver assistance framework where the controller gives
command to a driver to follow while considering the ability of the driver in
following control commands. Our proposed method uses vehicle-to-vehicle (V2V)
communication, exploits traffic lights' Signal Phase and Timing (SPAT)
information, models driver error injection with Markov chain, and employs
scenario tree based stochastic model predictive control to improve vehicle fuel
economy and traffic mobility. The proposed strategy is decentralized in nature
as every vehicle evaluates its own strategy using only local information.
Simulation results show the effect of consideration of driver error injection
when synthesizing fuel economic controllers in a driver assistance fashion.
",1,0,0,0,0,0
8043,8044,Equivariant mirror symmetry for the weighted projective line,"  In this paper, we establish equivariant mirror symmetry for the weighted
projective line. This extends the results by B. Fang, C.C. Liu and Z. Zong,
where the projective line was considered [{\it Geometry \& Topology}
24:2049-2092, 2017]. More precisely, we prove the equivalence of the
$R$-matrices for A-model and B-model at large radius limit, and establish
isomorphism for $R$-matrices for general radius. We further demonstrate that
the graph sum of higher genus cases are the same for both models, hence
establish equivariant mirror symmetry for the weighted projective line.
",0,0,1,0,0,0
13536,13537,Pathwise Least Angle Regression and a Significance Test for the Elastic Net,"  Least angle regression (LARS) by Efron et al. (2004) is a novel method for
constructing the piece-wise linear path of Lasso solutions. For several years,
it remained also as the de facto method for computing the Lasso solution before
more sophisticated optimization algorithms preceded it. LARS method has
recently again increased its popularity due to its ability to find the values
of the penalty parameters, called knots, at which a new parameter enters the
active set of non-zero coefficients. Significance test for the Lasso by
Lockhart et al. (2014), for example, requires solving the knots via the LARS
algorithm. Elastic net (EN), on the other hand, is a highly popular extension
of Lasso that uses a linear combination of Lasso and ridge regression
penalties. In this paper, we propose a new novel algorithm, called pathwise
(PW-)LARS-EN, that is able to compute the EN knots over a grid of EN tuning
parameter {\alpha} values. The developed PW-LARS-EN algorithm decreases the EN
tuning parameter and exploits the previously found knot values and the original
LARS algorithm. A covariance test statistic for the Lasso is then generalized
to the EN for testing the significance of the predictors. Our simulation
studies validate the fact that the test statistic has an asymptotic Exp(1)
distribution.
",0,0,1,1,0,0
19741,19742,Models of compact stars on paraboloidal spacetime satisfying Karmarkar condition,"  A new exact solution of Einstein's field equations on the background of
paraboloidal spacetime using Karmarkar condition is reported. The physical
acceptability conditions of the model are investigated and found that the model
is compatible with a number of compact star candidates like Her X-1, LMC X-4,
EXO 1785-248, PSR J1903+327, Vela X-1 and PSR J1614-2230. A noteworthy feature
of the model is that it is geometrically significant and simple in form.
",0,1,0,0,0,0
11022,11023,"Fog Robotics for Efficient, Fluent and Robust Human-Robot Interaction","  Active communication between robots and humans is essential for effective
human-robot interaction. To accomplish this objective, Cloud Robotics (CR) was
introduced to make robots enhance their capabilities. It enables robots to
perform extensive computations in the cloud by sharing their outcomes. Outcomes
include maps, images, processing power, data, activities, and other robot
resources. But due to the colossal growth of data and traffic, CR suffers from
serious latency issues. Therefore, it is unlikely to scale a large number of
robots particularly in human-robot interaction scenarios, where responsiveness
is paramount. Furthermore, other issues related to security such as privacy
breaches and ransomware attacks can increase. To address these problems, in
this paper, we have envisioned the next generation of social robotic
architectures based on Fog Robotics (FR) that inherits the strengths of Fog
Computing to augment the future social robotic systems. These new architectures
can escalate the dexterity of robots by shoving the data closer to the robot.
Additionally, they can ensure that human-robot interaction is more responsive
by resolving the problems of CR. Moreover, experimental results are further
discussed by considering a scenario of FR and latency as a primary factor
comparing to CR models.
",1,0,0,0,0,0
19247,19248,A construction of trivial Beltrami coefficients,"  A measurable function $\mu$ on the unit disk $\mathbb{D}$ of the complex
plane with $\|\mu\|_\infty<1$ is sometimes called a Beltrami coefficient. We
say that $\mu$ is trivial if it is the complex dilatation $f_{\bar z}/f_z$ of a
quasiconformal automorphism $f$ of $\mathbb{D}$ satisfying the trivial boundary
condition $f(z)=z,~|z|=1.$ Since it is not easy to solve the Beltrami equation
explicitly, to detect triviality of a given Beltrami coefficient is a hard
problem, in general. In the present article, we offer a sufficient condition
for a Beltrami coefficient to be trivial. Our proof is based on Betker's
theorem on Löwner chains.
",0,0,1,0,0,0
13129,13130,A Proximity-Aware Hierarchical Clustering of Faces,"  In this paper, we propose an unsupervised face clustering algorithm called
""Proximity-Aware Hierarchical Clustering"" (PAHC) that exploits the local
structure of deep representations. In the proposed method, a similarity measure
between deep features is computed by evaluating linear SVM margins. SVMs are
trained using nearest neighbors of sample data, and thus do not require any
external training data. Clusters are then formed by thresholding the similarity
scores. We evaluate the clustering performance using three challenging
unconstrained face datasets, including Celebrity in Frontal-Profile (CFP),
IARPA JANUS Benchmark A (IJB-A), and JANUS Challenge Set 3 (JANUS CS3)
datasets. Experimental results demonstrate that the proposed approach can
achieve significant improvements over state-of-the-art methods. Moreover, we
also show that the proposed clustering algorithm can be applied to curate a set
of large-scale and noisy training dataset while maintaining sufficient amount
of images and their variations due to nuisance factors. The face verification
performance on JANUS CS3 improves significantly by finetuning a DCNN model with
the curated MS-Celeb-1M dataset which contains over three million face images.
",1,0,0,0,0,0
6439,6440,Automatic White-Box Testing of First-Order Logic Ontologies,"  Formal ontologies are axiomatizations in a logic-based formalism. The
development of formal ontologies, and their important role in the Semantic Web
area, is generating considerable research on the use of automated reasoning
techniques and tools that help in ontology engineering. One of the main aims is
to refine and to improve axiomatizations for enabling automated reasoning tools
to efficiently infer reliable information. Defects in the axiomatization can
not only cause wrong inferences, but can also hinder the inference of expected
information, either by increasing the computational cost of, or even
preventing, the inference. In this paper, we introduce a novel, fully automatic
white-box testing framework for first-order logic ontologies. Our methodology
is based on the detection of inference-based redundancies in the given
axiomatization. The application of the proposed testing method is fully
automatic since a) the automated generation of tests is guided only by the
syntax of axioms and b) the evaluation of tests is performed by automated
theorem provers. Our proposal enables the detection of defects and serves to
certify the grade of suitability --for reasoning purposes-- of every axiom. We
formally define the set of tests that are generated from any axiom and prove
that every test is logically related to redundancies in the axiom from which
the test has been generated. We have implemented our method and used this
implementation to automatically detect several non-trivial defects that were
hidden in various first-order logic ontologies. Throughout the paper we provide
illustrative examples of these defects, explain how they were found, and how
each proof --given by an automated theorem-prover-- provides useful hints on
the nature of each defect. Additionally, by correcting all the detected
defects, we have obtained an improved version of one of the tested ontologies:
Adimen-SUMO.
",1,0,0,0,0,0
11498,11499,Statistical Challenges in Modeling Big Brain Signals,"  Brain signal data are inherently big: massive in amount, complex in
structure, and high in dimensions. These characteristics impose great
challenges for statistical inference and learning. Here we review several key
challenges, discuss possible solutions, and highlight future research
directions.
",0,0,0,1,0,0
10797,10798,Text-Independent Speaker Verification Using 3D Convolutional Neural Networks,"  In this paper, a novel method using 3D Convolutional Neural Network (3D-CNN)
architecture has been proposed for speaker verification in the text-independent
setting. One of the main challenges is the creation of the speaker models. Most
of the previously-reported approaches create speaker models based on averaging
the extracted features from utterances of the speaker, which is known as the
d-vector system. In our paper, we propose an adaptive feature learning by
utilizing the 3D-CNNs for direct speaker model creation in which, for both
development and enrollment phases, an identical number of spoken utterances per
speaker is fed to the network for representing the speakers' utterances and
creation of the speaker model. This leads to simultaneously capturing the
speaker-related information and building a more robust system to cope with
within-speaker variation. We demonstrate that the proposed method significantly
outperforms the traditional d-vector verification system. Moreover, the
proposed system can also be an alternative to the traditional d-vector system
which is a one-shot speaker modeling system by utilizing 3D-CNNs.
",1,0,0,0,0,0
18594,18595,On the topology of real Bott manifolds,"  The main aim of this article is to give a necessary and sufficient condition
for a real Bott manifold to admit a spin structure and further give a
combinatorial characterization for the spin structure in terms of the
associated acyclic digraph.
",0,0,1,0,0,0
4194,4195,Analyzing IO Amplification in Linux File Systems,"  We present the first systematic analysis of read, write, and space
amplification in Linux file systems. While many researchers are tackling write
amplification in key-value stores, IO amplification in file systems has been
largely unexplored. We analyze data and metadata operations on five widely-used
Linux file systems: ext2, ext4, XFS, btrfs, and F2FS. We find that data
operations result in significant write amplification (2-32X) and that metadata
operations have a large IO cost. For example, a single rename requires 648 KB
write IO in btrfs. We also find that small random reads result in read
amplification of 2-13X. Based on these observations, we present the CReWS
conjecture about the relationship between IO amplification, consistency, and
storage space utilization. We hope this paper spurs people to design future
file systems with less IO amplification, especially for non-volatile memory
technologies.
",1,0,0,0,0,0
1495,1496,Improvement of training set structure in fusion data cleaning using Time-Domain Global Similarity method,"  Traditional data cleaning identifies dirty data by classifying original data
sequences, which is a class$-$imbalanced problem since the proportion of
incorrect data is much less than the proportion of correct ones for most
diagnostic systems in Magnetic Confinement Fusion (MCF) devices. When using
machine learning algorithms to classify diagnostic data based on
class$-$imbalanced training set, most classifiers are biased towards the major
class and show very poor classification rates on the minor class. By
transforming the direct classification problem about original data sequences
into a classification problem about the physical similarity between data
sequences, the class$-$balanced effect of Time$-$Domain Global Similarity
(TDGS) method on training set structure is investigated in this paper.
Meanwhile, the impact of improved training set structure on data cleaning
performance of TDGS method is demonstrated with an application example in EAST
POlarimetry$-$INTerferometry (POINT) system.
",1,0,0,0,0,0
3364,3365,Quantifying the uncertainties in an ensemble of decadal climate predictions,"  Meaningful climate predictions must be accompanied by their corresponding
range of uncertainty. Quantifying the uncertainties is non-trivial, and
different methods have been suggested and used in the past. Here, we propose a
method that does not rely on any assumptions regarding the distribution of the
ensemble member predictions. The method is tested using the CMIP5 1981-2010
decadal predictions and is shown to perform better than two other methods
considered here. The improved estimate of the uncertainties is of great
importance for both practical use and for better assessing the significance of
the effects seen in theoretical studies.
",0,1,0,0,0,0
18588,18589,VUNet: Dynamic Scene View Synthesis for Traversability Estimation using an RGB Camera,"  We present VUNet, a novel view(VU) synthesis method for mobile robots in
dynamic environments, and its application to the estimation of future
traversability. Our method predicts future images for given virtual robot
velocity commands using only RGB images at previous and current time steps. The
future images result from applying two types of image changes to the previous
and current images: 1) changes caused by different camera pose, and 2) changes
due to the motion of the dynamic obstacles. We learn to predict these two types
of changes disjointly using two novel network architectures, SNet and DNet. We
combine SNet and DNet to synthesize future images that we pass to our
previously presented method GONet to estimate the traversable areas around the
robot. Our quantitative and qualitative evaluation indicate that our approach
for view synthesis predicts accurate future images in both static and dynamic
environments. We also show that these virtual images can be used to estimate
future traversability correctly. We apply our view synthesis-based
traversability estimation method to two applications for assisted
teleoperation.
",1,0,0,0,0,0
20034,20035,Triviality of the ground-state metastate in long-range Ising spin glasses in one dimension,"  We consider the one-dimensional model of a spin glass with independent
Gaussian-distributed random interactions, that have mean zero and variance
$1/|i-j|^{2\sigma}$, between the spins at sites $i$ and $j$ for all $i\neq j$.
It is known that, for $\sigma>1$, there is no phase transition at any non-zero
temperature in this model. We prove rigorously that, for $\sigma>3/2$, any
Newman-Stein metastate for the ground states (i.e.\ the frequencies with which
distinct ground states are observed in finite size samples in the limit of
infinite size, for given disorder) is trivial and unique. In other words, for
given disorder and asymptotically at large sizes, the same ground state, or its
global spin flip, is obtained (almost) always. The proof consists of two parts:
one is a theorem (based on one by Newman and Stein for short-range
two-dimensional models), valid for all $\sigma>1$, that establishes triviality
under a convergence hypothesis on something similar to the energies of domain
walls, and the other (based on older results for the one-dimensional model)
establishes that the hypothesis is true for $\sigma>3/2$. In addition, we
derive heuristic scaling arguments and rigorous exponent inequalities which
tend to support the validity of the hypothesis under broader conditions. The
constructions of various metastates are extended to all values $\sigma>1/2$.
Triviality of the metastate in bond-diluted power-law models for $\sigma>1$ is
proved directly.
",0,1,0,0,0,0
12930,12931,"Mermin-Wagner physics, (H,T) phase diagram, and candidate quantum spin-liquid phase in the spin-1/2 triangular-lattice antiferromagnet Ba8CoNb6O24","  Ba$_8$CoNb$_6$O$_{24}$ presents a system whose Co$^{2+}$ ions have an
effective spin 1/2 and construct a regular triangular-lattice antiferromagnet
(TLAFM) with a very large interlayer spacing, ensuring purely two-dimensional
character. We exploit this ideal realization to perform a detailed experimental
analysis of the $S = 1/2$ TLAFM, which is one of the keystone models in
frustrated quantum magnetism. We find strong low-energy spin fluctuations and
no magnetic ordering, but a diverging correlation length down to 0.1 K,
indicating a Mermin-Wagner trend towards zero-temperature order. Below 0.1 K,
however, our low-field measurements show an nexpected magnetically disordered
state, which is a candidate quantum spin liquid. We establish the $(H,T)$ phase
diagram, mapping in detail the quantum fluctuation corrections to the available
theoretical analysis. These include a strong upshift in field of the maximum
ordering temperature, qualitative changes to both low- and high-field phase
boundaries, and an ordered regime apparently dominated by the collinear
""up-up-down"" state. Ba$_8$CoNb$_6$O$_{24}$ therefore offers fresh input for the
development of theoretical approaches to the field-induced quantum phase
transitions of the $S = 1/2$ Heisenberg TLAFM.
",0,1,0,0,0,0
19533,19534,Impact of energy dissipation on interface shapes and on rates for dewetting from liquid substrates,"  We revisit the fundamental problem of liquid-liquid dewetting and perform a
detailed comparison of theoretical predictions based on thin-film models with
experimental measurements obtained by atomic force microscopy (AFM).
Specifically, we consider the dewetting of a liquid polystyrene (PS) layer from
a liquid polymethyl methacrylate (PMMA) layer, where the thicknesses and the
viscosities of PS and PMMA layers are similar. The excellent agreement of
experiment and theory reveals that dewetting rates for such systems follow no
universal power law, in contrast to dewetting scenarios on solid substrates.
Our new energetic approach allows to assess the physical importance of
different contributions to the energy-dissipation mechanism, for which we
analyze the local flow fields and the local dissipation rates.
",0,1,0,0,0,0
11621,11622,Training-induced inversion of spontaneous exchange bias field on La1.5Ca0.5CoMnO6,"  In this work we report the synthesis and structural, electronic and magnetic
properties of La1.5Ca0.5CoMnO6 double-perovskite. This is a re-entrant spin
cluster material which exhibits a non-negligible negative exchange bias effect
when it is cooled in zero magnetic field from an unmagnetized state down to low
temperature. X-ray powder diffraction, X-ray photoelectron spectroscopy and
magnetometry results indicate mixed valence state at Co site, leading to
competing magnetic phases and uncompensated spins at the magnetic interfaces.
We compare the results for this Ca-doped material with those reported for the
resemblant compound La1.5Sr0.5CoMnO6, and discuss the much smaller spontaneous
exchange bias effect observed for the former in terms of its structural and
magnetic particularities. For La1.5Ca0.5CoMnO6, when successive magnetization
loops are carried, the spontaneous exchange bias field inverts its sign from
negative to positive from the first to the second measurement. We discuss this
behavior based on the disorder at the magnetic interfaces, related to the
presence of a glassy phase. This compound also exhibits a large conventional
exchange bias, for which there is no sign inversion of the exchange bias field
for consecutive cycles.
",0,1,0,0,0,0
3708,3709,The Trace Criterion for Kernel Bandwidth Selection for Support Vector Data Description,"  Support vector data description (SVDD) is a popular anomaly detection
technique. The SVDD classifier partitions the whole data space into an
$\textit{inlier}$ region, which consists of the region $\textit{near}$ the
training data, and an $\textit{outlier}$ region, which consists of points
$\textit{away}$ from the training data. The computation of the SVDD classifier
requires a kernel function, for which the Gaussian kernel is a common choice.
The Gaussian kernel has a bandwidth parameter, and it is important to set the
value of this parameter correctly for good results. A small bandwidth leads to
overfitting such that the resulting SVDD classifier overestimates the number of
anomalies, whereas a large bandwidth leads to underfitting and an inability to
detect many anomalies. In this paper, we present a new unsupervised method for
selecting the Gaussian kernel bandwidth. Our method, which exploits the
low-rank representation of the kernel matrix to suggest a kernel bandwidth
value, is competitive with existing bandwidth selection methods.
",1,0,0,0,0,0
15266,15267,Performance of time delay estimation in a cognitive radar,"  A cognitive radar adapts the transmit waveform in response to changes in the
radar and target environment. In this work, we analyze the recently proposed
sub-Nyquist cognitive radar wherein the total transmit power in a multi-band
cognitive waveform remains the same as its full-band conventional counterpart.
For such a system, we derive lower bounds on the mean-squared-error (MSE) of a
single-target time delay estimate. We formulate a procedure to select the
optimal bands, and recommend distribution of the total power in different bands
to enhance the accuracy of delay estimation. In particular, using Cramér-Rao
bounds, we show that equi-width subbands in cognitive radar always have better
delay estimation than the conventional radar. Further analysis using Ziv-Zakai
bound reveals that cognitive radar performs well in low signal-to-noise (SNR)
regions.
",1,0,1,0,0,0
17821,17822,"Optimization of Ensemble Supervised Learning Algorithms for Increased Sensitivity, Specificity, and AUC of Population-Based Colorectal Cancer Screenings","  Over 150,000 new people in the United States are diagnosed with colorectal
cancer each year. Nearly a third die from it (American Cancer Society). The
only approved noninvasive diagnosis tools currently involve fecal blood count
tests (FOBTs) or stool DNA tests. Fecal blood count tests take only five
minutes and are available over the counter for as low as \$15. They are highly
specific, yet not nearly as sensitive, yielding a high percentage (25%) of
false negatives (Colon Cancer Alliance). Moreover, FOBT results are far too
generalized, meaning that a positive result could mean much more than just
colorectal cancer, and could just as easily mean hemorrhoids, anal fissure,
proctitis, Crohn's disease, diverticulosis, ulcerative colitis, rectal ulcer,
rectal prolapse, ischemic colitis, angiodysplasia, rectal trauma, proctitis
from radiation therapy, and others. Stool DNA tests, the modern benchmark for
CRC screening, have a much higher sensitivity and specificity, but also cost
\$600, take two weeks to process, and are not for high-risk individuals or
people with a history of polyps. To yield a cheap and effective CRC screening
alternative, a unique ensemble-based classification algorithm is put in place
that considers the FIT result, BMI, smoking history, and diabetic status of
patients. This method is tested under ten-fold cross validation to have a .95
AUC, 92% specificity, 89% sensitivity, .88 F1, and 90% precision. Once
clinically validated, this test promises to be cheaper, faster, and potentially
more accurate when compared to a stool DNA test.
",1,0,0,1,0,0
4142,4143,End-to-end Learning of Deterministic Decision Trees,"  Conventional decision trees have a number of favorable properties, including
interpretability, a small computational footprint and the ability to learn from
little training data. However, they lack a key quality that has helped fuel the
deep learning revolution: that of being end-to-end trainable, and to learn from
scratch those features that best allow to solve a given supervised learning
problem. Recent work (Kontschieder 2015) has addressed this deficit, but at the
cost of losing a main attractive trait of decision trees: the fact that each
sample is routed along a small subset of tree nodes only. We here propose a
model and Expectation-Maximization training scheme for decision trees that are
fully probabilistic at train time, but after a deterministic annealing process
become deterministic at test time. We also analyze the learned oblique split
parameters on image datasets and show that Neural Networks can be trained at
each split node. In summary, we present the first end-to-end learning scheme
for deterministic decision trees and present results on par with or superior to
published standard oblique decision tree algorithms.
",1,0,0,1,0,0
4538,4539,Accelerated Gossip via Stochastic Heavy Ball Method,"  In this paper we show how the stochastic heavy ball method (SHB) -- a popular
method for solving stochastic convex and non-convex optimization problems
--operates as a randomized gossip algorithm. In particular, we focus on two
special cases of SHB: the Randomized Kaczmarz method with momentum and its
block variant. Building upon a recent framework for the design and analysis of
randomized gossip algorithms, [Loizou Richtarik, 2016] we interpret the
distributed nature of the proposed methods. We present novel protocols for
solving the average consensus problem where in each step all nodes of the
network update their values but only a subset of them exchange their private
values. Numerical experiments on popular wireless sensor networks showing the
benefits of our protocols are also presented.
",1,0,0,0,0,0
2401,2402,The Unusual Effectiveness of Averaging in GAN Training,"  We show empirically that the optimal strategy of parameter averaging in a
minmax convex-concave game setting is also strikingly effective in the non
convex-concave GAN setting, specifically alleviating the convergence issues
associated with cycling behavior observed in GANs. We show that averaging over
generator parameters outside of the trainig loop consistently improves
inception and FID scores on different architectures and for different GAN
objectives. We provide comprehensive experimental results across a range of
datasets, bilinear games, mixture of Gaussians, CIFAR-10, STL-10, CelebA and
ImageNet, to demonstrate its effectiveness. We achieve state-of-the-art results
on CIFAR-10 and produce clean CelebA face images, demonstrating that averaging
is one of the most effective techniques for training highly performant GANs.
",0,0,0,1,0,0
14785,14786,Towards Understanding the Impact of Human Mobility on Police Allocation,"  Motivated by recent findings that human mobility is proxy for crime behavior
in big cities and that there is a superlinear relationship between the people's
movement and crime, this article aims to evaluate the impact of how these
findings influence police allocation. More precisely, we shed light on the
differences between an allocation strategy, in which the resources are
distributed by clusters of floating population, and conventional allocation
strategies, in which the police resources are distributed by an Administrative
Area (typically based on resident population). We observed a substantial
difference in the distributions of police resources allocated following these
strategies, what evidences the imprecision of conventional police allocation
methods.
",1,1,0,0,0,0
1771,1772,Invariant surface area functionals and singular Yamabe problem in 3-dimensional CR geometry,"  We express two CR invariant surface area elements in terms of quantities in
pseudohermitian geometry. We deduce the Euler-Lagrange equations of the
associated energy functionals. Many solutions are given and discussed. In
relation to the singular CR Yamabe problem, we show that one of the energy
functionals appears as the coefficient (up to a constant multiple) of the log
term in the associated volume renormalization.
",0,0,1,0,0,0
960,961,Complementary views on electron spectra: From Fluctuation Diagnostics to real space correlations,"  We study the relation between the microscopic properties of a many-body
system and the electron spectra, experimentally accessible by photoemission. In
a recent paper [Phys. Rev. Lett. 114, 236402 (2015)], we introduced the
""fluctuation diagnostics"" approach, to extract the dominant wave vector
dependent bosonic fluctuations from the electronic self-energy. Here, we first
reformulate the theory in terms of fermionic modes, to render its connection
with resonance valence bond (RVB) fluctuations more transparent. Secondly, by
using a large-U expansion, where U is the Coulomb interaction, we relate the
fluctuations to real space correlations. Therefore, it becomes possible to
study how electron spectra are related to charge, spin, superconductivity and
RVB-like real space correlations, broadening the analysis of an earlier work
[Phys. Rev. B 89, 245130 (2014)]. This formalism is applied to the pseudogap
physics of the two-dimensional Hubbard model, studied in the dynamical cluster
approximation. We perform calculations for embedded clusters with up to 32
sites, having three inequivalent K-points at the Fermi surface. We find that as
U is increased, correlation functions gradually attain values consistent with
an RVB state. This first happens for correlation functions involving the
antinodal point and gradually spreads to the nodal point along the Fermi
surface. Simultaneously a pseudogap opens up along the Fermi surface. We relate
this to a crossover from a Kondo-like state to an RVB-like localized cluster
state and to the presence of RVB and spin fluctuations. These changes are
caused by a strong momentum dependence in the cluster bath-couplings along the
Fermi surface. We also show, from a more algorithmic perspective, how the
time-consuming calculations in fluctuation diagnostics can be drastically
simplified.
",0,1,0,0,0,0
7897,7898,Robust and Fast Decoding of High-Capacity Color QR Codes for Mobile Applications,"  The use of color in QR codes brings extra data capacity, but also inflicts
tremendous challenges on the decoding process due to chromatic distortion,
cross-channel color interference and illumination variation. Particularly, we
further discover a new type of chromatic distortion in high-density color QR
codes, cross-module color interference, caused by the high density which also
makes the geometric distortion correction more challenging. To address these
problems, we propose two approaches, namely, LSVM-CMI and QDA-CMI, which
jointly model these different types of chromatic distortion. Extended from SVM
and QDA, respectively, both LSVM-CMI and QDA-CMI optimize over a particular
objective function to learn a color classifier. Furthermore, a robust geometric
transformation method and several pipeline refinements are proposed to boost
the decoding performance for mobile applications. We put forth and implement a
framework for high-capacity color QR codes equipped with our methods, called
HiQ. To evaluate the performance of HiQ, we collect a challenging large-scale
color QR code dataset, CUHK-CQRC, which consists of 5390 high-density color QR
code samples. The comparison with the baseline method [2] on CUHK-CQRC shows
that HiQ at least outperforms [2] by 188% in decoding success rate and 60% in
bit error rate. Our implementation of HiQ in iOS and Android also demonstrates
the effectiveness of our framework in real-world applications.
",1,0,0,0,0,0
11868,11869,Weak multiplier Hopf algebras III. Integrals and duality,"  Let $(A,\Delta)$ be a weak multiplier Hopf algebra. It is a pair of a
non-degenerate algebra $A$, with or without identity, and a coproduct $\Delta$
on $A$, satisfying certain properties. The main difference with multiplier Hopf
algebras is that now, the canonical maps $T_1$ and $T_2$ on $A\otimes A$,
defined by $$T_1(a\otimes b)=\Delta(a)(1\otimes b)
\qquad\quad\text{and}\qquad\quad T_2(c\otimes a)=(c\otimes 1)\Delta(a),$$ are
no longer assumed to be bijective. Also recall that a weak multiplier Hopf
algebra is called regular if its antipode is a bijective map from $A$ to
itself.
In this paper, we introduce and study the notion of integrals on such regular
weak multiplier Hopf algebras. A left integral is a non-zero linear functional
on $A$ that is left invariant (in an appropriate sense). Similarly for a right
integral. For a regular weak multiplier Hopf algebra $(A,\Delta)$ with
(sufficiently many) integrals, we construct the dual $(\widehat
A,\widehat\Delta)$. It is again a regular weak multiplier Hopf algebra with
(sufficiently many) integrals. This duality extends the known duality of
finite-dimensional weak Hopf algebras to this more general case. It also
extends the duality of multiplier Hopf algebras with integrals, the so-called
algebraic quantum groups. For this reason, we will sometimes call a regular
weak multiplier Hopf algebra with enough integrals an algebraic quantum
groupoid.
We discuss the relation of our work with the work on duality for algebraic
quantum groupoids by Timmermann.
We also illustrate this duality with a particular example in a separate
paper. In this paper, we only mention the main definitions and results for this
example. However, we do consider the two natural weak multiplier Hopf algebras
associated with a groupoid in detail and show that they are dual to each other
in the sense of the above duality.
",0,0,1,0,0,0
10475,10476,Differential Testing for Variational Analyses: Experience from Developing KConfigReader,"  Differential testing to solve the oracle problem has been applied in many
scenarios where multiple supposedly equivalent implementations exist, such as
multiple implementations of a C compiler. If the multiple systems disagree on
the output for a given test input, we have likely discovered a bug without
every having to specify what the expected output is. Research on variational
analyses (or variability-aware or family-based analyses) can benefit from
similar ideas. The goal of most variational analyses is to perform an analysis,
such as type checking or model checking, over a large number of configurations
much faster than an existing traditional analysis could by analyzing each
configuration separately. Variational analyses are very suitable for
differential testing, since the existence nonvariational analysis can provide
the oracle for test cases that would otherwise be tedious or difficult to
write. In this experience paper, I report how differential testing has helped
in developing KConfigReader, a tool for translating the Linux kernel's kconfig
model into a propositional formula. Differential testing allows us to quickly
build a large test base and incorporate external tests that avoided many
regressions during development and made KConfigReader likely the most precise
kconfig extraction tool available.
",1,0,0,0,0,0
17422,17423,Contextual Multi-armed Bandits under Feature Uncertainty,"  We study contextual multi-armed bandit problems under linear realizability on
rewards and uncertainty (or noise) on features. For the case of identical noise
on features across actions, we propose an algorithm, coined {\em NLinRel},
having $O\left(T^{\frac{7}{8}} \left(\log{(dT)}+K\sqrt{d}\right)\right)$ regret
bound for $T$ rounds, $K$ actions, and $d$-dimensional feature vectors. Next,
for the case of non-identical noise, we observe that popular linear hypotheses
including {\em NLinRel} are impossible to achieve such sub-linear regret.
Instead, under assumption of Gaussian feature vectors, we prove that a greedy
algorithm has $O\left(T^{\frac23}\sqrt{\log d}\right)$ regret bound with
respect to the optimal linear hypothesis. Utilizing our theoretical
understanding on the Gaussian case, we also design a practical variant of {\em
NLinRel}, coined {\em Universal-NLinRel}, for arbitrary feature distributions.
It first runs {\em NLinRel} for finding the `true' coefficient vector using
feature uncertainties and then adjust it to minimize its regret using the
statistical feature information. We justify the performance of {\em
Universal-NLinRel} on both synthetic and real-world datasets.
",1,0,0,1,0,0
6264,6265,An Effective Training Method For Deep Convolutional Neural Network,"  In this paper, we propose the nonlinearity generation method to speed up and
stabilize the training of deep convolutional neural networks. The proposed
method modifies a family of activation functions as nonlinearity generators
(NGs). NGs make the activation functions linear symmetric for their inputs to
lower model capacity, and automatically introduce nonlinearity to enhance the
capacity of the model during training. The proposed method can be considered an
unusual form of regularization: the model parameters are obtained by training a
relatively low-capacity model, that is relatively easy to optimize at the
beginning, with only a few iterations, and these parameters are reused for the
initialization of a higher-capacity model. We derive the upper and lower bounds
of variance of the weight variation, and show that the initial symmetric
structure of NGs helps stabilize training. We evaluate the proposed method on
different frameworks of convolutional neural networks over two object
recognition benchmark tasks (CIFAR-10 and CIFAR-100). Experimental results
showed that the proposed method allows us to (1) speed up the convergence of
training, (2) allow for less careful weight initialization, (3) improve or at
least maintain the performance of the model at negligible extra computational
cost, and (4) easily train a very deep model.
",1,0,0,1,0,0
2059,2060,Game-Theoretic Choice of Curing Rates Against Networked SIS Epidemics by Human Decision-Makers,"  We study networks of human decision-makers who independently decide how to
protect themselves against Susceptible-Infected-Susceptible (SIS) epidemics.
Motivated by studies in behavioral economics showing that humans perceive
probabilities in a nonlinear fashion, we examine the impacts of such
misperceptions on the equilibrium protection strategies. In our setting, nodes
choose their curing rates to minimize the infection probability under the
degree-based mean-field approximation of the SIS epidemic plus the cost of
their selected curing rate. We establish the existence of a degree based
equilibrium under both true and nonlinear perceptions of infection
probabilities (under suitable assumptions). When the per-unit cost of curing
rate is sufficiently high, we show that true expectation minimizers choose the
curing rate to be zero at the equilibrium, while curing rate is nonzero under
nonlinear probability weighting.
",1,0,0,0,0,0
3999,4000,Projection Free Rank-Drop Steps,"  The Frank-Wolfe (FW) algorithm has been widely used in solving nuclear norm
constrained problems, since it does not require projections. However, FW often
yields high rank intermediate iterates, which can be very expensive in time and
space costs for large problems. To address this issue, we propose a rank-drop
method for nuclear norm constrained problems. The goal is to generate descent
steps that lead to rank decreases, maintaining low-rank solutions throughout
the algorithm. Moreover, the optimization problems are constrained to ensure
that the rank-drop step is also feasible and can be readily incorporated into a
projection-free minimization method, e.g., Frank-Wolfe. We demonstrate that by
incorporating rank-drop steps into the Frank-Wolfe algorithm, the rank of the
solution is greatly reduced compared to the original Frank-Wolfe or its common
variants.
",0,0,0,1,0,0
10128,10129,Quotients of Buildings as $W$-Groupoids,"  We introduce structures which model the quotients of buildings by
type-preserving group actions. These structures, which we call W-groupoids,
generalize Bruhat decompositions, chambers systems of type M, and Tits
amalgams. We define the fundamental group of a W-groupoid, and characterize
buildings as connected simply connected W-groupoids. We give a brief outline of
the covering theory of W-groupoids, which produces buildings as the universal
covers of W-groupoids. The local-to-global theorem of Tits concerning spherical
3-resides allows for the construction of W-groupoids by gluing together
quotients of generalized polygons. In this way, W-groupoids can be used to
construct exotic, hyperbolic, and wild buildings.
",0,0,1,0,0,0
4406,4407,More lessons from the six box toy experiment,"  Following a paper in which the fundamental aspects of probabilistic inference
were introduced by means of a toy experiment, details of the analysis of
simulated long sequences of extractions are shown here. In fact, the striking
performance of probability-based inference and forecasting, compared to those
obtained by simple `rules', might impress those practitioners who are usually
underwhelmed by the philosophical foundation of the different methods. The
analysis of the sequences also shows how the smallness of the probability of
what has been actually observed, given the hypotheses of interest, is
irrelevant for the purpose of inference.
",0,1,1,0,0,0
15370,15371,Oxygen - Dislocation interaction in zirconium from first principles,"  Plasticity in zirconium alloys is mainly controlled by the interaction of 1/3
1210 screw dislocations with oxygen atoms in interstitial octahedral sites of
the hexagonal close-packed lattice. This process is studied here using ab
initio calculations based on the density functional theory. The atomic
simulations show that a strong repulsion exists only when the O atoms lie in
the dislocation core and belong to the prismatic dislocation habit plane. This
is a consequence of the destruction of the octahedral sites by the stacking
fault arising from the dislocation dissociation. Because of the repulsion, the
dislocation partially cross-slips to an adjacent prismatic plane, in agreement
with experiments where the lattice friction on screw dislocations in Zr-O
alloys has been attributed to the presence of jogs on the dislocations due to
local cross-slip.
",0,1,0,0,0,0
1265,1266,Discriminative Metric Learning with Deep Forest,"  A Discriminative Deep Forest (DisDF) as a metric learning algorithm is
proposed in the paper. It is based on the Deep Forest or gcForest proposed by
Zhou and Feng and can be viewed as a gcForest modification. The case of the
fully supervised learning is studied when the class labels of individual
training examples are known. The main idea underlying the algorithm is to
assign weights to decision trees in random forest in order to reduce distances
between objects from the same class and to increase them between objects from
different classes. The weights are training parameters. A specific objective
function which combines Euclidean and Manhattan distances and simplifies the
optimization problem for training the DisDF is proposed. The numerical
experiments illustrate the proposed distance metric algorithm.
",1,0,0,1,0,0
6002,6003,Linear-Time Sequence Classification using Restricted Boltzmann Machines,"  Classification of sequence data is the topic of interest for dynamic Bayesian
models and Recurrent Neural Networks (RNNs). While the former can explicitly
model the temporal dependencies between class variables, the latter have a
capability of learning representations. Several attempts have been made to
improve performance by combining these two approaches or increasing the
processing capability of the hidden units in RNNs. This often results in
complex models with a large number of learning parameters. In this paper, a
compact model is proposed which offers both representation learning and
temporal inference of class variables by rolling Restricted Boltzmann Machines
(RBMs) and class variables over time. We address the key issue of
intractability in this variant of RBMs by optimising a conditional
distribution, instead of a joint distribution. Experiments reported in the
paper on melody modelling and optical character recognition show that the
proposed model can outperform the state-of-the-art. Also, the experimental
results on optical character recognition, part-of-speech tagging and text
chunking demonstrate that our model is comparable to recurrent neural networks
with complex memory gates while requiring far fewer parameters.
",1,0,0,1,0,0
10488,10489,On Gallai's and Hajós' Conjectures for graphs with treewidth at most 3,"  A path (resp. cycle) decomposition of a graph $G$ is a set of edge-disjoint
paths (resp. cycles) of $G$ that covers the edge set of $G$. Gallai (1966)
conjectured that every graph on $n$ vertices admits a path decomposition of
size at most $\lfloor (n+1)/2\rfloor$, and Hajós (1968) conjectured that
every Eulerian graph on $n$ vertices admits a cycle decomposition of size at
most $\lfloor (n-1)/2\rfloor$. Gallai's Conjecture was verified for many
classes of graphs. In particular, Lovász (1968) verified this conjecture for
graphs with at most one vertex of even degree, and Pyber (1996) verified it for
graphs in which every cycle contains a vertex of odd degree. Hajós'
Conjecture, on the other hand, was verified only for graphs with maximum degree
$4$ and for planar graphs. In this paper, we verify Gallai's and Hajós'
Conjectures for graphs with treewidth at most $3$. Moreover, we show that the
only graphs with treewidth at most $3$ that do not admit a path decomposition
of size at most $\lfloor n/2\rfloor$ are isomorphic to $K_3$ or $K_5-e$.
Finally, we use the technique developed in this paper to present new proofs for
Gallai's and Hajós' Conjectures for graphs with maximum degree at most $4$,
and for planar graphs with girth at least $6$.
",1,0,0,0,0,0
16054,16055,"Stable Clustering Ansatz, Consistency Relations and Gravity Dual of Large-Scale Structure","  Gravitational clustering in the nonlinear regime remains poorly understood.
Gravity dual of gravitational clustering has recently been proposed as a means
to study the nonlinear regime. The stable clustering ansatz remains a key
ingredient to our understanding of gravitational clustering in the highly
nonlinear regime. We study certain aspects of violation of the stable
clustering ansatz in the gravity dual of Large Scale Structure (LSS). We extend
the recent studies of gravitational clustering using AdS gravity dual to take
into account possible departure from the stable clustering ansatz and to
arbitrary dimensions. Next, we extend the recently introduced consistency
relations to arbitrary dimensions. We use the consistency relations to test the
commonly used models of gravitational clustering including the halo models and
hierarchical ansätze. In particular we establish a tower of consistency
relations for the hierarchical amplitudes: $Q, R_a, R_b, S_a,S_b,S_c$ etc. as a
functions of the scaled peculiar velocity $h$. We also study the variants of
popular halo models in this context. In contrast to recent claims, none of
these models, in their simplest incarnation, seem to satisfy the consistency
relations in the soft limit.
",0,1,0,0,0,0
5533,5534,Vulnerability and co-susceptibility determine the size of network cascades,"  In a network, a local disturbance can propagate and eventually cause a
substantial part of the system to fail, in cascade events that are easy to
conceptualize but extraordinarily difficult to predict. Here, we develop a
statistical framework that can predict cascade size distributions by
incorporating two ingredients only: the vulnerability of individual components
and the co-susceptibility of groups of components (i.e., their tendency to fail
together). Using cascades in power grids as a representative example, we show
that correlations between component failures define structured and often
surprisingly large groups of co-susceptible components. Aside from their
implications for blackout studies, these results provide insights and a new
modeling framework for understanding cascades in financial systems, food webs,
and complex networks in general.
",1,1,0,0,0,0
10502,10503,Back to the Future: an Even More Nearly Optimal Cardinality Estimation Algorithm,"  We describe a new cardinality estimation algorithm that is extremely
space-efficient. It applies one of three novel estimators to the compressed
state of the Flajolet-Martin-85 coupon collection process. In an
apples-to-apples empirical comparison against compressed HyperLogLog sketches,
the new algorithm simultaneously wins on all three dimensions of the
time/space/accuracy tradeoff. Our prototype uses the zstd compression library,
and produces sketches that are smaller than the entropy of HLL, so no possible
implementation of compressed HLL can match its space efficiency. The paper's
technical contributions include analyses and simulations of the three new
estimators, accurate values for the entropies of FM85 and HLL, and a
non-trivial method for estimating a double asymptotic limit via simulation.
",1,0,0,0,0,0
17914,17915,Investigation on the use of Hidden-Markov Models in automatic transcription of music,"  Hidden Markov Models (HMMs) are a ubiquitous tool to model time series data,
and have been widely used in two main tasks of Automatic Music Transcription
(AMT): note segmentation, i.e. identifying the played notes after a multi-pitch
estimation, and sequential post-processing, i.e. correcting note segmentation
using training data. In this paper, we employ the multi-pitch estimation method
called Probabilistic Latent Component Analysis (PLCA), and develop AMT systems
by integrating different HMM-based modules in this framework. For note
segmentation, we use two different twostate on/o? HMMs, including a
higher-order one for duration modeling. For sequential post-processing, we
focused on a musicological modeling of polyphonic harmonic transitions, using a
first- and second-order HMMs whose states are defined through candidate note
mixtures. These different PLCA plus HMM systems have been evaluated
comparatively on two different instrument repertoires, namely the piano (using
the MAPS database) and the marovany zither. Our results show that the use of
HMMs could bring noticeable improvements to transcription results, depending on
the instrument repertoire.
",1,0,0,1,0,0
5240,5241,Up-down colorings of virtual-link diagrams and the necessity of Reidemeister moves of type II,"  We introduce an up-down coloring of a virtual-link diagram. The
colorabilities give a lower bound of the minimum number of Reidemeister moves
of type II which are needed between two 2-component virtual-link diagrams. By
using the notion of a quandle cocycle invariant, we determine the necessity of
Reidemeister moves of type II for a pair of diagrams of the trivial
virtual-knot. This implies that for any virtual-knot diagram $D$, there exists
a diagram $D'$ representing the same virtual-knot such that any sequence of
generalized Reidemeister moves between them includes at least one Reidemeister
move of type II.
",0,0,1,0,0,0
2542,2543,Few-Shot Learning with Graph Neural Networks,"  We propose to study the problem of few-shot learning with the prism of
inference on a partially observed graphical model, constructed from a
collection of input images whose label can be either observed or not. By
assimilating generic message-passing inference algorithms with their
neural-network counterparts, we define a graph neural network architecture that
generalizes several of the recently proposed few-shot learning models. Besides
providing improved numerical performance, our framework is easily extended to
variants of few-shot learning, such as semi-supervised or active learning,
demonstrating the ability of graph-based models to operate well on 'relational'
tasks.
",1,0,0,1,0,0
4527,4528,Network analysis of the COSMOS galaxy field,"  The galaxy data provided by COSMOS survey for 1 by 1 degree field of sky are
analysed by methods of complex networks. Three galaxy samples (slices) with
redshifts ranging within intervals 0.88-0.91, 0.91-0.94 and 0.94-0.97 are
studied as two-dimensional projections for the spatial distributions of
galaxies. We construct networks and calculate network measures for each sample,
in order to analyse the network similarity of different samples, distinguish
various topological environments, and find associations between galaxy
properties (colour index and stellar mass) and their topological environments.
Results indicate a high level of similarity between geometry and topology for
different galaxy samples and no clear evidence of evolutionary trends in
network measures. The distribution of local clustering coefficient C manifests
three modes which allow for discrimination between stand-alone singlets and
dumbbells (0 <= C <= 0.1), intermediately (0 < C < 0.9) and clique (0.9 <= C <=
1) like galaxies. Analysing astrophysical properties of galaxies (colour index
and stellar masses), we show that distributions are similar in all slices,
however weak evolutionary trends can also be seen across redshift slices. To
specify different topological environments we have extracted selections of
galaxies from each sample according to different modes of C distribution. We
have found statistically significant associations between evolutionary
parameters of galaxies and selections of C: the distribution of stellar mass
for galaxies with interim C differ from the corresponding distributions for
stand-alone and clique galaxies, and this difference holds for all redshift
slices. The colour index realises somewhat different behaviour.
",0,1,0,0,0,0
14655,14656,LSTM Fully Convolutional Networks for Time Series Classification,"  Fully convolutional neural networks (FCN) have been shown to achieve
state-of-the-art performance on the task of classifying time series sequences.
We propose the augmentation of fully convolutional networks with long short
term memory recurrent neural network (LSTM RNN) sub-modules for time series
classification. Our proposed models significantly enhance the performance of
fully convolutional networks with a nominal increase in model size and require
minimal preprocessing of the dataset. The proposed Long Short Term Memory Fully
Convolutional Network (LSTM-FCN) achieves state-of-the-art performance compared
to others. We also explore the usage of attention mechanism to improve time
series classification with the Attention Long Short Term Memory Fully
Convolutional Network (ALSTM-FCN). Utilization of the attention mechanism
allows one to visualize the decision process of the LSTM cell. Furthermore, we
propose fine-tuning as a method to enhance the performance of trained models.
An overall analysis of the performance of our model is provided and compared to
other techniques.
",1,0,0,1,0,0
20576,20577,Cherednik algebras and Calogero-Moser cells,"  Using the representation theory of Cherednik algebras at $t=0$ and a Galois
covering of the Calogero-Moser space, we define the notions of left, right and
two-sided Calogero-Moser cells for any finite complex reflection group. To each
Caloger-Moser two-sided cell is associated a Calogero-Moser family, while to
each Calogero-Moser left cell is associated a Calogero-Moser cellular
representation. We study properties of these objects and we conjecture that,
whenever the reflection group is real (i.e. is a Coxeter group), these notions
coincide with the one of Kazhdan-Lusztig left, right and two-sided cells,
Kazhdan-Lusztig families and Kazhdan-Lusztig cellular representations.
",0,0,1,0,0,0
6422,6423,Cosmological searches for a non-cold dark matter component,"  We explore an extended cosmological scenario where the dark matter is an
admixture of cold and additional non-cold species. The mass and temperature of
the non-cold dark matter particles are extracted from a number of cosmological
measurements. Among others, we consider tomographic weak lensing data and Milky
Way dwarf satellite galaxy counts. We also study the potential of these
scenarios in alleviating the existing tensions between local measurements and
Cosmic Microwave Background (CMB) estimates of the $S_8$ parameter, with
$S_8=\sigma_8\sqrt{\Omega_m}$, and of the Hubble constant $H_0$. In principle,
a sub-dominant, non-cold dark matter particle with a mass $m_X\sim$~keV, could
achieve the goals above. However, the preferred ranges for its temperature and
its mass are different when extracted from weak lensing observations and from
Milky Way dwarf satellite galaxy counts, since these two measurements require
suppressions of the matter power spectrum at different scales. Therefore,
solving simultaneously the CMB-weak lensing tensions and the small scale crisis
in the standard cold dark matter picture via only one non-cold dark matter
component seems to be challenging.
",0,1,0,0,0,0
14274,14275,Context Generation from Formal Specifications for C Analysis Tools,"  Analysis tools like abstract interpreters, symbolic execution tools and
testing tools usually require a proper context to give useful results when
analyzing a particular function. Such a context initializes the function
parameters and global variables to comply with function requirements. However
it may be error-prone to write it by hand: the handwritten context might
contain bugs or not match the intended specification. A more robust approach is
to specify the context in a dedicated specification language, and hold the
analysis tools to support it properly. This may mean to put significant
development efforts for enhancing the tools, something that is often not
feasible if ever possible.
This paper presents a way to systematically generate such a context from a
formal specification of a C function. This is applied to a subset of the ACSL
specification language in order to generate suitable contexts for the abstract
interpretation-based value analysis plug-ins of Frama-C, a framework for
analysis of code written in C. The idea here presented has been implemented in
a new Frama-C plug-in which is currently in use in an operational industrial
setting.
",1,0,0,0,0,0
9395,9396,On the Limitation of Local Intrinsic Dimensionality for Characterizing the Subspaces of Adversarial Examples,"  Understanding and characterizing the subspaces of adversarial examples aid in
studying the robustness of deep neural networks (DNNs) to adversarial
perturbations. Very recently, Ma et al. (ICLR 2018) proposed to use local
intrinsic dimensionality (LID) in layer-wise hidden representations of DNNs to
study adversarial subspaces. It was demonstrated that LID can be used to
characterize the adversarial subspaces associated with different attack
methods, e.g., the Carlini and Wagner's (C&W) attack and the fast gradient sign
attack.
In this paper, we use MNIST and CIFAR-10 to conduct two new sets of
experiments that are absent in existing LID analysis and report the limitation
of LID in characterizing the corresponding adversarial subspaces, which are (i)
oblivious attacks and LID analysis using adversarial examples with different
confidence levels; and (ii) black-box transfer attacks. For (i), we find that
the performance of LID is very sensitive to the confidence parameter deployed
by an attack, and the LID learned from ensembles of adversarial examples with
varying confidence levels surprisingly gives poor performance. For (ii), we
find that when adversarial examples are crafted from another DNN model, LID is
ineffective in characterizing their adversarial subspaces. These two findings
together suggest the limited capability of LID in characterizing the subspaces
of adversarial examples.
",0,0,0,1,0,0
6710,6711,On the Performance of Multi-Instrument Solar Flare Observations During Solar Cycle 24,"  The current fleet of space-based solar observatories offers us a wealth of
opportunities to study solar flares over a range of wavelengths. Significant
advances in our understanding of flare physics often come from coordinated
observations between multiple instruments. Consequently, considerable efforts
have been, and continue to be made to coordinate observations among instruments
(e.g. through the Max Millennium Program of Solar Flare Research). However,
there has been no study to date that quantifies how many flares have been
observed by combinations of various instruments. Here we describe a technique
that retrospectively searches archival databases for flares jointly observed by
RHESSI, SDO/EVE (MEGS-A and -B), Hinode/(EIS, SOT, and XRT), and IRIS. Out of
the 6953 flares of GOES magnitude C1 or greater that we consider over the 6.5
years after the launch of SDO, 40 have been observed by six or more instruments
simultaneously. Using each instrument's individual rate of success in observing
flares, we show that the numbers of flares co-observed by three or more
instruments are higher than the number expected under the assumption that the
instruments operated independently of one another. In particular, the number of
flares observed by larger numbers of instruments is much higher than expected.
Our study illustrates that these missions often acted in cooperation, or at
least had aligned goals. We also provide details on an interactive widget now
available in SSWIDL that allows a user to search for flaring events that have
been observed by a chosen set of instruments. This provides access to a broader
range of events in order to answer specific science questions. The difficulty
in scheduling coordinated observations for solar-flare research is discussed
with respect to instruments projected to begin operations during Solar Cycle
25, such as DKIST, Solar Orbiter, and Parker Solar Probe.
",0,1,0,0,0,0
2716,2717,Fast and Scalable Bayesian Deep Learning by Weight-Perturbation in Adam,"  Uncertainty computation in deep learning is essential to design robust and
reliable systems. Variational inference (VI) is a promising approach for such
computation, but requires more effort to implement and execute compared to
maximum-likelihood methods. In this paper, we propose new natural-gradient
algorithms to reduce such efforts for Gaussian mean-field VI. Our algorithms
can be implemented within the Adam optimizer by perturbing the network weights
during gradient evaluations, and uncertainty estimates can be cheaply obtained
by using the vector that adapts the learning rate. This requires lower memory,
computation, and implementation effort than existing VI methods, while
obtaining uncertainty estimates of comparable quality. Our empirical results
confirm this and further suggest that the weight-perturbation in our algorithm
could be useful for exploration in reinforcement learning and stochastic
optimization.
",0,0,0,1,0,0
8377,8378,A Volcanic Hydrogen Habitable Zone,"  The classical habitable zone is the circular region around a star in which
liquid water could exist on the surface of a rocky planet. The outer edge of
the traditional N2-CO2-H2O habitable zone (HZ) extends out to nearly 1.7 AU in
our Solar System, beyond which condensation and scattering by CO2 outstrips its
greenhouse capacity. Here, we show that volcanic outgassing of atmospheric H2
on a planet near the outer edge can extend the habitable zone out to ~2.4 AU in
our solar system. This wider volcanic hydrogen habitable zone (N2-CO2-H2O-H2)
can be sustained as long as volcanic H2 output offsets its escape from the top
of the atmosphere. We use a single-column radiative-convective climate model to
compute the HZ limits of this volcanic hydrogen habitable zone for hydrogen
concentrations between 1% and 50%, assuming diffusion-limited atmospheric
escape. At a hydrogen concentration of 50%, the effective stellar flux required
to support the outer edge decreases by ~35% to 60% for M to A stars. The
corresponding orbital distances increase by ~30% to 60%. The inner edge of this
HZ only moves out by ~0.1 to 4% relative to the classical HZ because H2 warming
is reduced in dense H2O atmospheres. The atmospheric scale heights of such
volcanic H2 atmospheres near the outer edge of the HZ also increase,
facilitating remote detection of atmospheric signatures.
",0,1,0,0,0,0
20729,20730,Unidirectional control of optically induced spin waves,"  Unidirectional control of optically induced spin waves in a rare-earth iron
garnet crystal is demonstrated. We observed the interference of two spin-wave
packets with different initial phases generated by circularly polarized light
pulses. This interference results in unidirectional propagation if the
spin-wave sources are spaced apart at 1/4 of the wavelength of the spin waves
and the initial phase difference is set to pi/2. The propagating direction of
the spin wave is switched by the polarization helicity of the light pulses.
Moreover, in a numerical simulation, applying more than two spin-wave sources
with a suitable polarization and spot shape, arbitrary manipulation of the spin
wave by the phased array method was replicated.
",0,1,0,0,0,0
10553,10554,Computational complexity and 3-manifolds and zombies,"  We show the problem of counting homomorphisms from the fundamental group of a
homology $3$-sphere $M$ to a finite, non-abelian simple group $G$ is
#P-complete, in the case that $G$ is fixed and $M$ is the computational input.
Similarly, deciding if there is a non-trivial homomorphism is NP-complete. In
both reductions, we can guarantee that every non-trivial homomorphism is a
surjection. As a corollary, for any fixed integer $m \ge 5$, it is NP-complete
to decide whether $M$ admits a connected $m$-sheeted covering.
Our construction is inspired by universality results in topological quantum
computation. Given a classical reversible circuit $C$, we construct $M$ so that
evaluations of $C$ with certain initialization and finalization conditions
correspond to homomorphisms $\pi_1(M) \to G$. An intermediate state of $C$
likewise corresponds to a homomorphism $\pi_1(\Sigma_g) \to G$, where
$\Sigma_g$ is a pointed Heegaard surface of $M$ of genus $g$. We analyze the
action on these homomorphisms by the pointed mapping class group
$\text{MCG}_*(\Sigma_g)$ and its Torelli subgroup $\text{Tor}_*(\Sigma_g)$. By
results of Dunfield-Thurston, the action of $\text{MCG}_*(\Sigma_g)$ is as
large as possible when $g$ is sufficiently large; we can pass to the Torelli
group using the congruence subgroup property of $\text{Sp}(2g,\mathbb{Z})$. Our
results can be interpreted as a sharp classical universality property of an
associated combinatorial $(2+1)$-dimensional TQFT.
",1,0,1,0,0,0
14667,14668,"Valued fields, Metastable groups","  We introduce a class of theories called metastable, including the theory of
algebraically closed valued fields (ACVF) as a motivating example. The key
local notion is that of definable types dominated by their stable part. A
theory is metastable (over a sort $\Gamma$) if every type over a sufficiently
rich base structure can be viewed as part of a $\Gamma$-parametrized family of
stably dominated types. We initiate a study of definable groups in metastable
theories of finite rank. Groups with a stably dominated generic type are shown
to have a canonical stable quotient. Abelian groups are shown to be
decomposable into a part coming from $\Gamma$, and a definable direct limit
system of groups with stably dominated generic. In the case of ACVF, among
definable subgroups of affine algebraic groups, we characterize the groups with
stably dominated generics in terms of group schemes over the valuation ring.
Finally, we classify all fields definable in ACVF.
",0,0,1,0,0,0
14438,14439,An Optimized Pattern Recognition Algorithm for Anomaly Detection in IoT Environment,"  With the advent of large-scale heterogeneous search engines comes the problem
of unified search control resulting in mismatches that could have otherwise
avoided. A mechanism is needed to determine exact patterns in web mining and
ubiquitous device searching. In this paper we demonstrate the use of an
optimized string searching algorithm to recognize exact patterns from a large
database. The underlying principle in designing the algorithm is that each
letter that maps to a fixed real values and some arithmetic operations which
are applied to compute corresponding pattern and substring values. We have
implemented this algorithm in C. We have tested the algorithm using a large
dataset. We created our own dataset using DNA sequences. The experimental
result shows the number of mismatch occurred in string search from a large
database. Furthermore, some of the inherent weaknesses in the use of this
algorithm are highlighted.
",1,0,0,0,0,0
14574,14575,Evaporation of dilute droplets in a turbulent jet: clustering and entrainment effects,"  Droplet evaporation in turbulent sprays involves unsteady, multiscale and
multiphase processes which make its comprehension and model capabilities still
limited. The present work aims to investigate droplet vaporization dynamics
within a turbulent spatial developing jet in dilute, non-reacting conditions.
We address the problem using a Direct Numerical Simulation of jet laden with
acetone droplets using an hybrid Eulerian/Lagrangian approach based on the
point droplet approximation. A detailed statistical analysis of both phases is
presented. In particular, we show how crucial is the preferential sampling of
the vapour phase induced by the inhomogeneous localization of the droplets
through the flow. The preferential segregation of droplets develops suddenly
downstream the inlet both within the turbulent core and in the mixing layer.
Two distinct mechanisms have been found to drive these phenomena, the inertial
small-scale clustering in the jet core and the intermittent dynamics of
droplets across the turbulent/non-turbulent interface in the mixing layer where
dry air entrainment occurs. These phenomenologies strongly affect the overall
vaporization process and lead to a spectacular widening of droplets size and
vaporization rate distributions in the downstream evolution of the turbulent
spray.
",0,1,0,0,0,0
20781,20782,External Prior Guided Internal Prior Learning for Real-World Noisy Image Denoising,"  Most of existing image denoising methods learn image priors from either
external data or the noisy image itself to remove noise. However, priors
learned from external data may not be adaptive to the image to be denoised,
while priors learned from the given noisy image may not be accurate due to the
interference of corrupted noise. Meanwhile, the noise in real-world noisy
images is very complex, which is hard to be described by simple distributions
such as Gaussian distribution, making real-world noisy image denoising a very
challenging problem. We propose to exploit the information in both external
data and the given noisy image, and develop an external prior guided internal
prior learning method for real-world noisy image denoising. We first learn
external priors from an independent set of clean natural images. With the aid
of learned external priors, we then learn internal priors from the given noisy
image to refine the prior model. The external and internal priors are
formulated as a set of orthogonal dictionaries to efficiently reconstruct the
desired image. Extensive experiments are performed on several real-world noisy
image datasets. The proposed method demonstrates highly competitive denoising
performance, outperforming state-of-the-art denoising methods including those
designed for real-world noisy images.
",1,0,0,0,0,0
11967,11968,Discrete-time Risk-sensitive Mean-field Games,"  In this paper, we study a class of discrete-time mean-field games under the
infinite-horizon risk-sensitive discounted-cost optimality criterion.
Risk-sensitivity is introduced for each agent (player) via an exponential
utility function. In this game model, each agent is coupled with the rest of
the population through the empirical distribution of the states, which affects
both the agent's individual cost and its state dynamics. Under mild
assumptions, we establish the existence of a mean-field equilibrium in the
infinite-population limit as the number of agents ($N$) goes to infinity, and
then show that the policy obtained from the mean-field equilibrium constitutes
an approximate Nash equilibrium when $N$ is sufficiently large.
",1,0,0,0,0,0
2914,2915,Cross-label Suppression: A Discriminative and Fast Dictionary Learning with Group Regularization,"  This paper addresses image classification through learning a compact and
discriminative dictionary efficiently. Given a structured dictionary with each
atom (columns in the dictionary matrix) related to some label, we propose
cross-label suppression constraint to enlarge the difference among
representations for different classes. Meanwhile, we introduce group
regularization to enforce representations to preserve label properties of
original samples, meaning the representations for the same class are encouraged
to be similar. Upon the cross-label suppression, we don't resort to
frequently-used $\ell_0$-norm or $\ell_1$-norm for coding, and obtain
computational efficiency without losing the discriminative power for
categorization. Moreover, two simple classification schemes are also developed
to take full advantage of the learnt dictionary. Extensive experiments on six
data sets including face recognition, object categorization, scene
classification, texture recognition and sport action categorization are
conducted, and the results show that the proposed approach can outperform lots
of recently presented dictionary algorithms on both recognition accuracy and
computational efficiency.
",1,0,0,1,0,0
9313,9314,Equipping weak equivalences with algebraic structure,"  We investigate the extent to which the weak equivalences in a model category
can be equipped with algebraic structure. We prove, for instance, that there
exists a monad T such that a morphism of topological spaces admits T-algebra
structure if and only it is a weak homotopy equivalence. Likewise for
quasi-isomorphisms and many other examples. The basic trick is to consider
injectivity in arrow categories. Using algebraic injectivity and cone
injectivity we obtain general results about the extent to which the weak
equivalences in a combinatorial model category can be equipped with algebraic
structure.
",0,0,1,0,0,0
15804,15805,Linguistic Diversities of Demographic Groups in Twitter,"  The massive popularity of online social media provides a unique opportunity
for researchers to study the linguistic characteristics and patterns of user's
interactions. In this paper, we provide an in-depth characterization of
language usage across demographic groups in Twitter. In particular, we extract
the gender and race of Twitter users located in the U.S. using advanced image
processing algorithms from Face++. Then, we investigate how demographic groups
(i.e. male/female, Asian/Black/White) differ in terms of linguistic styles and
also their interests. We extract linguistic features from 6 categories
(affective attributes, cognitive attributes, lexical density and awareness,
temporal references, social and personal concerns, and interpersonal focus), in
order to identify the similarities and differences in particular writing set of
attributes. In addition, we extract the absolute ranking difference of top
phrases between demographic groups. As a dimension of diversity, we also use
the topics of interest that we retrieve from each user. Our analysis unveils
clear differences in the writing styles (and the topics of interest) of
different demographic groups, with variation seen across both gender and race
lines. We hope our effort can stimulate the development of new studies related
to demographic information in the online space.
",1,0,0,0,0,0
18611,18612,Do planets remember how they formed?,"  One of the most directly observable features of a transiting multi-planet
system is their size-ordering when ranked in orbital separation. Kepler has
revealed a rich diversity of outcomes, from perfectly ordered systems, like
Kepler-80, to ostensibly disordered systems, like Kepler-20. Under the
hypothesis that systems are born via preferred formation pathways, one might
reasonably expect non-random size-orderings reflecting these processes.
However, subsequent dynamical evolution, often chaotic and turbulent in nature,
may erode this information and so here we ask - do systems remember how they
formed? To address this, we devise a model to define the entropy of a planetary
system's size-ordering, by first comparing differences between neighboring
planets and then extending to accommodate differences across the chain. We
derive closed-form solutions for many of the micro state occupancies and
provide public code with look-up tables to compute entropy for up to ten-planet
systems. All three proposed entropy definitions exhibit the expected property
that their credible interval increases with respect to a proxy for time. We
find that the observed Kepler multis display a highly significant deficit in
entropy compared to a randomly generated population. Incorporating a filter for
systems deemed likely to be dynamically packed, we show that this result is
robust against the possibility of missing planets too. Put together, our work
establishes that Kepler systems do indeed remember something of their younger
years and highlights the value of information theory for exoplanetary science.
",0,1,0,0,0,0
10436,10437,Towards automation of data quality system for CERN CMS experiment,"  Daily operation of a large-scale experiment is a challenging task,
particularly from perspectives of routine monitoring of quality for data being
taken. We describe an approach that uses Machine Learning for the automated
system to monitor data quality, which is based on partial use of data qualified
manually by detector experts. The system automatically classifies marginal
cases: both of good an bad data, and use human expert decision to classify
remaining ""grey area"" cases.
This study uses collision data collected by the CMS experiment at LHC in
2010. We demonstrate that proposed workflow is able to automatically process at
least 20\% of samples without noticeable degradation of the result.
",1,0,0,0,0,0
14051,14052,Cell-Probe Lower Bounds from Online Communication Complexity,"  In this work, we introduce an online model for communication complexity.
Analogous to how online algorithms receive their input piece-by-piece, our
model presents one of the players, Bob, his input piece-by-piece, and has the
players Alice and Bob cooperate to compute a result each time before the next
piece is revealed to Bob. This model has a closer and more natural
correspondence to dynamic data structures than classic communication models do,
and hence presents a new perspective on data structures.
We first present a tight lower bound for the online set intersection problem
in the online communication model, demonstrating a general approach for proving
online communication lower bounds. The online communication model prevents a
batching trick that classic communication complexity allows, and yields a
stronger lower bound. We then apply the online communication model to prove
data structure lower bounds for two dynamic data structure problems: the Group
Range problem and the Dynamic Connectivity problem for forests. Both of the
problems admit a worst case $O(\log n)$-time data structure. Using online
communication complexity, we prove a tight cell-probe lower bound for each:
spending $o(\log n)$ (even amortized) time per operation results in at best an
$\exp(-\delta^2 n)$ probability of correctly answering a
$(1/2+\delta)$-fraction of the $n$ queries.
",1,0,0,0,0,0
12291,12292,Modeling and Analysis of HetNets with mm-Wave Multi-RAT Small Cells Deployed Along Roads,"  We characterize a multi tier network with classical macro cells, and multi
radio access technology (RAT) small cells, which are able to operate in
microwave and millimeter-wave (mm-wave) bands. The small cells are assumed to
be deployed along roads modeled as a Poisson line process. This
characterization is more realistic as compared to the classical Poisson point
processes typically used in literature. In this context, we derive the
association and RAT selection probabilities of the typical user under various
system parameters such as the small cell deployment density and mm-wave antenna
gain, and with varying street densities. Finally, we calculate the signal to
interference plus noise ratio (SINR) coverage probability for the typical user
considering a tractable dominant interference based model for mm-wave
interference. Our analysis reveals the need of deploying more small cells per
street in cities with more streets to maintain coverage, and highlights that
mm-wave RAT in small cells can help to improve the SINR performance of the
users.
",1,0,0,0,0,0
11107,11108,The Role of Gender in Social Network Organization,"  The digital traces we leave behind when engaging with the modern world offer
an interesting lens through which we study behavioral patterns as expression of
gender. Although gender differentiation has been observed in a number of
settings, the majority of studies focus on a single data stream in isolation.
Here we use a dataset of high resolution data collected using mobile phones, as
well as detailed questionnaires, to study gender differences in a large cohort.
We consider mobility behavior and individual personality traits among a group
of more than $800$ university students. We also investigate interactions among
them expressed via person-to-person contacts, interactions on online social
networks, and telecommunication. Thus, we are able to study the differences
between male and female behavior captured through a multitude of channels for a
single cohort. We find that while the two genders are similar in a number of
aspects, there are robust deviations that include multiple facets of social
interactions, suggesting the existence of inherent behavioral differences.
Finally, we quantify how aspects of an individual's characteristics and social
behavior reveals their gender by posing it as a classification problem. We ask:
How well can we distinguish between male and female study participants based on
behavior alone? Which behavioral features are most predictive?
",1,0,0,0,0,0
18053,18054,Tunneling Field-Effect Junctions with WS$_2$ barrier,"  Transition metal dichalcogenides (TMDCs), with their two-dimensional
structures and sizable bandgaps, are good candidates for barrier materials in
tunneling field-effect transistor (TFET) formed from atomic precision vertical
stacks of graphene and insulating crystals of a few atomic layers in thickness.
We report first-principles study of the electronic properties of the
Graphene/WS$_2$/Graphene sandwich structure revealing strong interface effects
on dielectric properties and predicting a high ON/OFF ratio with an appropriate
WS$_2$ thickness and a suitable range of the gate voltage. Both the band
spin-orbit coupling splitting and the dielectric constant of the WS$_2$ layer
depend on its thickness when in contact with the graphene electrodes,
indicating strong influence from graphene across the interfaces. The dielectric
constant is significantly reduced from the bulk WS$_2$ value. The effective
barrier height varies with WS$_2$ thickness and can be tuned by a gate voltage.
These results are critical for future nanoelectronic device designs.
",0,1,0,0,0,0
71,72,The nature of the tensor order in Cd2Re2O7,"  The pyrochlore metal Cd2Re2O7 has been recently investigated by
second-harmonic generation (SHG) reflectivity. In this paper, we develop a
general formalism that allows for the identification of the relevant tensor
components of the SHG from azimuthal scans. We demonstrate that the secondary
order parameter identified by SHG at the structural phase transition is the
x2-y2 component of the axial toroidal quadrupole. This differs from the 3z2-r2
symmetry of the atomic displacements associated with the I-4m2 crystal
structure that was previously thought to be its origin. Within the same
formalism, we suggest that the primary order parameter detected in the SHG
experiment is the 3z2-r2 component of the magnetic quadrupole. We discuss the
general mechanism driving the phase transition in our proposed framework, and
suggest experiments, particularly resonant X-ray scattering ones, that could
clarify this issue.
",0,1,0,0,0,0
16838,16839,Measures of Tractography Convergence,"  In the present work, we use information theory to understand the empirical
convergence rate of tractography, a widely-used approach to reconstruct
anatomical fiber pathways in the living brain. Based on diffusion MRI data,
tractography is the starting point for many methods to study brain
connectivity. Of the available methods to perform tractography, most
reconstruct a finite set of streamlines, or 3D curves, representing probable
connections between anatomical regions, yet relatively little is known about
how the sampling of this set of streamlines affects downstream results, and how
exhaustive the sampling should be. Here we provide a method to measure the
information theoretic surprise (self-cross entropy) for tract sampling schema.
We then empirically assess four streamline methods. We demonstrate that the
relative information gain is very low after a moderate number of streamlines
have been generated for each tested method. The results give rise to several
guidelines for optimal sampling in brain connectivity analyses.
",0,0,0,1,1,0
6309,6310,Social Media Would Not Lie: Prediction of the 2016 Taiwan Election via Online Heterogeneous Data,"  The prevalence of online media has attracted researchers from various domains
to explore human behavior and make interesting predictions. In this research,
we leverage heterogeneous social media data collected from various online
platforms to predict Taiwan's 2016 presidential election. In contrast to most
existing research, we take a ""signal"" view of heterogeneous information and
adopt the Kalman filter to fuse multiple signals into daily vote predictions
for the candidates. We also consider events that influenced the election in a
quantitative manner based on the so-called event study model that originated in
the field of financial research. We obtained the following interesting
findings. First, public opinions in online media dominate traditional polls in
Taiwan election prediction in terms of both predictive power and timeliness.
But offline polls can still function on alleviating the sample bias of online
opinions. Second, although online signals converge as election day approaches,
the simple Facebook ""Like"" is consistently the strongest indicator of the
election result. Third, most influential events have a strong connection to
cross-strait relations, and the Chou Tzu-yu flag incident followed by the
apology video one day before the election increased the vote share of Tsai
Ing-Wen by 3.66%. This research justifies the predictive power of online media
in politics and the advantages of information fusion. The combined use of the
Kalman filter and the event study method contributes to the data-driven
political analytics paradigm for both prediction and attribution purposes.
",1,0,0,1,0,0
14037,14038,"A Multi-Ringed, Modestly-Inclined Protoplanetary Disk around AA Tau","  AA Tau is the archetype for a class of stars with a peculiar periodic
photometric variability thought to be related to a warped inner disk structure
with a nearly edge-on viewing geometry. We present high resolution ($\sim$0.2"")
ALMA observations of the 0.87 and 1.3~mm dust continuum emission from the disk
around AA Tau. These data reveal an evenly spaced three-ringed emission
structure, with distinct peaks at 0.34"", 0.66"", and 0.99"", all viewed at a
modest inclination of 59.1$^{\circ}\pm$0.3$^{\circ}$ (decidedly not edge-on).
In addition to this ringed substructure, we find non-axisymmetric features
including a `bridge' of emission that connects opposite sides of the innermost
ring. We speculate on the nature of this `bridge' in light of accompanying
observations of HCO$^+$ and $^{13}$CO (J=3--2) line emission. The HCO$^+$
emission is bright interior to the innermost dust ring, with a projected
velocity field that appears rotated with respect to the resolved disk geometry,
indicating the presence of a warp or inward radial flow. We suggest that the
continuum bridge and HCO$^+$ line kinematics could originate from gap-crossing
accretion streams, which may be responsible for the long-duration dimming of
optical light from AA Tau.
",0,1,0,0,0,0
11306,11307,Generative Bridging Network in Neural Sequence Prediction,"  In order to alleviate data sparsity and overfitting problems in maximum
likelihood estimation (MLE) for sequence prediction tasks, we propose the
Generative Bridging Network (GBN), in which a novel bridge module is introduced
to assist the training of the sequence prediction model (the generator
network). Unlike MLE directly maximizing the conditional likelihood, the bridge
extends the point-wise ground truth to a bridge distribution conditioned on it,
and the generator is optimized to minimize their KL-divergence. Three different
GBNs, namely uniform GBN, language-model GBN and coaching GBN, are proposed to
penalize confidence, enhance language smoothness and relieve learning burden.
Experiments conducted on two recognized sequence prediction tasks (machine
translation and abstractive text summarization) show that our proposed GBNs can
yield significant improvements over strong baselines. Furthermore, by analyzing
samples drawn from different bridges, expected influences on the generator are
verified.
",1,0,0,1,0,0
12038,12039,Efficient and Scalable View Generation from a Single Image using Fully Convolutional Networks,"  Single-image-based view generation (SIVG) is important for producing 3D
stereoscopic content. Here, handling different spatial resolutions as input and
optimizing both reconstruction accuracy and processing speed is desirable.
Latest approaches are based on convolutional neural network (CNN), and they
generate promising results. However, their use of fully connected layers as
well as pre-trained VGG forces a compromise between reconstruction accuracy and
processing speed. In addition, this approach is limited to the use of a
specific spatial resolution. To remedy these problems, we propose exploiting
fully convolutional networks (FCN) for SIVG. We present two FCN architectures
for SIVG. The first one is based on combination of an FCN and a view-rendering
network called DeepView$_{ren}$. The second one consists of decoupled networks
for luminance and chrominance signals, denoted by DeepView$_{dec}$. To train
our solutions we present a large dataset of 2M stereoscopic images. Results
show that both of our architectures improve accuracy and speed over the state
of the art. DeepView$_{ren}$ generates competitive accuracy to the state of the
art, however, with the fastest processing speed of all. That is x5 times faster
speed and x24 times lower memory consumption compared to the state of the art.
DeepView$_{dec}$ has much higher accuracy, but with x2.5 times faster speed and
x12 times lower memory consumption. We evaluated our approach with both
objective and subjective studies.
",1,0,0,0,0,0
18640,18641,Asymptotic and numerical analysis of a stochastic PDE model of volume transmission,"  Volume transmission is an important neural communication pathway in which
neurons in one brain region influence the neurotransmitter concentration in the
extracellular space of a distant brain region. In this paper, we apply
asymptotic analysis to a stochastic partial differential equation model of
volume transmission to calculate the neurotransmitter concentration in the
extracellular space. Our model involves the diffusion equation in a
three-dimensional domain with interior holes that randomly switch between being
either sources or sinks. These holes model nerve varicosities that alternate
between releasing and absorbing neurotransmitter, according to when they fire
action potentials. In the case that the holes are small, we compute
analytically the first two nonzero terms in an asymptotic expansion of the
average neurotransmitter concentration. The first term shows that the
concentration is spatially constant to leading order and that this constant is
independent of many details in the problem. Specifically, this constant first
term is independent of the number and location of nerve varicosities, neural
firing correlations, and the size and geometry of the extracellular space. The
second term shows how these factors affect the concentration at second order.
Interestingly, the second term is also spatially constant under some mild
assumptions. We verify our asymptotic results by high-order numerical
simulation using radial basis function-generated finite differences.
",0,0,0,0,1,0
8903,8904,Towards a Rigorous Methodology for Measuring Adoption of RPKI Route Validation and Filtering,"  A proposal to improve routing security---Route Origin Authorization
(ROA)---has been standardized. A ROA specifies which network is allowed to
announce a set of Internet destinations. While some networks now specify ROAs,
little is known about whether other networks check routes they receive against
these ROAs, a process known as Route Origin Validation (ROV). Which networks
blindly accept invalid routes? Which reject them outright? Which de-preference
them if alternatives exist?
Recent analysis attempts to use uncontrolled experiments to characterize ROV
adoption by comparing valid routes and invalid routes. However, we argue that
gaining a solid understanding of ROV adoption is impossible using currently
available data sets and techniques. Our measurements suggest that, although
some ISPs are not observed using invalid routes in uncontrolled experiments,
they are actually using different routes for (non-security) traffic engineering
purposes, without performing ROV. We conclude with a description of a
controlled, verifiable methodology for measuring ROV and present three ASes
that do implement ROV, confirmed by operators.
",1,0,0,0,0,0
2230,2231,Multi-district preference modelling,"  Generating realistic artificial preference distributions is an important part
of any simulation analysis of electoral systems. While this has been discussed
in some detail in the context of a single electoral district, many electoral
systems of interest are based on multiple districts. Neither treating
preferences between districts as independent nor ignoring the district
structure yields satisfactory results. We present a model based on an extension
of the classic Eggenberger-Pólya urn, in which each district is represented
by an urn and there is correlation between urns. We show in detail that this
procedure has a small number of tunable parameters, is computationally
efficient, and produces ""realistic-looking"" distributions. We intend to use it
in further studies of electoral systems.
",1,1,0,0,0,0
5100,5101,Maximal fluctuations of confined actomyosin gels: dynamics of the cell nucleus,"  We investigate the effect of stress fluctuations on the stochastic dynamics
of an inclusion embedded in a viscous gel. We show that, in non-equilibrium
systems, stress fluctuations give rise to an effective attraction towards the
boundaries of the confining domain, which is reminiscent of an active Casimir
effect. We apply this generic result to the dynamics of deformations of the
cell nucleus and we demonstrate the appearance of a fluctuation maximum at a
critical level of activity, in agreement with recent experiments [E. Makhija,
D. S. Jokhun, and G. V. Shivashankar, Proc. Natl. Acad. Sci. U.S.A. 113, E32
(2016)].
",0,1,0,0,0,0
14760,14761,Improving Vision-based Self-positioning in Intelligent Transportation Systems via Integrated Lane and Vehicle Detection,"  Traffic congestion is a widespread problem. Dynamic traffic routing systems
and congestion pricing are getting importance in recent research. Lane
prediction and vehicle density estimation is an important component of such
systems. We introduce a novel problem of vehicle self-positioning which
involves predicting the number of lanes on the road and vehicle's position in
those lanes using videos captured by a dashboard camera. We propose an
integrated closed-loop approach where we use the presence of vehicles to aid
the task of self-positioning and vice-versa. To incorporate multiple factors
and high-level semantic knowledge into the solution, we formulate this problem
as a Bayesian framework. In the framework, the number of lanes, the vehicle's
position in those lanes and the presence of other vehicles are considered as
parameters. We also propose a bounding box selection scheme to reduce the
number of false detections and increase the computational efficiency. We show
that the number of box proposals decreases by a factor of 6 using the selection
approach. It also results in large reduction in the number of false detections.
The entire approach is tested on real-world videos and is found to give
acceptable results.
",1,0,0,0,0,0
8372,8373,The Use of Unlabeled Data versus Labeled Data for Stopping Active Learning for Text Classification,"  Annotation of training data is the major bottleneck in the creation of text
classification systems. Active learning is a commonly used technique to reduce
the amount of training data one needs to label. A crucial aspect of active
learning is determining when to stop labeling data. Three potential sources for
informing when to stop active learning are an additional labeled set of data,
an unlabeled set of data, and the training data that is labeled during the
process of active learning. To date, no one has compared and contrasted the
advantages and disadvantages of stopping methods based on these three
information sources. We find that stopping methods that use unlabeled data are
more effective than methods that use labeled data.
",1,0,0,1,0,0
11644,11645,Anisotropic Fermi surface probed by the de Haas-van Alphen oscillation in proposed Dirac Semimetal TaSb$_{2}$,"  TaSb$_{2}$ has been predicted theoretically and proposed through
magnetotransport experiment to be a topological semimetal. In earlier reports,
the Shubnikov-de Haas oscillation has been analyzed to probe the Fermi surface,
with magnetic field along a particular crystallographic axis only. By employing
a sample rotator, we reveal highly anisotropic transverse magnetoresistance by
rotating the magnetic field along different crystallographic directions. To
probe the anisotropy in the Fermi surface, we have performed magnetization
measurements and detected strong de Haas-van Alphen (dHvA) oscillations for the
magnetic field applied along \textbf{b} and \textbf{c} axes as well as
perpendicular to \textbf{bc} plane of the crystals. Three Fermi pockets have
been identified by analyzing the dHvA oscillations. Hall measurement reveals
electron as the only charge carrier, i.e., all the three Fermi pockets are
electron type. With the application of magnetic field along different crystal
directions, the cross sectional areas of the Fermi pockets have been found
significantly different. Other physical parameters, such as the effective mass
of the charge carrier and Fermi velocity have also been calculated using the
Lifshitz-Kosevich formula.
",0,1,0,0,0,0
12137,12138,STWalk: Learning Trajectory Representations in Temporal Graphs,"  Analyzing the temporal behavior of nodes in time-varying graphs is useful for
many applications such as targeted advertising, community evolution and outlier
detection. In this paper, we present a novel approach, STWalk, for learning
trajectory representations of nodes in temporal graphs. The proposed framework
makes use of structural properties of graphs at current and previous time-steps
to learn effective node trajectory representations. STWalk performs random
walks on a graph at a given time step (called space-walk) as well as on graphs
from past time-steps (called time-walk) to capture the spatio-temporal behavior
of nodes. We propose two variants of STWalk to learn trajectory
representations. In one algorithm, we perform space-walk and time-walk as part
of a single step. In the other variant, we perform space-walk and time-walk
separately and combine the learned representations to get the final trajectory
embedding. Extensive experiments on three real-world temporal graph datasets
validate the effectiveness of the learned representations when compared to
three baseline methods. We also show the goodness of the learned trajectory
embeddings for change point detection, as well as demonstrate that arithmetic
operations on these trajectory representations yield interesting and
interpretable results.
",1,0,0,1,0,0
8363,8364,Deep Neural Networks for Multiple Speaker Detection and Localization,"  We propose to use neural networks for simultaneous detection and localization
of multiple sound sources in human-robot interaction. In contrast to
conventional signal processing techniques, neural network-based sound source
localization methods require fewer strong assumptions about the environment.
Previous neural network-based methods have been focusing on localizing a single
sound source, which do not extend to multiple sources in terms of detection and
localization. In this paper, we thus propose a likelihood-based encoding of the
network output, which naturally allows the detection of an arbitrary number of
sources. In addition, we investigate the use of sub-band cross-correlation
information as features for better localization in sound mixtures, as well as
three different network architectures based on different motivations.
Experiments on real data recorded from a robot show that our proposed methods
significantly outperform the popular spatial spectrum-based approaches.
",1,0,0,0,0,0
1788,1789,PythonRobotics: a Python code collection of robotics algorithms,"  This paper describes an Open Source Software (OSS) project: PythonRobotics.
This is a collection of robotics algorithms implemented in the Python
programming language. The focus of the project is on autonomous navigation, and
the goal is for beginners in robotics to understand the basic ideas behind each
algorithm. In this project, the algorithms which are practical and widely used
in both academia and industry are selected. Each sample code is written in
Python3 and only depends on some standard modules for readability and ease of
use. It includes intuitive animations to understand the behavior of the
simulation.
",1,0,0,0,0,0
16296,16297,Monodromy and Vinberg fusion for the principal degeneration of the space of G-bundles,"  We study the geometry and the singularities of the principal direction of the
Drinfeld-Lafforgue-Vinberg degeneration of the moduli space of G-bundles Bun_G
for an arbitrary reductive group G, and their relationship to the Langlands
dual group of G.
In the first part of the article we study the monodromy action on the nearby
cycles sheaf along the principal degeneration of Bun_G. We describe the
weight-monodromy filtration in terms of the combinatorics of the Langlands dual
group of G and generalizations of the Picard-Lefschetz oscillators found in
[Sch1]. Our proofs use certain local models for the principal degeneration
whose geometry is studied in the second part.
Our local models simultaneously provide two types of degenerations of the
Zastava spaces, which together equip the Zastava spaces with the geometric
analog of a Hopf algebra structure. The first degeneration corresponds to the
usual Beilinson-Drinfeld fusion of divisors on the curve. The second
degeneration is new and corresponds to what we call Vinberg fusion: It is
obtained not by degenerating divisors on the curve, but by degenerating the
group G via the Vinberg semigroup. On the level of cohomology the Vinberg
fusion gives rise to an algebra structure, while the Beilinson-Drinfeld fusion
gives rise to a coalgebra structure; the Hopf algebra axiom is a consequence of
the underlying geometry.
It is natural to conjecture that this Hopf algebra agrees with the universal
enveloping algebra of the positive part of the Langlands dual Lie algebra. The
above procedure would then yield a novel and highly geometric way to pass to
the Langlands dual side: Elements of the Langlands dual Lie algebra are
represented as cycles on the above moduli spaces, and the Lie bracket of two
elements is obtained by deforming the cartesian product cycle along the Vinberg
degeneration.
",0,0,1,0,0,0
8196,8197,Topological Dirac Nodal-net Fermions in AlB$_2$-type TiB$_2$ and ZrB$_2$,"  Based on first-principles calculations and effective model analysis, a Dirac
nodal-net semimetal state is recognized in AlB$_2$-type TiB$_2$ and ZrB$_2$
when spin-orbit coupling (SOC) is ignored. Taking TiB$_2$ as an example, there
are several topological excitations in this nodal-net structure including
triple point, nexus, and nodal link, which are protected by coexistence of
spatial-inversion symmetry and time reversal symmetry. This nodal-net state is
remarkably different from that of IrF$_4$, which requires sublattice chiral
symmetry. In addition, linearly and quadratically dispersed two-dimensional
surface Dirac points are identified as having emerged on the B-terminated and
Ti-terminated (001) surfaces of TiB$_2$ respectively, which are analogous to
those of monolayer and bilayer graphene.
",0,1,0,0,0,0
19562,19563,Transition of multi-diffusive states in a biased periodic potential,"  We study a frequency-dependent damping model of hyper-diffusion within the
generalized Langevin equation. The model allows for the colored noise defined
by its spectral density, assumed to be proportional to $\omega^{\delta-1}$ at
low frequencies with $0<\delta<1$ (sub-Ohmic damping) or $1<\delta<2$
(super-Ohmic damping), where the frequency-dependent damping is deduced from
the noise by means of the fluctuation-dissipation theorem. It is shown that for
super-Ohmic damping and certain parameters, the diffusive process of the
particle in a titled periodic potential undergos sequentially four
time-regimes: thermalization, hyper-diffusion, collapse and asymptotical
restoration. For analysing transition phenomenon of multi-diffusive states, we
demonstrate that the first exist time of the particle escaping from the locked
state into the running state abides by an exponential distribution. The concept
of equivalent velocity trap is introduced in the present model, moreover,
reformation of ballistic diffusive system is also considered as a marginal
situation, however there does not exhibit the collapsed state of diffusion.
",0,1,0,0,0,0
9196,9197,Relativistic Spacecraft Propelled by Directed Energy,"  Achieving relativistic flight to enable extrasolar exploration is one of the
dreams of humanity and the long term goal of our NASA Starlight program. We
derive a fully relativistic solution for the motion of a spacecraft propelled
by radiation pressure from a directed energy system. Depending on the system
parameters, low mass spacecraft can achieve relativistic speeds; thereby
enabling interstellar exploration. The diffraction of the directed energy
system plays an important role and limits the maximum speed of the spacecraft.
We consider 'photon recycling' as a possible method to achieving higher speeds.
We also discuss recent claims that our previous work on this topic is incorrect
and show that these claims arise from an improper treatment of causality.
",0,1,0,0,0,0
1798,1799,Efficient algorithms to discover alterations with complementary functional association in cancer,"  Recent large cancer studies have measured somatic alterations in an
unprecedented number of tumours. These large datasets allow the identification
of cancer-related sets of genetic alterations by identifying relevant
combinatorial patterns. Among such patterns, mutual exclusivity has been
employed by several recent methods that have shown its effectivenes in
characterizing gene sets associated to cancer. Mutual exclusivity arises
because of the complementarity, at the functional level, of alterations in
genes which are part of a group (e.g., a pathway) performing a given function.
The availability of quantitative target profiles, from genetic perturbations or
from clinical phenotypes, provides additional information that can be leveraged
to improve the identification of cancer related gene sets by discovering groups
with complementary functional associations with such targets.
In this work we study the problem of finding groups of mutually exclusive
alterations associated with a quantitative (functional) target. We propose a
combinatorial formulation for the problem, and prove that the associated
computation problem is computationally hard. We design two algorithms to solve
the problem and implement them in our tool UNCOVER. We provide analytic
evidence of the effectiveness of UNCOVER in finding high-quality solutions and
show experimentally that UNCOVER finds sets of alterations significantly
associated with functional targets in a variety of scenarios. In addition, our
algorithms are much faster than the state-of-the-art, allowing the analysis of
large datasets of thousands of target profiles from cancer cell lines. We show
that on one such dataset from project Achilles our methods identify several
significant gene sets with complementary functional associations with targets.
",0,0,0,0,1,0
10439,10440,Exchange constants in molecule-based magnets derived from density functional methods,"  Cu(pyz)(NO3)2 is a quasi one-dimensional molecular antiferromagnet that
exhibits three dimensional long-range magnetic order below TN=110 mK due to the
presence of weak inter-chain exchange couplings. Here we compare calculations
of the three largest exchange coupling constants in this system using two
techniques based on plane-wave basis-set density functional theory: (i) a dimer
fragment approach and (ii) an approach using periodic boundary conditions. The
calculated values of the large intrachain coupling constant are found to be
consistent with experiment, showing the expected level of variation between
different techniques and implementations. However, the interchain coupling
constants are found to be smaller than the current limits on the resolution of
the calculations. This is due to the computational limitations on convergence
of absolute energy differences with respect to basis set, which are larger than
the inter-chain couplings themselves. Our results imply that errors resulting
from such limitations are inherent in the evaluation of small exchange
constants in systems of this sort, and that many previously reported results
should therefore be treated with caution.
",0,1,0,0,0,0
1494,1495,Helium-like and Lithium-like ions: Ground state energy,"  It is shown that the non-relativistic ground state energy of helium-like and
lithium-like ions with static nuclei can be interpolated in full physics range
of nuclear charges $Z$ with accuracy of not less than 6 decimal digits (d.d.)
or 7-8 significant digits (s.d.) using a meromorphic function in appropriate
variable with a few free parameters. It is demonstrated that finite nuclear
mass effects do not change 4-5 s.d. for $Z \in [1,50]$ for 2-,3-electron
systems and the leading relativistic and QED corrections leave unchanged 3-4
s.d. for $Z \in [1,12]$ in the ground state energy for 2-electron system, thus,
the interpolation reproduces definitely those figures. A meaning of proposed
interpolation is in a construction of unified, {\it two-point} Pade approximant
(for both small and large $Z$ expansions) with fitting some parameters at
intermediate $Z$.
",0,1,0,0,0,0
12488,12489,Hedging in fractional Black-Scholes model with transaction costs,"  We consider conditional-mean hedging in a fractional Black-Scholes pricing
model in the presence of proportional transaction costs. We develop an explicit
formula for the conditional-mean hedging portfolio in terms of the recently
discovered explicit conditional law of the fractional Brownian motion.
",0,0,1,1,0,0
19244,19245,Socio-economic constraints to maximum human lifespan,"  The analysis of the demographic transition of the past century and a half,
using both empirical data and mathematical models, has rendered a wealth of
well-established facts, including the dramatic increases in life expectancy.
Despite these insights, such analyses have also occasionally triggered debates
which spill over many disciplines, from genetics, to biology, or demography.
Perhaps the hottest discussion is happening around the question of maximum
human lifespan, which --besides its fascinating historical and philosophical
interest-- poses urgent pragmatic warnings on a number of issues in public and
private decision-making. In this paper, we add to the controversy some results
which, based on purely statistical grounds, suggest that the maximum human
lifespan is not fixed, or has not reached yet a plateau. Quite the contrary,
analysis on reliable data for over 150 years in more than 20 industrialized
countries point at a sustained increase in the maximum age at death.
Furthermore, were this trend to continue, a limitless lifespan could be
achieved by 2102. Finally, we quantify the dependence of increases in the
maximum lifespan on socio-economic factors. Our analysis indicates that in some
countries the observed rising patterns can only be sustained by progressively
larger increases in GDP, setting the problem of longevity in a context of
diminishing returns.
",0,0,0,1,1,0
16125,16126,Hybrid integration of solid-state quantum emitters on a silicon photonic chip,"  Scalable quantum photonic systems require efficient single photon sources
coupled to integrated photonic devices. Solid-state quantum emitters can
generate single photons with high efficiency, while silicon photonic circuits
can manipulate them in an integrated device structure. Combining these two
material platforms could, therefore, significantly increase the complexity of
integrated quantum photonic devices. Here, we demonstrate hybrid integration of
solid-state quantum emitters to a silicon photonic device. We develop a
pick-and-place technique that can position epitaxially grown InAs/InP quantum
dots emitting at telecom wavelengths on a silicon photonic chip
deterministically with nanoscale precision. We employ an adiabatic tapering
approach to transfer the emission from the quantum dots to the waveguide with
high efficiency. We also incorporate an on-chip silicon-photonic beamsplitter
to perform a Hanbury-Brown and Twiss measurement. Our approach could enable
integration of pre-characterized III-V quantum photonic devices into
large-scale photonic structures to enable complex devices composed of many
emitters and photons.
",0,1,0,0,0,0
15781,15782,Development of a 32-channel ASIC for an X-ray APD Detector onboard the ISS,"  We report on the design and performance of a mixed-signal application
specific integrated circuit (ASIC) dedicated to avalanche photodiodes (APDs) in
order to detect hard X-ray emissions in a wide energy band onboard the
International Space Station. To realize wide-band detection from 20 keV to 1
MeV, we use Ce:GAGG scintillators, each coupled to an APD, with low-noise
front-end electronics capable of achieving a minimum energy detection threshold
of 20 keV. The developed ASIC has the ability to read out 32-channel APD
signals using 0.35 $\mu$m CMOS technology, and an analog amplifier at the input
stage is designed to suppress the capacitive noise primarily arising from the
large detector capacitance of the APDs. The ASIC achieves a performance of 2099
e$^{-}$ + 1.5 e$^{-}$/pF at root mean square (RMS) with a wide 300 fC dynamic
range. Coupling a reverse-type APD with a Ce:GAGG scintillator, we obtain an
energy resolution of 6.7% (FWHM) at 662 keV and a minimum detectable energy of
20 keV at room temperature (20 $^{\circ}$C). Furthermore, we examine the
radiation tolerance for space applications by using a 90 MeV proton beam,
confirming that the ASIC is free of single-event effects and can operate
properly without serious degradation in analog and digital processing.
",0,1,0,0,0,0
16394,16395,Are theoretical results 'Results'?,"  Yes.
",0,0,0,0,1,0
4224,4225,Pencilled regular parallelisms,"  Over any field $\mathbb K$, there is a bijection between regular spreads of
the projective space ${\rm PG}(3,{\mathbb K})$ and $0$-secant lines of the
Klein quadric in ${\rm PG}(5,{\mathbb K})$. Under this bijection, regular
parallelisms of ${\rm PG}(3,{\mathbb K})$ correspond to hyperflock determining
line sets (hfd line sets) with respect to the Klein quadric. An hfd line set is
defined to be \emph{pencilled} if it is composed of pencils of lines. We
present a construction of pencilled hfd line sets, which is then shown to
determine all such sets. Based on these results, we describe the corresponding
regular parallelisms. These are also termed as being \emph{pencilled}. Any
Clifford parallelism is regular and pencilled. From this, we derive necessary
and sufficient algebraic conditions for the existence of pencilled hfd line
sets.
",0,0,1,0,0,0
6067,6068,Observational signatures of linear warps in circumbinary discs,"  In recent years an increasing number of observational studies have hinted at
the presence of warps in protoplanetary discs, however a general comprehensive
description of observational diagnostics of warped discs was missing. We
performed a series of 3D SPH hydrodynamic simulations and combined them with 3D
radiative transfer calculations to study the observability of warps in
circumbinary discs, whose plane is misaligned with respect to the orbital plane
of the central binary. Our numerical hydrodynamic simulations confirm previous
analytical results on the dependence of the warp structure on the viscosity and
the initial misalignment between the binary and the disc. To study the
observational signatures of warps we calculate images in the continuum at
near-infrared and sub-millimetre wavelengths and in the pure rotational
transition of CO in the sub-millimetre. Warped circumbinary discs show surface
brightness asymmetry in near-infrared scattered light images as well as in
optically thick gas lines at sub-millimetre wavelengths. The asymmetry is
caused by self-shadowing of the disc by the inner warped regions, thus the
strength of the asymmetry depends on the strength of the warp. The projected
velocity field, derived from line observations, shows characteristic
deviations, twists and a change in the slope of the rotation curve, from that
of an unperturbed disc. In extreme cases even the direction of rotation appears
to change in the disc inwards of a characteristic radius. The strength of the
kinematical signatures of warps decreases with increasing inclination. The
strength of all warp signatures decreases with decreasing viscosity.
",0,1,0,0,0,0
2320,2321,"Structure of a Parabolic Partial Differential Equation on Graphs and Digital spaces. Solution of PDE on Digital Spaces: a Klein Bottle, a Projective Plane, a 4D Sphere and a Moebius Band","  This paper studies the structure of a parabolic partial differential equation
on graphs and digital n-dimensional manifolds, which are digital models of
continuous n-manifolds. Conditions for the existence of solutions of equations
are determined and investigated. Numerical solutions of the equation on a Klein
bottle, a projective plane, a 4D sphere and a Moebius strip are presented.
",1,0,1,0,0,0
10428,10429,Performance Evaluation of Spectrum Mobility in Multi-homed Mobile IPv6 Cognitive Radio Cellular Networks,"  Technological developments alongside VLSI achievements enable mobile devices
to be equipped with multiple radio interfaces which is known as multihoming. On
the other hand, the combination of various wireless access technologies, known
as Next Generation Wireless Networks (NGWNs) has been introduced to provide
continuous connection to mobile devices in any time and location. Cognitive
radio networks as a part of NGWNs aroused to overcome spectrum inefficiency and
spectrum scarcity issues. In order to provide seamless and ubiquitous
connection across heterogeneous wireless access networks in the context of
cognitive radio networks, utilizing Mobile IPv6 is beneficial. In this paper, a
mobile device equipped with two radio interfaces is considered in order to
evaluate performance of spectrum handover in terms of handover latency. The
analytical results show that the proposed model can achieve better performance
compared to other related mobility management protocols mainly in terms of
handover latency.
",1,0,0,0,0,0
18266,18267,Machine Learning as Statistical Data Assimilation,"  We identify a strong equivalence between neural network based machine
learning (ML) methods and the formulation of statistical data assimilation
(DA), known to be a problem in statistical physics. DA, as used widely in
physical and biological sciences, systematically transfers information in
observations to a model of the processes producing the observations. The
correspondence is that layer label in the ML setting is the analog of time in
the data assimilation setting. Utilizing aspects of this equivalence we discuss
how to establish the global minimum of the cost functions in the ML context,
using a variational annealing method from DA. This provides a design method for
optimal networks for ML applications and may serve as the basis for
understanding the success of ""deep learning"". Results from an ML example are
presented.
When the layer label is taken to be continuous, the Euler-Lagrange equation
for the ML optimization problem is an ordinary differential equation, and we
see that the problem being solved is a two point boundary value problem. The
use of continuous layers is denoted ""deepest learning"". The Hamiltonian version
provides a direct rationale for back propagation as a solution method for the
canonical momentum; however, it suggests other solution methods are to be
preferred.
",1,0,0,1,0,0
4479,4480,Geometric k-nearest neighbor estimation of entropy and mutual information,"  Nonparametric estimation of mutual information is used in a wide range of
scientific problems to quantify dependence between variables. The k-nearest
neighbor (knn) methods are consistent, and therefore expected to work well for
large sample size. These methods use geometrically regular local volume
elements. This practice allows maximum localization of the volume elements, but
can also induce a bias due to a poor description of the local geometry of the
underlying probability measure. We introduce a new class of knn estimators that
we call geometric knn estimators (g-knn), which use more complex local volume
elements to better model the local geometry of the probability measures. As an
example of this class of estimators, we develop a g-knn estimator of entropy
and mutual information based on elliptical volume elements, capturing the local
stretching and compression common to a wide range of dynamical systems
attractors. A series of numerical examples in which the thickness of the
underlying distribution and the sample sizes are varied suggest that local
geometry is a source of problems for knn methods such as the
Kraskov-Stögbauer-Grassberger (KSG) estimator when local geometric effects
cannot be removed by global preprocessing of the data. The g-knn method
performs well despite the manipulation of the local geometry. In addition, the
examples suggest that the g-knn estimators can be of particular relevance to
applications in which the system is large, but data size is limited.
",0,0,1,1,0,0
12175,12176,$L^p$ estimates for the Bergman projection on some Reinhardt domains,"  We obtain $L^p$ regularity for the Bergman projection on some Reinhardt
domains. We start with a bounded initial domain $\Omega$ with some symmetry
properties and generate successor domains in higher {dimensions}. We prove: If
the Bergman kernel on $\Omega$ satisfies appropriate estimates, then the
Bergman projection on the successor is $L^p$ bounded. For example, the Bergman
projection on successors of strictly pseudoconvex initial domains is bounded on
$L^p$ for $1<p<\infty$. The successor domains need not have smooth boundary nor
be strictly pseudoconvex.
",0,0,1,0,0,0
10962,10963,Adaptive Estimation for Nonlinear Systems using Reproducing Kernel Hilbert Spaces,"  This paper extends a conventional, general framework for online adaptive
estimation problems for systems governed by unknown nonlinear ordinary
differential equations. The central feature of the theory introduced in this
paper represents the unknown function as a member of a reproducing kernel
Hilbert space (RKHS) and defines a distributed parameter system (DPS) that
governs state estimates and estimates of the unknown function. This paper 1)
derives sufficient conditions for the existence and stability of the infinite
dimensional online estimation problem, 2) derives existence and stability of
finite dimensional approximations of the infinite dimensional approximations,
and 3) determines sufficient conditions for the convergence of finite
dimensional approximations to the infinite dimensional online estimates. A new
condition for persistency of excitation in a RKHS in terms of its evaluation
functionals is introduced in the paper that enables proof of convergence of the
finite dimensional approximations of the unknown function in the RKHS. This
paper studies two particular choices of the RKHS, those that are generated by
exponential functions and those that are generated by multiscale kernels
defined from a multiresolution analysis.
",1,0,0,0,0,0
6189,6190,Brownian dynamics of elongated particles in a quasi-2D isotropic liquid,"  We demonstrate experimentally that the long-range hydrodynamic interactions
in an incompressible quasi 2D isotropic fluid result in an anisotropic viscous
drag acting on elongated particles. The anisotropy of the drag is increasing
with increasing ratio of the particle length to the hydrodynamic scale given by
the Saffman-Delbrück length. The micro-rheology data for translational and
rotational drags collected over three orders of magnitude of the effective
particle length demonstrate the validity of the current theoretical approaches
to the hydrodynamics in restricted geometry. The results also demonstrate
crossovers between the hydrodynamical regimes determined by the characteristic
length scales.
",0,1,0,0,0,0
7270,7271,Improved Training of Wasserstein GANs,"  Generative Adversarial Networks (GANs) are powerful generative models, but
suffer from training instability. The recently proposed Wasserstein GAN (WGAN)
makes progress toward stable training of GANs, but sometimes can still generate
only low-quality samples or fail to converge. We find that these problems are
often due to the use of weight clipping in WGAN to enforce a Lipschitz
constraint on the critic, which can lead to undesired behavior. We propose an
alternative to clipping weights: penalize the norm of gradient of the critic
with respect to its input. Our proposed method performs better than standard
WGAN and enables stable training of a wide variety of GAN architectures with
almost no hyperparameter tuning, including 101-layer ResNets and language
models over discrete data. We also achieve high quality generations on CIFAR-10
and LSUN bedrooms.
",1,0,0,1,0,0
290,291,A XGBoost risk model via feature selection and Bayesian hyper-parameter optimization,"  This paper aims to explore models based on the extreme gradient boosting
(XGBoost) approach for business risk classification. Feature selection (FS)
algorithms and hyper-parameter optimizations are simultaneously considered
during model training. The five most commonly used FS methods including weight
by Gini, weight by Chi-square, hierarchical variable clustering, weight by
correlation, and weight by information are applied to alleviate the effect of
redundant features. Two hyper-parameter optimization approaches, random search
(RS) and Bayesian tree-structured Parzen Estimator (TPE), are applied in
XGBoost. The effect of different FS and hyper-parameter optimization methods on
the model performance are investigated by the Wilcoxon Signed Rank Test. The
performance of XGBoost is compared to the traditionally utilized logistic
regression (LR) model in terms of classification accuracy, area under the curve
(AUC), recall, and F1 score obtained from the 10-fold cross validation. Results
show that hierarchical clustering is the optimal FS method for LR while weight
by Chi-square achieves the best performance in XG-Boost. Both TPE and RS
optimization in XGBoost outperform LR significantly. TPE optimization shows a
superiority over RS since it results in a significantly higher accuracy and a
marginally higher AUC, recall and F1 score. Furthermore, XGBoost with TPE
tuning shows a lower variability than the RS method. Finally, the ranking of
feature importance based on XGBoost enhances the model interpretation.
Therefore, XGBoost with Bayesian TPE hyper-parameter optimization serves as an
operative while powerful approach for business risk modeling.
",1,0,0,1,0,0
11573,11574,Sensitivity Analysis of Deep Neural Networks,"  Deep neural networks (DNNs) have achieved superior performance in various
prediction tasks, but can be very vulnerable to adversarial examples or
perturbations. Therefore, it is crucial to measure the sensitivity of DNNs to
various forms of perturbations in real applications. We introduce a novel
perturbation manifold and its associated influence measure to quantify the
effects of various perturbations on DNN classifiers. Such perturbations include
various external and internal perturbations to input samples and network
parameters. The proposed measure is motivated by information geometry and
provides desirable invariance properties. We demonstrate that our influence
measure is useful for four model building tasks: detecting potential
'outliers', analyzing the sensitivity of model architectures, comparing network
sensitivity between training and test sets, and locating vulnerable areas.
Experiments show reasonably good performance of the proposed measure for the
popular DNN models ResNet50 and DenseNet121 on CIFAR10 and MNIST datasets.
",1,0,0,1,0,0
7252,7253,Statistical inference for misspecified ergodic Lévy driven stochastic differential equation models,"  This paper deals with the estimation problem of misspecified ergodic Lévy
driven stochastic differential equation models based on high-frequency samples.
We utilize the widely applicable and tractable Gaussian quasi-likelihood
approach which focuses on (conditional) mean and variance structure. It is
shown that the corresponding Gaussian quasi-likelihood estimators of drift and
scale parameters satisfy tail probability estimates and asymptotic normality at
the same rate as correctly specified case. In this process, extended Poisson
equation for time-homogeneous Feller Markov processes plays an important role
to handle misspecification effect. Our result confirms the practical usefulness
of the Gaussian quasi-likelihood approach for SDE models, more firmly.
",0,0,1,1,0,0
16701,16702,"Matrix Product Unitaries: Structure, Symmetries, and Topological Invariants","  Matrix Product Vectors form the appropriate framework to study and classify
one-dimensional quantum systems. In this work, we develop the structure theory
of Matrix Product Unitary operators (MPUs) which appear e.g. in the description
of time evolutions of one-dimensional systems. We prove that all MPUs have a
strict causal cone, making them Quantum Cellular Automata (QCAs), and derive a
canonical form for MPUs which relates different MPU representations of the same
unitary through a local gauge. We use this canonical form to prove an Index
Theorem for MPUs which gives the precise conditions under which two MPUs are
adiabatically connected, providing an alternative derivation to that of
[Commun. Math. Phys. 310, 419 (2012), arXiv:0910.3675] for QCAs. We also
discuss the effect of symmetries on the MPU classification. In particular, we
characterize the tensors corresponding to MPU that are invariant under
conjugation, time reversal, or transposition. In the first case, we give a full
characterization of all equivalence classes. Finally, we give several examples
of MPU possessing different symmetries.
",0,1,0,0,0,0
10986,10987,Effective interaction in a non-Fermi liquid conductor and spin correlations in under-doped cuprates,"  The effective interaction between the itinerant spin degrees of freedom in
the paramagnetic phases of hole doped quantum Heisenberg antiferromagnets is
investigated theoretically, based on the single-band t-J model on 1D lattice,
at zero temperature. The effective spin-spin interaction for this model in the
strong correlation limit, is studied in terms of the generalized spin stiffness
constant as a function of doping concentration. The plot of this generalized
spin stiffness constant against doping shows a very high value of stiffness in
the vicinity of zero doping and a very sharp fall with increase in doping
concentration, signifying the rapid decay of original coupling of
semi-localized spins in the system. Quite interestingly, this plot also shows a
maximum occurring at a finite value of doping, which strongly suggests the
tendency of the itinerant spins to couple again in the unconventional
paramagnetic phase. As the doping is further increased, this new coupling is
also suppressed and the spin response becomes analogous to almost Pauli-like.
The last two predictions of ours are quite novel and may be directly tested by
independent experiments and computational techniques in future. Our results in
general receive good support from other theoretical works and experimental
results extracted from the chains of YBa$_2$Cu$_3$O$_{6+x}$.
",0,1,0,0,0,0
13728,13729,Kondo destruction in a quantum paramagnet with magnetic frustration,"  We report results of isothermal magnetotransport and susceptibility
measurements at elevated magnetic fields B down to very low temperatures T on
high-quality single crystals of the frustrated Kondo-lattice system CePdAl.
They reveal a B*(T) line within the paramagnetic part of the phase diagram.
This line denotes a thermally broadened 'small'-to-'large' Fermi surface
crossover which substantially narrows upon cooling. At B_0* = B*(T=0) = (4.6
+/- 0.1) T, this B*(T) line merges with two other crossover lines, viz. Tp(B)
below and T_FL(B) above B_0*. Tp characterizes a frustration-dominated
spin-liquid state, while T_FL is the Fermi-liquid temperature associated with
the lattice Kondo effect. Non-Fermi-liquid phenomena which are commonly
observed near a 'Kondo destruction' quantum critical point cannot be resolved
in CePdAl. Our observations reveal a rare case where Kondo coupling,
frustration and quantum criticality are closely intertwined.
",0,1,0,0,0,0
14470,14471,Novel solid state vacuum quartz encapsulated growth of p-Terphenyl: the parent High Tc Oraganic Superconductor (HTOS),"  We report an easy and versatile route for the synthesis of the parent phase
of newest superconducting wonder material i.e. p-Terphenyl. Doped p-terphenyl
has recently shown superconductivity with transition temperature as high as
120K. For crystal growth, the commercially available p-Terphenyl powder is
pelletized, encapsulated in evacuated (10-4 Torr) quartz tube and subjected to
high temperature (260C) melt followed by slow cooling at 5C/hour. Simple
temperature controlled heating furnace is used during the process. The obtained
crystal is one piece, shiny and plate like. Single crystal surface XRD (X-ray
Diffraction) showed unidirectional (00l) lines, indicating that the crystal is
grown along c-direction. Powder XRD of the specimen showed that as grown
p-Terphenyl is crystallized in monoclinic structure with space group P21/a
space group, having lattice parameters a = 8.08(2) A, b = 5.62(5) A and c=
13.58(3) A. Scanning electron microscopy (SEM) pictures of the crystal showed
clear layered slab like growth without any visible contamination from oxygen.
Characteristic reported Raman active modes related to C-C-C bending, C-H
bending, C-C stretching and C-H stretching vibrations are seen clearly for the
studied p-Terphenyl crystal. The physical properties of crystal are yet
underway. The short letter reports an easy and versatile crystal growth method
for obtaining quality p-terphenyl. The same growth method may probably be
applied to doped p-terphenyl and to subsequently achieve superconductivity to
the tune of as high 120K for the newest superconductivity wonder i.e., High Tc
Oraganic Superconductor (HTOS).
",0,1,0,0,0,0
1427,1428,A proof of the Flaherty-Keller formula on the effective property of densely packed elastic composites,"  We prove in a mathematically rigorous way the asymptotic formula of Flaherty
and Keller on the effective property of densely packed periodic elastic
composites with hard inclusions. The proof is based on the primal-dual
variational principle, where the upper bound is derived by using the
Keller-type test functions and the lower bound by singular functions made of
nuclei of strain. Singular functions are solutions of the Lamé system and
capture precisely singular behavior of the stress in the narrow region between
two adjacent hard inclusions.
",0,0,1,0,0,0
6753,6754,Improving power of genetic association studies by extreme phenotype sampling: a review and some new results,"  Extreme phenotype sampling is a selective genotyping design for genetic
association studies where only individuals with extreme values of a continuous
trait are genotyped for a set of genetic variants. Under financial or other
limitations, this design is assumed to improve the power to detect associations
between genetic variants and the trait, compared to randomly selecting the same
number of individuals for genotyping. Here we present extensions of likelihood
models that can be used for inference when the data are sampled according to
the extreme phenotype sampling design. Computational methods for parameter
estimation and hypothesis testing are provided. We consider methods for common
variant genetic effects and gene-environment interaction effects in linear
regression models with a normally distributed trait. We use simulated and real
data to show that extreme phenotype sampling can be powerful compared to random
sampling, but that this does not hold for all extreme sampling methods and
situations.
",0,0,0,1,0,0
1469,1470,Generative Mixture of Networks,"  A generative model based on training deep architectures is proposed. The
model consists of K networks that are trained together to learn the underlying
distribution of a given data set. The process starts with dividing the input
data into K clusters and feeding each of them into a separate network. After
few iterations of training networks separately, we use an EM-like algorithm to
train the networks together and update the clusters of the data. We call this
model Mixture of Networks. The provided model is a platform that can be used
for any deep structure and be trained by any conventional objective function
for distribution modeling. As the components of the model are neural networks,
it has high capability in characterizing complicated data distributions as well
as clustering data. We apply the algorithm on MNIST hand-written digits and
Yale face datasets. We also demonstrate the clustering ability of the model
using some real-world and toy examples.
",1,0,0,1,0,0
6805,6806,Tomographic X-ray data of carved cheese,"  This is the documentation of the tomographic X-ray data of a carved cheese
slice. Data are available at www.fips.fi/dataset.php, and can be freely used
for scientific purposes with appropriate references to them, and to this
document in this http URL. The data set consists of (1) the X-ray sinogram
of a single 2D slice of the cheese slice with three different resolutions and
(2) the corresponding measurement matrices modeling the linear operation of the
X-ray transform. Each of these sinograms was obtained from a measured
360-projection fan-beam sinogram by down-sampling and taking logarithms. The
original (measured) sinogram is also provided in its original form and
resolution.
",0,1,0,0,0,0
3323,3324,Improved Point Source Detection in Crowded Fields using Probabilistic Cataloging,"  Cataloging is challenging in crowded fields because sources are extremely
covariant with their neighbors and blending makes even the number of sources
ambiguous. We present the first optical probabilistic catalog, cataloging a
crowded (~0.1 sources per pixel brighter than 22nd magnitude in F606W) Sloan
Digital Sky Survey r band image from M2. Probabilistic cataloging returns an
ensemble of catalogs inferred from the image and thus can capture source-source
covariance and deblending ambiguities. By comparing to a traditional catalog of
the same image and a Hubble Space Telescope catalog of the same region, we show
that our catalog ensemble better recovers sources from the image. It goes more
than a magnitude deeper than the traditional catalog while having a lower false
discovery rate brighter than 20th magnitude. We also present an algorithm for
reducing this catalog ensemble to a condensed catalog that is similar to a
traditional catalog, except it explicitly marginalizes over source-source
covariances and nuisance parameters. We show that this condensed catalog has a
similar completeness and false discovery rate to the catalog ensemble. Future
telescopes will be more sensitive, and thus more of their images will be
crowded. Probabilistic cataloging performs better than existing software in
crowded fields and so should be considered when creating photometric pipelines
in the Large Synoptic Space Telescope era.
",0,1,0,0,0,0
11454,11455,General Robust Bayes Pseudo-Posterior: Exponential Convergence results with Applications,"  Although Bayesian inference is an immensely popular paradigm among a large
segment of scientists including statisticians, most of the applications
consider the objective priors and need critical investigations (Efron, 2013,
Science). And although it has several optimal properties, one major drawback of
Bayesian inference is the lack of robustness against data contamination and
model misspecification, which becomes pernicious in the use of objective
priors. This paper presents the general formulation of a Bayes pseudo-posterior
distribution yielding robust inference. Exponential convergence results related
to the new pseudo-posterior and the corresponding Bayes estimators are
established under the general parametric set-up and illustrations are provided
for the independent stationary models and the independent non-homogenous
models. For the first case, the discrete priors and the corresponding maximum
posterior estimators are discussed with additional details. We further apply
this new pseudo-posterior to propose robust versions of the Bayes predictive
density estimators and the expected Bayes estimator for the fixed-design
(normal) linear regression models; their properties are illustrated both
theoretically as well as empirically.
",0,0,1,1,0,0
13575,13576,On iteration of Cox rings,"  We characterize all varieties with a torus action of complexity one that
admit iteration of Cox rings.
",0,0,1,0,0,0
1780,1781,Effect of Composition Gradient on Magnetothermal Instability Modified by Shear and Rotation,"  We model the intracluster medium as a weakly collisional plasma that is a
binary mixture of the hydrogen and the helium ions, along with free electrons.
When, owing to the helium sedimentation, the gradient of the mean molecular
weight (or equivalently, composition or helium ions' concentration) of the
plasma is not negligible, it can have appreciable influence on the stability
criteria of the thermal convective instabilities, e.g., the heat-flux-buoyancy
instability and the magnetothermal instability (MTI). These instabilities are
consequences of the anisotropic heat conduction occurring preferentially along
the magnetic field lines. In this paper, without ignoring the magnetic tension,
we first present the mathematical criterion for the onset of composition
gradient modified MTI. Subsequently, we relax the commonly adopted equilibrium
state in which the plasma is at rest, and assume that the plasma is in a
sheared state which may be due to differential rotation. We discuss how the
concentration gradient affects the coupling between the Kelvin--Helmholtz
instability and the MTI in rendering the plasma unstable or stable. We derive
exact stability criterion by working with the sharp boundary case in which the
physical variables---temperature, mean molecular weight, density, and magnetic
field---change discontinuously from one constant value to another on crossing
the boundary. Finally, we perform the linear stability analysis for the case of
the differentially rotating plasma that is thermally and compositionally
stratified as well. By assuming axisymmetric perturbations, we find the
corresponding dispersion relation and the explicit mathematical expression
determining the onset of the modified MTI.
",0,1,0,0,0,0
287,288,The anti-spherical category,"  We study a diagrammatic categorification (the ""anti-spherical category"") of
the anti-spherical module for any Coxeter group. We deduce that Deodhar's
(sign) parabolic Kazhdan-Lusztig polynomials have non-negative coefficients,
and that a monotonicity conjecture of Brenti's holds. The main technical
observation is a localisation procedure for the anti-spherical category, from
which we construct a ""light leaves"" basis of morphisms. Our techniques may be
used to calculate many new elements of the $p$-canonical basis in the
anti-spherical module. The results use generators and relations for Soergel
bimodules (""Soergel calculus"") in a crucial way.
",0,0,1,0,0,0
20077,20078,Explicitly correlated formalism for second-order single-particle Green's function,"  We present an explicitly correlated formalism for the second-order
single-particle Green's function method (GF2-F12) that does not assume the
popular diagonal approximation, and describes the energy dependence of the
explicitly correlated terms. For small and medium organic molecules the basis
set errors of ionization potentials of GF2-F12 are radically improved relative
to GF2: the performance of GF2-F12/aug- cc-pVDZ is better than that of
GF2/aug-cc-pVQZ, at a significantly lower cost.
",0,1,0,0,0,0
3925,3926,Symplectic rational $G$-surfaces and equivariant symplectic cones,"  We give characterizations of a finite group $G$ acting symplectically on a
rational surface ($\mathbb{C}P^2$ blown up at two or more points). In
particular, we obtain a symplectic version of the dichotomy of $G$-conic
bundles versus $G$-del Pezzo surfaces for the corresponding $G$-rational
surfaces, analogous to a classical result in algebraic geometry. Besides the
characterizations of the group $G$ (which is completely determined for the case
of $\mathbb{C}P^2\# N\overline{\mathbb{C}P^2}$, $N=2,3,4$), we also investigate
the equivariant symplectic minimality and equivariant symplectic cone of a
given $G$-rational surface.
",0,0,1,0,0,0
14053,14054,On the degree of incompleteness of an incomplete financial market,"  In order to find a way of measuring the degree of incompleteness of an
incomplete financial market, the rank of the vector price process of the traded
assets and the dimension of the associated acceptance set are introduced. We
show that they are equal and state a variety of consequences.
",0,0,0,0,0,1
5781,5782,Uniform Consistency in Stochastic Block Model with Continuous Community Label,"  \cite{bickel2009nonparametric} developed a general framework to establish
consistency of community detection in stochastic block model (SBM). In most
applications of this framework, the community label is discrete. For example,
in \citep{bickel2009nonparametric,zhao2012consistency} the degree corrected SBM
is assumed to have a discrete degree parameter. In this paper, we generalize
the method of \cite{bickel2009nonparametric} to give consistency analysis of
maximum likelihood estimator (MLE) in SBM with continuous community label. We
show that there is a standard procedure to transform the $||\cdot||_2$ error
bound to the uniform error bound. We demonstrate the application of our general
results by proving the uniform consistency (strong consistency) of the MLE in
the exponential network model with interaction effect. Unfortunately, in the
continuous parameter case, the condition ensuring uniform consistency we
obtained is much stronger than that in the discrete parameter case, namely
$n\mu_n^5/(\log n)^{8}\rightarrow\infty$ versus $n\mu_n/\log
n\rightarrow\infty$. Where $n\mu_n$ represents the average degree of the
network. But continuous is the limit of discrete. So it is not surprising as we
show that by discretizing the community label space into sufficiently small
(but not too small) pieces and applying the MLE on the discretized community
label space, uniform consistency holds under almost the same condition as in
discrete community label space. Such a phenomenon is surprising since the
discretization does not depend on the data or the model. This reminds us of the
thresholding method.
",0,0,0,1,0,0
2472,2473,On Symmetric Losses for Learning from Corrupted Labels,"  This paper aims to provide a better understanding of a symmetric loss. First,
we show that using a symmetric loss is advantageous in the balanced error rate
(BER) minimization and area under the receiver operating characteristic curve
(AUC) maximization from corrupted labels. Second, we prove general theoretical
properties of symmetric losses, including a classification-calibration
condition, excess risk bound, conditional risk minimizer, and AUC-consistency
condition. Third, since all nonnegative symmetric losses are non-convex, we
propose a convex barrier hinge loss that benefits significantly from the
symmetric condition, although it is not symmetric everywhere. Finally, we
conduct experiments on BER and AUC optimization from corrupted labels to
validate the relevance of the symmetric condition.
",1,0,0,1,0,0
17833,17834,Maximum likelihood estimation of determinantal point processes,"  Determinantal point processes (DPPs) have wide-ranging applications in
machine learning, where they are used to enforce the notion of diversity in
subset selection problems. Many estimators have been proposed, but surprisingly
the basic properties of the maximum likelihood estimator (MLE) have received
little attention. The difficulty is that it is a non-concave maximization
problem, and such functions are notoriously difficult to understand in high
dimensions, despite their importance in modern machine learning. Here we study
both the local and global geometry of the expected log-likelihood function. We
prove several rates of convergence for the MLE and give a complete
characterization of the case where these are parametric. We also exhibit a
potential curse of dimensionality where the asymptotic variance of the MLE
scales exponentially with the dimension of the problem. Moreover, we exhibit an
exponential number of saddle points, and give evidence that these may be the
only critical points.
",0,0,1,1,0,0
9663,9664,Cloth Manipulation Using Random-Forest-Based Imitation Learning,"  We present a novel approach for robust manipulation of high-DOF deformable
objects such as cloth. Our approach uses a random forest-based controller that
maps the observed visual features of the cloth to an optimal control action of
the manipulator. The topological structure of this random forest-based
controller is determined automatically based on the training data consisting
visual features and optimal control actions. This enables us to integrate the
overall process of training data classification and controller optimization
into an imitation learning (IL) approach. Our approach enables learning of
robust control policy for cloth manipulation with guarantees on convergence.We
have evaluated our approach on different multi-task cloth manipulation
benchmarks such as flattening, folding and twisting. In practice, our approach
works well with different deformable features learned based on the specific
task or deep learning. Moreover, our controller outperforms a simple or
piecewise linear controller in terms of robustness to noise. In addition, our
approach is easy to implement and does not require much parameter tuning.
",1,0,0,0,0,0
15710,15711,$J_1$-$J_2$ square lattice antiferromagnetism in the orbitally quenched insulator MoOPO$_4$,"  We report magnetic and thermodynamic properties of a $4d^1$ (Mo$^{5+}$)
magnetic insulator MoOPO$_4$ single crystal, which realizes a $J_1$-$J_2$
Heisenberg spin-$1/2$ model on a stacked square lattice. The specific-heat
measurements show a magnetic transition at 16 K which is also confirmed by
magnetic susceptibility, ESR, and neutron diffraction measurements. Magnetic
entropy deduced from the specific heat corresponds to a two-level degree of
freedom per Mo$^{5+}$ ion, and the effective moment from the susceptibility
corresponds to the spin-only value. Using {\it ab initio} quantum chemistry
calculations we demonstrate that the Mo$^{5+}$ ion hosts a purely spin-$1/2$
magnetic moment, indicating negligible effects of spin-orbit interaction. The
quenched orbital moments originate from the large displacement of Mo ions
inside the MoO$_6$ octahedra along the apical direction. The ground state is
shown by neutron diffraction to support a collinear Néel-type magnetic order,
and a spin-flop transition is observed around an applied magnetic field of 3.5
T. The magnetic phase diagram is reproduced by a mean-field calculation
assuming a small easy-axis anisotropy in the exchange interactions. Our results
suggest $4d$ molybdates as an alternative playground to search for model
quantum magnets.
",0,1,0,0,0,0
10605,10606,Functional limit laws for the increments of Lévy processes,"  We present a functional form of the Erdös-Renyi law of large numbers for
Levy processes.
",0,0,1,1,0,0
13478,13479,Atomistic study of hardening mechanism in Al-Cu nanostructure,"  Nanostructures have the immense potential to supplant the traditional
metallic structure as they show enhanced mechanical properties through strain
hardening. In this paper, the effect of grain size on the hardening mechanism
of Al-Cu nanostructure is elucidated by molecular dynamics simulation. Al-Cu
(50-54% Cu by weight) nanostructure having an average grain size of 4.57 to
7.26 nm are investigated for tensile simulation at different strain rate using
embedded atom method (EAM) potential at a temperature of 50~500K. It is found
that the failure mechanism of the nanostructure is governed by the temperature,
grain size as well as strain rate effect. At the high temperature of 300-500K,
the failure strength of Al-Cu nanostructure increases with the decrease of
average grain size following Hall-Petch relation. Dislocation motions are
hindered significantly when the grain size is decreased which play a vital role
on the hardening of the nanostructure. The failure is always found to initiate
at a particular Al grain due to its weak link and propagates through grain
boundary (GB) sliding, diffusion, dislocation nucleation and propagation. We
also visualize the dislocation density at different grain size to show how the
dislocation affects the material properties at the nanoscale. These results
will further aid investigation on the deformation mechanism of nanostructure.
",0,1,0,0,0,0
18459,18460,Rigidity of closed metric measure spaces with nonnegative curvature,"  We show that one-dimensional circle is the only case for closed smooth metric
measure spaces with nonnegative Bakry-Émery Ricci curvature whose spectrum
of the weighted Laplacian has an optimal positive upper bound. This result
extends the work of Hang-Wang in the manifold case (Int. Math. Res. Not. 18
(2007), Art. ID rnm064, 9pp).
",0,0,1,0,0,0
11130,11131,Alternative Lagrangians obtained by scalar deformations,"  We study non-conservative like SODEs admitting explicit Lagrangian
descriptions. Such systems are equivalent to the system of Lagrange equations
of some Lagrangian $L$, including a covariant force field which represents
non-conservative forces. We find necessary and sufficient conditions for the
existence of a differentiable function $\Phi:\mathbb{R}\rightarrow\mathbb{R}$
such that the initial system is equivalent to the system of Euler-Lagrange
equations of the deformed Lagrangian $\Phi(L)$. We give various examples of
such deformations.
",0,0,1,0,0,0
4552,4553,Weakly nonergodic dynamics in the Gross--Pitaevskii lattice,"  The microcanonical Gross--Pitaevskii (aka semiclassical Bose-Hubbard) lattice
model dynamics is characterized by a pair of energy and norm densities. The
grand canonical Gibbs distribution fails to describe a part of the density
space, due to the boundedness of its kinetic energy spectrum. We define
Poincare equilibrium manifolds and compute the statistics of microcanonical
excursion times off them. The tails of the distribution functions quantify the
proximity of the many-body dynamics to a weakly-nonergodic phase, which occurs
when the average excursion time is infinite. We find that a crossover to
weakly-nonergodic dynamics takes place inside the nonGibbs phase, being
unnoticed by the largest Lyapunov exponent. In the ergodic part of the
non-Gibbs phase, the Gibbs distribution should be replaced by an unknown
modified one. We relate our findings to the corresponding integrable limit,
close to which the actions are interacting through a short range coupling
network.
",0,1,0,0,0,0
3013,3014,Normalized Direction-preserving Adam,"  Adaptive optimization algorithms, such as Adam and RMSprop, have shown better
optimization performance than stochastic gradient descent (SGD) in some
scenarios. However, recent studies show that they often lead to worse
generalization performance than SGD, especially for training deep neural
networks (DNNs). In this work, we identify the reasons that Adam generalizes
worse than SGD, and develop a variant of Adam to eliminate the generalization
gap. The proposed method, normalized direction-preserving Adam (ND-Adam),
enables more precise control of the direction and step size for updating weight
vectors, leading to significantly improved generalization performance.
Following a similar rationale, we further improve the generalization
performance in classification tasks by regularizing the softmax logits. By
bridging the gap between SGD and Adam, we also hope to shed light on why
certain optimization algorithms generalize better than others.
",1,0,0,1,0,0
18739,18740,Proofs of life: molecular-biology reasoning simulates cell behaviors from first principles,"  We axiomatize the molecular-biology reasoning style, verify compliance of the
standard reference: Ptashne, A Genetic Switch, and present proof-theory-induced
technologies to predict phenotypes and life cycles from genotypes. The key is
to note that `reductionist discipline' entails constructive reasoning, i.e.,
that any argument for a compound property is constructed from more basic
arguments. Proof theory makes explicit the inner structure of the axiomatized
reasoning style and allows the permissible dynamics to be presented as a mode
of computation that can be executed and analyzed. Constructivity and
executability guarantee simulation when working over domain-specific languages.
Here, we exhibit phenotype properties for genotype reasons: a molecular-biology
argument is an open-system concurrent computation that results in compartment
changes and is performed among processes of physiology change as determined
from the molecular programming of given DNA. Life cycles are the possible
sequentializations of the processes. A main implication of our construction is
that technical correctness provides a complementary perspective on science that
is as fundamental there as it is for pure mathematics, provided mature
reductionism exists.
",0,0,0,0,1,0
10878,10879,Non Fermi liquid behavior and continuously tunable resistivity exponents in the Anderson-Hubbard model at finite temperature,"  We employ a recently developed computational many-body technique to study for
the first time the half-filled Anderson-Hubbard model at finite temperature and
arbitrary correlation ($U$) and disorder ($V$) strengths. Interestingly, the
narrow zero temperature metallic range induced by disorder from the Mott
insulator expands with increasing temperature in a manner resembling a quantum
critical point. Our study of the resistivity temperature scaling $T^{\alpha}$
for this metal reveals non Fermi liquid characteristics. Moreover, a continuous
dependence of $\alpha$ on $U$ and $V$ from linear to nearly quadratic was
observed. We argue that these exotic results arise from a systematic change
with $U$ and $V$ of the ""effective"" disorder, a combination of quenched
disorder and intrinsic localized spins.
",0,1,0,0,0,0
17164,17165,A Useful Motif for Flexible Task Learning in an Embodied Two-Dimensional Visual Environment,"  Animals (especially humans) have an amazing ability to learn new tasks
quickly, and switch between them flexibly. How brains support this ability is
largely unknown, both neuroscientifically and algorithmically. One reasonable
supposition is that modules drawing on an underlying general-purpose sensory
representation are dynamically allocated on a per-task basis. Recent results
from neuroscience and artificial intelligence suggest the role of the general
purpose visual representation may be played by a deep convolutional neural
network, and give some clues how task modules based on such a representation
might be discovered and constructed. In this work, we investigate module
architectures in an embodied two-dimensional touchscreen environment, in which
an agent's learning must occur via interactions with an environment that emits
images and rewards, and accepts touches as input. This environment is designed
to capture the physical structure of the task environments that are commonly
deployed in visual neuroscience and psychophysics. We show that in this
context, very simple changes in the nonlinear activations used by such a module
can significantly influence how fast it is at learning visual tasks and how
suitable it is for switching to new tasks.
",1,0,0,1,0,0
15989,15990,Decentralized Clustering based on Robust Estimation and Hypothesis Testing,"  This paper considers a network of sensors without fusion center that may be
difficult to set up in applications involving sensors embedded on autonomous
drones or robots. In this context, this paper considers that the sensors must
perform a given clustering task in a fully decentralized setup. Standard
clustering algorithms usually need to know the number of clusters and are very
sensitive to initialization, which makes them difficult to use in a fully
decentralized setup. In this respect, this paper proposes a decentralized
model-based clustering algorithm that overcomes these issues. The proposed
algorithm is based on a novel theoretical framework that relies on hypothesis
testing and robust M-estimation. More particularly, the problem of deciding
whether two data belong to the same cluster can be optimally solved via Wald's
hypothesis test on the mean of a Gaussian random vector. The p-value of this
test makes it possible to define a new type of score function, particularly
suitable for devising an M-estimation of the centroids. The resulting
decentralized algorithm efficiently performs clustering without prior knowledge
of the number of clusters. It also turns out to be less sensitive to
initialization than the already existing clustering algorithms, which makes it
appropriate for use in a network of sensors without fusion center.
",0,0,1,1,0,0
18169,18170,Weakly-Private Information Retrieval,"  Private information retrieval (PIR) protocols make it possible to retrieve a
file from a database without disclosing any information about the identity of
the file being retrieved. These protocols have been rigorously explored from an
information-theoretic perspective in recent years. While existing protocols
strictly impose that no information is leaked on the file's identity, this work
initiates the study of the tradeoffs that can be achieved by relaxing the
requirement of perfect privacy. In case the user is willing to leak some
information on the identity of the retrieved file, we study how the PIR rate,
as well as the upload cost and access complexity, can be improved. For the
particular case of replicated servers, we propose two weakly-private
information retrieval schemes based on two recent PIR protocols and a family of
schemes based on partitioning. Lastly, we compare the performance of the
proposed schemes.
",1,0,0,0,0,0
3162,3163,Virtual link and knot invariants from non-abelian Yang-Baxter 2-cocycle pairs,"  For a given $(X,S,\beta)$, where $S,\beta\colon X\times X\to X\times X$ are
set theoretical solutions of Yang-Baxter equation with a compatibility
condition, we define an invariant for virtual (or classical) knots/links using
non commutative 2-cocycles pairs $(f,g)$ that generalizes the one defined in
[FG2]. We also define, a group $U_{nc}^{fg}=U_{nc}^{fg}(X,S,\beta)$ and
functions $\pi_f, \pi_g\colon X\times X\to U_{nc}^{fg}(X)$ governing all
2-cocycles in $X$. We exhibit examples of computations achieved using GAP.
",0,0,1,0,0,0
7168,7169,Unsupervised Motion Artifact Detection in Wrist-Measured Electrodermal Activity Data,"  One of the main benefits of a wrist-worn computer is its ability to collect a
variety of physiological data in a minimally intrusive manner. Among these
data, electrodermal activity (EDA) is readily collected and provides a window
into a person's emotional and sympathetic responses. EDA data collected using a
wearable wristband are easily influenced by motion artifacts (MAs) that may
significantly distort the data and degrade the quality of analyses performed on
the data if not identified and removed. Prior work has demonstrated that MAs
can be successfully detected using supervised machine learning algorithms on a
small data set collected in a lab setting. In this paper, we demonstrate that
unsupervised learning algorithms perform competitively with supervised
algorithms for detecting MAs on EDA data collected in both a lab-based setting
and a real-world setting comprising about 23 hours of data. We also find,
somewhat surprisingly, that incorporating accelerometer data as well as EDA
improves detection accuracy only slightly for supervised algorithms and
significantly degrades the accuracy of unsupervised algorithms.
",1,0,0,0,0,0
17477,17478,Opportunities for Two-color Experiments at the SASE3 undulator line of the European XFEL,"  X-ray Free Electron Lasers (XFELs) have been proven to generate short and
powerful radiation pulses allowing for a wide class of novel experiments. If an
XFEL facility supports the generation of two X-ray pulses with different
wavelengths and controllable delay, the range of possible experiments is
broadened even further to include X-ray-pump/X-ray-probe applications. In this
work we discuss the possibility of applying a simple and cost-effective method
for producing two-color pulses at the SASE3 soft X-ray beamline of the European
XFEL. The technique is based on the installation of a magnetic chicane in the
baseline undulator and can be accomplished in several steps. We discuss the
scientific interest of this upgrade for the Small Quantum Systems (SQS)
instrument, in connection with the high-repetition rate of the European XFEL,
and we provide start-to-end simulations up to the radiation focus on the
sample, proving the feasibility of our concept.
",0,1,0,0,0,0
15385,15386,Independently Controllable Factors,"  It has been postulated that a good representation is one that disentangles
the underlying explanatory factors of variation. However, it remains an open
question what kind of training framework could potentially achieve that.
Whereas most previous work focuses on the static setting (e.g., with images),
we postulate that some of the causal factors could be discovered if the learner
is allowed to interact with its environment. The agent can experiment with
different actions and observe their effects. More specifically, we hypothesize
that some of these factors correspond to aspects of the environment which are
independently controllable, i.e., that there exists a policy and a learnable
feature for each such aspect of the environment, such that this policy can
yield changes in that feature with minimal changes to other features that
explain the statistical variations in the observed data. We propose a specific
objective function to find such factors and verify experimentally that it can
indeed disentangle independently controllable aspects of the environment
without any extrinsic reward signal.
",1,0,0,1,0,0
11136,11137,The rigorous derivation of the linear Landau equation from a particle system in a weak-coupling limit,"  We consider a system of N particles interacting via a short-range smooth
potential, in a intermediate regime between the weak-coupling and the
low-density. We provide a rigorous derivation of the Linear Landau equation
from this particle system. The strategy of the proof consists in showing the
asymptotic equivalence between the one-particle marginal and the solution of
the linear Boltzmann equation with vanishing mean free path.Then, following the
ideas of Landau, we prove the asympotic equivalence between the solutions of
the Boltzmann and Landau linear equation in the grazing collision limit.
",0,0,1,0,0,0
6871,6872,Quivers with potentials for cluster varieties associated to braid semigroups,"  Let $C$ be a simply laced generalized Cartan matrix. Given an element $b$ of
the generalized braid semigroup related to $C$, we construct a collection of
mutation-equivalent quivers with potentials. A quiver with potential in such a
collection corresponds to an expression of $b$ in terms of the standard
generators. For two expressions that differ by a braid relation, the
corresponding quivers with potentials are related by a mutation.
The main application of this result is a construction of a family of $CY_3$
$A_\infty$-categories associated to elements of the braid semigroup related to
$C$. In particular, we construct a canonical up to equivalence $CY_3$
$A_\infty$-category associated to quotient of any Double Bruhat cell
$G^{u,v}/{\rm Ad} H$ in a simply laced reductive Lie group $G$.
We describe the full set of parameters these categories depend on by defining
a 2-dimensional CW-complex and proving that the set of parameters is identified
with second cohomology group of this complex.
",0,0,1,0,0,0
11462,11463,Adaptive Cardinality Estimation,"  In this paper we address cardinality estimation problem which is an important
subproblem in query optimization. Query optimization is a part of every
relational DBMS responsible for finding the best way of the execution for the
given query. These ways are called plans. The execution time of different plans
may differ by several orders, so query optimizer has a great influence on the
whole DBMS performance. We consider cost-based query optimization approach as
the most popular one. It was observed that cost-based optimization quality
depends much on cardinality estimation quality. Cardinality of the plan node is
the number of tuples returned by it.
In the paper we propose a novel cardinality estimation approach with the use
of machine learning methods. The main point of the approach is using query
execution statistics of the previously executed queries to improve cardinality
estimations. We called this approach adaptive cardinality estimation to reflect
this point. The approach is general, flexible, and easy to implement. The
experimental evaluation shows that this approach significantly increases the
quality of cardinality estimation, and therefore increases the DBMS performance
for some queries by several times or even by several dozens of times.
",1,0,0,1,0,0
3691,3692,Fractional Driven Damped Oscillator,"  The resonances associated with a fractional damped oscillator which is driven
by an oscillatory external force are studied. It is shown that such resonances
can be manipulated by tuning up either the coefficient of the fractional
damping or the order of the corresponding fractional derivatives.
",0,1,0,0,0,0
19366,19367,D-optimal design for multivariate polynomial regression via the Christoffel function and semidefinite relaxations,"  We present a new approach to the design of D-optimal experiments with
multivariate polynomial regressions on compact semi-algebraic design spaces. We
apply the moment-sum-of-squares hierarchy of semidefinite programming problems
to solve numerically and approximately the optimal design problem. The geometry
of the design is recovered with semidefinite programming duality theory and the
Christoffel polynomial.
",0,0,1,1,0,0
508,509,A Multiple Source Framework for the Identification of Activities of Daily Living Based on Mobile Device Data,"  The monitoring of the lifestyles may be performed based on a system for the
recognition of Activities of Daily Living (ADL) and their environments,
combining the results obtained with the user agenda. The system may be
developed with the use of the off-the-shelf mobile devices commonly used,
because they have several types of sensors available, including motion,
magnetic, acoustic, and location sensors. Data acquisition, data processing,
data fusion, and artificial intelligence methods are applied in different
stages of the system developed, which recognizes the ADL with pattern
recognition methods. The motion and magnetic sensors allow the recognition of
activities with movement, but the acoustic sensors allow the recognition of the
environments. The fusion of the motion, magnetic and acoustic sensors allows
the differentiation of other ADL. On the other hand, the location sensors
allows the recognition of ADL with large movement, and the combination of these
sensors with the other sensors increases the number of ADL recognized by the
system. This study consists on the comparison of different types of ANN for
choosing the best methods for the recognition of several ADL, which they are
implemented in a system for the recognition of ADL that combines the sensors
data with the users agenda for the monitoring of the lifestyles. Conclusions
point to the use of Deep Neural Networks (DNN) with normalized data for the
identification of ADL with 85.89% of accuracy, the use of Feedforward neural
networks with non-normalized data for the identification of the environments
with 86.50% of accuracy, and the use of DNN with normalized data for the
identification of standing activities with 100% of accuracy, proving the
reliability of the framework presented in this study.
",1,0,0,0,0,0
9658,9659,Higher-genus quasimap wall-crossing via localization,"  We give a new proof of Ciocan-Fontanine and Kim's wall-crossing formula
relating the virtual classes of the moduli spaces of $\epsilon$-stable
quasimaps for different $\epsilon$ in any genus, whenever the target is a
complete intersection in projective space and there is at least one marked
point.
Our techniques involve a twisted graph space, which we expect to generalize
to yield wall-crossing formulas for general gauged linear sigma models.
",0,0,1,0,0,0
17292,17293,Structure-Based Subspace Method for Multi-Channel Blind System Identification,"  In this work, a novel subspace-based method for blind identification of
multichannel finite impulse response (FIR) systems is presented. Here, we
exploit directly the impeded Toeplitz channel structure in the signal linear
model to build a quadratic form whose minimization leads to the desired channel
estimation up to a scalar factor. This method can be extended to estimate any
predefined linear structure, e.g. Hankel, that is usually encountered in linear
systems. Simulation findings are provided to highlight the appealing advantages
of the new structure-based subspace (SSS) method over the standard subspace
(SS) method in certain adverse identification scenarii.
",1,0,0,1,0,0
2604,2605,Method of Reduction of Variables for Bilinear Matrix Inequality Problems in System and Control Designs,"  Bilinear matrix inequality (BMI) problems in system and control designs are
investigated in this paper. A solution method of reduction of variables (MRVs)
is proposed. This method consists of a principle of variable classification, a
procedure for problem transformation, and a hybrid algorithm that combines
deterministic and stochastic search engines. The classification principle is
used to classify the decision variables of a BMI problem into two categories:
1) external and 2) internal variables. Theoretical analysis is performed to
show that when the classification principle is applicable, a BMI problem can be
transformed into an unconstrained optimization problem that has fewer decision
variables. Stochastic search and deterministic search are then applied to
determine the decision variables of the unconstrained problem externally and
explore the internal problem structure, respectively. The proposed method can
address feasibility, single-objective, and multiobjective problems constrained
by BMIs in a unified manner. A number of numerical examples in system and
control designs are provided to validate the proposed methodology. Simulations
show that the MRVs can outperform existing BMI solution methods in most
benchmark problems and achieve similar levels of performance in the remaining
problems.
",1,0,0,0,0,0
15591,15592,Photometric and radial-velocity time-series of RR Lyrae stars in M3: analysis of single-mode variables,"  We present the first simultaneous photometric and spectroscopic investigation
of a large set of RR Lyrae variables in a globular cluster. The radial-velocity
data presented comprise the largest sample of RVs of RR Lyrae stars ever
obtained. The target is M3; $BVI_{\mathrm{C}}$ time-series of 111 and $b$ flux
data of further 64 RRab stars, and RV data of 79 RR Lyrae stars are published.
Blazhko modulation of the light curves of 47 percent of the RRab stars are
detected. The mean value of the center-of-mass velocities of RR Lyrae stars is
$-146.8$ km s$^{-1}$ with 4.52 km s$^{-1}$ standard deviation, which is in good
agreement with the results obtained for the red giants of the cluster. The
${\Phi_{21}}^{\mathrm RV}$ phase difference of the RV curves of RRab stars is
found to be uniformly constant both for the M3 and for Galactic field RRab
stars; no period or metallicity dependence of the ${\Phi_{21}}^{\mathrm RV}$ is
detected. The Baade-Wesselink distances of 26 non-Blazhko variables with the
best phase-coverage radial-velocity curves are determined; the corresponding
distance of the cluster, $10480\pm210$ pc, agrees with the previous literature
information. A quadratic formula for the $A_{\mathrm{puls}}-A_V$ relation of
RRab stars is given, which is valid for both OoI and OoII variables. We also
show that the $(V-I)_0$ of RRab stars measured at light minimum is period
dependent, there is at least 0.1 mag difference between the colours at minimum
light of the shortest- and longest-period variables.
",0,1,0,0,0,0
19898,19899,A framework for cascade size calculations on random networks,"  We present a framework to calculate the cascade size evolution for a large
class of cascade models on random network ensembles in the limit of infinite
network size. Our method is exact and applies to network ensembles with almost
arbitrary degree distribution, degree-degree correlations and, in case of
threshold models, with arbitrary threshold distribution. With our approach, we
shift the perspective from the known branching process approximations to the
iterative update of suitable probability distributions. Such distributions are
key to capture cascade dynamics that involve possibly continuous quantities and
that depend on the cascade history, e.g. if load is accumulated over time.
These distributions respect the Markovian nature of the studied random
processes. Random variables capture the impact of nodes that have failed at any
point in the past on their neighborhood. As a proof of concept, we provide two
examples: (a) Constant load models that cover many of the analytically
tractable cascade models, and, as a highlight, (b) a fiber bundle model that
was not tractable by branching process approximations before. Our derivations
cover the whole cascade dynamics, not only their steady state. This allows to
include interventions in time or further model complexity in the analysis.
",1,1,0,0,0,0
15515,15516,Free deterministic equivalent Z-scores of compound Wishart models: A goodness of fit test of 2DARMA models,"  We introduce a new method to qualify the goodness of fit parameter estimation
of compound Wishart models. Our method based on the free deterministic
equivalent Z-score, which we introduce in this paper. Furthermore, an
application to two dimensional autoregressive moving-average model is provided.
Our proposal method is a generalization of statistical hypothesis testing to
one dimensional moving average model based on fluctuations of real compound
Wishart matrices, which is a recent result by Hasegawa, Sakuma and Yoshida.
",0,0,1,1,0,0
7186,7187,Almost h-conformal semi-invariant submersions from almost quaternionic Hermitian manifolds,"  As a generalization of Riemannian submersions, horizontally conformal
submersions, semi-invariant submersions, h-semi-invariant submersions, almost
h-semi-invariant submersions, conformal semi-invariant submersions, we
introduce h-conformal semi-invariant submersions and almost h-conformal
semi-invariant submersions from almost quaternionic Hermitian manifolds onto
Riemannian manifolds.
We study their properties: the geometry of foliations, the conditions for
total manifolds to be locally product manifolds, the conditions for such maps
to be totally geodesic, etc. Finally, we give some examples of such maps.
",0,0,1,0,0,0
11488,11489,Event Analysis of Pulse-reclosers in Distribution Systems Through Sparse Representation,"  The pulse-recloser uses pulse testing technology to verify that the line is
clear of faults before initiating a reclose operation, which significantly
reduces stress on the system components (e.g. substation transformers) and
voltage sags on adjacent feeders. Online event analysis of pulse-reclosers are
essential to increases the overall utility of the devices, especially when
there are numerous devices installed throughout the distribution system. In
this paper, field data recorded from several devices were analyzed to identify
specific activity and fault locations. An algorithm is developed to screen the
data to identify the status of each pole and to tag time windows with a
possible pulse event. In the next step, selected time windows are further
analyzed and classified using a sparse representation technique by solving an
l1-regularized least-square problem. This classification is obtained by
comparing the pulse signature with the reference dictionary to find a set that
most closely matches the pulse features. This work also sheds additional light
on the possibility of fault classification based on the pulse signature. Field
data collected from a distribution system are used to verify the effectiveness
and reliability of the proposed method.
",1,0,0,0,0,0
20304,20305,Strongly exchange-coupled and surface-state-modulated magnetization dynamics in Bi2Se3/YIG heterostructures,"  We report strong interfacial exchange coupling in Bi2Se3/yttrium iron garnet
(YIG) bilayers manifested as large in-plane interfacial magnetic anisotropy
(IMA) and enhancement of damping probed by ferromagnetic resonance (FMR). The
IMA and spin mixing conductance reached a maximum when Bi2Se3 was around 6
quintuple-layer (QL) thick. The unconventional Bi2Se3 thickness dependence of
the IMA and spin mixing conductance are correlated with the evolution of
surface band structure of Bi2Se3, indicating that topological surface states
play an important role in the magnetization dynamics of YIG.
Temperature-dependent FMR of Bi2Se3/YIG revealed signatures of magnetic
proximity effect of $T_c$ as high as 180 K, and an effective field parallel to
the YIG magnetization direction at low temperature. Our study sheds light on
the effects of topological insulators on magnetization dynamics, essential for
development of TI-based spintronic devices.
",0,1,0,0,0,0
18978,18979,Rokhlin dimension for compact quantum group actions,"  We show that, for a given compact or discrete quantum group $G$, the class of
actions of $G$ on C*-algebras is first-order axiomatizable in the logic for
metric structures. As an application, we extend the notion of Rokhlin property
for $G$-C*-algebra, introduced by Barlak, Szabó, and Voigt in the case when
$G$ is second countable and coexact, to an arbitrary compact quantum group $G$.
All the the preservations and rigidity results for Rokhlin actions of second
countable coexact compact quantum groups obtained by Barlak, Szabó, and
Voigt are shown to hold in this general context. As a further application, we
extend the notion of equivariant order zero dimension for equivariant
*-homomorphisms, introduced in the classical setting by the first and third
authors, to actions of compact quantum groups. This allows us to define the
Rokhlin dimension of an action of a compact quantum group on a C*-algebra,
recovering the Rokhlin property as Rokhlin dimension zero. We conclude by
establishing a preservation result for finite nuclear dimension and finite
decomposition rank when passing to fixed point algebras and crossed products by
compact quantum group actions with finite Rokhlin dimension.
",0,0,1,0,0,0
18060,18061,Learning Edge Representations via Low-Rank Asymmetric Projections,"  We propose a new method for embedding graphs while preserving directed edge
information. Learning such continuous-space vector representations (or
embeddings) of nodes in a graph is an important first step for using network
information (from social networks, user-item graphs, knowledge bases, etc.) in
many machine learning tasks.
Unlike previous work, we (1) explicitly model an edge as a function of node
embeddings, and we (2) propose a novel objective, the ""graph likelihood"", which
contrasts information from sampled random walks with non-existent edges.
Individually, both of these contributions improve the learned representations,
especially when there are memory constraints on the total size of the
embeddings. When combined, our contributions enable us to significantly improve
the state-of-the-art by learning more concise representations that better
preserve the graph structure.
We evaluate our method on a variety of link-prediction task including social
networks, collaboration networks, and protein interactions, showing that our
proposed method learn representations with error reductions of up to 76% and
55%, on directed and undirected graphs. In addition, we show that the
representations learned by our method are quite space efficient, producing
embeddings which have higher structure-preserving accuracy but are 10 times
smaller.
",1,0,0,1,0,0
19595,19596,"Turing Completeness of Finite, Epistemic Programs","  In this note, we show the class of finite, epistemic programs to be Turing
complete. Epistemic programs is a widely used update mechanism used in
epistemic logic, where it such are a special type of action models: One which
does not contain postconditions.
",1,0,0,0,0,0
19007,19008,A new topological insulator - β-InTe strained in the layer plane,"  We have investigated the band structure of the bulk crystal and the (001)
surface of the \beta-InTe layered crystal subjected to biaxial stretching in
the layer plane. The calculation has been carried out using the full-potential
linearized augmented plane wave method (FP LAPW) implemented in WIEN2k. It has
been shown that at the strain \Deltaa/a=0.06, where a is the lattice parameter
in the layer plane, the band gap in the electronic spectrum collapses. With
further strain increase a band inversion occurs. The inclusion of the
spin-orbit interaction reopens the gap in the electronic spectrum of a bulk
crystal, and our calculations show that the spectrum of the surface states has
the form of a Dirac cone, typical for topological insulators.
",0,1,0,0,0,0
4583,4584,Onset of a modulational instability in trapped dipolar Bose-Einstein condensates,"  We explore the phase diagram of a finite-sized dysprosium dipolar
Bose-Einstein condensate in a cylindrical harmonic trap. We monitor the final
state after the scattering length is lowered from the repulsive BEC regime to
the quantum droplet regime. Either an adiabatic transformation between a BEC
and a quantum droplet is obtained or, above a critical trap aspect ratio
$\lambda_{\rm c}=1.87(14)$, a modulational instability results in the formation
of multiple droplets. This is in full agreement with the predicted structure of
the phase diagram with a crossover region below $\lambda_{\rm c}$ and a
multistable region above. Our results provide the missing piece connecting the
previously explored regimes resulting in a single or multiple dipolar quantum
droplets.
",0,1,0,0,0,0
14861,14862,Reply to comment on `Poynting flux in the neighbourhood of a point charge in arbitrary motion and the radiative power losses',"  Doubts have been expressed in a comment (Eur. J. Phys., 39, 018001, 2018),
about the tenability of the formulation for radiative losses in our recent
published work (Eur. J. Phys., 37, 045210, 2016). We provide our reply to the
comment. In particular, it is pointed out that one need to clearly distinguish
between the rate of the energy-momentum being carried by the electromagnetic
radiation to far-off space, and that of the mechanical energy-momentum losses
being incurred by the radiating charge. It is also demonstrated that while the
Poynting flux is always positive through a spherical surface centred on the
retarded position of the charge, it could surprisingly be negative through a
surface centred on the ""present"" position of the charge. It is further shown
that the mysterious Schott term, hitherto thought in literature to arise from
some acceleration-dependent energy in fields, is actually nothing but the
difference in rate of change of energy in self-fields of the charge between the
retarded and present times.
",0,1,0,0,0,0
19604,19605,Unbiased inference for discretely observed hidden Markov model diffusions,"  We develop an importance sampling (IS) type estimator for Bayesian joint
inference on the model parameters and latent states of a class of hidden Markov
models. The hidden state dynamics is a diffusion process and noisy observations
are obtained at discrete points in time. We suppose that the diffusion dynamics
can not be simulated exactly and hence one must time-discretise the diffusion.
Our approach is based on particle marginal Metropolis--Hastings, particle
filters, and multilevel Monte Carlo. The resulting IS type estimator leads to
inference without a bias from the time-discretisation. We give convergence
results and recommend allocations for algorithm inputs. In contrast to existing
unbiased methods requiring strong conditions on the diffusion and tailored
solutions, our method relies on standard Euler approximations of the diffusion.
Our method is parallelisable, and can be computationally efficient. The
user-friendly approach is illustrated with two examples.
",0,0,0,1,0,0
3523,3524,Model Selection for Explosive Models,"  This paper examines the limit properties of information criteria (such as
AIC, BIC, HQIC) for distinguishing between the unit root model and the various
kinds of explosive models. The explosive models include the local-to-unit-root
model, the mildly explosive model and the regular explosive model. Initial
conditions with different order of magnitude are considered. Both the OLS
estimator and the indirect inference estimator are studied. It is found that
BIC and HQIC, but not AIC, consistently select the unit root model when data
come from the unit root model. When data come from the local-to-unit-root
model, both BIC and HQIC select the wrong model with probability approaching 1
while AIC has a positive probability of selecting the right model in the limit.
When data come from the regular explosive model or from the mildly explosive
model in the form of $1+n^{\alpha }/n$ with $\alpha \in (0,1)$, all three
information criteria consistently select the true model. Indirect inference
estimation can increase or decrease the probability for information criteria to
select the right model asymptotically relative to OLS, depending on the
information criteria and the true model. Simulation results confirm our
asymptotic results in finite sample.
",0,0,1,1,0,0
9117,9118,BOOK: Storing Algorithm-Invariant Episodes for Deep Reinforcement Learning,"  We introduce a novel method to train agents of reinforcement learning (RL) by
sharing knowledge in a way similar to the concept of using a book. The recorded
information in the form of a book is the main means by which humans learn
knowledge. Nevertheless, the conventional deep RL methods have mainly focused
either on experiential learning where the agent learns through interactions
with the environment from the start or on imitation learning that tries to
mimic the teacher. Contrary to these, our proposed book learning shares key
information among different agents in a book-like manner by delving into the
following two characteristic features: (1) By defining the linguistic function,
input states can be clustered semantically into a relatively small number of
core clusters, which are forwarded to other RL agents in a prescribed manner.
(2) By defining state priorities and the contents for recording, core
experiences can be selected and stored in a small container. We call this
container as `BOOK'. Our method learns hundreds to thousand times faster than
the conventional methods by learning only a handful of core cluster
information, which shows that deep RL agents can effectively learn through the
shared knowledge from other agents.
",1,0,0,0,0,0
7255,7256,Automated and Robust Quantification of Colocalization in Dual-Color Fluorescence Microscopy: A Nonparametric Statistical Approach,"  Colocalization is a powerful tool to study the interactions between
fluorescently labeled molecules in biological fluorescence microscopy. However,
existing techniques for colocalization analysis have not undergone continued
development especially in regards to robust statistical support. In this paper,
we examine two of the most popular quantification techniques for colocalization
and argue that they could be improved upon using ideas from nonparametric
statistics and scan statistics. In particular, we propose a new colocalization
metric that is robust, easily implementable, and optimal in a rigorous
statistical testing framework. Application to several benchmark datasets, as
well as biological examples, further demonstrates the usefulness of the
proposed technique.
",0,0,0,1,0,0
6590,6591,Statistically Optimal and Computationally Efficient Low Rank Tensor Completion from Noisy Entries,"  In this article, we develop methods for estimating a low rank tensor from
noisy observations on a subset of its entries to achieve both statistical and
computational efficiencies. There have been a lot of recent interests in this
problem of noisy tensor completion. Much of the attention has been focused on
the fundamental computational challenges often associated with problems
involving higher order tensors, yet very little is known about their
statistical performance. To fill in this void, in this article, we characterize
the fundamental statistical limits of noisy tensor completion by establishing
minimax optimal rates of convergence for estimating a $k$th order low rank
tensor under the general $\ell_p$ ($1\le p\le 2$) norm which suggest
significant room for improvement over the existing approaches. Furthermore, we
propose a polynomial-time computable estimating procedure based upon power
iteration and a second-order spectral initialization that achieves the optimal
rates of convergence. Our method is fairly easy to implement and numerical
experiments are presented to further demonstrate the practical merits of our
estimator.
",0,0,1,1,0,0
3257,3258,Learning to Parse and Translate Improves Neural Machine Translation,"  There has been relatively little attention to incorporating linguistic prior
to neural machine translation. Much of the previous work was further
constrained to considering linguistic prior on the source side. In this paper,
we propose a hybrid model, called NMT+RNNG, that learns to parse and translate
by combining the recurrent neural network grammar into the attention-based
neural machine translation. Our approach encourages the neural machine
translation model to incorporate linguistic prior during training, and lets it
translate on its own afterward. Extensive experiments with four language pairs
show the effectiveness of the proposed NMT+RNNG.
",1,0,0,0,0,0
20144,20145,Lurking Variable Detection via Dimensional Analysis,"  Lurking variables represent hidden information, and preclude a full
understanding of phenomena of interest. Detection is usually based on
serendipity -- visual detection of unexplained, systematic variation. However,
these approaches are doomed to fail if the lurking variables do not vary. In
this article, we address these challenges by introducing formal hypothesis
tests for the presence of lurking variables, based on Dimensional Analysis.
These procedures utilize a modified form of the Buckingham Pi theorem to
provide structure for a suitable null hypothesis. We present analytic tools for
reasoning about lurking variables in physical phenomena, construct procedures
to handle cases of increasing complexity, and present examples of their
application to engineering problems. The results of this work enable
algorithm-driven lurking variable detection, complementing a traditionally
inspection-based approach.
",0,0,0,1,0,0
13355,13356,Quermassintegral preserving curvature flow in Hyperbolic space,"  We consider the quermassintegral preserving flow of closed \emph{h-convex}
hypersurfaces in hyperbolic space with the speed given by any positive power of
a smooth symmetric, strictly increasing, and homogeneous of degree one function
$f$ of the principal curvatures which is inverse concave and has dual $f_*$
approaching zero on the boundary of the positive cone. We prove that if the
initial hypersurface is \emph{h-convex}, then the solution of the flow becomes
strictly \emph{h-convex} for $t>0$, the flow exists for all time and converges
to a geodesic sphere exponentially in the smooth topology.
",0,0,1,0,0,0
2289,2290,MM Algorithms for Variance Component Estimation and Selection in Logistic Linear Mixed Model,"  Logistic linear mixed model is widely used in experimental designs and
genetic analysis with binary traits. Motivated by modern applications, we
consider the case with many groups of random effects and each group corresponds
to a variance component. When the number of variance components is large,
fitting the logistic linear mixed model is challenging. We develop two
efficient and stable minorization-maximization (MM) algorithms for the
estimation of variance components based on the Laplace approximation of the
logistic model. One of them leads to a simple iterative soft-thresholding
algorithm for variance component selection using maximum penalized approximated
likelihood. We demonstrate the variance component estimation and selection
performance of our algorithms by simulation studies and a real data analysis.
",0,0,0,1,0,0
12970,12971,Bloch-type spaces and extended Cesàro operators in the unit ball of a complex Banach space,"  Let $\mathbb{B}$ be the unit ball of a complex Banach space $X$. In this
paper, we will generalize the Bloch-type spaces and the little Bloch-type
spaces to the open unit ball $\mathbb{B}$ by using the radial derivative. Next,
we define an extended Cesàro operator $T_{\varphi}$ with holomorphic symbol
$\varphi$ and characterize those $\varphi$ for which $T_{\varphi}$ is bounded
between the Bloch-type spaces and the little Bloch-type spaces. We also
characterize those $\varphi$ for which $T_{\varphi}$ is compact between the
Bloch-type spaces and the little Bloch-type spaces under some additional
assumption on the symbol $\varphi$. When $\mathbb{B}$ is the open unit ball of
a finite dimensional complex Banach space $X$, this additional assumption is
automatically satisfied.
",0,0,1,0,0,0
362,363,Preventing Hospital Acquired Infections Through a Workflow-Based Cyber-Physical System,"  Hospital acquired infections (HAI) are infections acquired within the
hospital from healthcare workers, patients or from the environment, but which
have no connection to the initial reason for the patient's hospital admission.
HAI are a serious world-wide problem, leading to an increase in mortality
rates, duration of hospitalisation as well as significant economic burden on
hospitals. Although clear preventive guidelines exist, studies show that
compliance to them is frequently poor. This paper details the software
perspective for an innovative, business process software based cyber-physical
system that will be implemented as part of a European Union-funded research
project. The system is composed of a network of sensors mounted in different
sites around the hospital, a series of wearables used by the healthcare workers
and a server side workflow engine. For better understanding, we describe the
system through the lens of a single, simple clinical workflow that is
responsible for a significant portion of all hospital infections. The goal is
that when completed, the system will be configurable in the sense of
facilitating the creation and automated monitoring of those clinical workflows
that when combined, account for over 90\% of hospital infections.
",1,0,0,0,0,0
16998,16999,AI Challenges in Human-Robot Cognitive Teaming,"  Among the many anticipated roles for robots in the future is that of being a
human teammate. Aside from all the technological hurdles that have to be
overcome with respect to hardware and control to make robots fit to work with
humans, the added complication here is that humans have many conscious and
subconscious expectations of their teammates - indeed, we argue that teaming is
mostly a cognitive rather than physical coordination activity. This introduces
new challenges for the AI and robotics community and requires fundamental
changes to the traditional approach to the design of autonomy. With this in
mind, we propose an update to the classical view of the intelligent agent
architecture, highlighting the requirements for mental modeling of the human in
the deliberative process of the autonomous agent. In this article, we outline
briefly the recent efforts of ours, and others in the community, towards
developing cognitive teammates along these guidelines.
",1,0,0,0,0,0
8351,8352,A Unified Approach to Adaptive Regularization in Online and Stochastic Optimization,"  We describe a framework for deriving and analyzing online optimization
algorithms that incorporate adaptive, data-dependent regularization, also
termed preconditioning. Such algorithms have been proven useful in stochastic
optimization by reshaping the gradients according to the geometry of the data.
Our framework captures and unifies much of the existing literature on adaptive
online methods, including the AdaGrad and Online Newton Step algorithms as well
as their diagonal versions. As a result, we obtain new convergence proofs for
these algorithms that are substantially simpler than previous analyses. Our
framework also exposes the rationale for the different preconditioned updates
used in common stochastic optimization methods.
",1,0,1,1,0,0
962,963,Dynamics of a Camphoric Acid boat at the air-water interface,"  We report experiments on an agarose gel tablet loaded with camphoric acid
(c-boat) set into self-motion by interfacial tension gradients at the air-water
interface. We observe three distinct modes of c-boat motion: harmonic mode
where the c-boat speed oscillates sinusoidally in time, a steady mode where the
c-boat maintains constant speed, and a relaxation oscillation mode where the
c-boat maintains near-zero speed between sudden jumps in speed and position at
regular time intervals. Whereas all three modes have been separately reported
before in different systems, we show they belong to a common description.
Through control of the air-water surface tension with Sodium Dodecyl Sulfate
(SDS), we experimentally deduce the three self-propulsive modes result from
surface tension difference between Camphoric Acid (CA) and the ambient
surroundings.
",0,1,0,0,0,0
9953,9954,Some remarks on Huisken's monotonicity formula for mean curvature flow,"  We discuss a monotone quantity related to Huisken's monotonicity formula and
some technical consequences for mean curvature flow.
",0,0,1,0,0,0
17134,17135,Competing Ferromagnetic and Anti-Ferromagnetic interactions in Iron Nitride $ζ$-Fe$_2$N,"  The paper discusses the magnetic state of zeta phase of iron nitride viz.
$\zeta$-Fe$_2$N on the basis of spin polarized first principles electronic
structure calculations together with a review of already published data.
Results of our first principles study suggest that the ground state of
$\zeta$-Fe$_2$N is ferromagnetic (FM) with a magnetic moment of 1.528
$\mu_\text{B}$ on the Fe site. The FM ground state is lower than the
anti-ferromagnetic (AFM) state by 8.44 meV and non-magnetic(NM) state by 191
meV per formula unit. These results are important in view of reports which
claim that $\zeta$-Fe$_2$N undergoes an AFM transition below 10K and others
which do not observe any magnetic transition up to 4.2K. We argue that the
experimental results of AFM transition below 10K are inconclusive and we
propose the presence of competing FM and AFM superexchange interactions between
Fe sites mediated by nitrogen atoms, which are consistent with
Goodenough-Kanamori-Anderson rules. We find that the anti-ferromagnetically
coupled Fe sites are outnumbered by ferromagnetically coupled Fe sites leading
to a stable FM ground state. A Stoner analysis of the results also supports our
claim of a FM ground state.
",0,1,0,0,0,0
10197,10198,Hyperrigid subsets of Cuntz-Krieger algebras and the property of rigidity at zero,"  A subset $\mathcal{G}$ generating a $C^*$-algebra $A$ is said to be
hyperrigid if for every faithful nondegenerate $*$-representation $A\subseteq
B(H)$ and a sequence $\phi_n:B(H) \to B(H)$ of unital completely positive maps,
we have that \[ \lim_{n\to\infty}\phi_n(g)= g~~\text{for all } g\in \mathcal{G}
~~ \implies ~~ \lim_{n\to\infty}\phi_n(a)= a~~\text{for all } a\in A \] where
all convergence are in norm. In this paper, we show that for the Cuntz-Krieger
algebra $\mathcal{O}(G)$ associated to a row-finite directed graph $G$ with no
isolated vertices, the set of partial isometries $\mathcal{E}=\{S_e:e\in E\}$
is hyperrigid.
In addition, we define and examine a closely related notion: the property of
rigidity at $0$. A generating subset $\mathcal{G}$ of a $C^*$-algebra $A$ is
said to be rigid at $0$ if for every sequence of contractive positive maps
$\varphi_n:A\to \mathbb C$ satisfying $\lim_{n\to \infty}\varphi_n(g)=0$ for
every $g\in \mathcal{G}$, we have that $\lim_{n\to \infty}\varphi_n(a)=0$ for
every $a\in A$.
We show that, when combined, hyperrigidity and rigidity at $0$ are equivalent
to a somewhat stronger notion of hyperrigidity, and we connect this to the
unique extension property. This, however, is not the case for the generating
set $\mathcal{E}$. More precisely, we show that for any graph $G$, subsets of
the Cuntz-Krieger family generating $\mathcal{O}(G)$ are rigid at $0$ if and
only if they contain every vertex projection.
",0,0,1,0,0,0
738,739,Morse geodesics in torsion groups,"  In this paper we exhibit Morse geodesics, often called ""hyperbolic
directions"", in infinite unbounded torsion groups. The groups studied are
lacunary hyperbolic groups and constructed using graded small cancellation
conditions. In all previously known examples, Morse geodesics were found in
groups which also contained Morse elements, infinite order elements whose
cyclic subgroup gives a Morse quasi-geodesic. Our result presents the first
example of a group which contains Morse geodesics but no Morse elements. In
fact, we show that there is an isometrically embedded $7$-regular tree inside
such groups where every infinite, simple path is a Morse geodesic.
",0,0,1,0,0,0
19209,19210,Adversarial Active Learning for Deep Networks: a Margin Based Approach,"  We propose a new active learning strategy designed for deep neural networks.
The goal is to minimize the number of data annotation queried from an oracle
during training. Previous active learning strategies scalable for deep networks
were mostly based on uncertain sample selection. In this work, we focus on
examples lying close to the decision boundary. Based on theoretical works on
margin theory for active learning, we know that such examples may help to
considerably decrease the number of annotations. While measuring the exact
distance to the decision boundaries is intractable, we propose to rely on
adversarial examples. We do not consider anymore them as a threat instead we
exploit the information they provide on the distribution of the input space in
order to approximate the distance to decision boundaries. We demonstrate
empirically that adversarial active queries yield faster convergence of CNNs
trained on MNIST, the Shoe-Bag and the Quick-Draw datasets.
",0,0,0,1,0,0
4029,4030,An optimization approach for dynamical Tucker tensor approximation,"  An optimization-based approach for the Tucker tensor approximation of
parameter-dependent data tensors and solutions of tensor differential equations
with low Tucker rank is presented. The problem of updating the tensor
decomposition is reformulated as fitting problem subject to the tangent space
without relying on an orthogonality gauge condition. A discrete Euler scheme is
established in an alternating least squares framework, where the quadratic
subproblems reduce to trace optimization problems, that are shown to be
explicitly solvable and accessible using SVD of small size. In the presence of
small singular values, instability for larger ranks is reduced, since the
method does not need the (pseudo) inverse of matricizations of the core tensor.
Regularization of Tikhonov type can be used to compensate for the lack of
uniqueness in the tangent space. The method is validated numerically and shown
to be stable also for larger ranks in the case of small singular values of the
core unfoldings. Higher order explicit integrators of Runge-Kutta type can be
composed.
",0,1,0,0,0,0
8629,8630,Multilayer Network Model of Movie Script,"  Network models have been increasingly used in the past years to support
summarization and analysis of narratives, such as famous TV series, books and
news. Inspired by social network analysis, most of these models focus on the
characters at play. The network model well captures all characters
interactions, giving a broad picture of the narration's content. A few works
went beyond by introducing additional semantic elements, always captured in a
single layer network. In contrast, we introduce in this work a multilayer
network model to capture more elements of the narration of a movie from its
script: people, locations, and other semantic elements. This model enables new
measures and insights on movies. We demonstrate this model on two very popular
movies.
",1,0,0,0,0,0
1889,1890,Real-Time Illegal Parking Detection System Based on Deep Learning,"  The increasing illegal parking has become more and more serious. Nowadays the
methods of detecting illegally parked vehicles are based on background
segmentation. However, this method is weakly robust and sensitive to
environment. Benefitting from deep learning, this paper proposes a novel
illegal vehicle parking detection system. Illegal vehicles captured by camera
are firstly located and classified by the famous Single Shot MultiBox Detector
(SSD) algorithm. To improve the performance, we propose to optimize SSD by
adjusting the aspect ratio of default box to accommodate with our dataset
better. After that, a tracking and analysis of movement is adopted to judge the
illegal vehicles in the region of interest (ROI). Experiments show that the
system can achieve a 99% accuracy and real-time (25FPS) detection with strong
robustness in complex environments.
",1,0,0,1,0,0
15479,15480,Robust and Efficient Transfer Learning with Hidden-Parameter Markov Decision Processes,"  We introduce a new formulation of the Hidden Parameter Markov Decision
Process (HiP-MDP), a framework for modeling families of related tasks using
low-dimensional latent embeddings. Our new framework correctly models the joint
uncertainty in the latent parameters and the state space. We also replace the
original Gaussian Process-based model with a Bayesian Neural Network, enabling
more scalable inference. Thus, we expand the scope of the HiP-MDP to
applications with higher dimensions and more complex dynamics.
",1,0,0,1,0,0
9432,9433,A Connection between Feed-Forward Neural Networks and Probabilistic Graphical Models,"  Two of the most popular modelling paradigms in computer vision are
feed-forward neural networks (FFNs) and probabilistic graphical models (GMs).
Various connections between the two have been studied in recent works, such as
e.g. expressing mean-field based inference in a GM as an FFN. This paper
establishes a new connection between FFNs and GMs. Our key observation is that
any FFN implements a certain approximation of a corresponding Bayesian network
(BN). We characterize various benefits of having this connection. In
particular, it results in a new learning algorithm for BNs. We validate the
proposed methods for a classification problem on CIFAR-10 dataset and for
binary image segmentation on Weizmann Horse dataset. We show that statistically
learned BNs improve performance, having at the same time essentially better
generalization capability, than their FFN counterparts.
",1,0,0,1,0,0
1597,1598,HNCcorr: A Novel Combinatorial Approach for Cell Identification in Calcium-Imaging Movies,"  Calcium imaging has emerged as a workhorse method in neuroscience to
investigate patterns of neuronal activity. Instrumentation to acquire calcium
imaging movies has rapidly progressed and has become standard across labs.
Still, algorithms to automatically detect and extract activity signals from
calcium imaging movies are highly variable from~lab~to~lab and more advanced
algorithms are continuously being developed. Here we present HNCcorr, a novel
algorithm for cell identification in calcium imaging movies based on
combinatorial optimization. The algorithm identifies cells by finding distinct
groups of highly similar pixels in correlation space, where a pixel is
represented by the vector of correlations to a set of other pixels. The HNCcorr
algorithm achieves the best known results for the cell identification benchmark
of Neurofinder, and guarantees an optimal solution to the underlying
deterministic optimization model resulting in a transparent mapping from input
data to outcome.
",0,0,1,0,0,0
11261,11262,Deriving Verb Predicates By Clustering Verbs with Arguments,"  Hand-built verb clusters such as the widely used Levin classes (Levin, 1993)
have proved useful, but have limited coverage. Verb classes automatically
induced from corpus data such as those from VerbKB (Wijaya, 2016), on the other
hand, can give clusters with much larger coverage, and can be adapted to
specific corpora such as Twitter. We present a method for clustering the
outputs of VerbKB: verbs with their multiple argument types, e.g.
""marry(person, person)"", ""feel(person, emotion)."" We make use of a novel
low-dimensional embedding of verbs and their arguments to produce high quality
clusters in which the same verb can be in different clusters depending on its
argument type. The resulting verb clusters do a better job than hand-built
clusters of predicting sarcasm, sentiment, and locus of control in tweets.
",1,0,0,0,0,0
14253,14254,Spectral Estimation of Plasma Fluctuations I: Comparison of Methods,"  The relative root mean squared errors (RMSE) of nonparametric methods for
spectral estimation is compared for microwave scattering data of plasma
fluctuations. These methods reduce the variance of the periodogram estimate by
averaging the spectrum over a frequency bandwidth. As the bandwidth increases,
the variance decreases, but the bias error increases. The plasma spectra vary
by over four orders of magnitude, and therefore, using a spectral window is
necessary. We compare the smoothed tapered periodogram with the adaptive
multiple taper methods and hybrid methods. We find that a hybrid method, which
uses four orthogonal tapers and then applies a kernel smoother, performs best.
For 300 point data segments, even an optimized smoothed tapered periodogram has
a 24 \% larger relative RMSE than the hybrid method. We present two new
adaptive multi-taper weightings which outperform Thomson's original adaptive
weighting.
",0,0,0,1,0,0
2643,2644,A Finite-Tame-Wild Trichotomy Theorem for Tensor Diagrams,"  In this paper, we consider the problem of determining when two tensor
networks are equivalent under a heterogeneous change of basis. In particular,
to a string diagram in a certain monoidal category (which we call tensor
diagrams), we formulate an associated abelian category of representations. Each
representation corresponds to a tensor network on that diagram. We then
classify which tensor diagrams give rise to categories that are finite, tame,
or wild in the traditional sense of representation theory. For those tensor
diagrams of finite and tame type, we classify the indecomposable
representations. Our main result is that a tensor diagram is wild if and only
if it contains a vertex of degree at least three. Otherwise, it is of tame or
finite type.
",0,0,1,0,0,0
5035,5036,First constraints on fuzzy dark matter from Lyman-$α$ forest data and hydrodynamical simulations,"  We present constraints on the masses of extremely light bosons dubbed fuzzy
dark matter from Lyman-$\alpha$ forest data. Extremely light bosons with a De
Broglie wavelength of $\sim 1$ kpc have been suggested as dark matter
candidates that may resolve some of the current small scale problems of the
cold dark matter model. For the first time we use hydrodynamical simulations to
model the Lyman-$\alpha$ flux power spectrum in these models and compare with
the observed flux power spectrum from two different data sets: the XQ-100 and
HIRES/MIKE quasar spectra samples. After marginalization over nuisance and
physical parameters and with conservative assumptions for the thermal history
of the IGM that allow for jumps in the temperature of up to $5000\rm\,K$,
XQ-100 provides a lower limit of 7.1$\times 10^{-22}$ eV, HIRES/MIKE returns a
stronger limit of 14.3$\times 10^{-22}$ eV, while the combination of both data
sets results in a limit of 20 $\times 10^{-22}$ eV (2$\sigma$ C.L.). The limits
for the analysis of the combined data sets increases to 37.5$\times 10^{-22}$
eV (2$\sigma$ C.L.) when a smoother thermal history is assumed where the
temperature of the IGM evolves as a power-law in redshift. Light boson masses
in the range $1-10 \times10^{-22}$ eV are ruled out at high significance by our
analysis, casting strong doubts that FDM helps solve the ""small scale crisis""
of the cold dark matter models.
",0,1,0,0,0,0
20066,20067,One side continuity of meromorphic mappings between real analytic hypersurfaces,"  We prove that a meromorphic mapping, which sends a peace of a real analytic
strictly pseudoconvex hypersurface in $\cc^2$ to a compact subset of $\cc^N$
which doesn't contain germs of non-constant complex curves is continuous from
the concave side of the hypersurface. This implies the analytic continuability
along CR-paths of germs of holomorphic mappings from real analytic
hypersurfaces with non-vanishing Levi form to the locally spherical ones in all
dimensions.
",0,0,1,0,0,0
15091,15092,The $u^n$-invariant and the Symbol Length of $H_2^n(F)$,"  Given a field $F$ of $\operatorname{char}(F)=2$, we define $u^n(F)$ to be the
maximal dimension of an anisotropic form in $I_q^n F$. For $n=1$ it recaptures
the definition of $u(F)$. We study the relations between this value and the
symbol length of $H_2^n(F)$, denoted by $sl_2^n(F)$. We show for any $n \geq 2$
that if $2^n \leq u^n(F) \leq u^2(F) < \infty$ then $sl_2^n(F) \leq
\prod_{i=2}^n (\frac{u^i(F)}{2}+1-2^{i-1})$. As a result, if $u(F)$ is finite
then $sl_2^n(F)$ is finite for any $n$, a fact which was previously proven when
$\operatorname{char}(F) \neq 2$ by Saltman and Krashen. We also show that if
$sl_2^n(F)=1$ then $u^n(F)$ is either $2^n$ or $2^{n+1}$.
",0,0,1,0,0,0
364,365,On orbifold constructions associated with the Leech lattice vertex operator algebra,"  In this article, we study orbifold constructions associated with the Leech
lattice vertex operator algebra. As an application, we prove that the structure
of a strongly regular holomorphic vertex operator algebra of central charge
$24$ is uniquely determined by its weight one Lie algebra if the Lie algebra
has the type $A_{3,4}^3A_{1,2}$, $A_{4,5}^2$, $D_{4,12}A_{2,6}$, $A_{6,7}$,
$A_{7,4}A_{1,1}^3$, $D_{5,8}A_{1,2}$ or $D_{6,5}A_{1,1}^2$ by using the reverse
orbifold construction. Our result also provides alternative constructions of
these vertex operator algebras (except for the case $A_{6,7}$) from the Leech
lattice vertex operator algebra.
",0,0,1,0,0,0
6239,6240,Dynamical transport measurement of the Luttinger parameter in helical edges states of 2D topological insulators,"  One-dimensional (1D) electron systems in the presence of Coulomb interaction
are described by Luttinger liquid theory. The strength of Coulomb interaction
in the Luttinger liquid, as parameterized by the Luttinger parameter K, is in
general difficult to measure. This is because K is usually hidden in powerlaw
dependencies of observables as a function of temperature or applied bias. We
propose a dynamical way to measure K on the basis of an electronic
time-of-flight experiment. We argue that the helical Luttinger liquid at the
edge of a 2D topological insulator constitutes a preeminently suited
realization of a 1D system to test our proposal. This is based on the
robustness of helical liquids against elastic backscattering in the presence of
time reversal symmetry.
",0,1,0,0,0,0
4159,4160,Fraction of the X-ray selected AGNs with optical emission lines in galaxy groups,"  Compared with numerous X-ray dominant active galactic nuclei (AGNs) without
emission-line signatures in their optical spectra, the X-ray selected AGNs with
optical emission lines are probably still in the high-accretion phase of black
hole growth. This paper presents an investigation on the fraction of these
X-ray detected AGNs with optical emission-line spectra in 198 galaxy groups at
$z<1$ in a rest frame 0.1-2.4 keV luminosity range 41.3 <log(L_X/erg s-1) <
44.1 within the COSMOS field, as well as its variations with redshift and group
richness. For various selection criteria of member galaxies, the numbers of
galaxies and the AGNs with optical emission lines in each galaxy group are
obtained. It is found that, in total 198 X-ray groups, there are 27 AGNs
detected in 26 groups. AGN fraction is on everage less than $4.6 (\pm 1.2)\%$
for individual groups hosting at least one AGN. The corrected overall AGN
fraction for whole group sample is less than $0.98 (\pm 0.11) \%$. The
normalized locations of group AGNs show that 15 AGNs are found to be located in
group centers, including all 6 low-luminosity group AGNs. A week rising
tendency with $z$ are found: overall AGN fraction is 0.30-0.43% for the groups
at $z<0.5$, and 0.55-0.64% at 0.5 < z < 1.0. For the X-ray groups at $z>0.5$,
most member AGNs are X-ray bright, optically dull, which results in a lower AGN
fractions at higher redshifts. The AGN fraction in isolated fields also
exhibits a rising trend with redshift, and the slope is consistent with that in
groups. The environment of galaxy groups seems to make no difference in
detection probability of the AGNs with emission lines. Additionally, a larger
AGN fractions are found in poorer groups, which implies that the AGNs in poorer
groups might still be in the high-accretion phase, whereas the AGN population
in rich clusters is mostly in the low-accretion, X-ray dominant phase.
",0,1,0,0,0,0
17461,17462,"Lusin-type approximation of Sobolev by Lipschitz functions, in Gaussian and $RCD(K,\infty)$ spaces","  We establish new approximation results, in the sense of Lusin, of Sobolev
functions by Lipschitz ones, in some classes of non-doubling metric measure
structures. Our proof technique relies upon estimates for heat semigroups and
applies to Gaussian and $RCD(K, \infty)$ spaces. As a consequence, we obtain
quantitative stability for regular Lagrangian flows in Gaussian settings.
",0,0,1,0,0,0
4564,4565,Penalty Alternating Direction Methods for Mixed-Integer Optimization: A New View on Feasibility Pumps,"  Feasibility pumps are highly effective primal heuristics for mixed-integer
linear and nonlinear optimization. However, despite their success in practice
there are only few works considering their theoretical properties. We show that
feasibility pumps can be seen as alternating direction methods applied to
special reformulations of the original problem, inheriting the convergence
theory of these methods. Moreover, we propose a novel penalty framework that
encompasses this alternating direction method, which allows us to refrain from
random perturbations that are applied in standard versions of feasibility pumps
in case of failure. We present a convergence theory for the new penalty based
alternating direction method and compare the new variant of the feasibility
pump with existing versions in an extensive numerical study for mixed-integer
linear and nonlinear problems.
",0,0,1,0,0,0
17415,17416,MIMO Graph Filters for Convolutional Neural Networks,"  Superior performance and ease of implementation have fostered the adoption of
Convolutional Neural Networks (CNNs) for a wide array of inference and
reconstruction tasks. CNNs implement three basic blocks: convolution, pooling
and pointwise nonlinearity. Since the two first operations are well-defined
only on regular-structured data such as audio or images, application of CNNs to
contemporary datasets where the information is defined in irregular domains is
challenging. This paper investigates CNNs architectures to operate on signals
whose support can be modeled using a graph. Architectures that replace the
regular convolution with a so-called linear shift-invariant graph filter have
been recently proposed. This paper goes one step further and, under the
framework of multiple-input multiple-output (MIMO) graph filters, imposes
additional structure on the adopted graph filters, to obtain three new (more
parsimonious) architectures. The proposed architectures result in a lower
number of model parameters, reducing the computational complexity, facilitating
the training, and mitigating the risk of overfitting. Simulations show that the
proposed simpler architectures achieve similar performance as more complex
models.
",0,0,0,1,0,0
13686,13687,Variations of BPS structure and a large rank limit,"  We study a class of flat bundles, of finite rank $N$, which arise naturally
from the Donaldson-Thomas theory of a Calabi-Yau threefold $X$ via the notion
of a variation of BPS structure. We prove that in a large $N$ limit their flat
sections converge to the solutions to certain infinite dimensional
Riemann-Hilbert problems recently found by Bridgeland. In particular this
implies an expression for the positive degree, genus $0$ Gopakumar-Vafa
contribution to the Gromov-Witten partition function of $X$ in terms of
solutions to confluent hypergeometric differential equations.
",0,0,1,0,0,0
10769,10770,Monitoring of Wild Pseudomonas Biofilm Strain Conditions Using Statistical Characterisation of Scanning Electron Microscopy Images,"  The present paper proposes a novel method of quantification of the variation
in biofilm architecture, in correlation with the alteration of growth
conditions that include, variations of substrate and conditioning layer. The
polymeric biomaterial serving as substrates are widely used in implants and
indwelling medical devices, while the plasma proteins serve as the conditioning
layer. The present method uses descriptive statistics of FESEM images of
biofilms obtained during a variety of growth conditions. We aim to explore here
the texture and fractal analysis techniques, to identify the most
discriminatory features which are capable of predicting the difference in
biofilm growth conditions. We initially extract some statistical features of
biofilm images on bare polymer surfaces, followed by those on the same
substrates adsorbed with two different types of plasma proteins, viz. Bovine
serum albumin (BSA) and Fibronectin (FN), for two different adsorption times.
The present analysis has the potential to act as a futuristic technology for
developing a computerized monitoring system in hospitals with automated image
analysis and feature extraction, which may be used to predict the growth
profile of an emerging biofilm on surgical implants or similar medical
applications.
",0,0,0,1,1,0
654,655,Rotating Rayleigh-Taylor turbulence,"  The turbulent Rayleigh--Taylor system in a rotating reference frame is
investigated by direct numerical simulations within the Oberbeck-Boussinesq
approximation. On the basis of theoretical arguments, supported by our
simulations, we show that the Rossby number decreases in time, and therefore
the Coriolis force becomes more important as the system evolves and produces
many effects on Rayleigh--Taylor turbulence. We find that rotation reduces the
intensity of turbulent velocity fluctuations and therefore the growth rate of
the temperature mixing layer. Moreover, in presence of rotation the conversion
of potential energy into turbulent kinetic energy is found to be less effective
and the efficiency of the heat transfer is reduced. Finally, during the
evolution of the mixing layer we observe the development of a
cyclone-anticyclone asymmetry.
",0,1,0,0,0,0
9302,9303,Parametric uncertainty in complex environmental models: a cheap emulation approach for models with high-dimensional output,"  In order to understand underlying processes governing environmental and
physical processes, and predict future outcomes, a complex computer model is
frequently required to simulate these dynamics. However there is inevitably
uncertainty related to the exact parametric form or the values of such
parameters to be used when developing these simulators, with \emph{ranges} of
plausible values prevalent in the literature. Systematic errors introduced by
failing to account for these uncertainties have the potential to have a large
effect on resulting estimates in unknown quantities of interest. Due to the
complexity of these types of models, it is often unfeasible to run large
numbers of training runs that are usually required for full statistical
emulators of the environmental processes. We therefore present a method for
accounting for uncertainties in complex environmental simulators without the
need for very large numbers of training runs and illustrate the method through
an application to the Met Office's atmospheric transport model NAME. We
conclude that there are two principle parameters that are linked with
variability in NAME outputs, namely the free tropospheric turbulence parameter
and particle release height. Our results suggest the former should be
significantly larger than is currently implemented as a default in NAME, whilst
changes in the latter most likely stem from inconsistencies between the model
specified ground height at the observation locations and the true height at
this location. Estimated discrepancies from independent data are consistent
with the discrepancy between modelled and true ground height.
",0,0,0,1,0,0
4079,4080,Hamiltonian analogs of combustion engines: a systematic exception to adiabatic decoupling,"  Workhorse theories throughout all of physics derive effective Hamiltonians to
describe slow time evolution, even though low-frequency modes are actually
coupled to high-frequency modes. Such effective Hamiltonians are accurate
because of \textit{adiabatic decoupling}: the high-frequency modes `dress' the
low-frequency modes, and renormalize their Hamiltonian, but they do not
steadily inject energy into the low-frequency sector. Here, however, we
identify a broad class of dynamical systems in which adiabatic decoupling fails
to hold, and steady energy transfer across a large gap in natural frequency
(`steady downconversion') instead becomes possible, through nonlinear
resonances of a certain form. Instead of adiabatic decoupling, the special
features of multiple time scale dynamics lead in these cases to efficiency
constraints that somewhat resemble thermodynamics.
",0,1,0,0,0,0
1177,1178,On Evaluation of Embodied Navigation Agents,"  Skillful mobile operation in three-dimensional environments is a primary
topic of study in Artificial Intelligence. The past two years have seen a surge
of creative work on navigation. This creative output has produced a plethora of
sometimes incompatible task definitions and evaluation protocols. To coordinate
ongoing and future research in this area, we have convened a working group to
study empirical methodology in navigation research. The present document
summarizes the consensus recommendations of this working group. We discuss
different problem statements and the role of generalization, present evaluation
measures, and provide standard scenarios that can be used for benchmarking.
",1,0,0,0,0,0
18021,18022,Self-Organization and The Origins of Life: The Managed-Metabolism Hypothesis,"  The managed-metabolism hypothesis suggests that a cooperation barrier must be
overcome if self-producing chemical organizations are to transition from
non-life to life. This barrier prevents un-managed, self-organizing,
autocatalytic networks of molecular species from individuating into complex,
cooperative organizations. The barrier arises because molecular species that
could otherwise make significant cooperative contributions to the success of an
organization will often not be supported within the organization, and because
side reactions and other free-riding processes will undermine cooperation. As a
result, the barrier seriously limits the possibility space that can be explored
by un-managed organizations, impeding individuation, complex functionality and
the transition to life. The barrier can be overcome comprehensively by
appropriate management which implements a system of evolvable constraints. The
constraints support beneficial co-operators and suppress free riders. In this
way management can manipulate the chemical processes of an autocatalytic
organization, producing novel processes that serve the interests of the
organization as a whole and that could not arise and persist spontaneously in
an un-managed chemical organization. Management self-organizes because it is
able to capture some of the benefits that are produced when its management of
an autocatalytic organization promotes beneficial cooperation. Selection
therefore favours the emergence of managers that take over and manage chemical
organizations so as to overcome the cooperation barrier. The managed-metabolism
hypothesis shows that if management is to overcome the cooperation barrier
comprehensively, its interventions must be digitally coded. In this way, the
hypothesis accounts for the two-tiered structure of all living cells in which a
digitally-coded genetic apparatus manages an analogically-informed metabolism.
",0,1,0,0,0,0
10977,10978,Counterexample to Gronwall's Conjecture,"  We present a projectively invariant description of planar linear 3-webs and
construct a counterexample to Gronwall's conjecture.
",0,0,1,0,0,0
15701,15702,Probabilistic Projection of Subnational Total Fertility Rates,"  We consider the problem of probabilistic projection of the total fertility
rate (TFR) for subnational regions. We seek a method that is consistent with
the UN's recently adopted Bayesian method for probabilistic TFR projections for
all countries, and works well for all countries. We assess various possible
methods using subnational TFR data for 47 countries. We find that the method
that performs best in terms of out-of-sample predictive performance and also in
terms of reproducing the within-country correlation in TFR is a method that
scales the national trajectory by a region-specific scale factor that is
allowed to vary slowly over time. This supports the hypothesis of Watkins
(1990, 1991) that within-country TFR converges over time in response to
country-specific factors, and extends the Watkins hypothesis to the last 50
years and to a much wider range of countries around the world.
",0,0,0,1,0,0
12347,12348,Risk ratios for contagious outcomes,"  The risk ratio is a popular tool for summarizing the relationship between a
binary covariate and outcome, even when outcomes may be dependent.
Investigations of infectious disease outcomes in cohort studies of individuals
embedded within clusters -- households, villages, or small groups -- often
report risk ratios. Epidemiologists have warned that risk ratios may be
misleading when outcomes are contagious, but the nature and severity of this
error is not well understood. In this study, we assess the epidemiologic
meaning of the risk ratio when outcomes are contagious. We first give a
structural definition of infectious disease transmission within clusters, based
on the canonical susceptible-infective epidemic model. From this standard
characterization, we define the individual-level ratio of instantaneous risks
(hazard ratio) as the inferential target, and evaluate the properties of the
risk ratio as an estimate of this quantity. We exhibit analytically and by
simulation the circumstances under which the risk ratio implies an effect whose
direction is opposite that of the true individual-level hazard ratio. In
particular, the risk ratio can be greater than one even when the covariate of
interest reduces both individual-level susceptibility to infection, and
transmissibility once infected. We explain these findings in the epidemiologic
language of confounding and relate the direction bias to Simpson's paradox.
",0,0,0,1,0,0
9878,9879,Robust and Real-time Deep Tracking Via Multi-Scale Domain Adaptation,"  Visual tracking is a fundamental problem in computer vision. Recently, some
deep-learning-based tracking algorithms have been achieving record-breaking
performances. However, due to the high complexity of deep learning, most deep
trackers suffer from low tracking speed, and thus are impractical in many
real-world applications. Some new deep trackers with smaller network structure
achieve high efficiency while at the cost of significant decrease on precision.
In this paper, we propose to transfer the feature for image classification to
the visual tracking domain via convolutional channel reductions. The channel
reduction could be simply viewed as an additional convolutional layer with the
specific task. It not only extracts useful information for object tracking but
also significantly increases the tracking speed. To better accommodate the
useful feature of the target in different scales, the adaptation filters are
designed with different sizes. The yielded visual tracker is real-time and also
illustrates the state-of-the-art accuracies in the experiment involving two
well-adopted benchmarks with more than 100 test videos.
",1,0,0,0,0,0
19810,19811,N-body simulations of planet formation via pebble accretion I: First Results,"  Context. Planet formation with pebbles has been proposed to solve a couple of
long-standing issues in the classical formation model. Some sophisticated
simulations have been done to confirm the efficiency of pebble accretion.
However, there has not been any global N-body simulations that compare the
outcomes of planet formation via pebble accretion with observed extrasolar
planetary systems. Aims. In this paper, we study the effects of a range of
initial parameters of planet formation via pebble accretion, and present the
first results of our simulations. Methods. We incorporate the pebble accretion
model by Ida et al. (2016) in the N-body code SyMBA (Duncan et al. 1998), along
with the effects of gas accretion, eccentricity and inclination damping and
planet migration in the disc. Results. We confirm that pebble accretion leads
to a variety of planetary systems, but have difficulty in reproducing observed
properties of exoplanetary systems, such as planetary mass, semimajor axis, and
eccentricity distributions. The main reason behind this is a too-efficient type
I migration, which sensitively depends on the disc model. However, our
simulations also lead to a few interesting predictions. First, we find that
formation efficiencies of planets depend on the stellar metallicities, not only
for giant planets, but also for Earths (Es) and Super-Earths (SEs). The
dependency for Es/SEs is subtle. Although higher metallicity environments lead
to faster formation of a larger number of Es/SEs, they also tend to be lost
later via dynamical instability. Second, our results indicate that a wide range
of bulk densities observed for Es and SEs is a natural consequence of dynamical
evolution of planetary systems. Third, the ejection trend of our simulations
suggest that one free-floating E/SE may be expected for two smaller-mass
planets.
",0,1,0,0,0,0
17377,17378,Beamspace SU-MIMO for Future Millimeter Wave Wireless Communications,"  For future networks (i.e., the fifth generation (5G) wireless networks and
beyond), millimeter-wave (mmWave) communication with large available unlicensed
spectrum is a promising technology that enables gigabit multimedia
applications. Thanks to the short wavelength of mmWave radio, massive antenna
arrays can be packed into the limited dimensions of mmWave transceivers.
Therefore, with directional beamforming (BF), both mmWave transmitters (MTXs)
and mmWave receivers (MRXs) are capable of supporting multiple beams in 5G
networks. However, for the transmission between an MTX and an MRX, most works
have only considered a single beam, which means that they do not make full
potential use of mmWave. Furthermore, the connectivity of single beam
transmission can easily be blocked. In this context, we propose a single-user
multi-beam concurrent transmission scheme for future mmWave networks with
multiple reflected paths. Based on spatial spectrum reuse, the scheme can be
described as a multiple-input multiple-output (MIMO) technique in beamspace
(i.e., in the beam-number domain). Moreover, this study investigates the
challenges and potential solutions for implementing this scheme, including
multibeam selection, cooperative beam tracking, multi-beam power allocation and
synchronization. The theoretical and numerical results show that the proposed
beamspace SU-MIMO can largely improve the achievable rate of the transmission
between an MTX and an MRX and, meanwhile, can maintain the connectivity.
",1,0,0,0,0,0
3780,3781,Environmental impact assessment for climate change policy with the simulation-based integrated assessment model E3ME-FTT-GENIE,"  A high degree of consensus exists in the climate sciences over the role that
human interference with the atmosphere is playing in changing the climate.
Following the Paris Agreement, a similar consensus exists in the policy
community over the urgency of policy solutions to the climate problem. The
context for climate policy is thus moving from agenda setting, which has now
been mostly established, to impact assessment, in which we identify policy
pathways to implement the Paris Agreement. Most integrated assessment models
currently used to address the economic and technical feasibility of avoiding
climate change are based on engineering perspectives with a normative systems
optimisation philosophy, suitable for agenda setting, but unsuitable to assess
the socio-economic impacts of a realistic baskets of climate policies. Here, we
introduce a fully descriptive, simulation-based integrated assessment model
designed specifically to assess policies, formed by the combination of (1) a
highly disaggregated macro-econometric simulation of the global economy based
on time series regressions (E3ME), (2) a family of bottom-up evolutionary
simulations of technology diffusion based on cross-sectional discrete choice
models (FTT), and (3) a carbon cycle and atmosphere circulation model of
intermediate complexity (GENIE-1). We use this combined model to create a
detailed global and sectoral policy map and scenario that sets the economy on a
pathway that achieves the goals of the Paris Agreement with >66% probability of
not exceeding 2$^\circ$C of global warming. We propose a blueprint for a new
role for integrated assessment models in this upcoming policy assessment
context.
",0,1,0,0,0,0
14562,14563,Influence des mécanismes dissociés de ludifications sur l'apprentissage en support numérique de la lecture en classe primaire,"  The introduction of serious games as pedagogical supports in the field of
education is a process gaining in popularity amongst the teaching community.
This article creates a link between the integration of new pedagogical
solutions in first-year primary class and the fundamental research on the
motivation of the players/learners, detailing an experiment based on a game
specifically developed, named QCM. QCM considers the learning worksheets issued
from the Freinet pedagogy using various gameplay mechanisms. The main
contribution of QCM in relation to more traditional games is the dissociation
of immersion mechanisms, in order to improve the understanding of the user
experience. This game also contains a system of gameplay metrics, the analysis
of which shows a relative increase in the motivation of students using QCM
instead of paper worksheets, while revealing large differences in students
behavior in conjunction with the mechanisms of gamification employed. Keywords
: Serious games, learning analytics, gamification, flow.
",1,0,0,0,0,0
20149,20150,CSGNet: Neural Shape Parser for Constructive Solid Geometry,"  We present a neural architecture that takes as input a 2D or 3D shape and
outputs a program that generates the shape. The instructions in our program are
based on constructive solid geometry principles, i.e., a set of boolean
operations on shape primitives defined recursively. Bottom-up techniques for
this shape parsing task rely on primitive detection and are inherently slow
since the search space over possible primitive combinations is large. In
contrast, our model uses a recurrent neural network that parses the input shape
in a top-down manner, which is significantly faster and yields a compact and
easy-to-interpret sequence of modeling instructions. Our model is also more
effective as a shape detector compared to existing state-of-the-art detection
techniques. We finally demonstrate that our network can be trained on novel
datasets without ground-truth program annotations through policy gradient
techniques.
",1,0,0,0,0,0
3871,3872,Automatic Analysis of EEGs Using Big Data and Hybrid Deep Learning Architectures,"  Objective: A clinical decision support tool that automatically interprets
EEGs can reduce time to diagnosis and enhance real-time applications such as
ICU monitoring. Clinicians have indicated that a sensitivity of 95% with a
specificity below 5% was the minimum requirement for clinical acceptance. We
propose a highperformance classification system based on principles of big data
and machine learning. Methods: A hybrid machine learning system that uses
hidden Markov models (HMM) for sequential decoding and deep learning networks
for postprocessing is proposed. These algorithms were trained and evaluated
using the TUH EEG Corpus, which is the world's largest publicly available
database of clinical EEG data. Results: Our approach delivers a sensitivity
above 90% while maintaining a specificity below 5%. This system detects three
events of clinical interest: (1) spike and/or sharp waves, (2) periodic
lateralized epileptiform discharges, (3) generalized periodic epileptiform
discharges. It also detects three events used to model background noise: (1)
artifacts, (2) eye movement (3) background. Conclusions: A hybrid HMM/deep
learning system can deliver a low false alarm rate on EEG event detection,
making automated analysis a viable option for clinicians. Significance: The TUH
EEG Corpus enables application of highly data consumptive machine learning
algorithms to EEG analysis. Performance is approaching clinical acceptance for
real-time applications.
",1,0,0,1,0,0
1689,1690,De Rham and twisted cohomology of Oeljeklaus-Toma manifolds,"  Oeljeklaus-Toma (OT) manifolds are complex non-Kähler manifolds whose
construction arises from specific number fields. In this note, we compute their
de Rham cohomology in terms of invariants associated to the background number
field. This is done by two distinct approaches, one using invariant cohomology
and the other one using the Leray-Serre spectral sequence. In addition, we
compute also their Morse-Novikov cohomology. As an application, we show that
the low degree Chern classes of any complex vector bundle on an OT manifold
vanish in the real cohomology. Other applications concern the OT manifolds
admitting locally conformally Kähler (LCK) metrics: we show that there is
only one possible Lee class of an LCK metric, and we determine all the possible
Morse-Novikov classes of an LCK metric, which implies the nondegeneracy of
certain Lefschetz maps in cohomology.
",0,0,1,0,0,0
11080,11081,Neural Machine Translation and Sequence-to-sequence Models: A Tutorial,"  This tutorial introduces a new and powerful set of techniques variously
called ""neural machine translation"" or ""neural sequence-to-sequence models"".
These techniques have been used in a number of tasks regarding the handling of
human language, and can be a powerful tool in the toolbox of anyone who wants
to model sequential data of some sort. The tutorial assumes that the reader
knows the basics of math and programming, but does not assume any particular
experience with neural networks or natural language processing. It attempts to
explain the intuition behind the various methods covered, then delves into them
with enough mathematical detail to understand them concretely, and culiminates
with a suggestion for an implementation exercise, where readers can test that
they understood the content in practice.
",1,0,0,1,0,0
3742,3743,An Alternative Approach to Functional Linear Partial Quantile Regression,"  We have previously proposed the partial quantile regression (PQR) prediction
procedure for functional linear model by using partial quantile covariance
techniques and developed the simple partial quantile regression (SIMPQR)
algorithm to efficiently extract PQR basis for estimating functional
coefficients. However, although the PQR approach is considered as an attractive
alternative to projections onto the principal component basis, there are
certain limitations to uncovering the corresponding asymptotic properties
mainly because of its iterative nature and the non-differentiability of the
quantile loss function. In this article, we propose and implement an
alternative formulation of partial quantile regression (APQR) for functional
linear model by using block relaxation method and finite smoothing techniques.
The proposed reformulation leads to insightful results and motivates new
theory, demonstrating consistency and establishing convergence rates by
applying advanced techniques from empirical process theory. Two simulations and
two real data from ADHD-200 sample and ADNI are investigated to show the
superiority of our proposed methods.
",0,0,1,1,0,0
15521,15522,Baryogenesis at a Lepton-Number-Breaking Phase Transition,"  We study a scenario in which the baryon asymmetry of the universe arises from
a cosmological phase transition where lepton-number is spontaneously broken. If
the phase transition is first order, a lepton-number asymmetry can arise at the
bubble wall, through dynamics similar to electroweak baryogenesis, but
involving right-handed neutrinos. In addition to the usual neutrinoless double
beta decay in nuclear experiments, the model may be probed through a variety of
""baryogenesis by-products,"" which include a stochastic background of
gravitational waves created by the colliding bubbles. Depending on the model,
other aspects may include a network of topological defects that produce their
own gravitational waves, additional contribution to dark radiation, and a light
pseudo-Goldstone boson (majoron) as dark matter candidate.
",0,1,0,0,0,0
18166,18167,CORRECT: Code Reviewer Recommendation in GitHub Based on Cross-Project and Technology Experience,"  Peer code review locates common coding rule violations and simple logical
errors in the early phases of software development, and thus reduces overall
cost. However, in GitHub, identifying an appropriate code reviewer for a pull
request is a non-trivial task given that reliable information for reviewer
identification is often not readily available. In this paper, we propose a code
reviewer recommendation technique that considers not only the relevant
cross-project work history (e.g., external library experience) but also the
experience of a developer in certain specialized technologies associated with a
pull request for determining her expertise as a potential code reviewer. We
first motivate our technique using an exploratory study with 10 commercial
projects and 10 associated libraries external to those projects. Experiments
using 17,115 pull requests from 10 commercial projects and six open source
projects show that our technique provides 85%--92% recommendation accuracy,
about 86% precision and 79%--81% recall in code reviewer recommendation, which
are highly promising. Comparison with the state-of-the-art technique also
validates the empirical findings and the superiority of our recommendation
technique.
",1,0,0,0,0,0
20625,20626,Resistance distance criterion for optimal slack bus selection,"  We investigate the dependence of transmission losses on the choice of a slack
bus in high voltage AC transmission networks. We formulate a transmission loss
minimization problem in terms of slack variables representing the additional
power injection that each generator provides to compensate the transmission
losses. We show analytically that for transmission lines having small,
homogeneous resistance over reactance ratios ${r/x\ll1}$, transmission losses
are generically minimal in the case of a unique \textit{slack bus} instead of a
distributed slack bus. For the unique slack bus scenario, to lowest order in
${r/x}$, transmission losses depend linearly on a resistance distance based
indicator measuring the separation of the slack bus candidate from the rest of
the network. We confirm these results numerically for several IEEE and Pegase
testcases, and show that our predictions qualitatively hold also in the case of
lines having inhomogeneous ${r/x}$ ratios, with optimal slack bus choices
reducing transmission losses by ${10}\%$ typically.
",1,1,0,0,0,0
19397,19398,The OSIRIS-REx Visible and InfraRed Spectrometer (OVIRS): Spectral Maps of the Asteroid Bennu,"  The OSIRIS-REx Visible and Infrared Spectrometer (OVIRS) is a point
spectrometer covering the spectral range of 0.4 to 4.3 microns (25,000-2300
cm-1). Its primary purpose is to map the surface composition of the asteroid
Bennu, the target asteroid of the OSIRIS-REx asteroid sample return mission.
The information it returns will help guide the selection of the sample site. It
will also provide global context for the sample and high spatial resolution
spectra that can be related to spatially unresolved terrestrial observations of
asteroids. It is a compact, low-mass (17.8 kg), power efficient (8.8 W
average), and robust instrument with the sensitivity needed to detect a 5%
spectral absorption feature on a very dark surface (3% reflectance) in the
inner solar system (0.89-1.35 AU). It, in combination with the other
instruments on the OSIRIS-REx Mission, will provide an unprecedented view of an
asteroid's surface.
",0,1,0,0,0,0
3681,3682,ISS Property with Respect to Boundary Disturbances for a Class of Riesz-Spectral Boundary Control Systems,"  This paper deals with the establishment of Input-to-State Stability (ISS)
properties for infinite dimensional systems with respect to both boundary and
distributed disturbances. First, an ISS estimate is established with respect to
finite dimensional boundary disturbances for a class of Riesz-spectral boundary
control systems satisfying certain eigenvalue constraints. Second, a concept of
weak solutions is introduced in order to relax the disturbances regularity
assumptions required to ensure the existence of strong solutions. The proposed
concept of weak solutions, that applies to a large class of boundary control
systems which is not limited to the Riesz-spectral ones, provides a natural
extension of the concept of both strong and mild solutions. Assuming that an
ISS estimate holds true for strong solutions, we show the existence, the
uniqueness, and the ISS property of the weak solutions.
",1,0,0,0,0,0
9680,9681,A Deep Reinforcement Learning Chatbot,"  We present MILABOT: a deep reinforcement learning chatbot developed by the
Montreal Institute for Learning Algorithms (MILA) for the Amazon Alexa Prize
competition. MILABOT is capable of conversing with humans on popular small talk
topics through both speech and text. The system consists of an ensemble of
natural language generation and retrieval models, including template-based
models, bag-of-words models, sequence-to-sequence neural network and latent
variable neural network models. By applying reinforcement learning to
crowdsourced data and real-world user interactions, the system has been trained
to select an appropriate response from the models in its ensemble. The system
has been evaluated through A/B testing with real-world users, where it
performed significantly better than many competing systems. Due to its machine
learning architecture, the system is likely to improve with additional data.
",1,0,0,1,0,0
12271,12272,Neural SLAM: Learning to Explore with External Memory,"  We present an approach for agents to learn representations of a global map
from sensor data, to aid their exploration in new environments. To achieve
this, we embed procedures mimicking that of traditional Simultaneous
Localization and Mapping (SLAM) into the soft attention based addressing of
external memory architectures, in which the external memory acts as an internal
representation of the environment. This structure encourages the evolution of
SLAM-like behaviors inside a completely differentiable deep neural network. We
show that this approach can help reinforcement learning agents to successfully
explore new environments where long-term memory is essential. We validate our
approach in both challenging grid-world environments and preliminary Gazebo
experiments. A video of our experiments can be found at: this https URL.
",1,0,0,0,0,0
14294,14295,Experiments of posture estimation on vehicles using wearable acceleration sensors,"  In this paper, we study methods to estimate drivers' posture in vehicles
using acceleration data of wearable sensor and conduct a field test. Recently,
sensor technologies have been progressed. Solutions of safety management to
analyze vital data acquired from wearable sensor and judge work status are
proposed. To prevent huge accidents, demands for safety management of bus and
taxi are high. However, acceleration of vehicles is added to wearable sensor in
vehicles, and there is no guarantee to estimate drivers' posture accurately.
Therefore, in this paper, we study methods to estimate driving posture using
acceleration data acquired from T-shirt type wearable sensor hitoe, conduct
field tests and implement a sample application.
",1,0,0,0,0,0
20707,20708,GEANT4 Simulation of Nuclear Interaction Induced Soft Errors in Digital Nanoscale Electronics: Interrelation Between Proton and Heavy Ion Impacts,"  A simple and self-consistent approach has been proposed for simulation of the
proton-induced soft error rate based on the heavy ion induced single event
upset cross-section data and vice versa. The approach relies on the GEANT4
assisted Monte Carlo simulation of the secondary particle LET spectra produced
by nuclear interactions. The method has been validated with the relevant
in-flight soft error rate data for space protons and heavy ions. An approximate
analytical relation is proposed and validated for a fast recalculation between
the two types of experimental data.
",0,1,0,0,0,0
9537,9538,Towards Smart Proof Search for Isabelle,"  Despite the recent progress in automatic theorem provers, proof engineers are
still suffering from the lack of powerful proof automation. In this position
paper we first report our proof strategy language based on a meta-tool
approach. Then, we propose an AI-based approach to drastically improve proof
automation for Isabelle, while identifying three major challenges we plan to
address for this objective.
",1,0,0,0,0,0
15493,15494,Perfect Sequences and Arrays over the Unit Quaternions,"  We introduce several new constructions for perfect periodic autocorrelation
sequences and arrays over the unit quaternions. This paper uses both
mathematical proofs and com- puter experiments to prove the (bounded) array
constructions have perfect periodic auto- correlation. Furthermore, the first
sequence construction generates odd-perfect sequences of unbounded lengths,
with good ZCZ.
",1,0,1,0,0,0
14086,14087,An educational distributed Cosmic Ray detector network based on ArduSiPM,"  The advent of microcontrollers with enough CPU power and with analog and
digital peripherals makes possible to design a complete particle detector with
relative acquisition system around one microcontroller chip. The existence of a
world wide data infrastructure as internet allows for devising a distributed
network of cheap detectors capable to elaborate and send data or respond to
settings commands. The internet infrastructure enables to distribute the
absolute time (with precision of few milliseconds), to the simple devices far
apart, with few milliseconds precision, from a few meters to thousands of
kilometres. So it is possible to create a crowdsourcing experiment of citizen
science that use small scintillation-based particle detectors to monitor the
high energetic cosmic ray and the radiation environment.
",0,1,0,0,0,0
17151,17152,When intuition fails in assessing conditional risks: the example of the frog riddle,"  Recently, the educational initiative TED-Ed has published a popular brain
teaser coined the 'frog riddle', which illustrates non-intuitive implications
of conditional probabilities. In its intended form, the frog riddle is a
reformulation of the classic boy-girl paradox. However, the authors alter the
narrative of the riddle in a form, that subtly changes the way information is
conveyed. The presented solution, unfortunately, does not take this point into
full account, and as a consequence, lacks consistency in the sense that
different parts of the problem are treated on unequal footing. We here review,
how the mechanism of receiving information matters, and why this is exactly the
reason that such kind of problems challenge intuitive thinking. Subsequently,
we present a generalized solution, that accounts for the above difficulties,
and preserves full logical consistency. Eventually, the relation to the
boy-girl paradox is discussed.
",0,1,0,0,0,0
13309,13310,Large Margin Learning in Set to Set Similarity Comparison for Person Re-identification,"  Person re-identification (Re-ID) aims at matching images of the same person
across disjoint camera views, which is a challenging problem in multimedia
analysis, multimedia editing and content-based media retrieval communities. The
major challenge lies in how to preserve similarity of the same person across
video footages with large appearance variations, while discriminating different
individuals. To address this problem, conventional methods usually consider the
pairwise similarity between persons by only measuring the point to point (P2P)
distance. In this paper, we propose to use deep learning technique to model a
novel set to set (S2S) distance, in which the underline objective focuses on
preserving the compactness of intra-class samples for each camera view, while
maximizing the margin between the intra-class set and inter-class set. The S2S
distance metric is consisted of three terms, namely the class-identity term,
the relative distance term and the regularization term. The class-identity term
keeps the intra-class samples within each camera view gathering together, the
relative distance term maximizes the distance between the intra-class class set
and inter-class set across different camera views, and the regularization term
smoothness the parameters of deep convolutional neural network (CNN). As a
result, the final learned deep model can effectively find out the matched
target to the probe object among various candidates in the video gallery by
learning discriminative and stable feature representations. Using the CUHK01,
CUHK03, PRID2011 and Market1501 benchmark datasets, we extensively conducted
comparative evaluations to demonstrate the advantages of our method over the
state-of-the-art approaches.
",1,0,0,1,0,0
4034,4035,Approximate Ranking from Pairwise Comparisons,"  A common problem in machine learning is to rank a set of n items based on
pairwise comparisons. Here ranking refers to partitioning the items into sets
of pre-specified sizes according to their scores, which includes identification
of the top-k items as the most prominent special case. The score of a given
item is defined as the probability that it beats a randomly chosen other item.
Finding an exact ranking typically requires a prohibitively large number of
comparisons, but in practice, approximate rankings are often adequate.
Accordingly, we study the problem of finding approximate rankings from pairwise
comparisons. We analyze an active ranking algorithm that counts the number of
comparisons won, and decides whether to stop or which pair of items to compare
next, based on confidence intervals computed from the data collected in
previous steps. We show that this algorithm succeeds in recovering approximate
rankings using a number of comparisons that is close to optimal up to
logarithmic factors. We also present numerical results, showing that in
practice, approximation can drastically reduce the number of comparisons
required to estimate a ranking.
",0,0,0,1,0,0
6606,6607,Truncated Variational EM for Semi-Supervised Neural Simpletrons,"  Inference and learning for probabilistic generative networks is often very
challenging and typically prevents scalability to as large networks as used for
deep discriminative approaches. To obtain efficiently trainable, large-scale
and well performing generative networks for semi-supervised learning, we here
combine two recent developments: a neural network reformulation of hierarchical
Poisson mixtures (Neural Simpletrons), and a novel truncated variational EM
approach (TV-EM). TV-EM provides theoretical guarantees for learning in
generative networks, and its application to Neural Simpletrons results in
particularly compact, yet approximately optimal, modifications of learning
equations. If applied to standard benchmarks, we empirically find, that
learning converges in fewer EM iterations, that the complexity per EM iteration
is reduced, and that final likelihood values are higher on average. For the
task of classification on data sets with few labels, learning improvements
result in consistently lower error rates if compared to applications without
truncation. Experiments on the MNIST data set herein allow for comparison to
standard and state-of-the-art models in the semi-supervised setting. Further
experiments on the NIST SD19 data set show the scalability of the approach when
a manifold of additional unlabeled data is available.
",0,0,0,1,0,0
10670,10671,Security for 4G and 5G Cellular Networks: A Survey of Existing Authentication and Privacy-preserving Schemes,"  This paper presents a comprehensive survey of existing authentication and
privacy-preserving schemes for 4G and 5G cellular networks. We start by
providing an overview of existing surveys that deal with 4G and 5G
communications, applications, standardization, and security. Then, we give a
classification of threat models in 4G and 5G cellular networks in four
categories, including, attacks against privacy, attacks against integrity,
attacks against availability, and attacks against authentication. We also
provide a classification of countermeasures into three types of categories,
including, cryptography methods, humans factors, and intrusion detection
methods. The countermeasures and informal and formal security analysis
techniques used by the authentication and privacy preserving schemes are
summarized in form of tables. Based on the categorization of the authentication
and privacy models, we classify these schemes in seven types, including,
handover authentication with privacy, mutual authentication with privacy, RFID
authentication with privacy, deniable authentication with privacy,
authentication with mutual anonymity, authentication and key agreement with
privacy, and three-factor authentication with privacy. In addition, we provide
a taxonomy and comparison of authentication and privacy-preserving schemes for
4G and 5G cellular networks in form of tables. Based on the current survey,
several recommendations for further research are discussed at the end of this
paper.
",1,0,0,0,0,0
6160,6161,Khovanov-Rozansky homology and higher Catalan sequences,"  We give a simple recursion which computes the triply graded Khovanov-Rozansky
homology of several infinite families of knots and links, including the
$(n,nm\pm 1)$ and $(n,nm)$ torus links for $n,m\geq 1$. We interpret our
results in terms of Catalan combinatorics, proving a conjecture of Gorsky's.
Our computations agree with predictions coming from Hilbert schemes and
rational DAHA, which also proves the Gorsky-Oblomkov-Rasmussen-Shende
conjectures in these cases. Additionally, our results suggest a topological
interpretation of the symmetric functions which appear in the context of the
$m$-shuffle conjecture of Haglund-Haiman-Loehr-Remmel-Ulyanov.
",0,0,1,0,0,0
1534,1535,Driving Interactive Graph Exploration Using 0-Dimensional Persistent Homology Features,"  Graphs are commonly used to encode relationships among entities, yet, their
abstractness makes them incredibly difficult to analyze. Node-link diagrams are
a popular method for drawing graphs. Classical techniques for the node-link
diagrams include various layout methods that rely on derived information to
position points, which often lack interactive exploration functionalities; and
force-directed layouts, which ignore global structures of the graph. This paper
addresses the graph drawing challenge by leveraging topological features of a
graph as derived information for interactive graph drawing. We first discuss
extracting topological features from a graph using persistent homology. We then
introduce an interactive persistence barcodes to study the substructures of a
force-directed graph layout; in particular, we add contracting and repulsing
forces guided by the 0-dimensional persistent homology features. Finally, we
demonstrate the utility of our approach across three datasets.
",1,0,0,0,0,0
14736,14737,Statistical analysis of the first passage path ensemble of jump processes,"  The transition mechanism of jump processes between two different subsets in
state space reveals important dynamical information of the processes and
therefore has attracted considerable attention in the past years. In this
paper, we study the first passage path ensemble of both discrete-time and
continuous-time jump processes on a finite state space. The main approach is to
divide each first passage path into nonreactive and reactive segments and to
study them separately. The analysis can be applied to jump processes which are
non-ergodic, as well as continuous-time jump processes where the waiting time
distributions are non-exponential. In the particular case that the jump
processes are both Markovian and ergodic, our analysis elucidates the relations
between the study of the first passage paths and the study of the transition
paths in transition path theory. We provide algorithms to numerically compute
statistics of the first passage path ensemble. The computational complexity of
these algorithms scales with the complexity of solving a linear system, for
which efficient methods are available. Several examples demonstrate the wide
applicability of the derived results across research areas.
",0,0,1,0,0,0
4574,4575,Bäcklund Transformations for the Boussinesq Equation and Merging Solitons,"  The Bäcklund transformation (BT) for the ""good"" Boussinesq equation and its
superposition principles are presented and applied. Unlike many other standard
integrable equations, the Boussinesq equation does not have a strictly
algebraic superposition principle for 2 BTs, but it does for 3. We present
associated lattice systems. Applying the BT to the trivial solution generates
standard solitons but also what we call ""merging solitons"" --- solutions in
which two solitary waves (with related speeds) merge into a single one. We use
the superposition principles to generate a variety of interesting solutions,
including superpositions of a merging soliton with $1$ or $2$ regular solitons,
and solutions that develop a singularity in finite time which then disappears
at some later finite time. We prove a Wronskian formula for the solutions
obtained by applying a general sequence of BTs on the trivial solution.
Finally, we show how to obtain the standard conserved quantities of the
Boussinesq equation from the BT, and how the hierarchy of local symmetries
follows in a simple manner from the superposition principle for 3 BTs.
",0,1,1,0,0,0
18030,18031,A BERT Baseline for the Natural Questions,"  This technical note describes a new baseline for the Natural Questions. Our
model is based on BERT and reduces the gap between the model F1 scores reported
in the original dataset paper and the human upper bound by 30% and 50% relative
for the long and short answer tasks respectively. This baseline has been
submitted to the official NQ leaderboard at
ai.google.com/research/NaturalQuestions and we plan to opensource the code for
it in the near future.
",1,0,0,0,0,0
964,965,$q$-deformed quadrature operator and optical tomogram,"  In this letter, we define the homodyne $q$-deformed quadrature operator.
Analytic expression for the wavefunctions of $q$-deformed oscillator in the
quadrature basis are found. Furthermore, we compute the explicit analytical
expression for the tomogram of the $q$-deformed coherent states by finding the
eigenstates of the $q$-deformed quadrature operator.
",0,1,1,0,0,0
10596,10597,Continuous Relaxations for the Traveling Salesman Problem,"  In this work, we aim to explore connections between dynamical systems
techniques and combinatorial optimization problems. In particular, we construct
heuristic approaches for the traveling salesman problem (TSP) based on
embedding the relaxed discrete optimization problem into appropriate manifolds.
We explore multiple embedding techniques -- namely, the construction of new
dynamical systems on the manifold of orthogonal matrices and associated
Procrustes approximations of the TSP cost function. Using these dynamical
systems, we analyze the local neighborhood around the optimal TSP solutions
(which are equilibria) using computations to approximate the associated
\emph{stable manifolds}. We find that these flows frequently converge to
undesirable equilibria. However, the solutions of the dynamical systems and the
associated Procrustes approximation provide an interesting biasing approach for
the popular Lin--Kernighan heuristic which yields fast convergence. The
Lin--Kernighan heuristic is typically based on the computation of edges that
have a `high probability' of being in the shortest tour, thereby effectively
pruning the search space. Our new approach, instead, relies on a natural
relaxation of the combinatorial optimization problem to the manifold of
orthogonal matrices and the subsequent use of this solution to bias the
Lin--Kernighan heuristic. Although the initial cost of computing these edges
using the Procrustes solution is higher than existing methods, we find that the
Procrustes solution, when coupled with a homotopy computation, contains
valuable information regarding the optimal edges. We explore the Procrustes
based approach on several TSP instances and find that our approach often
requires fewer $k$-opt moves than existing approaches. Broadly, we hope that
this work initiates more work in the intersection of dynamical systems theory
and combinatorial optimization.
",1,0,1,0,0,0
10859,10860,On Tackling the Limits of Resolution in SAT Solving,"  The practical success of Boolean Satisfiability (SAT) solvers stems from the
CDCL (Conflict-Driven Clause Learning) approach to SAT solving. However, from a
propositional proof complexity perspective, CDCL is no more powerful than the
resolution proof system, for which many hard examples exist. This paper
proposes a new problem transformation, which enables reducing the decision
problem for formulas in conjunctive normal form (CNF) to the problem of solving
maximum satisfiability over Horn formulas. Given the new transformation, the
paper proves a polynomial bound on the number of MaxSAT resolution steps for
pigeonhole formulas. This result is in clear contrast with earlier results on
the length of proofs of MaxSAT resolution for pigeonhole formulas. The paper
also establishes the same polynomial bound in the case of modern core-guided
MaxSAT solvers. Experimental results, obtained on CNF formulas known to be hard
for CDCL SAT solvers, show that these can be efficiently solved with modern
MaxSAT solvers.
",1,0,0,0,0,0
8833,8834,Zero-Delay Source-Channel Coding with a One-Bit ADC Front End and Correlated Side Information at the Receiver,"  Zero-delay transmission of a Gaussian source over an additive white Gaussian
noise (AWGN) channel is considered with a one-bit analog-to-digital converter
(ADC) front end and a correlated side information at the receiver. The design
of the optimal encoder and decoder is studied for two performance criteria,
namely, the mean squared error (MSE) distortion and the distortion outage
probability (DOP), under an average power constraint on the channel input. For
both criteria, necessary optimality conditions for the encoder and the decoder
are derived. Using these conditions, it is observed that the numerically
optimized encoder (NOE) under the MSE distortion criterion is periodic, and its
period increases with the correlation between the source and the receiver side
information. For the DOP, it is instead seen that the NOE mappings periodically
acquire positive and negative values, which decay to zero with increasing
source magnitude, and the interval over which the mapping takes non-zero
values, becomes wider with the correlation between the source and the side
information.
",1,0,0,0,0,0
20514,20515,Solar wind turbulent cascade from MHD to sub-ion scales: large-size 3D hybrid particle-in-cell simulations,"  Spectral properties of the turbulent cascade from fluid to kinetic scales in
collisionless plasmas are investigated by means of large-size three-dimensional
(3D) hybrid (fluid electrons, kinetic protons) particle-in-cell simulations.
Initially isotropic Alfvènic fluctuations rapidly develop a strongly
anisotropic turbulent cascade, mainly in the direction perpendicular to the
ambient magnetic field. The omnidirectional magnetic field spectrum shows a
double power-law behavior over almost two decades in wavenumber, with a
Kolmogorov-like index at large scales, a spectral break around ion scales, and
a steepening at sub-ion scales. Power laws are also observed in the spectra of
the ion bulk velocity, density, and electric field, both at magnetohydrodynamic
(MHD) and at kinetic scales. Despite the complex structure, the omnidirectional
spectra of all fields at ion and sub-ion scales are in remarkable quantitative
agreement with those of a two-dimensional (2D) simulation with similar physical
parameters. This provides a partial, a-posteriori validation of the 2D
approximation at kinetic scales. Conversely, at MHD scales, the spectra of the
density and of the velocity (and, consequently, of the electric field) exhibit
differences between the 2D and 3D cases. Although they can be partly ascribed
to the lower spatial resolution, the main reason is likely the larger
importance of compressible effects in a full geometry. Our findings are also in
remarkable quantitative agreement with solar wind observations.
",0,1,0,0,0,0
2240,2241,A new cosine series antialiasing function and its application to aliasing-free glottal source models for speech and singing synthesis,"  We formulated and implemented a procedure to generate aliasing-free
excitation source signals. It uses a new antialiasing filter in the continuous
time domain followed by an IIR digital filter for response equalization. We
introduced a cosine-series-based general design procedure for the new
antialiasing function. We applied this new procedure to implement the
antialiased Fujisaki-Ljungqvist model. We also applied it to revise our
previous implementation of the antialiased Fant-Liljencrants model. A
combination of these signals and a lattice implementation of the time varying
vocal tract model provides a reliable and flexible basis to test fo extractors
and source aperiodicity analysis methods. MATLAB implementations of these
antialiased excitation source models are available as part of our open source
tools for speech science.
",1,0,0,0,0,0
14046,14047,A Dictionary Approach to Identifying Transient RFI,"  As radio telescopes become more sensitive, the damaging effects of radio
frequency interference (RFI) become more apparent. Near radio telescope arrays,
RFI sources are often easily removed or replaced; the challenge lies in
identifying them. Transient (impulsive) RFI is particularly difficult to
identify. We propose a novel dictionary-based approach to transient RFI
identification. RFI events are treated as sequences of sub-events, drawn from
particular labelled classes. We demonstrate an automated method of extracting
and labelling sub-events using a dataset of transient RFI. A dictionary of
labels may be used in conjunction with hidden Markov models to identify the
sources of RFI events reliably. We attain improved classification accuracy over
traditional approaches such as SVMs or a naïve kNN classifier. Finally, we
investigate why transient RFI is difficult to classify. We show that cluster
separation in the principal components domain is influenced by the mains supply
phase for certain sources.
",0,1,0,0,0,0
7465,7466,Unified Gas-kinetic Scheme with Multigrid Convergence for Rarefied Flow Study,"  The unified gas kinetic scheme (UGKS) is a direct modeling method based on
the gas dynamical model on the mesh size and time step scales. With the
implementation of particle transport and collision in a time-dependent flux
function, the UGKS can recover multiple flow physics from the kinetic particle
transport to the hydrodynamic wave propagation. In comparison with direct
simulation Monte Carlo (DSMC), the equations-based UGKS can use the implicit
techniques in the updates of macroscopic conservative variables and microscopic
distribution function. The implicit UGKS significantly increases the
convergence speed for steady flow computations, especially in the highly
rarefied and near continuum regime. In order to further improve the
computational efficiency, for the first time a geometric multigrid technique is
introduced into the implicit UGKS, where the prediction step for the
equilibrium state and the evolution step for the distribution function are both
treated with multigrid acceleration. The multigrid implicit UGKS (MIUGKS) is
used in the non-equilibrium flow study, which includes microflow, such as
lid-driven cavity flow and the flow passing through a finite-length flat plate,
and high speed one, such as supersonic flow over a square cylinder. The MIUGKS
shows 5 to 9 times efficiency increase over the previous implicit scheme. For
the low speed microflow, the efficiency of MIUGKS is several orders of
magnitude higher than the DSMC. Even for the hypersonic flow at Mach number 5
and Knudsen number 0.1, the MIUGKS is still more than 100 times faster than the
DSMC method for a convergent steady state solution.
",0,1,0,0,0,0
13859,13860,Stock Market Visualization,"  We provide complete source code for a front-end GUI and its back-end
counterpart for a stock market visualization tool. It is built based on the
""functional visualization"" concept we discuss, whereby functionality is not
sacrificed for fancy graphics. The GUI, among other things, displays a
color-coded signal (computed by the back-end code) based on how ""out-of-whack""
each stock is trading compared with its peers (""mean-reversion""), and the most
sizable changes in the signal (""momentum""). The GUI also allows to efficiently
filter/tier stocks by various parameters (e.g., sector, exchange, signal,
liquidity, market cap) and functionally display them. The tool can be run as a
web-based or local application.
",0,0,0,0,0,1
7106,7107,Non-Hamiltonian isotopic Lagrangians on the one-point blow-up of CP^2,"  We show that two Hamiltonian isotopic Lagrangians in
(CP^2,\omega_\textup{FS}) induce two Lagrangian submanifolds in the one-point
blow-up (\widetilde{CP}^2,\widetilde{\omega}_\rho) that are not Hamiltonian
isotopic. Furthermore, we show that for any integer k>1 there are k Hamiltonian
isotopic Lagrangians in (CP^2,\omega_\textup{FS}) that induce k Lagrangian
submanifolds in the one-point blow-up such that no two of them are Hamiltonian
isotopic.
",0,0,1,0,0,0
6444,6445,Weight Spectrum of Quasi-Perfect Binary Codes with Distance 4,"  We consider the weight spectrum of a class of quasi-perfect binary linear
codes with code distance 4. For example, extended Hamming code and Panchenko
code are the known members of this class. Also, it is known that in many cases
Panchenko code has the minimal number of weight 4 codewords. We give exact
recursive formulas for the weight spectrum of quasi-perfect codes and their
dual codes. As an example of application of the weight spectrum we derive a
lower estimate for the conditional probability of correction of erasure
patterns of high weights (equal to or greater than code distance).
",1,0,0,0,0,0
9818,9819,Universal geometric constraints during epithelial jamming,"  As an injury heals, an embryo develops, or a carcinoma spreads, epithelial
cells systematically change their shape. In each of these processes cell shape
is studied extensively, whereas variation of shape from cell-to-cell is
dismissed most often as biological noise. But where do cell shape and variation
of cell shape come from? Here we report that cell shape and shape variation are
mutually constrained through a relationship that is purely geometrical. That
relationship is shown to govern maturation of the pseudostratified bronchial
epithelial layer cultured from both non-asthmatic and asthmatic donors as well
as formation of the ventral furrow in the epithelial monolayer of the
Drosophila embryo in vivo. Across these and other vastly different epithelial
systems, cell shape variation collapses to a family of distributions that is
common to all and potentially universal. That distribution, in turn, is
accounted for quantitatively by a mechanistic theory of cell-cell interaction
showing that cell shape becomes progressively less elongated and less variable
as the layer becomes progressively more jammed. These findings thus uncover a
connection between jamming and geometry that is generic -spanning jammed living
and inert systems alike- and demonstrate that proximity of the cell layer to
the jammed state is the principal determinant of the most primitive features of
epithelial cell shape and shape variation.
",0,1,0,0,0,0
20851,20852,Introducing Geometric Algebra to Geometric Computing Software Developers: A Computational Thinking Approach,"  Designing software systems for Geometric Computing applications can be a
challenging task. Software engineers typically use software abstractions to
hide and manage the high complexity of such systems. Without the presence of a
unifying algebraic system to describe geometric models, the use of software
abstractions alone can result in many design and maintenance problems.
Geometric Algebra (GA) can be a universal abstract algebraic language for
software engineering geometric computing applications. Few sources, however,
provide enough information about GA-based software implementations targeting
the software engineering community. In particular, successfully introducing GA
to software engineers requires quite different approaches from introducing GA
to mathematicians or physicists. This article provides a high-level
introduction to the abstract concepts and algebraic representations behind the
elegant GA mathematical structure. The article focuses on the conceptual and
representational abstraction levels behind GA mathematics with sufficient
references for more details. In addition, the article strongly recommends
applying the methods of Computational Thinking in both introducing GA to
software engineers, and in using GA as a mathematical language for developing
Geometric Computing software systems.
",1,0,0,0,0,0
7456,7457,Persistent Entropy for Separating Topological Features from Noise in Vietoris-Rips Complexes,"  Persistent homology studies the evolution of k-dimensional holes along a
nested sequence of simplicial complexes (called a filtration). The set of bars
(i.e. intervals) representing birth and death times of k-dimensional holes
along such sequence is called the persistence barcode. k-Dimensional holes with
short lifetimes are informally considered to be ""topological noise"", and those
with long lifetimes are considered to be ""topological features"" associated to
the filtration. Persistent entropy is defined as the Shannon entropy of the
persistence barcode of a given filtration. In this paper we present new
important properties of persistent entropy of Cech and Vietoris-Rips
filtrations. Among the properties, we put a focus on the stability theorem that
allows to use persistent entropy for comparing persistence barcodes. Later, we
derive a simple method for separating topological noise from features in
Vietoris-Rips filtrations.
",1,0,0,0,0,0
275,276,On algebraically integrable domains in Euclidean spaces,"  Let $D$ be a bounded domain $D$ in $\mathbb R^n $ with infinitely smooth
boundary and $n$ is odd. We prove that if the volume cut off from the domain by
a hyperplane is an algebraic function of the hyperplane, free of real singular
points, then the domain is an ellipsoid. This partially answers a question of
V.I. Arnold: whether odd-dimensional ellipsoids are the only algebraically
integrable domains?
",0,0,1,0,0,0
19946,19947,The cohomology ring of some Hopf algebras,"  Let p be a prime, and k be a field of characteristic p. We investigate the
algebra structure and the structure of the cohomology ring for the connected
Hopf algebras of dimension p^3, which appear in the classification obtained in
[V.C. Nguyen, L.-H. Wang and X.-T. Wang, Classification of connected Hopf
algebras of dimension p^3, J. Algebra 424 (2015), 473-505]. The list consists
of 23 algebras together with two infinite families. We identify the Morita type
of the algebra, and in almost all cases this is sufficient to clarify the
structure of the cohomology ring.
",0,0,1,0,0,0
13597,13598,Polynomial configurations in sets of positive upper density over local fields,"  Let $F(x)=(f_1(x), \dots, f_m(x))$ be such that $1, f_1, \dots, f_m$ are
linearly independent polynomials with real coefficients. Based on ideas of
Bachoc, DeCorte, Oliveira and Vallentin in combination with estimating certain
oscillatory integrals with polynomial phase we will show that the independence
ratio of the Cayley graph of $\mathbb{R}^m$ with respect to the portion of the
graph of $F$ defined by $a\leq \log |s| \leq T$ is at most $O(1/(T-a))$. We
conclude that if $I \subseteq \mathbb{R}^m$ has positive upper density, then
the difference set $I-I$ contains vectors of the form $F(s)$ for an unbounded
set of values $s \in \mathbb{R}$. It follows that the Borel chromatic number of
the Cayley graph of $\mathbb{R}^m$ with respect to the set $\{ \pm F(s): s \in
\mathbb{R} \}$ is infinite. Analogous results are also proven when $\mathbb{R}$
is replaced by the field of $p$-adic numbers $\mathbb{Q}_p$. At the end, we
will also the existence of real analytic functions $f_1, \dots, f_m$, for which
the analogous statements no longer hold.
",0,0,1,0,0,0
15092,15093,An Affective Robot Companion for Assisting the Elderly in a Cognitive Game Scenario,"  Being able to recognize emotions in human users is considered a highly
desirable trait in Human-Robot Interaction (HRI) scenarios. However, most
contemporary approaches rarely attempt to apply recognized emotional features
in an active manner to modulate robot decision-making and dialogue for the
benefit of the user. In this position paper, we propose a method of
incorporating recognized emotions into a Reinforcement Learning (RL) based
dialogue management module that adapts its dialogue responses in order to
attempt to make cognitive training tasks, like the 2048 Puzzle Game, more
enjoyable for the users.
",1,0,0,0,0,0
3758,3759,Director Field Analysis (DFA): Exploring Local White Matter Geometric Structure in diffusion MRI,"  In Diffusion Tensor Imaging (DTI) or High Angular Resolution Diffusion
Imaging (HARDI), a tensor field or a spherical function field (e.g., an
orientation distribution function field), can be estimated from measured
diffusion weighted images. In this paper, inspired by the microscopic
theoretical treatment of phases in liquid crystals, we introduce a novel
mathematical framework, called Director Field Analysis (DFA), to study local
geometric structural information of white matter based on the reconstructed
tensor field or spherical function field: 1) We propose a set of mathematical
tools to process general director data, which consists of dyadic tensors that
have orientations but no direction. 2) We propose Orientational Order (OO) and
Orientational Dispersion (OD) indices to describe the degree of alignment and
dispersion of a spherical function in a single voxel or in a region,
respectively; 3) We also show how to construct a local orthogonal coordinate
frame in each voxel exhibiting anisotropic diffusion; 4) Finally, we define
three indices to describe three types of orientational distortion (splay, bend,
and twist) in a local spatial neighborhood, and a total distortion index to
describe distortions of all three types. To our knowledge, this is the first
work to quantitatively describe orientational distortion (splay, bend, and
twist) in general spherical function fields from DTI or HARDI data. The
proposed DFA and its related mathematical tools can be used to process not only
diffusion MRI data but also general director field data, and the proposed
scalar indices are useful for detecting local geometric changes of white matter
for voxel-based or tract-based analysis in both DTI and HARDI acquisitions. The
related codes and a tutorial for DFA will be released in DMRITool.
",1,1,0,0,0,0
10368,10369,Experimental observation of self excited co--rotating multiple vortices in a dusty plasma with inhomogeneous plasma background,"  We report an experimental observation of multiple co--rotating vortices in a
extended dust column in the background of non--uniform diffused plasma.
Inductively coupled RF discharge is initiated in the background of argon gas in
the source region which later found to diffuse in the main experimental
chamber. A secondary DC glow discharge plasma is produced to introduce the dust
particles into the plasma. These micron sized poly-disperse dust particles get
charged in the plasma environment and transported by the ambipolar electric
field of the diffused plasma and found to confine in the potential well, where
the resultant electric field of the diffused plasma (ambipolar E--field) and
glass wall charging (sheath E--field) hold the micron sized particles against
the gravity. Multiple co--rotating (anti--clockwise) dust vortices are observed
in the dust cloud for a particular discharge condition. The transition from
multiple to single dust vortex is observed when input RF power is lowered.
Occurrence of these vortices are explained on the basis of the charge gradient
of dust particles which is orthogonal to the ion drag force. The charge
gradient is a consequence of the plasma inhomogeneity along the dust cloud
length. The detailed nature and the reason for multiple vortices are still
under investigation through further experiments, however, preliminary
qualitative understanding is discussed based on characteristic scale length of
dust vortex. There is a characteristic size of the vortex in the dusty plasma
so that multiple vortices is possible to form in the extended dusty plasma with
inhomogeneous plasma background. The experimental results on the vortex motion
of particles are compared with a theoretical model and found some agreement.
",0,1,0,0,0,0
10608,10609,ALMA reveals starburst-like interstellar medium conditions in a compact star-forming galaxy at z ~ 2 using [CI] and CO,"  We present ALMA detections of the [CI] 1-0, CO J=3-2, and CO J=4-3 emission
lines, as well as the ALMA band 4 continuum for a compact star-forming galaxy
(cSFG) at z=2.225, 3D-HST GS30274. As is typical for cSFGs, this galaxy has a
stellar mass of $1.89 \pm 0.47\,\times 10^{11}\,\rm{M}_\odot$, with a star
formation rate of $214\pm44\,\rm{M}_\odot\,\rm{yr}^{-1}$ putting it on the
star-forming `main-sequence', but with an H-band effective radius of 2.5 kpc,
making it much smaller than the bulk of `main-sequence' star-forming galaxies.
The intensity ratio of the line detections yield an ISM density (~ 6 $\times
10^{4}\,\rm{cm}^{-3}$) and a UV-radiation field ( ~2 $\times 10^4\,\rm{G}_0$),
similar to the values in local starburst and ultra-luminous infrared galaxy
environments. A starburst phase is consistent with the short depletion times
($t_{\rm H2, dep} \leq 140$ Myr) we find using three different proxies for the
H2 mass ([CI], CO, dust mass). This depletion time is significantly shorter
than in more extended SFGs with similar stellar masses and SFRs. Moreover, the
gas fraction of 3D-HST GS30274 is smaller than typically found in extended
galaxies. We measure the CO and [CI] kinematics and find a FWHM line width of
~$750 \pm 41 $ km s$^{-1}$. The CO and [CI] FWHM are consistent with a
previously measured H$\alpha$ FWHM for this source. The line widths are
consistent with gravitational motions, suggesting we are seeing a compact
molecular gas reservoir. A previous merger event, as suggested by the
asymmetric light profile, may be responsible for the compact distribution of
gas and has triggered a central starburst event. This event gives rise to the
starburst-like ISM properties and short depletion times. The centrally located
and efficient star formation is quickly building up a dense core of stars,
responsible for the compact distribution of stellar light in 3D-HST GS30274.
",0,1,0,0,0,0
11282,11283,Reduction and specialization of hyperelliptic continued fractions,"  For a monic polynomial $D(X)$ of even degree, express $\sqrt D$ as a Laurent
series in $X^{-1}$; this yields a continued fraction expansion (similar to
continued fractions of real numbers): \[\sqrt
D=a_0+\dfrac{1}{a_1+\dfrac{1}{a_2+\dfrac{1}{\ddots}}},\quad a_i\text{
polynomials in }X.\] Such continued fractions were first considered by Abel in
1826, and later by Chebyshev. It turns out they are rarely periodic unless $D$
is defined over a finite field.
Around 2001 van der Poorten studied non-periodic continued fractions of
$\sqrt D$, with $D$ defined over the rationals, and simultaneously the
continued fraction of $\sqrt D$ modulo a suitable prime $p$; the latter
continued fraction is automatically periodic. He found that one recovers all
the convergents (rational function approximations to $\sqrt D$ obtained by
cutting off the continued fraction) of $\sqrt D \mod{p}$ by appropriately
normalising and then reducing the convergents of $\sqrt D$.
By developing a general specialization theory for continued fractions of
Laurent series, I produced a rigorous proof of this result stated by van der
Poorten and further was able to show the following:
If $D$ is defined over the rationals and the continued fraction of $\sqrt D$
is non-periodic, then for all but finitely many primes $p \in \mathbb Z$, this
prime $p$ occurs in the denominator of the leading coefficient of infinitely
many $a_i$.
For $\mathrm{deg}\,D = 4$, I can even give a description of the orders in
which the prime appears, and the $p$-adic Gauss norms of the $a_i$ and the
convergents. These results also generalise to number fields.
Moreover, I derive optimised formulae for computing quadratic continued
fractions, along with several example expansions. I discuss a few known results
on the heights of the convergents, and explain some relations with the
reduction of hyperelliptic curves and Jacobians.
",0,0,1,0,0,0
14892,14893,$H$-compactness of elliptic operators on weighted Riemannian Manifolds,"  In this paper we study the asymptotic behavior of second-order uniformly
elliptic operators on weighted Riemannian manifolds. We appeal to the notion of
\mbox{$H$-convergence} introduced by Murat and Tartar. In our main result we
establish an \mbox{$H$-compactness} result that applies to elliptic operators
with measurable, uniformly elliptic coefficients on weighted Riemannian
manifolds. We further discuss the special case of ""locally periodic""
coefficients and study the asymptotic behavior of the Laplace-Beltrami operator
on families of weighted manifolds obtained from a reference manifold by a
conformal (rapidly oscillating) change of metrics.
",0,0,1,0,0,0
13480,13481,The altmetric performance of publications authored by Brazilian researchers: analysis of CNPq productivity scholarship holders,"  The present work seeks to analyse the altmetric performance of Brazilian
publications authored by researchers who are productivity scholarship holders
(PQ) of the National Council of Scientific and Technological Development
(CNPq). It was considered, within the scope of this research, the PQs in
activity in October, 2017 (n = 14.609). The scientific production registered on
Lattes was collected via GetLattesData and filtered by articles from academic
journals published between 2016 and October 2017 that hold the Digital Object
Identifier (n = 99064). The online attention data are analysed according to
their distribution by density and variation; language of the publication and
field of knowledge; and by average performance of the type of source that has
provided its altmetric values. The density evidences the long tail behavior of
the variable, with most part of the articles with altmetrics score = 0, while
few articles have a high index. The average of the online attention indicates a
better performance of articles written in English and belonging to the Health
and Biological Sciences field of knowledge. As for the sources, there was a
good performance from Mendeley, followed by Twitter and a low coverage from
Facebook
",1,0,0,0,0,0
14965,14966,Training Group Orthogonal Neural Networks with Privileged Information,"  Learning rich and diverse representations is critical for the performance of
deep convolutional neural networks (CNNs). In this paper, we consider how to
use privileged information to promote inherent diversity of a single CNN model
such that the model can learn better representations and offer stronger
generalization ability. To this end, we propose a novel group orthogonal
convolutional neural network (GoCNN) that learns untangled representations
within each layer by exploiting provided privileged information and enhances
representation diversity effectively. We take image classification as an
example where image segmentation annotations are used as privileged information
during the training process. Experiments on two benchmark datasets -- ImageNet
and PASCAL VOC -- clearly demonstrate the strong generalization ability of our
proposed GoCNN model. On the ImageNet dataset, GoCNN improves the performance
of state-of-the-art ResNet-152 model by absolute value of 1.2% while only uses
privileged information of 10% of the training images, confirming effectiveness
of GoCNN on utilizing available privileged knowledge to train better CNNs.
",1,0,0,0,0,0
17207,17208,An Adaptive Strategy for Active Learning with Smooth Decision Boundary,"  We present the first adaptive strategy for active learning in the setting of
classification with smooth decision boundary. The problem of adaptivity (to
unknown distributional parameters) has remained opened since the seminal work
of Castro and Nowak (2007), which first established (active learning) rates for
this setting. While some recent advances on this problem establish adaptive
rates in the case of univariate data, adaptivity in the more practical setting
of multivariate data has so far remained elusive. Combining insights from
various recent works, we show that, for the multivariate case, a careful
reduction to univariate-adaptive strategies yield near-optimal rates without
prior knowledge of distributional parameters.
",1,0,0,1,0,0
8026,8027,Scalable k-Means Clustering via Lightweight Coresets,"  Coresets are compact representations of data sets such that models trained on
a coreset are provably competitive with models trained on the full data set. As
such, they have been successfully used to scale up clustering models to massive
data sets. While existing approaches generally only allow for multiplicative
approximation errors, we propose a novel notion of lightweight coresets that
allows for both multiplicative and additive errors. We provide a single
algorithm to construct lightweight coresets for k-means clustering as well as
soft and hard Bregman clustering. The algorithm is substantially faster than
existing constructions, embarrassingly parallel, and the resulting coresets are
smaller. We further show that the proposed approach naturally generalizes to
statistical k-means clustering and that, compared to existing results, it can
be used to compute smaller summaries for empirical risk minimization. In
extensive experiments, we demonstrate that the proposed algorithm outperforms
existing data summarization strategies in practice.
",1,0,0,1,0,0
16358,16359,Compact Multi-Class Boosted Trees,"  Gradient boosted decision trees are a popular machine learning technique, in
part because of their ability to give good accuracy with small models. We
describe two extensions to the standard tree boosting algorithm designed to
increase this advantage. The first improvement extends the boosting formalism
from scalar-valued trees to vector-valued trees. This allows individual trees
to be used as multiclass classifiers, rather than requiring one tree per class,
and drastically reduces the model size required for multiclass problems. We
also show that some other popular vector-valued gradient boosted trees
modifications fit into this formulation and can be easily obtained in our
implementation. The second extension, layer-by-layer boosting, takes smaller
steps in function space, which is empirically shown to lead to a faster
convergence and to a more compact ensemble. We have added both improvements to
the open-source TensorFlow Boosted trees (TFBT) package, and we demonstrate
their efficacy on a variety of multiclass datasets. We expect these extensions
will be of particular interest to boosted tree applications that require small
models, such as embedded devices, applications requiring fast inference, or
applications desiring more interpretable models.
",1,0,0,1,0,0
19691,19692,Facilitating information system development with Panoramic view on data,"  The increasing amount of information and the absence of an effective tool for
assisting users with minimal technical knowledge lead us to use associative
thinking paradigm for implementation of a software solution - Panorama. In this
study, we present object recognition process, based on context + focus
information visualization techniques, as a foundation for realization of
Panorama. We show that user can easily define data vocabulary of selected
domain that is furthermore used as the application framework. The purpose of
Panorama approach is to facilitate software development of certain problem
domains by shortening the Software Development Life Cycle with minimizing the
impact of implementation, review and maintenance phase. Our approach is focused
on using and updating data vocabulary by users without extensive programming
skills. Panorama therefore facilitates traversing through data by following
associations where user does not need to be familiar with the query language,
the data structure and does not need to know the problem domain fully. Our
approach has been verified by detailed comparison to existing approaches and in
an experiment by implementing selected use cases. The results confirmed that
Panorama fits problem domains with emphasis on data oriented rather than ones
with process oriented aspects. In such cases the development of selected
problem domains is shortened up to 25%, where emphasis is mainly on analysis,
logical design and testing, while omitting physical design and programming,
which is performed automatically by Panorama tool.
",1,0,0,0,0,0
16194,16195,Virtual-to-Real: Learning to Control in Visual Semantic Segmentation,"  Collecting training data from the physical world is usually time-consuming
and even dangerous for fragile robots, and thus, recent advances in robot
learning advocate the use of simulators as the training platform.
Unfortunately, the reality gap between synthetic and real visual data prohibits
direct migration of the models trained in virtual worlds to the real world.
This paper proposes a modular architecture for tackling the virtual-to-real
problem. The proposed architecture separates the learning model into a
perception module and a control policy module, and uses semantic image
segmentation as the meta representation for relating these two modules. The
perception module translates the perceived RGB image to semantic image
segmentation. The control policy module is implemented as a deep reinforcement
learning agent, which performs actions based on the translated image
segmentation. Our architecture is evaluated in an obstacle avoidance task and a
target following task. Experimental results show that our architecture
significantly outperforms all of the baseline methods in both virtual and real
environments, and demonstrates a faster learning curve than them. We also
present a detailed analysis for a variety of variant configurations, and
validate the transferability of our modular architecture.
",1,0,0,0,0,0
18035,18036,Structure of $^{20}$Ne states in the resonance $^{16}$O+$α$ elastic scattering,"  Background
The nuclear structure of the cluster bands in $^{20}$Ne presents a challenge
for different theoretical approaches. It is especially difficult to explain the
broad 0$^+$, 2$^+$ states at 9 MeV excitation energy. Simultaneously, it is
important to obtain more reliable experimental data for these levels in order
to quantitatively assess the theoretical framework.
Purpose
To obtain new data on $^{20}$Ne $\alpha$ cluster structure. Method Thick
target inverse kinematics technique was used to study the $^{16}$O+$\alpha$
resonance elastic scattering and the data were analyzed using an \textit{R}
matrix approach. The $^{20}$Ne spectrum, the cluster and nucleon spectroscopic
factors were calculated using cluster-nucleon configuration interaction model
(CNCIM).
Results
We determined the parameters of the broad resonances in
\textsuperscript{20}Ne: 0$^+$ level at 8.77 $\pm$ 0.150 MeV with a width of 750
(+500/-220) keV; 2$^+$ level at 8.75 $\pm$ 0.100 MeV with the width of 695
$\pm$ 120 keV; the width of 9.48 MeV level of 65 $\pm$ 20 keV and showed that
9.19 MeV, 2$^+$ level (if exists) should have width $\leq$ 10 keV. The detailed
comparison of the theoretical CNCIM predictions with the experimental data on
cluster states was made.
Conclusions
Our experimental results by the TTIK method generally confirm the adopted
data on $\alpha$ cluster levels in $^{20}$Ne. The CNCIM gives a good
description of the $^{20}$Ne positive parity states up to an excitation energy
of $\sim$ 7 MeV, predicting reasonably well the excitation energy of the states
and their cluster and single particle properties. At higher excitations, the
qualitative disagreement with the experimentally observed structure is evident,
especially for broad resonances.
",0,1,0,0,0,0
13923,13924,Search for electromagnetic super-preshowers using gamma-ray telescopes,"  Any considerations on propagation of particles through the Universe must
involve particle interactions: processes leading to production of particle
cascades. While one expects existence of such cascades, the state of the art
cosmic-ray research is oriented purely on a detection of single particles,
gamma rays or associated extensive air showers. The natural extension of the
cosmic-ray research with the studies on ensembles of particles and air showers
is being proposed by the CREDO Collaboration. Within the CREDO strategy the
focus is put on generalized super-preshowers (SPS): spatially and/or temporally
extended cascades of particles originated above the Earth atmosphere, possibly
even at astrophysical distances. With CREDO we want to find out whether SPS can
be at least partially observed by a network of terrestrial and/or satellite
detectors receiving primary or secondary cosmic-ray signal. This paper
addresses electromagnetic SPS, e.g. initiated by VHE photons interacting with
the cosmic microwave background, and the SPS signatures that can be seen by
gamma-ray telescopes, exploring the exampleof Cherenkov Telescope Array. The
energy spectrum of secondary electrons and photons in an electromagnetic
super-preshower might be extended over awide range of energy, down to TeV or
even lower, as it is evident from the simulation results. This means that
electromagnetic showers induced by such particles in the Earth atmosphere could
be observed by imaging atmospheric Cherenkov telescopes. We present preliminary
results from the study of response of the Cherenkov Telescope Array to SPS
events, including the analysis of the simulated shower images on the camera
focal plane and implementedgeneric reconstruction chains based on the Hillas
parameters.
",0,1,0,0,0,0
4561,4562,Implicit Cooperative Positioning in Vehicular Networks,"  Absolute positioning of vehicles is based on Global Navigation Satellite
Systems (GNSS) combined with on-board sensors and high-resolution maps. In
Cooperative Intelligent Transportation Systems (C-ITS), the positioning
performance can be augmented by means of vehicular networks that enable
vehicles to share location-related information. This paper presents an Implicit
Cooperative Positioning (ICP) algorithm that exploits the Vehicle-to-Vehicle
(V2V) connectivity in an innovative manner, avoiding the use of explicit V2V
measurements such as ranging. In the ICP approach, vehicles jointly localize
non-cooperative physical features (such as people, traffic lights or inactive
cars) in the surrounding areas, and use them as common noisy reference points
to refine their location estimates. Information on sensed features are fused
through V2V links by a consensus procedure, nested within a message passing
algorithm, to enhance the vehicle localization accuracy. As positioning does
not rely on explicit ranging information between vehicles, the proposed ICP
method is amenable to implementation with off-the-shelf vehicular communication
hardware. The localization algorithm is validated in different traffic
scenarios, including a crossroad area with heterogeneous conditions in terms of
feature density and V2V connectivity, as well as a real urban area by using
Simulation of Urban MObility (SUMO) for traffic data generation. Performance
results show that the proposed ICP method can significantly improve the vehicle
location accuracy compared to the stand-alone GNSS, especially in harsh
environments, such as in urban canyons, where the GNSS signal is highly
degraded or denied.
",1,0,0,1,0,0
9430,9431,Which Distribution Distances are Sublinearly Testable?,"  Given samples from an unknown distribution $p$ and a description of a
distribution $q$, are $p$ and $q$ close or far? This question of ""identity
testing"" has received significant attention in the case of testing whether $p$
and $q$ are equal or far in total variation distance. However, in recent work,
the following questions have been been critical to solving problems at the
frontiers of distribution testing:
-Alternative Distances: Can we test whether $p$ and $q$ are far in other
distances, say Hellinger?
-Tolerance: Can we test when $p$ and $q$ are close, rather than equal? And if
so, close in which distances?
Motivated by these questions, we characterize the complexity of distribution
testing under a variety of distances, including total variation, $\ell_2$,
Hellinger, Kullback-Leibler, and $\chi^2$. For each pair of distances $d_1$ and
$d_2$, we study the complexity of testing if $p$ and $q$ are close in $d_1$
versus far in $d_2$, with a focus on identifying which problems allow strongly
sublinear testers (i.e., those with complexity $O(n^{1 - \gamma})$ for some
$\gamma > 0$ where $n$ is the size of the support of the distributions $p$ and
$q$). We provide matching upper and lower bounds for each case. We also study
these questions in the case where we only have samples from $q$ (equivalence
testing), showing qualitative differences from identity testing in terms of
when tolerance can be achieved. Our algorithms fall into the classical paradigm
of $\chi^2$-statistics, but require crucial changes to handle the challenges
introduced by each distance we consider. Finally, we survey other recent
results in an attempt to serve as a reference for the complexity of various
distribution testing problems.
",1,0,1,1,0,0
13738,13739,Limit on graviton mass from galaxy cluster Abell 1689,"  To date, the only limit on graviton mass using galaxy clusters was obtained
by Goldhaber and Nieto in 1974, using the fact that the orbits of galaxy
clusters are bound and closed, and extend up to 580 kpc. From positing that
only a Newtonian potential gives rise to such stable bound orbits, a limit on
the graviton mass $m_g<10^{-29}$ eV was obtained (PRD 9,1119, 1974). Recently,
it has been shown that one can obtain closed bound orbits for Yukawa potential
(arXiv:1705.02444), thus invalidating the main \emph{ansatz} used in Goldhaber
and Nieto to obtain the graviton mass bound. In order to obtain a revised
estimate using galaxy clusters, we use dynamical mass models of the Abell 1689
(A1689) galaxy cluster to check their compatibility with a Yukawa gravitational
potential. We assume mass models for the gas, dark matter, and galaxies for
A1689 from arXiv:1703.10219 and arXiv:1610.01543, who used this cluster to test
various alternate gravity theories, which dispense with the need for dark
matter. We quantify the deviations in the acceleration profile using these mass
models assuming a Yukawa potential and that obtained assuming a Newtonian
potential by calculating the $\chi^2$ residuals between the two profiles. Our
estimated bound on the graviton mass ($m_g$) is thereby given by, $m_g < 1.37
\times 10^{-29}$ eV or in terms of the graviton Compton wavelength of,
$\lambda_g>9.1 \times 10^{19}$ km at 90\% confidence level.
",0,1,0,0,0,0
16229,16230,Large polaron evolution in anatase TiO2 due to carrier and temperature dependence of electron-phonon coupling,"  The electronic and magneto transport properties of reduced anatase TiO2
epitaxial thin films are analyzed considering various polaronic effects.
Unexpectedly, with increasing carrier concentration, the mobility increases,
which rarely happens in common metallic systems. We find that the screening of
the electron-phonon (e-ph) coupling by excess carriers is necessary to explain
this unusual dependence. We also find that the magnetoresistance (MR) could be
decomposed into a linear and a quadratic component, separately characterizing
the transport and trap behavior of carriers as a function of temperature. The
various transport behaviors could be organized into a single phase diagram
which clarifies the nature of large polaron in this material.
",0,1,0,0,0,0
17566,17567,"Decoupled molecules with binding polynomials of bidegree (n,2)","  We present a result on the number of decoupled molecules for systems binding
two different types of ligands. In the case of $n$ and $2$ binding sites
respectively, we show that, generically, there are $2(n!)^{2}$ decoupled
molecules with the same binding polynomial. For molecules with more binding
sites for the second ligand, we provide computational results.
",1,1,0,0,0,0
3352,3353,Posterior Asymptotic Normality for an Individual Coordinate in High-dimensional Linear Regression,"  We consider the sparse high-dimensional linear regression model
$Y=Xb+\epsilon$ where $b$ is a sparse vector. For the Bayesian approach to this
problem, many authors have considered the behavior of the posterior
distribution when, in truth, $Y=X\beta+\epsilon$ for some given $\beta$. There
have been numerous results about the rate at which the posterior distribution
concentrates around $\beta$, but few results about the shape of that posterior
distribution. We propose a prior distribution for $b$ such that the marginal
posterior distribution of an individual coordinate $b_i$ is asymptotically
normal centered around an asymptotically efficient estimator, under the truth.
Such a result gives Bayesian credible intervals that match with the confidence
intervals obtained from an asymptotically efficient estimator for $b_i$. We
also discuss ways of obtaining such asymptotically efficient estimators on
individual coordinates. We compare the two-step procedure proposed by Zhang and
Zhang (2014) and a one-step modified penalization method.
",0,0,1,1,0,0
8680,8681,Efficient Algorithms for t-distributed Stochastic Neighborhood Embedding,"  t-distributed Stochastic Neighborhood Embedding (t-SNE) is a method for
dimensionality reduction and visualization that has become widely popular in
recent years. Efficient implementations of t-SNE are available, but they scale
poorly to datasets with hundreds of thousands to millions of high dimensional
data-points. We present Fast Fourier Transform-accelerated Interpolation-based
t-SNE (FIt-SNE), which dramatically accelerates the computation of t-SNE. The
most time-consuming step of t-SNE is a convolution that we accelerate by
interpolating onto an equispaced grid and subsequently using the fast Fourier
transform to perform the convolution. We also optimize the computation of input
similarities in high dimensions using multi-threaded approximate nearest
neighbors. We further present a modification to t-SNE called ""late
exaggeration,"" which allows for easier identification of clusters in t-SNE
embeddings. Finally, for datasets that cannot be loaded into the memory, we
present out-of-core randomized principal component analysis (oocPCA), so that
the top principal components of a dataset can be computed without ever fully
loading the matrix, hence allowing for t-SNE of large datasets to be computed
on resource-limited machines.
",1,0,0,1,0,0
11096,11097,A Practical Approach to Insertion with Variable Socket Position Using Deep Reinforcement Learning,"  Insertion is a challenging haptic and visual control problem with significant
practical value for manufacturing. Existing approaches in the model-based
robotics community can be highly effective when task geometry is known, but are
complex and cumbersome to implement, and must be tailored to each individual
problem by a qualified engineer. Within the learning community there is a long
history of insertion research, but existing approaches are typically either too
sample-inefficient to run on real robots, or assume access to high-level object
features, e.g. socket pose. In this paper we show that relatively minor
modifications to an off-the-shelf Deep-RL algorithm (DDPG), combined with a
small number of human demonstrations, allows the robot to quickly learn to
solve these tasks efficiently and robustly. Our approach requires no modeling
or simulation, no parameterized search or alignment behaviors, no vision system
aside from raw images, and no reward shaping. We evaluate our approach on a
narrow-clearance peg-insertion task and a deformable clip-insertion task, both
of which include variability in the socket position. Our results show that
these tasks can be solved reliably on the real robot in less than 10 minutes of
interaction time, and that the resulting policies are robust to variance in the
socket position and orientation.
",1,0,0,0,0,0
7735,7736,Learning to Optimize Neural Nets,"  Learning to Optimize is a recently proposed framework for learning
optimization algorithms using reinforcement learning. In this paper, we explore
learning an optimization algorithm for training shallow neural nets. Such
high-dimensional stochastic optimization problems present interesting
challenges for existing reinforcement learning algorithms. We develop an
extension that is suited to learning optimization algorithms in this setting
and demonstrate that the learned optimization algorithm consistently
outperforms other known optimization algorithms even on unseen tasks and is
robust to changes in stochasticity of gradients and the neural net
architecture. More specifically, we show that an optimization algorithm trained
with the proposed method on the problem of training a neural net on MNIST
generalizes to the problems of training neural nets on the Toronto Faces
Dataset, CIFAR-10 and CIFAR-100.
",1,0,1,1,0,0
1055,1056,Lord Kelvin's method of images approach to the Rotenberg model and its asymptotics,"  We study a mathematical model of cell populations dynamics proposed by M.
Rotenberg and investigated by M. Boulanouar. Here, a cell is characterized by
her maturity and speed of maturation. The growth of cell populations is
described by a partial differential equation with a boundary condition. In the
first part of the paper we exploit semigroup theory approach and apply Lord
Kelvin's method of images in order to give a new proof that the model is well
posed. Next, we use a semi-explicit formula for the semigroup related to the
model obtained by the method of images in order to give growth estimates for
the semigroup. The main part of the paper is devoted to the asymptotic
behaviour of the semigroup. We formulate conditions for the asymptotic
stability of the semigroup in the case in which the average number of viable
daughters per mitosis equals one. To this end we use methods developed by K.
Pichór and R. Rudnicki.
",0,0,1,0,0,0
15384,15385,Generic partiality for $\frac{3}{2}$-institutions,"  $\frac{3}{2}$-institutions have been introduced as an extension of
institution theory that accommodates implicitly partiality of the signature
morphisms together with its syntactic and semantic effects. In this paper we
show that ordinary institutions that are equipped with an inclusion system for
their categories of signatures generate naturally $\frac{3}{2}$ -institutions
with explicit partiality for their signature morphisms. This provides a general
uniform way to build 3 -institutions for the foundations of conceptual blending
and software evolution. Moreover our general construction allows for an uniform
derivation of some useful technical properties.
",0,0,1,0,0,0
2290,2291,Settling the query complexity of non-adaptive junta testing,"  We prove that any non-adaptive algorithm that tests whether an unknown
Boolean function $f: \{0, 1\}^n\to \{0, 1\}$ is a $k$-junta or $\epsilon$-far
from every $k$-junta must make $\widetilde{\Omega}(k^{3/2} / \epsilon)$ many
queries for a wide range of parameters $k$ and $\epsilon$. Our result
dramatically improves previous lower bounds from [BGSMdW13, STW15], and is
essentially optimal given Blais's non-adaptive junta tester from [Blais08],
which makes $\widetilde{O}(k^{3/2})/\epsilon$ queries. Combined with the
adaptive tester of [Blais09] which makes $O(k\log k + k /\epsilon)$ queries,
our result shows that adaptivity enables polynomial savings in query complexity
for junta testing.
",1,0,0,0,0,0
561,562,Design of the Artificial: lessons from the biological roots of general intelligence,"  Our desire and fascination with intelligent machines dates back to the
antiquity's mythical automaton Talos, Aristotle's mode of mechanical thought
(syllogism) and Heron of Alexandria's mechanical machines and automata.
However, the quest for Artificial General Intelligence (AGI) is troubled with
repeated failures of strategies and approaches throughout the history. This
decade has seen a shift in interest towards bio-inspired software and hardware,
with the assumption that such mimicry entails intelligence. Though these steps
are fruitful in certain directions and have advanced automation, their singular
design focus renders them highly inefficient in achieving AGI. Which set of
requirements have to be met in the design of AGI? What are the limits in the
design of the artificial? Here, a careful examination of computation in
biological systems hints that evolutionary tinkering of contextual processing
of information enabled by a hierarchical architecture is the key to build AGI.
",1,1,0,0,0,0
15830,15831,Hilbert isometries and maximal deviation preserving maps on JB-algebras,"  In this paper we characterize the surjective linear variation norm isometries
on JB-algebras. Variation norm isometries are precisely the maps that preserve
the maximal deviation, the quantum analogue of the standard deviation, which
plays an important role in quantum statistics. Consequently, we characterize
the Hilbert's metric isometries on cones in JB-algebras.
",0,0,1,0,0,0
2680,2681,Multi-Scale Pipeline for the Search of String-Induced CMB Anisotropies,"  We propose a multi-scale edge-detection algorithm to search for the
Gott-Kaiser-Stebbins imprints of a cosmic string (CS) network on the Cosmic
Microwave Background (CMB) anisotropies. Curvelet decomposition and extended
Canny algorithm are used to enhance the string detectability. Various
statistical tools are then applied to quantify the deviation of CMB maps having
a cosmic string contribution with respect to pure Gaussian anisotropies of
inflationary origin. These statistical measures include the one-point
probability density function, the weighted two-point correlation function
(TPCF) of the anisotropies, the unweighted TPCF of the peaks and of the
up-crossing map, as well as their cross-correlation. We use this algorithm on a
hundred of simulated Nambu-Goto CMB flat sky maps, covering approximately
$10\%$ of the sky, and for different string tensions $G\mu$. On noiseless sky
maps with an angular resolution of $0.9'$, we show that our pipeline detects
CSs with $G\mu$ as low as $G\mu\gtrsim 4.3\times 10^{-10}$. At the same
resolution, but with a noise level typical to a CMB-S4 phase II experiment, the
detection threshold would be to $G\mu\gtrsim 1.2 \times 10^{-7}$.
",0,1,0,1,0,0
11269,11270,Conditions for the equivalence between IQC and graph separation stability results,"  This paper provides a link between time-domain and frequency-domain stability
results in the literature. Specifically, we focus on the comparison between
stability results for a feedback interconnection of two nonlinear systems
stated in terms of frequency-domain conditions. While the Integral Quadratic
Constrain (IQC) theorem can cope with them via a homotopy argument for the
Lurye problem, graph separation results require the transformation of the
frequency-domain conditions into truncated time-domain conditions. To date,
much of the literature focuses on ""hard"" factorizations of the multiplier,
considering only one of the two frequency-domain conditions. Here it is shown
that a symmetric, ""doubly-hard"" factorization is required to convert both
frequency-domain conditions into truncated time-domain conditions. By using the
appropriate factorization, a novel comparison between the results obtained by
IQC and separation theories is then provided. As a result, we identify under
what conditions the IQC theorem may provide some advantage.
",1,0,0,0,0,0
18198,18199,Efficiency Analysis of ASP Encodings for Sequential Pattern Mining Tasks,"  This article presents the use of Answer Set Programming (ASP) to mine
sequential patterns. ASP is a high-level declarative logic programming paradigm
for high level encoding combinatorial and optimization problem solving as well
as knowledge representation and reasoning. Thus, ASP is a good candidate for
implementing pattern mining with background knowledge, which has been a data
mining issue for a long time. We propose encodings of the classical sequential
pattern mining tasks within two representations of embeddings (fill-gaps vs
skip-gaps) and for various kinds of patterns: frequent, constrained and
condensed. We compare the computational performance of these encodings with
each other to get a good insight into the efficiency of ASP encodings. The
results show that the fill-gaps strategy is better on real problems due to
lower memory consumption. Finally, compared to a constraint programming
approach (CPSM), another declarative programming paradigm, our proposal showed
comparable performance.
",1,0,0,1,0,0
4627,4628,Asteroid 2017 FZ2 et al.: signs of recent mass-shedding from YORP?,"  The first direct detection of the asteroidal YORP effect, a phenomenon that
changes the spin states of small bodies due to thermal reemission of sunlight
from their surfaces, was obtained for (54509) YORP 2000 PH5. Such an alteration
can slowly increase the rotation rate of asteroids, driving them to reach their
fission limit and causing their disruption. This process can produce binaries
and unbound asteroid pairs. Secondary fission opens the door to the eventual
formation of transient but genetically-related groupings. Here, we show that
the small near-Earth asteroid (NEA) 2017 FZ2 was a co-orbital of our planet of
the quasi-satellite type prior to their close encounter on 2017 March 23.
Because of this flyby with the Earth, 2017 FZ2 has become a non-resonant NEA.
Our N-body simulations indicate that this object may have experienced
quasi-satellite engagements with our planet in the past and it may return as a
co-orbital in the future. We identify a number of NEAs that follow similar
paths, the largest named being YORP, which is also an Earth's co-orbital. An
apparent excess of NEAs moving in these peculiar orbits is studied within the
framework of two orbit population models. A possibility that emerges from this
analysis is that such an excess, if real, could be the result of mass shedding
from YORP itself or a putative larger object that produced YORP. Future
spectroscopic observations of 2017 FZ2 during its next visit in 2018 (and of
related objects when feasible) may be able to confirm or reject this
interpretation.
",0,1,0,0,0,0
1141,1142,Large deviation theorem for random covariance matrices,"  We establish a large deviation theorem for the empirical spectral
distribution of random covariance matrices whose entries are independent random
variables with mean 0, variance 1 and having controlled forth moments. Some new
properties of Laguerre polynomials are also given.
",0,0,1,0,0,0
12220,12221,"AWAKE readiness for the study of the seeded self-modulation of a 400\,GeV proton bunch","  AWAKE is a proton-driven plasma wakefield acceleration experiment. % We show
that the experimental setup briefly described here is ready for systematic
study of the seeded self-modulation of the 400\,GeV proton bunch in the
10\,m-long rubidium plasma with density adjustable from 1 to
10$\times10^{14}$\,cm$^{-3}$. % We show that the short laser pulse used for
ionization of the rubidium vapor propagates all the way along the column,
suggesting full ionization of the vapor. % We show that ionization occurs along
the proton bunch, at the laser time and that the plasma that follows affects
the proton bunch. %
",0,1,0,0,0,0
10222,10223,Optimal Prediction for Additive Function-on-Function Regression,"  As with classic statistics, functional regression models are invaluable in
the analysis of functional data. While there are now extensive tools with
accompanying theory available for linear models, there is still a great deal of
work to be done concerning nonlinear models for functional data. In this work
we consider the Additive Function-on-Function Regression model, a type of
nonlinear model that uses an additive relationship between the functional
outcome and functional covariate. We present an estimation methodology built
upon Reproducing Kernel Hilbert Spaces, and establish optimal rates of
convergence for our estimates in terms of prediction error. We also discuss
computational challenges that arise with such complex models, developing a
representer theorem for our estimate as well as a more practical and
computationally efficient approximation. Simulations and an application to
Cumulative Intraday Returns around the 2008 financial crisis are also provided.
",0,0,1,1,0,0
9306,9307,Distortions of the Cosmic Microwave Background through cooling lines during the epoch of Reionization,"  By using N-body hydrodynamical cosmological simulations in which the
chemistry of major metals and molecules is consistently solved for, we study
the interaction of metallic fine-structure lines with the CMB. Our analysis
shows that the collisional induced emissions in the OI 145 $\mu$m and CII 158
$\mu$m lines during reionization introduce a distortion of the CMB spectrum at
low frequencies ($\nu < 300$ GHz) with amplitudes up to $\Delta
I_{\nu}/B_{\nu}(T_{\rm CMB})\sim 10^{-8}$-$10^{-7}$, i.e., at the $\sim 0.1$
percent level of FIRAS upper limits. Shorter wavelength fine-structure
transitions (OI 63 $\mu$m, FeII 26 $\mu$m, and SiII 35 $\mu$m) typically sample
the reionization epoch at higher observing frequencies ($\nu > 400$ GHz). This
corresponds to the Wien tail of the CMB spectrum and the distortion level
induced by those lines may be as high as $\Delta I_{\nu}/B_{\nu}(T_{\rm
CMB})\sim 10^{-4}$. The angular anisotropy produced by these lines should be
more relevant at higher frequencies: while practically negligible at $\nu=145
$GHz, signatures from CII 158 $\mu$m and OI 145 $\mu$m should amount to 1%-5%
of the anisotropy power measured at $l \sim 5000$ and $\nu=220 $GHz by the ACT
and SPT collaborations (after assuming $\Delta \nu_{\rm obs}/\nu_{\rm
obs}\simeq 0.005$ for the line observations). Our simulations show that
anisotropy maps from different lines (e.g., OI 145 $\mu$m and CII 158 $\mu$m)
at the same redshift show a very high degree ($>0.8$) of spatial correlation,
allowing for the use of observations at different frequencies to unveil the
same snapshot of the reionization epoch. Finally, our simulations demonstrate
that line-emission anisotropies extracted in narrow frequency/redshift shells
are practically uncorrelated in frequency space, thus enabling standard methods
for removal of foregrounds that vary smoothly in frequency, just as in HI 21 cm
studies.
",0,1,0,0,0,0
3054,3055,Thinking Fast and Slow with Deep Learning and Tree Search,"  Sequential decision making problems, such as structured prediction, robotic
control, and game playing, require a combination of planning policies and
generalisation of those plans. In this paper, we present Expert Iteration
(ExIt), a novel reinforcement learning algorithm which decomposes the problem
into separate planning and generalisation tasks. Planning new policies is
performed by tree search, while a deep neural network generalises those plans.
Subsequently, tree search is improved by using the neural network policy to
guide search, increasing the strength of new plans. In contrast, standard deep
Reinforcement Learning algorithms rely on a neural network not only to
generalise plans, but to discover them too. We show that ExIt outperforms
REINFORCE for training a neural network to play the board game Hex, and our
final tree search agent, trained tabula rasa, defeats MoHex 1.0, the most
recent Olympiad Champion player to be publicly released.
",1,0,0,0,0,0
1476,1477,Variational methods for steady-state Darcy/Fick flow in swollen and poroelastic solids,"  Existence of steady states in elastic media at small strains with diffusion
of a solvent or fluid due to Fick's or Darcy's laws is proved by combining
usage of variational methods inspired from static situations with Schauder's
fixed-point arguments. In the plain variant, the problem consists in the force
equilibrium coupled with the continuity equation, and the underlying operator
is non-potential and non-pseudomonotone so that conventional methods are not
applicable. In advanced variants, electrically-charged multi-component flows
through an electrically charged elastic solid are treated, employing critical
points of the saddle-point type. Eventually, anisothermal variants involving
heat-transfer equation are treated, too.
",0,0,1,0,0,0
13325,13326,Bad Primes in Computational Algebraic Geometry,"  Computations over the rational numbers often suffer from intermediate
coefficient swell. One solution to this problem is to apply the given algorithm
modulo a number of primes and then lift the modular results to the rationals.
This method is guaranteed to work if we use a sufficiently large set of good
primes. In many applications, however, there is no efficient way of excluding
bad primes. In this note, we describe a technique for rational reconstruction
which will nevertheless return the correct result, provided the number of good
primes in the selected set of primes is large enough. We give a number of
illustrating examples which are implemented using the computer algebra system
Singular and the programming language Julia. We discuss applications of our
technique in computational algebraic geometry.
",1,0,1,0,0,0
14147,14148,On the Effectiveness of Discretizing Quantitative Attributes in Linear Classifiers,"  Learning algorithms that learn linear models often have high representation
bias on real-world problems. In this paper, we show that this representation
bias can be greatly reduced by discretization. Discretization is a common
procedure in machine learning that is used to convert a quantitative attribute
into a qualitative one. It is often motivated by the limitation of some
learners to qualitative data. Discretization loses information, as fewer
distinctions between instances are possible using discretized data relative to
undiscretized data. In consequence, where discretization is not essential, it
might appear desirable to avoid it. However, it has been shown that
discretization often substantially reduces the error of the linear generative
Bayesian classifier naive Bayes. This motivates a systematic study of the
effectiveness of discretizing quantitative attributes for other linear
classifiers. In this work, we study the effect of discretization on the
performance of linear classifiers optimizing three distinct discriminative
objective functions --- logistic regression (optimizing negative
log-likelihood), support vector classifiers (optimizing hinge loss) and a
zero-hidden layer artificial neural network (optimizing mean-square-error). We
show that discretization can greatly increase the accuracy of these linear
discriminative learners by reducing their representation bias, especially on
big datasets. We substantiate our claims with an empirical study on $42$
benchmark datasets.
",1,0,0,0,0,0
18197,18198,Accelerating solutions of one-dimensional unsteady PDEs with GPU-based swept time-space decomposition,"  The expedient design of precision components in aerospace and other high-tech
industries requires simulations of physical phenomena often described by
partial differential equations (PDEs) without exact solutions. Modern design
problems require simulations with a level of resolution difficult to achieve in
reasonable amounts of time---even in effectively parallelized solvers. Though
the scale of the problem relative to available computing power is the greatest
impediment to accelerating these applications, significant performance gains
can be achieved through careful attention to the details of memory
communication and access. The swept time-space decomposition rule reduces
communication between sub-domains by exhausting the domain of influence before
communicating boundary values. Here we present a GPU implementation of the
swept rule, which modifies the algorithm for improved performance on this
processing architecture by prioritizing use of private (shared) memory,
avoiding interblock communication, and overwriting unnecessary values. It shows
significant improvement in the execution time of finite-difference solvers for
one-dimensional unsteady PDEs, producing speedups of 2--9$\times$ for a range
of problem sizes, respectively, compared with simple GPU versions and
7--300$\times$ compared with parallel CPU versions. However, for a more
sophisticated one-dimensional system of equations discretized with a
second-order finite-volume scheme, the swept rule performs 1.2--1.9$\times$
worse than a standard implementation for all problem sizes.
",1,1,0,0,0,0
13813,13814,Constraints on the pre-impact orbits of Solar System giant impactors,"  We provide a fast method for computing constraints on impactor pre-impact
orbits, applying this to the late giant impacts in the Solar System. These
constraints can be used to make quick, broad comparisons of different collision
scenarios, identifying some immediately as low-probability events, and
narrowing the parameter space in which to target follow-up studies with
expensive N-body simulations. We benchmark our parameter space predictions,
finding good agreement with existing N-body studies for the Moon. We suggest
that high-velocity impact scenarios in the inner Solar System, including all
currently proposed single impact scenarios for the formation of Mercury, should
be disfavoured. This leaves a multiple hit-and-run scenario as the most
probable currently proposed for the formation of Mercury.
",0,1,0,0,0,0
12782,12783,Dynamics of quantum information in many-body localized systems,"  We characterize the information dynamics of strongly disordered systems using
a combination of analytics, exact diagonalization, and matrix product operator
simulations. More specifically, we study the spreading of quantum information
in three different scenarios: thermalizing, Anderson localized, and many-body
localized. We qualitatively distinguish these cases by quantifying the amount
of remnant information in a local region. The nature of the dynamics is further
explored by computing the propagation of mutual information with respect to
varying partitions. Finally, we demonstrate that classical simulability, as
captured by the magnitude of MPO truncation errors, exhibits enhanced
fluctuations near the localization transition, suggesting the possibility of
its use as a diagnostic of the critical point.
",0,1,0,0,0,0
2422,2423,Vortex states and spin textures of rotating spin-orbit-coupled Bose-Einstein condensates in a toroidal trap,"  We consider the ground-state properties of Rashba spin-orbit-coupled
pseudo-spin-1/2 Bose-Einstein condensates (BECs) in a rotating two-dimensional
(2D) toroidal trap. In the absence of spin-orbit coupling (SOC), the increasing
rotation frequency enhances the creation of giant vortices for the initially
miscible BECs, while it can lead to the formation of semiring density patterns
with irregular hidden vortex structures for the initially immiscible BECs.
Without rotation, strong 2D isotropic SOC yields a heliciform-stripe phase for
the initially immiscible BECs. Combined effects of rotation, SOC, and
interatomic interactions on the vortex structures and typical spin textures of
the ground state of the system are discussed systematically. In particular, for
fixed rotation frequency above the critical value, the increasing isotropic SOC
favors a visible vortex ring in each component which is accompanied by a hidden
giant vortex plus a (several) hidden vortex ring(s) in the central region. In
the case of 1D anisotropic SOC, large SOC strength results in the generation of
hidden linear vortex string and the transition from initial phase separation
(phase mixing) to phase mixing (phase separation). Furthermore, the peculiar
spin textures including skyrmion lattice, skyrmion pair and skyrmion string are
revealed in this system.
",0,1,0,0,0,0
642,643,One-Step Time-Dependent Future Video Frame Prediction with a Convolutional Encoder-Decoder Neural Network,"  There is an inherent need for autonomous cars, drones, and other robots to
have a notion of how their environment behaves and to anticipate changes in the
near future. In this work, we focus on anticipating future appearance given the
current frame of a video. Existing work focuses on either predicting the future
appearance as the next frame of a video, or predicting future motion as optical
flow or motion trajectories starting from a single video frame. This work
stretches the ability of CNNs (Convolutional Neural Networks) to predict an
anticipation of appearance at an arbitrarily given future time, not necessarily
the next video frame. We condition our predicted future appearance on a
continuous time variable that allows us to anticipate future frames at a given
temporal distance, directly from the input video frame. We show that CNNs can
learn an intrinsic representation of typical appearance changes over time and
successfully generate realistic predictions at a deliberate time difference in
the near future.
",1,0,0,0,0,0
12821,12822,Assessing the Privacy Cost in Centralized Event-Based Demand Response for Microgrids,"  Demand response (DR) programs have emerged as a potential key enabling
ingredient in the context of smart grid (SG). Nevertheless, the rising concerns
over privacy issues raised by customers subscribed to these programs constitute
a major threat towards their effective deployment and utilization. This has
driven extensive research to resolve the hindrance confronted, resulting in a
number of methods being proposed for preserving customers' privacy. While these
methods provide stringent privacy guarantees, only limited attention has been
paid to their computational efficiency and performance quality. Under the
paradigm of differential privacy, this paper initiates a systematic empirical
study on quantifying the trade-off between privacy and optimality in
centralized DR systems for maximizing cumulative customer utility. Aiming to
elucidate the factors governing this trade-off, we analyze the cost of privacy
in terms of the effect incurred on the objective value of the DR optimization
problem when applying the employed privacy-preserving strategy based on Laplace
mechanism. The theoretical results derived from the analysis are complemented
with empirical findings, corroborated extensively by simulations on a 4-bus MG
system with up to thousands of customers. By evaluating the impact of privacy,
this pilot study serves DR practitioners when considering the social and
economic implications of deploying privacy-preserving DR programs in practice.
Moreover, it stimulates further research on exploring more efficient approaches
with bounded performance guarantees for optimizing energy procurement of MGs
without infringing the privacy of customers on demand side.
",1,0,1,0,0,0
17585,17586,InfoVAE: Information Maximizing Variational Autoencoders,"  A key advance in learning generative models is the use of amortized inference
distributions that are jointly trained with the models. We find that existing
training objectives for variational autoencoders can lead to inaccurate
amortized inference distributions and, in some cases, improving the objective
provably degrades the inference quality. In addition, it has been observed that
variational autoencoders tend to ignore the latent variables when combined with
a decoding distribution that is too flexible. We again identify the cause in
existing training criteria and propose a new class of objectives (InfoVAE) that
mitigate these problems. We show that our model can significantly improve the
quality of the variational posterior and can make effective use of the latent
features regardless of the flexibility of the decoding distribution. Through
extensive qualitative and quantitative analyses, we demonstrate that our models
outperform competing approaches on multiple performance metrics.
",1,0,0,1,0,0
17550,17551,Thermal properties of graphene from path-integral simulations,"  Thermal properties of graphene monolayers are studied by path-integral
molecular dynamics (PIMD) simulations, which take into account the quantization
of vibrational modes in the crystalline membrane, and allow one to consider
anharmonic effects in these properties. This system was studied at temperatures
in the range from 12 to 2000~K and zero external stress, by describing the
interatomic interactions through the LCBOPII effective potential. We analyze
the internal energy and specific heat and compare the results derived from the
simulations with those yielded by a harmonic approximation for the vibrational
modes. This approximation turns out to be rather precise up to temperatures of
about 400~K. At higher temperatures, we observe an influence of the elastic
energy, due to the thermal expansion of the graphene sheet. Zero-point and
thermal effects on the in-plane and ""real"" surface of graphene are discussed.
The thermal expansion coefficient $\alpha$ of the real area is found to be
positive at all temperatures, in contrast to the expansion coefficient
$\alpha_p$ of the in-plane area, which is negative at low temperatures, and
becomes positive for $T \gtrsim$ 1000~K.
",0,1,0,0,0,0
6556,6557,Poisson brackets symmetry from the pentagon-wheel cocycle in the graph complex,"  Kontsevich designed a scheme to generate infinitesimal symmetries
$\dot{\mathcal{P}} = \mathcal{Q}(\mathcal{P})$ of Poisson brackets
$\mathcal{P}$ on all affine manifolds $M^r$; every such deformation is encoded
by oriented graphs on $n+2$ vertices and $2n$ edges. In particular, these
symmetries can be obtained by orienting sums of non-oriented graphs $\gamma$ on
$n$ vertices and $2n-2$ edges. The bi-vector flow $\dot{\mathcal{P}} =
\text{Or}(\gamma)(\mathcal{P})$ preserves the space of Poisson structures if
$\gamma$ is a cocycle with respect to the vertex-expanding differential in the
graph complex.
A class of such cocycles $\boldsymbol{\gamma}_{2\ell+1}$ is known to exist:
marked by $\ell \in \mathbb{N}$, each of them contains a $(2\ell+1)$-gon wheel
with a nonzero coefficient. At $\ell=1$ the tetrahedron $\boldsymbol{\gamma}_3$
itself is a cocycle; at $\ell=2$ the Kontsevich--Willwacher pentagon-wheel
cocycle $\boldsymbol{\gamma}_5$ consists of two graphs. We reconstruct the
symmetry $\mathcal{Q}_5(\mathcal{P}) =
\text{Or}(\boldsymbol{\gamma}_5)(\mathcal{P})$ and verify that $\mathcal{Q}_5$
is a Poisson cocycle indeed:
$[\![\mathcal{P},\mathcal{Q}_5(\mathcal{P})]\!]\doteq 0$ via
$[\![\mathcal{P},\mathcal{P}]\!]=0$.
",0,0,1,0,0,0
9547,9548,The Case for Learned Index Structures,"  Indexes are models: a B-Tree-Index can be seen as a model to map a key to the
position of a record within a sorted array, a Hash-Index as a model to map a
key to a position of a record within an unsorted array, and a BitMap-Index as a
model to indicate if a data record exists or not. In this exploratory research
paper, we start from this premise and posit that all existing index structures
can be replaced with other types of models, including deep-learning models,
which we term learned indexes. The key idea is that a model can learn the sort
order or structure of lookup keys and use this signal to effectively predict
the position or existence of records. We theoretically analyze under which
conditions learned indexes outperform traditional index structures and describe
the main challenges in designing learned index structures. Our initial results
show, that by using neural nets we are able to outperform cache-optimized
B-Trees by up to 70% in speed while saving an order-of-magnitude in memory over
several real-world data sets. More importantly though, we believe that the idea
of replacing core components of a data management system through learned models
has far reaching implications for future systems designs and that this work
just provides a glimpse of what might be possible.
",1,0,0,0,0,0
17349,17350,"Nikol'ski\uı, Jackson and Ul'yanov type inequalities with Muckenhoupt weights","  In the present work we prove a Nikol'ski inequality for trigonometric
polynomials and Ul'yanov type inequalities for functions in Lebesgue spaces
with Muckenhoupt weights. Realization result and Jackson inequalities are
obtained. Simultaneous approximation by polynomials is considered. Some uniform
norm inequalities are transferred to weighted Lebesgue space.
",0,0,1,0,0,0
13403,13404,Adaptive recurrence quantum entanglement distillation for two-Kraus-operator channels,"  Quantum entanglement serves as a valuable resource for many important quantum
operations. A pair of entangled qubits can be shared between two agents by
first preparing a maximally entangled qubit pair at one agent, and then sending
one of the qubits to the other agent through a quantum channel. In this
process, the deterioration of entanglement is inevitable since the noise
inherent in the channel contaminates the qubit. To address this challenge,
various quantum entanglement distillation (QED) algorithms have been developed.
Among them, recurrence algorithms have advantages in terms of implementability
and robustness. However, the efficiency of recurrence QED algorithms has not
been investigated thoroughly in the literature. This paper put forth two
recurrence QED algorithms that adapt to the quantum channel to tackle the
efficiency issue. The proposed algorithms have guaranteed convergence for
quantum channels with two Kraus operators, which include phase-damping and
amplitude-damping channels. Analytical results show that the convergence speed
of these algorithms is improved from linear to quadratic and one of the
algorithms achieves the optimal speed. Numerical results confirm that the
proposed algorithms significantly improve the efficiency of QED.
",0,0,1,0,0,0
15262,15263,"Korea Microlensing Telescope Network Microlensing Events from 2015: Event-Finding Algorithm, Vetting, and Photometry","  We present microlensing events in the 2015 Korea Microlensing Telescope
Network (KMTNet) data and our procedure for identifying these events. In
particular, candidates were detected with a novel ""completed event""
microlensing event-finder algorithm. The algorithm works by making linear fits
to a (t0,teff,u0) grid of point-lens microlensing models. This approach is
rendered computationally efficient by restricting u0 to just two values (0 and
1), which we show is quite adequate. The implementation presented here is
specifically tailored to the commission-year character of the 2015 data, but
the algorithm is quite general and has already been applied to a completely
different (non-KMTNet) data set. We outline expected improvements for 2016 and
future KMTNet data. The light curves of the 660 ""clear microlensing"" and 182
""possible microlensing"" events that were found in 2015 are presented along with
our policy for their public release.
",0,1,0,0,0,0
15164,15165,A summation formula for triples of quadratic spaces,"  Let $V_1,V_2,V_3$ be a triple of even dimensional vector spaces over a number
field $F$ equipped with nondegenerate quadratic forms
$\mathcal{Q}_1,\mathcal{Q}_2,\mathcal{Q}_3$, respectively. Let \begin{align*} Y
\subset \prod_{i=1}V_i \end{align*} be the closed subscheme consisting of
$(v_1,v_2,v_3)$ on which
$\mathcal{Q}_1(v_1)=\mathcal{Q}_2(v_2)=\mathcal{Q}_3(v_3)$. Motivated by
conjectures of Braverman and Kazhdan and related work of Lafforgue, Ngô, and
Sakellaridis we prove an analogue of the Poisson summation formula for certain
functions on this space.
",0,0,1,0,0,0
20717,20718,Riemannian Gaussian distributions on the space of positive-definite quaternion matrices,"  Recently, Riemannian Gaussian distributions were defined on spaces of
positive-definite real and complex matrices. The present paper extends this
definition to the space of positive-definite quaternion matrices. In order to
do so, it develops the Riemannian geometry of the space of positive-definite
quaternion matrices, which is shown to be a Riemannian symmetric space of
non-positive curvature. The paper gives original formulae for the Riemannian
metric of this space, its geodesics, and distance function. Then, it develops
the theory of Riemannian Gaussian distributions, including the exact expression
of their probability density, their sampling algorithm and statistical
inference.
",0,0,1,1,0,0
14007,14008,F-index of graphs based on four operations related to the lexicographic product,"  The forgotten topological index or F-index of a graph is defined as the sum
of cubes of the degree of all the vertices of the graph. In this paper we study
the F-index of four operations related to the lexicographic product on graphs
which were introduced by Sarala et al. [D. Sarala, H. Deng, S.K. Ayyaswamya and
S. Balachandrana, The Zagreb indices of graphs based on four new operations
related to the lexicographic product, \textit{Applied Mathematics and
Computation}, 309 (2017) 156--169.].
",1,0,0,0,0,0
323,324,Bayesian Optimization for Probabilistic Programs,"  We present the first general purpose framework for marginal maximum a
posteriori estimation of probabilistic program variables. By using a series of
code transformations, the evidence of any probabilistic program, and therefore
of any graphical model, can be optimized with respect to an arbitrary subset of
its sampled variables. To carry out this optimization, we develop the first
Bayesian optimization package to directly exploit the source code of its
target, leading to innovations in problem-independent hyperpriors, unbounded
optimization, and implicit constraint satisfaction; delivering significant
performance improvements over prominent existing packages. We present
applications of our method to a number of tasks including engineering design
and parameter optimization.
",1,0,0,1,0,0
2114,2115,Detecting Near Duplicates in Software Documentation,"  Contemporary software documentation is as complicated as the software itself.
During its lifecycle, the documentation accumulates a lot of near duplicate
fragments, i.e. chunks of text that were copied from a single source and were
later modified in different ways. Such near duplicates decrease documentation
quality and thus hamper its further utilization. At the same time, they are
hard to detect manually due to their fuzzy nature. In this paper we give a
formal definition of near duplicates and present an algorithm for their
detection in software documents. This algorithm is based on the exact software
clone detection approach: the software clone detection tool Clone Miner was
adapted to detect exact duplicates in documents. Then, our algorithm uses these
exact duplicates to construct near ones. We evaluate the proposed algorithm
using the documentation of 19 open source and commercial projects. Our
evaluation is very comprehensive - it covers various documentation types:
design and requirement specifications, programming guides and API
documentation, user manuals. Overall, the evaluation shows that all kinds of
software documentation contain a significant number of both exact and near
duplicates. Next, we report on the performed manual analysis of the detected
near duplicates for the Linux Kernel Documentation. We present both quantative
and qualitative results of this analysis, demonstrate algorithm strengths and
weaknesses, and discuss the benefits of duplicate management in software
documents.
",1,0,0,0,0,0
14748,14749,On the character degrees of a Sylow $p$-subgroup of a finite Chevalley group $G(p^f)$ over a bad prime,"  Let $q$ be a power of a prime $p$ and let $U(q)$ be a Sylow $p$-subgroup of a
finite Chevalley group $G(q)$ defined over the field with $q$ elements. We
first give a parametrization of the set $\text{Irr}(U(q))$ of irreducible
characters of $U(q)$ when $G(q)$ is of type $\mathrm{G}_2$. This is uniform for
primes $p \ge 5$, while the bad primes $p=2$ and $p=3$ have to be considered
separately. We then use this result and the contribution of several authors to
show a general result, namely that if $G(q)$ is any finite Chevalley group with
$p$ a bad prime, then there exists a character $\chi \in \text{Irr}(U(q))$ such
that $\chi(1)=q^n/p$ for some $n \in \mathbb{Z}_{\ge_0}$. In particular, for
each $G(q)$ and every bad prime $p$, we construct a family of characters of
such degree as inflation followed by an induction of linear characters of an
abelian subquotient $V(q)$ of $U(q)$.
",0,0,1,0,0,0
13627,13628,An overview of knot Floer homology,"  Knot Floer homology is an invariant for knots discovered by the authors and,
independently, Jacob Rasmussen. The discovery of this invariant grew naturally
out of studying how a certain three-manifold invariant, Heegaard Floer
homology, changes as the three-manifold undergoes Dehn surgery along a knot.
Since its original definition, thanks to the contributions of many researchers,
knot Floer homology has emerged as a useful tool for studying knots in its own
right. We give here a few selected highlights of this theory, and then move on
to some new algebraic developments in the computation of knot Floer homology.
",0,0,1,0,0,0
1475,1476,On recognizing shapes of polytopes from their shadows,"  Let $P$ and $Q$ be two convex polytopes both contained in the interior of an
Euclidean ball $r\textbf{B}^{d}$. We prove that $P=Q$ provided that their sight
cones from any point on the sphere $rS^{d-1}$ are congruent. We also prove an
analogous result for spherical projections.
",0,0,1,0,0,0
11516,11517,The study on quantum material WTe2,"  WTe2 and its sister alloys have attracted tremendous attentions recent years
due to the large non-saturating magnetoresistance and topological non-trivial
properties. Herein, we briefly review the electrical property studies on this
new quantum material.
",0,1,0,0,0,0
9852,9853,Transferrable End-to-End Learning for Protein Interface Prediction,"  While there has been an explosion in the number of experimentally determined,
atomically detailed structures of proteins, how to represent these structures
in a machine learning context remains an open research question. In this work
we demonstrate that representations learned from raw atomic coordinates can
outperform hand-engineered structural features while displaying a much higher
degree of transferrability. To do so, we focus on a central problem in biology:
predicting how proteins interact with one another--that is, which surfaces of
one protein bind to which surfaces of another protein. We present Siamese
Atomic Surfacelet Network (SASNet), the first end-to-end learning method for
protein interface prediction. Despite using only spatial coordinates and
identities of atoms as inputs, SASNet outperforms state-of-the-art methods that
rely on hand-engineered, high-level features. These results are particularly
striking because we train the method entirely on a significantly biased data
set that does not account for the fact that proteins deform when binding to one
another. Demonstrating the first successful application of transfer learning to
atomic-level data, our network maintains high performance, without retraining,
when tested on real cases in which proteins do deform.
",0,0,0,1,1,0
6256,6257,Contraction par Frobenius et modules de Steinberg,"  For a reductive group G defined over an algebraically closed field of
positive characteristic, we show that the Frobenius contraction functor of
G-modules is right adjoint to the Frobenius twist of the modules tensored with
the Steinberg module twice. It follows that the Frobenius contraction functor
preserves injectivity, good filtrations, but not semisiplicity.
",0,0,1,0,0,0
14982,14983,The CODALEMA/EXTASIS experiment: Contributions to the 35th International Cosmic Ray Conference (ICRC 2017),"  Contributions of the CODALEMA/EXTASIS experiment to the 35th International
Cosmic Ray Conference, 12-20 July 2017, Busan, South Korea.
",0,1,0,0,0,0
1907,1908,Multi-Erasure Locally Recoverable Codes Over Small Fields For Flash Memory Array,"  Erasure codes play an important role in storage systems to prevent data loss.
In this work, we study a class of erasure codes called Multi-Erasure Locally
Recoverable Codes (ME-LRCs) for flash memory array. Compared to previous
related works, we focus on the construction of ME-LRCs over small fields. We
first develop upper and lower bounds on the minimum distance of ME-LRCs. These
bounds explicitly take the field size into account. Our main contribution is to
propose a general construction of ME-LRCs based on generalized tensor product
codes, and study their erasure-correcting property. A decoding algorithm
tailored for erasure recovery is given. We then prove that our construction
yields optimal ME-LRCs with a wide range of code parameters. Finally, we
present several families of ME-LRCs over different fields.
",1,0,0,0,0,0
17003,17004,Significance of Side Information in the Graph Matching Problem,"  Percolation based graph matching algorithms rely on the availability of seed
vertex pairs as side information to efficiently match users across networks.
Although such algorithms work well in practice, there are other types of side
information available which are potentially useful to an attacker. In this
paper, we consider the problem of matching two correlated graphs when an
attacker has access to side information, either in the form of community labels
or an imperfect initial matching. In the former case, we propose a naive graph
matching algorithm by introducing the community degree vectors which harness
the information from community labels in an efficient manner. Furthermore, we
analyze a variant of the basic percolation algorithm proposed in literature for
graphs with community structure. In the latter case, we propose a novel
percolation algorithm with two thresholds which uses an imperfect matching as
input to match correlated graphs.
We evaluate the proposed algorithms on synthetic as well as real world
datasets using various experiments. The experimental results demonstrate the
importance of communities as side information especially when the number of
seeds is small and the networks are weakly correlated.
",1,1,0,0,0,0
2431,2432,The Faraday room of the CUORE Experiment,"  The paper describes the Faraday room that shields the CUORE experiment
against electromagnetic fields, from 50 Hz up to high frequency. Practical
contraints led to choose panels made of light shielding materials. The seams
between panels were optimized with simulations to minimize leakage.
Measurements of shielding performance show attenuation of a factor 15 at 50 Hz,
and a factor 1000 above 1 KHz up to about 100 MHz.
",0,1,0,0,0,0
4944,4945,Putting Self-Supervised Token Embedding on the Tables,"  Information distribution by electronic messages is a privileged means of
transmission for many businesses and individuals, often under the form of
plain-text tables. As their number grows, it becomes necessary to use an
algorithm to extract text and numbers instead of a human. Usual methods are
focused on regular expressions or on a strict structure in the data, but are
not efficient when we have many variations, fuzzy structure or implicit labels.
In this paper we introduce SC2T, a totally self-supervised model for
constructing vector representations of tokens in semi-structured messages by
using characters and context levels that address these issues. It can then be
used for an unsupervised labeling of tokens, or be the basis for a
semi-supervised information extraction system.
",1,0,0,0,0,0
15690,15691,"Run, skeleton, run: skeletal model in a physics-based simulation","  In this paper, we present our approach to solve a physics-based reinforcement
learning challenge ""Learning to Run"" with objective to train
physiologically-based human model to navigate a complex obstacle course as
quickly as possible. The environment is computationally expensive, has a
high-dimensional continuous action space and is stochastic. We benchmark state
of the art policy-gradient methods and test several improvements, such as layer
normalization, parameter noise, action and state reflecting, to stabilize
training and improve its sample-efficiency. We found that the Deep
Deterministic Policy Gradient method is the most efficient method for this
environment and the improvements we have introduced help to stabilize training.
Learned models are able to generalize to new physical scenarios, e.g. different
obstacle courses.
",1,0,0,1,0,0
9376,9377,Placing your Coins on a Shelf,"  We consider the problem of packing a family of disks ""on a shelf"", that is,
such that each disk touches the $x$-axis from above and such that no two disks
overlap. We prove that the problem of minimizing the distance between the
leftmost point and the rightmost point of any disk is NP-hard. On the positive
side, we show how to approximate this problem within a factor of 4/3 in $O(n
\log n)$ time, and provide an $O(n \log n)$-time exact algorithm for a special
case, in particular when the ratio between the largest and smallest radius is
at most four.
",1,0,1,0,0,0
10135,10136,Detecting in-plane tension induced crystal plasticity transition with nanoindentation,"  We present experimental data and simulations on the effects of in-plane
tension on nanoindentation hardness and pop-in noise. Nanoindentation
experiments using a Berkovich tip are performed on bulk polycrystaline Al
samples, under tension in a custom 4pt-bending fixture. The hardness displays a
transition, for indentation depths smaller than 10nm, as function of the
in-plane stress at a value consistent with the bulk tensile yield stress.
Displacement bursts appear insensitive to in-plane tension and this transition
disappears for larger indentation depths. Two dimensional discrete dislocation
dynamics simulations confirm that a regime exists where hardness is sensitive
to tension-induced pre-existing dislocations.
",0,1,0,0,0,0
3688,3689,Finding Bottlenecks: Predicting Student Attrition with Unsupervised Classifier,"  With pressure to increase graduation rates and reduce time to degree in
higher education, it is important to identify at-risk students early. Automated
early warning systems are therefore highly desirable. In this paper, we use
unsupervised clustering techniques to predict the graduation status of declared
majors in five departments at California State University Northridge (CSUN),
based on a minimal number of lower division courses in each major. In addition,
we use the detected clusters to identify hidden bottleneck courses.
",1,0,0,1,0,0
11377,11378,Effect of annealing temperatures on the electrical conductivity and dielectric properties of Ni1.5Fe1.5O4 spinel ferrite prepared by chemical reaction at different pH values,"  The electrical conductivity and dielectric properties of Ni1.5Fe1.5O4 ferrite
has been controlled by varying the annealing temperature of the chemical routed
samples. The frequency activated conductivity obeyed Jonschers power law and
universal scaling suggested semiconductor nature. An unusual metal like state
has been revealed in the measurement temperature scale in between two
semiconductor states with different activation energy. The metal like state has
been affected by thermal annealing of the material. The analysis of electrical
impedance and modulus spectra has confirmed non-Debye dielectric relaxation
with contributions from grains and grain boundaries. The dielectric relaxation
process is thermally activated in terms of measurement temperature and
annealing temperature of the samples. The hole hopping process, due to presence
of Ni3+ ions in the present Ni rich ferrite, played a significant role in
determining the thermal activated conduction mechanism. This work has
successfully applied the technique of a combined variation of annealing
temperature and pH value during chemical reaction for tuning electrical
parameters in a wide range; for example dc limit of conductivity 10power(-4)
-10power(-12) S/cm, and unusually high activation energy 0.17-1.36 eV.
",0,1,0,0,0,0
3088,3089,Transport properties across the many-body localization transition in quasiperiodic and random systems,"  We theoretically study transport properties in one-dimensional interacting
quasiperiodic systems at infinite temperature. We compare and contrast the
dynamical transport properties across the many-body localization (MBL)
transition in quasiperiodic and random models. Using exact diagonalization we
compute the optical conductivity $\sigma(\omega)$ and the return probability
$R(\tau)$ and study their average low-frequency and long-time power-law
behavior, respectively. We show that the low-energy transport dynamics is
markedly distinct in both the thermal and MBL phases in quasiperiodic and
random models and find that the diffusive and MBL regimes of the quasiperiodic
model are more robust than those in the random system. Using the distribution
of the DC conductivity, we quantify the contribution of sample-to-sample and
state-to-state fluctuations of $\sigma(\omega)$ across the MBL transition. We
find that the activated dynamical scaling ansatz works poorly in the
quasiperiodic model but holds in the random model with an estimated activation
exponent $\psi\approx 0.9$. We argue that near the MBL transition in
quasiperiodic systems, critical eigenstates give rise to a subdiffusive
crossover regime on finite-size systems.
",0,1,0,0,0,0
19155,19156,"Existence of Stein Kernels under a Spectral Gap, and Discrepancy Bound","  We establish existence of Stein kernels for probability measures on
$\mathbb{R}^d$ satisfying a Poincaré inequality, and obtain bounds on the
Stein discrepancy of such measures. Applications to quantitative central limit
theorems are discussed, including a new CLT in Wasserstein distance $W_2$ with
optimal rate and dependence on the dimension. As a byproduct, we obtain a
stability version of an estimate of the Poincaré constant of probability
measures under a second moment constraint. The results extend more generally to
the setting of converse weighted Poincaré inequalities. The proof is based on
simple arguments of calculus of variations.
Further, we establish two general properties enjoyed by the Stein
discrepancy, holding whenever a Stein kernel exists: Stein discrepancy is
strictly decreasing along the CLT, and it controls the skewness of a random
vector.
",1,0,1,0,0,0
1115,1116,Bayesian Unification of Gradient and Bandit-based Learning for Accelerated Global Optimisation,"  Bandit based optimisation has a remarkable advantage over gradient based
approaches due to their global perspective, which eliminates the danger of
getting stuck at local optima. However, for continuous optimisation problems or
problems with a large number of actions, bandit based approaches can be
hindered by slow learning. Gradient based approaches, on the other hand,
navigate quickly in high-dimensional continuous spaces through local
optimisation, following the gradient in fine grained steps. Yet, apart from
being susceptible to local optima, these schemes are less suited for online
learning due to their reliance on extensive trial-and-error before the optimum
can be identified. In this paper, we propose a Bayesian approach that unifies
the above two paradigms in one single framework, with the aim of combining
their advantages. At the heart of our approach we find a stochastic linear
approximation of the function to be optimised, where both the gradient and
values of the function are explicitly captured. This allows us to learn from
both noisy function and gradient observations, and predict these properties
across the action space to support optimisation. We further propose an
accompanying bandit driven exploration scheme that uses Bayesian credible
bounds to trade off exploration against exploitation. Our empirical results
demonstrate that by unifying bandit and gradient based learning, one obtains
consistently improved performance across a wide spectrum of problem
environments. Furthermore, even when gradient feedback is unavailable, the
flexibility of our model, including gradient prediction, still allows us
outperform competing approaches, although with a smaller margin. Due to the
pervasiveness of bandit based optimisation, our scheme opens up for improved
performance both in meta-optimisation and in applications where gradient
related information is readily available.
",1,0,0,0,0,0
11544,11545,Carlsson's rank conjecture and a conjecture on square-zero upper triangular matrices,"  Let $k$ be an algebraically closed field and $A$ the polynomial algebra in
$r$ variables with coefficients in $k$. In case the characteristic of $k$ is
$2$, Carlsson conjectured that for any $DG$-$A$-module $M$ of dimension $N$ as
a free $A$-module, if the homology of $M$ is nontrivial and finite dimensional
as a $k$-vector space, then $2^r\leq N$. Here we state a stronger conjecture
about varieties of square-zero upper-triangular $N\times N$ matrices with
entries in $A$. Using stratifications of these varieties via Borel orbits, we
show that the stronger conjecture holds when $N < 8$ or $r < 3$ without any
restriction on the characteristic of $k$. As a consequence, we attain a new
proof for many of the known cases of Carlsson's conjecture and give new results
when $N > 4$ and $r = 2$.
",0,0,1,0,0,0
6755,6756,Invitation to Alexandrov geometry: CAT[0] spaces,"  The idea is to demonstrate the beauty and power of Alexandrov geometry by
reaching interesting applications with a minimum of preparation.
The topics include
1. Estimates on the number of collisions in billiards.
2. Construction of exotic aspherical manifolds.
3. The geometry of two-convex sets in Euclidean space.
",0,0,1,0,0,0
15668,15669,Adversarial Training Versus Weight Decay,"  Performance-critical machine learning models should be robust to input
perturbations not seen during training. Adversarial training is a method for
improving a model's robustness to some perturbations by including them in the
training process, but this tends to exacerbate other vulnerabilities of the
model. The adversarial training framework has the effect of translating the
data with respect to the cost function, while weight decay has a scaling
effect. Although weight decay could be considered a crude regularization
technique, it appears superior to adversarial training as it remains stable
over a broader range of regimes and reduces all generalization errors. Equipped
with these abstractions, we provide key baseline results and methodology for
characterizing robustness. The two approaches can be combined to yield one
small model that demonstrates good robustness to several white-box attacks
associated with different metrics.
",0,0,0,1,0,0
11908,11909,Regrasp Planning Considering Bipedal Stability Constraints,"  This paper presents a Center of Mass (CoM) based manipulation and regrasp
planner that implements stability constraints to preserve the robot balance.
The planner provides a graph of IK-feasible, collision-free and stable motion
sequences, constructed using an energy based motion planning algorithm. It
assures that the assembly motions are stable and prevent the robot from falling
while performing dexterous tasks in different situations. Furthermore, the
constraints are also used to perform an RRT-inspired task-related stability
estimation in several simulations. The estimation can be used to select between
single-arm and dual-arm regrasping configurations to achieve more stability and
robustness for a given manipulation task. To validate the planner and the
task-related stability estimations, several tests are performed in simulations
and real-world experiments involving the HRP5P humanoid robot, the 5th
generation of the HRP robot family. The experiment results suggest that the
planner and the task-related stability estimation provide robust behavior for
the humanoid robot while performing regrasp tasks.
",1,0,0,0,0,0
11693,11694,Approximation by generalized Kantorovich sampling type series,"  In the present article, we analyse the behaviour of a new family of
Kantorovich type sampling operators $(K_w^{\varphi}f)_{w>0}.$ First, we give a
Voronovskaya type theorem for these Kantorovich generalized sampling series and
a corresponding quantitative version in terms of the first order of modulus of
continuity. Further, we study the order of approximation in $C({\mathbb{R}})$
(the set of all uniformly continuous and bounded functions on ${\mathbb{R}}$)
for the family $(K_w^{\varphi}f)_{w>0}.$ Finally, we give some examples of
kernels such as B-spline kernels and Blackman-Harris kernel to which the theory
can be applied.
",0,0,1,0,0,0
11886,11887,Exploiting OxRAM Resistive Switching for Dynamic Range Improvement of CMOS Image Sensors,"  We present a unique application of OxRAM devices in CMOS Image Sensors (CIS)
for dynamic range (DR) improvement. We propose a modified 3T-APS (Active Pixel
Sensor) circuit that incorporates OxRAM in 1T-1R configuration. DR improvement
is achieved by resistive compression of the pixel output signal through
autonomous programming of OxRAM device resistance during exposure. We show that
by carefully preconditioning the OxRAM resistance, pixel DR can be enhanced.
Detailed impact of OxRAM SET-to-RESET and RESET-to-SET transitions on pixel DR
is discussed. For experimental validation with specific OxRAM preprogrammed
states, a 4 Kb 10 nm thick HfOx (1T-1R) matrix was fabricated and
characterized. Best case, relative pixel DR improvement of ~ 50 dB was obtained
for our design.
",1,0,0,0,0,0
18261,18262,Estimation in the convolution structure density model. Part I: oracle inequalities,"  We study the problem of nonparametric estimation under $\bL_p$-loss, $p\in
[1,\infty)$, in the framework of the convolution structure density model on
$\bR^d$. This observation scheme is a generalization of two classical
statistical models, namely density estimation under direct and indirect
observations. In Part I the original pointwise selection rule from a family of
""kernel-type"" estimators is proposed. For the selected estimator, we prove an
$\bL_p$-norm oracle inequality and several of its consequences. In Part II the
problem of adaptive minimax estimation under $\bL_p$--loss over the scale of
anisotropic Nikol'skii classes is addressed. We fully characterize the behavior
of the minimax risk for different relationships between regularity parameters
and norm indexes in the definitions of the functional class and of the risk. We
prove that the selection rule proposed in Part I leads to the construction of
an optimally or nearly optimally (up to logarithmic factor) adaptive estimator.
",0,0,1,1,0,0
12244,12245,A neural network trained to predict future video frames mimics critical properties of biological neuronal responses and perception,"  While deep neural networks take loose inspiration from neuroscience, it is an
open question how seriously to take the analogies between artificial deep
networks and biological neuronal systems. Interestingly, recent work has shown
that deep convolutional neural networks (CNNs) trained on large-scale image
recognition tasks can serve as strikingly good models for predicting the
responses of neurons in visual cortex to visual stimuli, suggesting that
analogies between artificial and biological neural networks may be more than
superficial. However, while CNNs capture key properties of the average
responses of cortical neurons, they fail to explain other properties of these
neurons. For one, CNNs typically require large quantities of labeled input data
for training. Our own brains, in contrast, rarely have access to this kind of
supervision, so to the extent that representations are similar between CNNs and
brains, this similarity must arise via different training paths. In addition,
neurons in visual cortex produce complex time-varying responses even to static
inputs, and they dynamically tune themselves to temporal regularities in the
visual environment. We argue that these differences are clues to fundamental
differences between the computations performed in the brain and in deep
networks. To begin to close the gap, here we study the emergent properties of a
previously-described recurrent generative network that is trained to predict
future video frames in a self-supervised manner. Remarkably, the model is able
to capture a wide variety of seemingly disparate phenomena observed in visual
cortex, ranging from single unit response dynamics to complex perceptual motion
illusions. These results suggest potentially deep connections between recurrent
predictive neural network models and the brain, providing new leads that can
enrich both fields.
",0,0,0,0,1,0
3760,3761,Advantages of versatile neural-network decoding for topological codes,"  Finding optimal correction of errors in generic stabilizer codes is a
computationally hard problem, even for simple noise models. While this task can
be simplified for codes with some structure, such as topological stabilizer
codes, developing good and efficient decoders still remains a challenge. In our
work, we systematically study a very versatile class of decoders based on
feedforward neural networks. To demonstrate adaptability, we apply neural
decoders to the triangular color and toric codes under various noise models
with realistic features, such as spatially-correlated errors. We report that
neural decoders provide significant improvement over leading efficient decoders
in terms of the error-correction threshold. Using neural networks simplifies
the process of designing well-performing decoders, and does not require prior
knowledge of the underlying noise model.
",0,0,0,1,0,0
18882,18883,Bug or Not? Bug Report Classification Using N-Gram IDF,"  Previous studies have found that a significant number of bug reports are
misclassified between bugs and non-bugs, and that manually classifying bug
reports is a time-consuming task. To address this problem, we propose a bug
reports classification model with N-gram IDF, a theoretical extension of
Inverse Document Frequency (IDF) for handling words and phrases of any length.
N-gram IDF enables us to extract key terms of any length from texts, these key
terms can be used as the features to classify bug reports. We build
classification models with logistic regression and random forest using features
from N-gram IDF and topic modeling, which is widely used in various software
engineering tasks. With a publicly available dataset, our results show that our
N-gram IDF-based models have a superior performance than the topic-based models
on all of the evaluated cases. Our models show promising results and have a
potential to be extended to other software engineering tasks.
",1,0,0,0,0,0
3611,3612,Novel Structured Low-rank algorithm to recover spatially smooth exponential image time series,"  We propose a structured low rank matrix completion algorithm to recover a
time series of images consisting of linear combination of exponential
parameters at every pixel, from under-sampled Fourier measurements. The spatial
smoothness of these parameters is exploited along with the exponential
structure of the time series at every pixel, to derive an annihilation relation
in the $k-t$ domain. This annihilation relation translates into a structured
low rank matrix formed from the $k-t$ samples. We demonstrate the algorithm in
the parameter mapping setting and show significant improvement over state of
the art methods.
",1,0,0,0,0,0
11647,11648,Dispersion for the wave equation outside a ball and counterexamples,"  The purpose of this note is to prove dispersive estimates for the wave
equation outside a ball in R^d. If d = 3, we show that the linear flow
satisfies the dispersive estimates as in R^3. In higher dimensions d $\ge$ 4 we
show that losses in dispersion do appear and this happens at the Poisson spot.
",0,0,1,0,0,0
15590,15591,Applications of noncommutative deformations,"  For a general class of contractions of a variety X to a base Y, I discuss
recent joint work with M. Wemyss defining a noncommutative enhancement of the
locus in Y over which the contraction is not an isomorphism, along with
applications to the derived symmetries of X. This note is based on a talk given
at the Kinosaki Symposium in 2016.
",0,0,1,0,0,0
9534,9535,On the coherent emission of radio frequency radiation from high energy particle showers,"  Extended Air Showers produced by cosmic rays impinging on the earth
atmosphere irradiate radio frequency radiation through different mechanisms.
Upon certain conditions, the emission has a coherent nature, with the
consequence that the emitted power is not proportional to the energy of the
primary cosmic rays, but to the energy squared. The effect was predicted in
1962 by Askaryan and it is nowadays experimentally well established and
exploited for the detection of ultra high energy cosmic rays.
In this paper we discuss in details the conditions for coherence, which in
literature have been too often taken for granted, and calculate them
analytically, finding a formulation which comprehends both the coherent and the
incoherent emissions. We apply the result to the Cherenkov effect, obtaining
the same conclusions derived by Askaryan, and to the geosynchrotron radiation.
",0,1,0,0,0,0
6602,6603,Quickest Change Detection under Transient Dynamics: Theory and Asymptotic Analysis,"  The problem of quickest change detection (QCD) under transient dynamics is
studied, where the change from the initial distribution to the final persistent
distribution does not happen instantaneously, but after a series of transient
phases. The observations within the different phases are generated by different
distributions. The objective is to detect the change as quickly as possible,
while controlling the average run length (ARL) to false alarm, when the
durations of the transient phases are completely unknown. Two algorithms are
considered, the dynamic Cumulative Sum (CuSum) algorithm, proposed in earlier
work, and a newly constructed weighted dynamic CuSum algorithm. Both algorithms
admit recursions that facilitate their practical implementation, and they are
adaptive to the unknown transient durations. Specifically, their asymptotic
optimality is established with respect to both Lorden's and Pollak's criteria
as the ARL to false alarm and the durations of the transient phases go to
infinity at any relative rate. Numerical results are provided to demonstrate
the adaptivity of the proposed algorithms, and to validate the theoretical
results.
",0,0,1,1,0,0
20151,20152,Deep Neural Networks as 0-1 Mixed Integer Linear Programs: A Feasibility Study,"  Deep Neural Networks (DNNs) are very popular these days, and are the subject
of a very intense investigation. A DNN is made by layers of internal units (or
neurons), each of which computes an affine combination of the output of the
units in the previous layer, applies a nonlinear operator, and outputs the
corresponding value (also known as activation). A commonly-used nonlinear
operator is the so-called rectified linear unit (ReLU), whose output is just
the maximum between its input value and zero. In this (and other similar cases
like max pooling, where the max operation involves more than one input value),
one can model the DNN as a 0-1 Mixed Integer Linear Program (0-1 MILP) where
the continuous variables correspond to the output values of each unit, and a
binary variable is associated with each ReLU to model its yes/no nature. In
this paper we discuss the peculiarity of this kind of 0-1 MILP models, and
describe an effective bound-tightening technique intended to ease its solution.
We also present possible applications of the 0-1 MILP model arising in feature
visualization and in the construction of adversarial examples. Preliminary
computational results are reported, aimed at investigating (on small DNNs) the
computational performance of a state-of-the-art MILP solver when applied to a
known test case, namely, hand-written digit recognition.
",1,0,0,0,0,0
10421,10422,Visualizing spreading phenomena on complex networks,"  Graph drawings are useful tools for exploring the structure and dynamics of
data that can be represented by pair-wise relationships among a set of objects.
Typical real-world social, biological or technological networks exhibit high
complexity resulting from a large number and broad heterogeneity of objects and
relationships. Thus, mapping these networks into a low-dimensional space to
visualize the dynamics of network-driven processes is a challenging task. Often
we want to analyze how a single node is influenced by or is influencing its
local network as the source of a spreading process. Here I present a network
layout algorithm for graphs with millions of nodes that visualizes spreading
phenomena from the perspective of a single node. The algorithm consists of
three stages to allow for an interactive graph exploration: First, a global
solution for the network layout is found in spherical space that minimizes
distance errors between all nodes. Second, a focal node is interactively
selected, and distances to this node are further optimized. Third, node
coordinates are mapped to a circular representation and drawn with additional
features to represent the network-driven phenomenon. The effectiveness and
scalability of this method are shown for a large collaboration network of
scientists, where we are interested in the citation dynamics around a focal
author.
",1,0,0,0,0,0
8578,8579,Variational Dropout Sparsifies Deep Neural Networks,"  We explore a recently proposed Variational Dropout technique that provided an
elegant Bayesian interpretation to Gaussian Dropout. We extend Variational
Dropout to the case when dropout rates are unbounded, propose a way to reduce
the variance of the gradient estimator and report first experimental results
with individual dropout rates per weight. Interestingly, it leads to extremely
sparse solutions both in fully-connected and convolutional layers. This effect
is similar to automatic relevance determination effect in empirical Bayes but
has a number of advantages. We reduce the number of parameters up to 280 times
on LeNet architectures and up to 68 times on VGG-like networks with a
negligible decrease of accuracy.
",1,0,0,1,0,0
1155,1156,Learning Interpretable Models with Causal Guarantees,"  Machine learning has shown much promise in helping improve the quality of
medical, legal, and economic decision-making. In these applications, machine
learning models must satisfy two important criteria: (i) they must be causal,
since the goal is typically to predict individual treatment effects, and (ii)
they must be interpretable, so that human decision makers can validate and
trust the model predictions. There has recently been much progress along each
direction independently, yet the state-of-the-art approaches are fundamentally
incompatible. We propose a framework for learning causal interpretable
models---from observational data---that can be used to predict individual
treatment effects. Our framework can be used with any algorithm for learning
interpretable models. Furthermore, we prove an error bound on the treatment
effects predicted by our model. Finally, in an experiment on real-world data,
we show that the models trained using our framework significantly outperform a
number of baselines.
",1,0,0,1,0,0
17057,17058,High quality mesh generation using cross and asterisk fields: Application on coastal domains,"  This paper presents a method to generate high quality triangular or
quadrilateral meshes that uses direction fields and a frontal point insertion
strategy. Two types of direction fields are considered: asterisk fields and
cross fields. With asterisk fields we generate high quality triangulations,
while with cross fields we generate right-angled triangulations that are
optimal for transformation to quadrilateral meshes. The input of our algorithm
is an initial triangular mesh and a direction field calculated on it. The goal
is to compute the vertices of the final mesh by an advancing front strategy
along the direction field. We present an algorithm that enables to efficiently
generate the points using solely information from the base mesh. A
multi-threaded implementation of our algorithm is presented, allowing us to
achieve significant speedup of the point generation. Regarding the
quadrangulation process, we develop a quality criterion for right-angled
triangles with respect to the local cross field and an optimization process
based on it. Thus we are able to further improve the quality of the output
quadrilaterals. The algorithm is demonstrated on the sphere and examples of
high quality triangular and quadrilateral meshes of coastal domains are
presented.
",1,0,0,0,0,0
8772,8773,Accelerated Extra-Gradient Descent: A Novel Accelerated First-Order Method,"  We provide a novel accelerated first-order method that achieves the
asymptotically optimal convergence rate for smooth functions in the first-order
oracle model. To this day, Nesterov's Accelerated Gradient Descent (AGD) and
variations thereof were the only methods achieving acceleration in this
standard blackbox model. In contrast, our algorithm is significantly different
from AGD, as it relies on a predictor-corrector approach similar to that used
by Mirror-Prox [Nemirovski, 2004] and Extra-Gradient Descent [Korpelevich,
1977] in the solution of convex-concave saddle point problems. For this reason,
we dub our algorithm Accelerated Extra-Gradient Descent (AXGD). Its
construction is motivated by the discretization of an accelerated
continuous-time dynamics [Krichene et al., 2015] using the classical method of
implicit Euler discretization. Our analysis explicitly shows the effects of
discretization through a conceptually novel primal-dual viewpoint. Moreover, we
show that the method is quite general: it attains optimal convergence rates for
other classes of objectives (e.g., those with generalized smoothness properties
or that are non-smooth and Lipschitz-continuous) using the appropriate choices
of step lengths. Finally, we present experiments showing that our algorithm
matches the performance of Nesterov's method, while appearing more robust to
noise in some cases.
",1,0,1,0,0,0
18025,18026,Gap and rings carved by vortices in protoplanetary dust,"  Large-scale vortices in protoplanetary disks are thought to form and survive
for long periods of time. Hence, they can significantly change the global disk
evolution and particularly the distribution of the solid particles embedded in
the gas, possibly explaining asymmetries and dust concentrations recently
observed at sub-millimeter and millimeter wavelengths. We investigate the
spatial distribution of dust grains using a simple model of protoplanetary disk
hosted by a giant gaseous vortex. We explore the dependence of the results on
grain size and deduce possible consequences and predictions for observations of
the dust thermal emission at sub-millimeter and millimeter wavelengths. Global
2D simulations with a bi-fluid code are used to follow the evolution of a
single population of solid particles aerodynamically coupled to the gas.
Possible observational signatures of the dust thermal emission are obtained
using simulators of ALMA and ngVLA observations. We find that a giant vortex
not only captures dust grains with Stokes number St < 1 but can also affect the
distribution of larger grains (with St '~' 1) carving a gap associated to a
ring composed of incompletely trapped particles. The results are presented for
different particle size and associated to their possible signatures in disk
observations. Gap clearing in the dust spatial distribution could be due to the
interaction with a giant gaseous vortex and their associated spiral waves,
without the gravitational assistance of a planet. Hence, strong dust
concentrations at short sub-mm wavelengths associated with a gap and an
irregular ring at longer mm and cm wavelengths could indicate the presence of
an unseen gaseous vortex.
",0,1,0,0,0,0
9457,9458,Large Sample Asymptotics of the Pseudo-Marginal Method,"  The pseudo-marginal algorithm is a variant of the Metropolis-Hastings
algorithm which samples asymptotically from a probability distribution when it
is only possible to estimate unbiasedly an unnormalized version of its density.
Practically, one has to trade-off the computational resources used to obtain
this estimator against the asymptotic variances of the ergodic averages
obtained by the pseudo-marginal algorithm. Recent works optimizing this
trade-off rely on some strong assumptions which can cast doubts over their
practical relevance. In particular, they all assume that the distribution of
the additive error in the log-likelihood estimator is independent of the
parameter value at which it is evaluated. Under weak regularity conditions we
show here that, as the number of data points tends to infinity, a
space-rescaled version of the pseudo-marginal chain converges weakly towards
another pseudo-marginal chain for which this assumption indeed holds. A study
of this limiting chain allows us to provide parameter dimension-dependent
guidelines on how to optimally scale a normal random walk proposal and the
number of Monte Carlo samples for the pseudo-marginal method in the large
sample regime. This complements and validates currently available results.
",0,0,0,1,0,0
3434,3435,Direct Multitype Cardiac Indices Estimation via Joint Representation and Regression Learning,"  Cardiac indices estimation is of great importance during identification and
diagnosis of cardiac disease in clinical routine. However, estimation of
multitype cardiac indices with consistently reliable and high accuracy is still
a great challenge due to the high variability of cardiac structures and
complexity of temporal dynamics in cardiac MR sequences. While efforts have
been devoted into cardiac volumes estimation through feature engineering
followed by a independent regression model, these methods suffer from the
vulnerable feature representation and incompatible regression model. In this
paper, we propose a semi-automated method for multitype cardiac indices
estimation. After manual labelling of two landmarks for ROI cropping, an
integrated deep neural network Indices-Net is designed to jointly learn the
representation and regression models. It comprises two tightly-coupled
networks: a deep convolution autoencoder (DCAE) for cardiac image
representation, and a multiple output convolution neural network (CNN) for
indices regression. Joint learning of the two networks effectively enhances the
expressiveness of image representation with respect to cardiac indices, and the
compatibility between image representation and indices regression, thus leading
to accurate and reliable estimations for all the cardiac indices.
When applied with five-fold cross validation on MR images of 145 subjects,
Indices-Net achieves consistently low estimation error for LV wall thicknesses
(1.44$\pm$0.71mm) and areas of cavity and myocardium (204$\pm$133mm$^2$). It
outperforms, with significant error reductions, segmentation method (55.1% and
17.4%) and two-phase direct volume-only methods (12.7% and 14.6%) for wall
thicknesses and areas, respectively. These advantages endow the proposed method
a great potential in clinical cardiac function assessment.
",1,0,0,0,0,0
1257,1258,Neural IR Meets Graph Embedding: A Ranking Model for Product Search,"  Recently, neural models for information retrieval are becoming increasingly
popular. They provide effective approaches for product search due to their
competitive advantages in semantic matching. However, it is challenging to use
graph-based features, though proved very useful in IR literature, in these
neural approaches. In this paper, we leverage the recent advances in graph
embedding techniques to enable neural retrieval models to exploit
graph-structured data for automatic feature extraction. The proposed approach
can not only help to overcome the long-tail problem of click-through data, but
also incorporate external heterogeneous information to improve search results.
Extensive experiments on a real-world e-commerce dataset demonstrate
significant improvement achieved by our proposed approach over multiple strong
baselines both as an individual retrieval model and as a feature used in
learning-to-rank frameworks.
",1,0,0,0,0,0
1401,1402,Transitions from a Kondo-like diamagnetic insulator into a modulated ferromagnetic metal in $\bm{\mathrm{FeGa}_{3-y}\mathrm{Ge}_y}$,"  One initial and essential question of magnetism is whether the magnetic
properties of a material are governed by localized moments or itinerant
electrons. Here we expose the case for the weakly ferromagnetic system
FeGa$_{3-y}$Ge$_y$ wherein these two opposite models are reconciled, such that
the magnetic susceptibility is quantitatively explained by taking into account
the effects of spin-spin correlation. With the electron doping introduced by Ge
substitution, the diamagnetic insulating parent compound FeGa$_3$ becomes a
paramagnetic metal as early as at $ y=0.01 $, and turns into a weakly
ferromagnetic metal around the quantum critical point $ y=0.15 $. Within the
ferromagnetic regime of FeGa$_{3-y}$Ge$_y$, the magnetic properties are of a
weakly itinerant ferromagnetic nature, located in the intermediate regime
between the localized and the itinerant dominance. Our analysis implies a
potential universality for all itinerant-electron ferromagnets.
",0,1,0,0,0,0
11112,11113,Symbolic Computation via Program Transformation,"  Symbolic computation is an important approach in automated program analysis.
Most state-of-the-art tools perform symbolic computation as interpreters and
directly maintain symbolic data. In this paper, we show that it is feasible,
and in fact practical, to use a compiler-based strategy instead. Using compiler
tooling, we propose and implement a transformation which takes a standard
program and outputs a program that performs semantically equivalent, but
partially symbolic, computation. The transformed program maintains symbolic
values internally and operates directly on them hence the program can be
processed by a tool without support for symbolic manipulation.
The main motivation for the transformation is in symbolic verification, but
there are many other possible use-cases, including test generation and concolic
testing. Moreover using the transformation simplifies tools, since the symbolic
computation is handled by the program directly. We have implemented the
transformation at the level of LLVM bitcode. The paper includes an experimental
evaluation, based on an explicit-state software model checker as a verification
backend.
",1,0,0,0,0,0
4127,4128,Multi-Entity Dependence Learning with Rich Context via Conditional Variational Auto-encoder,"  Multi-Entity Dependence Learning (MEDL) explores conditional correlations
among multiple entities. The availability of rich contextual information
requires a nimble learning scheme that tightly integrates with deep neural
networks and has the ability to capture correlation structures among
exponentially many outcomes. We propose MEDL_CVAE, which encodes a conditional
multivariate distribution as a generating process. As a result, the variational
lower bound of the joint likelihood can be optimized via a conditional
variational auto-encoder and trained end-to-end on GPUs. Our MEDL_CVAE was
motivated by two real-world applications in computational sustainability: one
studies the spatial correlation among multiple bird species using the eBird
data and the other models multi-dimensional landscape composition and human
footprint in the Amazon rainforest with satellite images. We show that
MEDL_CVAE captures rich dependency structures, scales better than previous
methods, and further improves on the joint likelihood taking advantage of very
large datasets that are beyond the capacity of previous methods.
",1,0,0,1,0,0
17122,17123,Spatial Models of Vector-Host Epidemics with Directed Movement of Vectors Over Long Distances,"  We investigate a time-dependent spatial vector-host epidemic model with
non-coincident domains for the vector and host populations. The host population
resides in small non-overlapping sub-regions, while the vector population
resides throughout a much larger region. The dynamics of the populations are
modeled by a reaction-diffusion-advection compartmental system of partial
differential equations. The disease is transmitted through vector and host
populations in criss-cross fashion. We establish global well-posedness and
uniform a prior bounds as well as the long-term behavior. The model is applied
to simulate the outbreak of bluetongue disease in sheep transmitted by midges
infected with bluetongue virus. We show that the long-range directed movement
of the midge population, due to wind-aided movement, enhances the transmission
of the disease to sheep in distant sites.
",0,0,0,0,1,0
12980,12981,Inflationary $α$-attractor cosmology: A global dynamical systems perspective,"  We study flat FLRW $\alpha$-attractor $\mathrm{E}$- and $\mathrm{T}$-models
by introducing a dynamical systems framework that yields regularized
unconstrained field equations on two-dimensional compact state spaces. This
results in both illustrative figures and a complete description of the entire
solution spaces of these models, including asymptotics. In particular, it is
shown that observational viability, which requires a sufficient number of
$e$-folds, is associated with a solution given by a one-dimensional center
manifold of a past asymptotic de Sitter state, where the center manifold
structure also explains why nearby solutions are attracted to this
`inflationary attractor solution.' A center manifold expansion yields a
description of the inflationary regime with arbitrary analytic accuracy, where
the slow-roll approximation asymptotically describes the tangency condition of
the center manifold at the asymptotic de Sitter state.
",0,1,0,0,0,0
20917,20918,Land Cover Classification via Multi-temporal Spatial Data by Recurrent Neural Networks,"  Nowadays, modern earth observation programs produce huge volumes of satellite
images time series (SITS) that can be useful to monitor geographical areas
through time. How to efficiently analyze such kind of information is still an
open question in the remote sensing field. Recently, deep learning methods
proved suitable to deal with remote sensing data mainly for scene
classification (i.e. Convolutional Neural Networks - CNNs - on single images)
while only very few studies exist involving temporal deep learning approaches
(i.e Recurrent Neural Networks - RNNs) to deal with remote sensing time series.
In this letter we evaluate the ability of Recurrent Neural Networks, in
particular the Long-Short Term Memory (LSTM) model, to perform land cover
classification considering multi-temporal spatial data derived from a time
series of satellite images. We carried out experiments on two different
datasets considering both pixel-based and object-based classification. The
obtained results show that Recurrent Neural Networks are competitive compared
to state-of-the-art classifiers, and may outperform classical approaches in
presence of low represented and/or highly mixed classes. We also show that
using the alternative feature representation generated by LSTM can improve the
performances of standard classifiers.
",1,0,0,0,0,0
5553,5554,Adaptive p-value weighting with power optimality,"  Weighting the p-values is a well-established strategy that improves the power
of multiple testing procedures while dealing with heterogeneous data. However,
how to achieve this task in an optimal way is rarely considered in the
literature. This paper contributes to fill the gap in the case of
group-structured null hypotheses, by introducing a new class of procedures
named ADDOW (for Adaptive Data Driven Optimal Weighting) that adapts both to
the alternative distribution and to the proportion of true null hypotheses. We
prove the asymptotical FDR control and power optimality among all weighted
procedures of ADDOW, which shows that it dominates all existing procedures in
that framework. Some numerical experiments show that the proposed method
preserves its optimal properties in the finite sample setting when the number
of tests is moderately large.
",0,0,1,1,0,0
2842,2843,Nilpotence order growth of recursion operators in characteristic p,"  We prove that the killing rate of certain degree-lowering ""recursion
operators"" on a polynomial algebra over a finite field grows slower than
linearly in the degree of the polynomial attacked. We also explain the
motivating application: obtaining a lower bound for the Krull dimension of a
local component of a big mod-p Hecke algebra in the genus-zero case. We sketch
the application for p=2 and p=3 in level one. The case p=2 was first
established in by Nicolas and Serre in 2012 using different methods.
",0,0,1,0,0,0
3886,3887,Life-span of blowup solutions to semilinear wave equation with space-dependent critical damping,"  This paper is concerned with the blowup phenomena for initial value problem
of semilinear wave equation with critical space-dependent damping term
(DW:$V$). The main result of the present paper is to give a solution of the
problem and to provide a sharp estimate for lifespan for such a solution when
$\frac{N}{N-1}<p\leq p_S(N+V_0)$, where $p_S(N)$ is the Strauss exponent for
(DW:$0$). The main idea of the proof is due to the technique of test functions
for (DW:$0$) originated by Zhou--Han (2014, MR3169791). Moreover, we find a new
threshold value $V_0=\frac{(N-1)^2}{N+1}$ for the coefficient of critical and
singular damping $|x|^{-1}$.
",0,0,1,0,0,0
12237,12238,Weak subsolutions to complex Monge-Ampère equations,"  We compare various notions of weak subsolutions to degenerate complex
Monge-Ampère equations, showing that they all coincide. This allows us to
give an alternative proof of mixed Monge-Ampère inequalities due to Kolodziej
and Dinew.
",0,0,1,0,0,0
14156,14157,A Short Note on Almost Sure Convergence of Bayes Factors in the General Set-Up,"  Although there is a significant literature on the asymptotic theory of Bayes
factor, the set-ups considered are usually specialized and often involves
independent and identically distributed data. Even in such specialized cases,
mostly weak consistency results are available. In this article, for the first
time ever, we derive the almost sure convergence theory of Bayes factor in the
general set-up that includes even dependent data and misspecified models.
Somewhat surprisingly, the key to the proof of such a general theory is a
simple application of a result of Shalizi (2009) to a well-known identity
satisfied by the Bayes factor.
",0,0,1,1,0,0
9606,9607,Learning with Correntropy-induced Losses for Regression with Mixture of Symmetric Stable Noise,"  In recent years, correntropy and its applications in machine learning have
been drawing continuous attention owing to its merits in dealing with
non-Gaussian noise and outliers. However, theoretical understanding of
correntropy, especially in the statistical learning context, is still limited.
In this study, within the statistical learning framework, we investigate
correntropy based regression in the presence of non-Gaussian noise or outliers.
Motivated by the practical way of generating non-Gaussian noise or outliers, we
introduce mixture of symmetric stable noise, which include Gaussian noise,
Cauchy noise, and their mixture as special cases, to model non-Gaussian noise
or outliers. We demonstrate that under the mixture of symmetric stable noise
assumption, correntropy based regression can learn the conditional mean
function or the conditional median function well without resorting to the
finite-variance or even the finite first-order moment condition on the noise.
In particular, for the above two cases, we establish asymptotic optimal
learning rates for correntropy based regression estimators that are
asymptotically of type $\mathcal{O}(n^{-1})$. These results justify the
effectiveness of the correntropy based regression estimators in dealing with
outliers as well as non-Gaussian noise. We believe that the present study
completes our understanding towards correntropy based regression from a
statistical learning viewpoint, and may also shed some light on robust
statistical learning for regression.
",0,0,0,1,0,0
15236,15237,An Efficient Keyless Fragmentation Algorithm for Data Protection,"  The family of Information Dispersal Algorithms is applied to distributed
systems for secure and reliable storage and transmission. In comparison with
perfect secret sharing it achieves a significantly smaller memory overhead and
better performance, but provides only incremental confidentiality. Therefore,
even if it is not possible to explicitly reconstruct data from less than the
required amount of fragments, it is still possible to deduce some information
about the nature of data by looking at preserved data patterns inside a
fragment. The idea behind this paper is to provide a lightweight data
fragmentation scheme, that would combine the space efficiency and simplicity
that could be find in Information Dispersal Algorithms with a computational
level of data confidentiality.
",1,0,0,0,0,0
18988,18989,Spot dynamics in a reaction-diffusion model of plant root hair initiation,"  We study pattern formation in a 2-D reaction-diffusion (RD) sub-cellular
model characterizing the effect of a spatial gradient of a plant hormone
distribution on a family of G-proteins associated with root-hair (RH)
initiation in the plant cell Arabidopsis thaliana. The activation of these
G-proteins, known as the Rho of Plants (ROPs), by the plant hormone auxin, is
known to promote certain protuberances on root hair cells, which are crucial
for both anchorage and the uptake of nutrients from the soil. Our mathematical
model for the activation of ROPs by the auxin gradient is an extension of the
model of Payne and Grierson [PLoS ONE, 12(4), (2009)], and consists of a
two-component Schnakenberg-type RD system with spatially heterogeneous
coefficients on a 2-D domain. The nonlinear kinetics in this RD system model
the nonlinear interactions between the active and inactive forms of ROPs. By
using a singular perturbation analysis to study 2-D localized spatial patterns
of active ROPs, it is shown that the spatial variations in the nonlinear
reaction kinetics, due to the auxin gradient, lead to a slow spatial alignment
of the localized regions of active ROPs along the longitudinal midline of the
plant cell. Numerical bifurcation analysis, together with time-dependent
numerical simulations of the RD system are used to illustrate both 2-D
localized patterns in the model, and the spatial alignment of localized
structures.
",0,1,0,0,0,0
11755,11756,Relativistic wide-angle galaxy bispectrum on the light-cone,"  Given the important role that the galaxy bispectrum has recently acquired in
cosmology and the scale and precision of forthcoming galaxy clustering
observations, it is timely to derive the full expression of the large-scale
bispectrum going beyond approximated treatments which neglect integrated terms
or higher-order bias terms or use the Limber approximation. On cosmological
scales, relativistic effects that arise from observing on the past light-cone
alter the observed galaxy number counts, therefore leaving their imprints on
N-point correlators at all orders. In this paper we compute for the first time
the bispectrum including all general relativistic, local and integrated,
effects at second order, the tracers' bias at second order, geometric effects
as well as the primordial non-Gaussianity contribution. This is timely
considering that future surveys will probe scales comparable to the horizon
where approximations widely used currently may not hold; neglecting these
effects may introduce biases in estimation of cosmological parameters as well
as primordial non-Gaussianity.
",0,1,0,0,0,0
6406,6407,The role of relativistic many-body theory in probing new physics beyond the standard model via the electric dipole moments of diamagnetic atoms,"  The observation of electric dipole moments (EDMs) in atomic systems due to
parity and time-reversal violating (P,T-odd) interactions can probe new physics
beyond the standard model and also provide insights into the matter-antimatter
asymmetry in the Universe. The EDMs of open-shell atomic systems are sensitive
to the electron EDM and the P,T-odd scalar-pseudoscalar (S-PS) semi-leptonic
interaction, but the dominant contributions to the EDMs of diamagnetic atoms
come from the hadronic and tensor-pseudotensor (T-PT) semi-leptonic
interactions. Several diamagnetic atoms like $^{129}$Xe, $^{171}$Yb,
$^{199}$Hg, $^{223}$Rn, and $^{225}$Ra are candidates for the experimental
search for the possible existence of EDMs, and among these $^{199}$Hg has
yielded the lowest limit till date. The T or CP violating coupling constants of
the aforementioned interactions can be extracted from these measurements by
combining with atomic and nuclear calculations. In this work, we report the
calculations of the EDMs of the above atoms by including both the
electromagnetic and P,T-odd violating interactions simultaneously. These
calculations are performed by employing relativistic many-body methods based on
the random phase approximation (RPA) and the singles and doubles
coupled-cluster (CCSD) method starting with the Dirac-Hartree-Fock (DHF) wave
function in both cases. The differences in the results from both the methods
shed light on the importance of the non-core-polarization electron correlation
effects that are accounted for by the CCSD method. We also determine electric
dipole polarizabilities of these atoms, which have computational similarities
with EDMs and compare them with the available experimental and other
theoretical results to assess the accuracy of our calculations.
",0,1,0,0,0,0
14341,14342,Recovering Sparse Nonnegative Signals via Non-convex Fraction Function Penalty,"  Many real world practical problems can be formulated as
$\ell_{0}$-minimization problems with nonnegativity constraints, which seek the
sparsest nonnegative signals to underdetermined linear systems. They have been
widely applied in signal and image processing, machine learning, pattern
recognition and computer vision. Unfortunately, this $\ell_{0}$-minimization
problem with nonnegativity constraint is computational and NP-hard because of
the discrete and discontinuous nature of the $\ell_{0}$-norm. In this paper, we
replace the $\ell_{0}$-norm with a non-convex fraction function, and study the
minimization problem of this non-convex fraction function in recovering the
sparse nonnegative signals from an underdetermined linear system. Firstly, we
discuss the equivalence between $(P_{0}^{\geq})$ and $(FP_{a}^{\geq})$, and the
equivalence between $(FP_{a}^{\geq})$ and $(FP_{a,\lambda}^{\geq})$. It is
proved that the optimal solution of the problem $(P_{0}^{\geq})$ could be
approximately obtained by solving the regularization problem
$(FP_{a,\lambda}^{\geq})$ if some specific conditions satisfied. Secondly, we
propose a nonnegative iterative thresholding algorithm to solve the
regularization problem $(FP_{a,\lambda}^{\geq})$ for all $a>0$. Finally, some
numerical experiments on sparse nonnegative siganl recovery problems show that
our method performs effective in finding sparse nonnegative signals compared
with the linear programming.
",0,0,1,0,0,0
1924,1925,Compiling Deep Learning Models for Custom Hardware Accelerators,"  Convolutional neural networks (CNNs) are the core of most state-of-the-art
deep learning algorithms specialized for object detection and classification.
CNNs are both computationally complex and embarrassingly parallel. Two
properties that leave room for potential software and hardware optimizations
for embedded systems. Given a programmable hardware accelerator with a CNN
oriented custom instructions set, the compiler's task is to exploit the
hardware's full potential, while abiding with the hardware constraints and
maintaining generality to run different CNN models with varying workload
properties. Snowflake is an efficient and scalable hardware accelerator
implemented on programmable logic devices. It implements a control pipeline for
a custom instruction set. The goal of this paper is to present Snowflake's
compiler that generates machine level instructions from Torch7 model
description files. The main software design points explored in this work are:
model structure parsing, CNN workload breakdown, loop rearrangement for memory
bandwidth optimizations and memory access balancing. The performance achieved
by compiler generated instructions matches against hand optimized code for
convolution layers. Generated instructions also efficiently execute AlexNet and
ResNet18 inference on Snowflake. Snowflake with $256$ processing units was
synthesized on Xilinx's Zynq XC7Z045 FPGA. At $250$ MHz, AlexNet achieved in
$93.6$ frames/s and $1.2$ GB/s of off-chip memory bandwidth, and $21.4$
frames/s and $2.2$ GB/s for ResNet18. Total on-chip power is $5$ W.
",1,0,0,0,0,0
6935,6936,Improved upper bounds in the moving sofa problem,"  The moving sofa problem, posed by L. Moser in 1966, asks for the planar shape
of maximal area that can move around a right-angled corner in a hallway of unit
width. It is known that a maximal area shape exists, and that its area is at
least 2.2195... - the area of an explicit construction found by Gerver in 1992
- and at most $2\sqrt{2}=2.82...$, with the lower bound being conjectured as
the true value. We prove a new and improved upper bound of 2.37. The method
involves a computer-assisted proof scheme that can be used to rigorously derive
further improved upper bounds that converge to the correct value.
",0,0,1,0,0,0
4913,4914,"An Adaptive, Multivariate Partitioning Algorithm for Global Optimization of Nonconvex Programs","  In this work, we develop an adaptive, multivariate partitioning algorithm for
solving mixed-integer nonlinear programs (MINLP) with multi-linear terms to
global optimality. This iterative algorithm primarily exploits the advantages
of piecewise polyhedral relaxation approaches via disjunctive formulations to
solve MINLPs to global optimality in contrast to the conventional spatial
branch-and-bound approaches. In order to maintain relatively small-scale
mixed-integer linear programs at every iteration of the algorithm, we
adaptively partition the variable domains appearing in the multi-linear terms.
We also provide proofs on convergence guarantees of the proposed algorithm to a
global solution. Further, we discuss a few algorithmic enhancements based on
the sequential bound-tightening procedure as a presolve step, where we observe
the importance of solving piecewise relaxations compared to basic convex
relaxations to speed-up the convergence of the algorithm to global optimality.
We demonstrate the effectiveness of our disjunctive formulations and the
algorithm on well-known benchmark problems (including Pooling and Blending
instances) from MINLPLib and compare with state-of-the-art global optimization
solvers. With this novel approach, we solve several large-scale instances which
are, in some cases, intractable by the global optimization solver. We also
shrink the best known optimality gap for one of the hard, generalized pooling
problem instance.
",1,0,1,0,0,0
19506,19507,Gamma factors of intertwining periods and distinction for inner forms of $\GL(n)$,"  Let $F$ be a $p$-adic fied, $E$ be a quadratic extension of $F$, and $D$ be
an $F$-division algebra of odd index. Set $H=\mathrm{GL}m,D)$ and
$G=\mathrm{GL}(m,D\otimes_F E)$, we carry out a fine study of local
intertwining open periods attached to $H$-distinguished induced representations
of inner forms of $G$. These objects have been studied globally in \cite{JLR}
and \cite{LR}, and locally in \cite{BD08}. Here we give sufficient conditions
for the local intertwining periods to have singularities. By a local/global
method, we also compute in terms of Asai gamma factors the proportionality
constants involved in their functional equations with respect to certain
intertwining operators. As a consequence, we classify distinguished unitary and
ladder representations of $G$, extending respectively the results of \cite{M14}
and \cite{G15} for $D=F$, which both relied at some crucial step on the theory
of Bernstein-Zelevinsky derivatives. We make use of one of the main results of
\cite{BP17} in our setting, which in the case of the group $G$, asserts that
the Jacquet-Langlands correspondence preserves distinction. Such a result is
for discrete series representations, but our method in fact allows us to use it
only for cuspidal representations of $G$.
",0,0,1,0,0,0
3510,3511,Efficient Decision Trees for Multi-class Support Vector Machines Using Entropy and Generalization Error Estimation,"  We propose new methods for Support Vector Machines (SVMs) using tree
architecture for multi-class classi- fication. In each node of the tree, we
select an appropriate binary classifier using entropy and generalization error
estimation, then group the examples into positive and negative classes based on
the selected classi- fier and train a new classifier for use in the
classification phase. The proposed methods can work in time complexity between
O(log2N) to O(N) where N is the number of classes. We compared the performance
of our proposed methods to the traditional techniques on the UCI machine
learning repository using 10-fold cross-validation. The experimental results
show that our proposed methods are very useful for the problems that need fast
classification time or problems with a large number of classes as the proposed
methods run much faster than the traditional techniques but still provide
comparable accuracy.
",1,0,0,1,0,0
1892,1893,Segmentation of Instances by Hashing,"  We propose a novel approach to address the Simultaneous Detection and
Segmentation problem. Using hierarchical structures we use an efficient and
accurate procedure that exploits the hierarchy feature information using
Locality Sensitive Hashing. We build on recent work that utilizes convolutional
neural networks to detect bounding boxes in an image and then use the top
similar hierarchical region that best fits each bounding box after hashing, we
call this approach CZ Segmentation. We then refine our final segmentation
results by automatic hierarchy pruning. CZ Segmentation introduces a train-free
alternative to Hypercolumns. We conduct extensive experiments on PASCAL VOC
2012 segmentation dataset, showing that CZ gives competitive state-of-the-art
object segmentations.
",1,0,0,0,0,0
8630,8631,On the Performance of Millimeter Wave-based RF-FSO Multi-hop and Mesh Networks,"  This paper studies the performance of multi-hop and mesh networks composed of
millimeter wave (MMW)-based radio frequency (RF) and free-space optical (FSO)
links. The results are obtained in cases with and without hybrid automatic
repeat request (HARQ). Taking the MMW characteristics of the RF links into
account, we derive closed-form expressions for the networks' outage probability
and ergodic achievable rates. We also evaluate the effect of various parameters
such as power amplifiers efficiency, number of antennas as well as different
coherence times of the RF and the FSO links on the system performance. Finally,
we determine the minimum number of the transmit antennas in the RF link such
that the same rate is supported in the RF- and the FSO-based hops. The results
show the efficiency of the RF-FSO setups in different conditions. Moreover,
HARQ can effectively improve the outage probability/energy efficiency, and
compensate for the effect of hardware impairments in RF-FSO networks. For
common parameter settings of the RF-FSO dual-hop networks, outage probability
of 10^{-4} and code rate of 3 nats-per-channel-use, the implementation of HARQ
with a maximum of 2 and 3 retransmissions reduces the required power, compared
to cases with open-loop communication, by 13 and 17 dB, respectively.
",1,0,0,0,0,0
1646,1647,Dynamic nested sampling: an improved algorithm for parameter estimation and evidence calculation,"  We introduce dynamic nested sampling: a generalisation of the nested sampling
algorithm in which the number of ""live points"" varies to allocate samples more
efficiently. In empirical tests the new method significantly improves
calculation accuracy compared to standard nested sampling with the same number
of samples; this increase in accuracy is equivalent to speeding up the
computation by factors of up to ~72 for parameter estimation and ~7 for
evidence calculations. We also show that the accuracy of both parameter
estimation and evidence calculations can be improved simultaneously. In
addition, unlike in standard nested sampling, more accurate results can be
obtained by continuing the calculation for longer. Popular standard nested
sampling implementations can be easily adapted to perform dynamic nested
sampling, and several dynamic nested sampling software packages are now
publicly available.
",0,1,0,1,0,0
760,761,Revealing Hidden Potentials of the q-Space Signal in Breast Cancer,"  Mammography screening for early detection of breast lesions currently suffers
from high amounts of false positive findings, which result in unnecessary
invasive biopsies. Diffusion-weighted MR images (DWI) can help to reduce many
of these false-positive findings prior to biopsy. Current approaches estimate
tissue properties by means of quantitative parameters taken from generative,
biophysical models fit to the q-space encoded signal under certain assumptions
regarding noise and spatial homogeneity. This process is prone to fitting
instability and partial information loss due to model simplicity. We reveal
unexplored potentials of the signal by integrating all data processing
components into a convolutional neural network (CNN) architecture that is
designed to propagate clinical target information down to the raw input images.
This approach enables simultaneous and target-specific optimization of image
normalization, signal exploitation, global representation learning and
classification. Using a multicentric data set of 222 patients, we demonstrate
that our approach significantly improves clinical decision making with respect
to the current state of the art.
",1,0,0,0,0,0
6172,6173,Distribution of the periodic points of the Farey map,"  We expand the cross section of the geodesic flow in the tangent bundle of the
modular surface given by Series to produce another section whose return map
under the geodesic flow is a double cover of the natural extension of the Farey
map. We use this cross section to extend the correspondence between the closed
geodesics on the modular surface and the periodic points of the Gauss map to
include the periodic points of the Farey map. Then, analogous to the work of
Pollicott, we prove an equidistribution result for the periodic points of the
Farey map when they are ordered according to the length of their corresponding
closed geodesics.
",0,0,1,0,0,0
3031,3032,Rigidity of volume-minimizing hypersurfaces in Riemannian 5-manifolds,"  In this paper we generalize the main result of [4] for manifolds that are not
necessarily Einstein. In fact, we obtain an upper bound for the volume of a
locally volume-minimizing closed hypersurface $\Sigma$ of a Riemannian
5-manifold $M$ with scalar curvature bounded from below by a positive constant
in terms of the total traceless Ricci curvature of $\Sigma$. Furthermore, if
$\Sigma$ saturates the respective upper bound and $M$ has nonnegative Ricci
curvature, then $\Sigma$ is isometric to $\mathbb{S}^4$ up to scaling and $M$
splits in a neighborhood of $\Sigma$. Also, we obtain a rigidity result for the
Riemannian cover of $M$ when $\Sigma$ minimizes the volume in its homotopy
class and saturates the upper bound.
",0,0,1,0,0,0
2525,2526,Bose-Hubbard lattice as a controllable environment for open quantum systems,"  We investigate the open dynamics of an atomic impurity embedded in a
one-dimensional Bose-Hubbard lattice. We derive the reduced evolution equation
for the impurity and show that the Bose-Hubbard lattice behaves as a tunable
engineered environment allowing to simulate both Markovian and non-Markovian
dynamics in a controlled and experimentally realisable way. We demonstrate that
the presence or absence of memory effects is a signature of the nature of the
excitations induced by the impurity, being delocalized or localized in the two
limiting cases of superfluid and Mott insulator, respectively. Furthermore, our
findings show how the excitations supported in the two phases can be
characterized as information carriers.
",0,1,0,0,0,0
5761,5762,"A representation theorem for stochastic processes with separable covariance functions, and its implications for emulation","  Many applications require stochastic processes specified on two- or
higher-dimensional domains; spatial or spatial-temporal modelling, for example.
In these applications it is attractive, for conceptual simplicity and
computational tractability, to propose a covariance function that is separable;
e.g., the product of a covariance function in space and one in time. This paper
presents a representation theorem for such a proposal, and shows that all
processes with continuous separable covariance functions are second-order
identical to the product of second-order uncorrelated processes. It discusses
the implications of separable or nearly separable prior covariances for the
statistical emulation of complicated functions such as computer codes, and
critically reexamines the conventional wisdom concerning emulator structure,
and size of design.
",0,0,1,1,0,0
13642,13643,Exact Formulas for the Generalized Sum-of-Divisors Functions,"  We prove new exact formulas for the generalized sum-of-divisors functions.
The formulas for $\sigma_{\alpha}(x)$ when $\alpha \in \mathbb{C}$ is fixed and
$x \geq 1$ involves a finite sum over all of the prime factors $n \leq x$ and
terms involving the $r$-order harmonic number sequences. The generalized
harmonic number sequences correspond to the partial sums of the Riemann zeta
function when $r > 1$ and are related to the generalized Bernoulli numbers when
$r \leq 0$ is integer-valued. A key part of our expansions of the Lambert
series generating functions for the generalized divisor functions is formed by
taking logarithmic derivatives of the cyclotomic polynomials, $\Phi_n(q)$,
which completely factorize the Lambert series terms $(1-q^n)^{-1}$ into
irreducible polynomials in $q$. We also consider applications of our new
results to asymptotic approximations for sums over these divisor functions and
to the forms of perfect numbers defined by the special case of the divisor
function, $\sigma(n)$, when $\alpha := 1$.
Keywords: divisor function; sum-of-divisors function; Lambert series; perfect
number.
MSC (2010): 30B50; 11N64; 11B83
",0,0,1,0,0,0
18062,18063,The Weisfeiler-Leman algorithm and the diameter of Schreier graphs,"  We prove that the number of iterations taken by the Weisfeiler-Leman
algorithm for configurations coming from Schreier graphs is closely linked to
the diameter of the graphs themselves: an upper bound is found for general
Schreier graphs, and a lower bound holds for particular cases, such as for
Schreier graphs with $G=\mbox{SL}_{n}({\mathbb F}_{q})$ ($q>2$) acting on
$k$-tuples of vectors in ${\mathbb F}_{q}^{n}$; moreover, an exact expression
is found in the case of Cayley graphs.
",0,0,1,0,0,0
11209,11210,On Convergence of Extended Dynamic Mode Decomposition to the Koopman Operator,"  Extended Dynamic Mode Decomposition (EDMD) is an algorithm that approximates
the action of the Koopman operator on an $N$-dimensional subspace of the space
of observables by sampling at $M$ points in the state space. Assuming that the
samples are drawn either independently or ergodically from some measure $\mu$,
it was shown that, in the limit as $M\rightarrow\infty$, the EDMD operator
$\mathcal{K}_{N,M}$ converges to $\mathcal{K}_N$, where $\mathcal{K}_N$ is the
$L_2(\mu)$-orthogonal projection of the action of the Koopman operator on the
finite-dimensional subspace of observables. In this work, we show that, as $N
\rightarrow \infty$, the operator $\mathcal{K}_N$ converges in the strong
operator topology to the Koopman operator. This in particular implies
convergence of the predictions of future values of a given observable over any
finite time horizon, a fact important for practical applications such as
forecasting, estimation and control. In addition, we show that accumulation
points of the spectra of $\mathcal{K}_N$ correspond to the eigenvalues of the
Koopman operator with the associated eigenfunctions converging weakly to an
eigenfunction of the Koopman operator, provided that the weak limit of
eigenfunctions is nonzero. As a by-product, we propose an analytic version of
the EDMD algorithm which, under some assumptions, allows one to construct
$\mathcal{K}_N$ directly, without the use of sampling. Finally, under
additional assumptions, we analyze convergence of $\mathcal{K}_{N,N}$ (i.e.,
$M=N$), proving convergence, along a subsequence, to weak eigenfunctions (or
eigendistributions) related to the eigenmeasures of the Perron-Frobenius
operator. No assumptions on the observables belonging to a finite-dimensional
invariant subspace of the Koopman operator are required throughout.
",0,0,1,0,0,0
19886,19887,Inverse problems for the wave equation with under-determined data,"  We consider the inverse problems of determining the potential or the damping
coefficient appearing in the wave equation. We will prove the unique
determination of these coefficients from the one point measurement. Since our
problem is under-determined, so some extra assumption on the coefficients is
required to prove the uniqueness.
",0,0,1,0,0,0
2549,2550,Typesafe Abstractions for Tensor Operations,"  We propose a typesafe abstraction to tensors (i.e. multidimensional arrays)
exploiting the type-level programming capabilities of Scala through
heterogeneous lists (HList), and showcase typesafe abstractions of common
tensor operations and various neural layers such as convolution or recurrent
neural networks. This abstraction could lay the foundation of future typesafe
deep learning frameworks that runs on Scala/JVM.
",1,0,0,0,0,0
11906,11907,"Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour","  Deep learning thrives with large neural networks and large datasets. However,
larger networks and larger datasets result in longer training times that impede
research and development progress. Distributed synchronous SGD offers a
potential solution to this problem by dividing SGD minibatches over a pool of
parallel workers. Yet to make this scheme efficient, the per-worker workload
must be large, which implies nontrivial growth in the SGD minibatch size. In
this paper, we empirically show that on the ImageNet dataset large minibatches
cause optimization difficulties, but when these are addressed the trained
networks exhibit good generalization. Specifically, we show no loss of accuracy
when training with large minibatch sizes up to 8192 images. To achieve this
result, we adopt a hyper-parameter-free linear scaling rule for adjusting
learning rates as a function of minibatch size and develop a new warmup scheme
that overcomes optimization challenges early in training. With these simple
techniques, our Caffe2-based system trains ResNet-50 with a minibatch size of
8192 on 256 GPUs in one hour, while matching small minibatch accuracy. Using
commodity hardware, our implementation achieves ~90% scaling efficiency when
moving from 8 to 256 GPUs. Our findings enable training visual recognition
models on internet-scale data with high efficiency.
",1,0,0,0,0,0
20910,20911,CURE: Curvature Regularization For Missing Data Recovery,"  Missing data recovery is an important and yet challenging problem in imaging
and data science. Successful models often adopt certain carefully chosen
regularization. Recently, the low dimension manifold model (LDMM) was
introduced by S.Osher et al. and shown effective in image inpainting. They
observed that enforcing low dimensionality on image patch manifold serves as a
good image regularizer. In this paper, we observe that having only the low
dimension manifold regularization is not enough sometimes, and we need
smoothness as well. For that, we introduce a new regularization by combining
the low dimension manifold regularization with a higher order Curvature
Regularization, and we call this new regularization CURE for short. The key
step of solving CURE is to solve a biharmonic equation on a manifold. We
further introduce a weighted version of CURE, called WeCURE, in a similar
manner as the weighted nonlocal Laplacian (WNLL) method. Numerical experiments
for image inpainting and semi-supervised learning show that the proposed CURE
and WeCURE significantly outperform LDMM and WNLL respectively.
",1,0,0,0,0,0
14208,14209,StealthDB: a Scalable Encrypted Database with Full SQL Query Support,"  Encrypted database systems provide a great method for protecting sensitive
data in untrusted infrastructures. These systems are built using either
special-purpose cryptographic algorithms that support operations over encrypted
data, or by leveraging trusted computing co-processors. Strong cryptographic
algorithms usually result in high performance overheads (e.g., public-key
encryptions, garbled circuits), while weaker algorithms (e.g., order-preserving
encryption) result in large leakage profiles. On the other hand, some encrypted
database systems (e.g., Cipherbase, TrustedDB) leverage non-standard trusted
computing devices, and are designed to work around their specific architectural
limitations.
In this work we build StealthDB -- an encrypted database system from Intel
SGX. Our system can run on any newer generation Intel CPU. StealthDB has a very
small trusted computing base, scales to large datasets, requires no DBMS
changes, and provides strong security guarantees at steady state and during
query execution.
",1,0,0,0,0,0
18247,18248,Klout Topics for Modeling Interests and Expertise of Users Across Social Networks,"  This paper presents Klout Topics, a lightweight ontology to describe social
media users' topics of interest and expertise. Klout Topics is designed to: be
human-readable and consumer-friendly; cover multiple domains of knowledge in
depth; and promote data extensibility via knowledge base entities. We discuss
why this ontology is well-suited for text labeling and interest modeling
applications, and how it compares to available alternatives. We show its
coverage against common social media interest sets, and examples of how it is
used to model the interests of over 780M social media users on Klout.com.
Finally, we open the ontology for external use.
",1,0,0,0,0,0
721,722,"Graph Theoretical Models of Closed n-Dimensional Manifolds: Digital Models of a Moebius Strip, a Torus, a Projective Plane a Klein Bottle and n-Dimensional Spheres","  In this paper, we show how to construct graph theoretical models of
n-dimensional continuous objects and manifolds. These models retain topological
properties of their continuous counterparts. An LCL collection of n-cells in
Euclidean space is introduced and investigated. If an LCL collection of n-cells
is a cover of a continuous n-dimensional manifold then the intersection graph
of this cover is a digital closed n-dimensional manifold with the same topology
as its continuous counterpart. As an example, we prove that the digital model
of a continuous n-dimensional sphere is a digital n-sphere with at least 2n+2
points, the digital model of a continuous projective plane is a digital
projective plane with at least eleven points, the digital model of a continuous
Klein bottle is the digital Klein bottle with at least sixteen points, the
digital model of a continuous torus is the digital torus with at least sixteen
points and the digital model of a continuous Moebius band is the digital
Moebius band with at least twelve points.
",1,0,1,0,0,0
17690,17691,Detection of low dimensionality and data denoising via set estimation techniques,"  This work is closely related to the theories of set estimation and manifold
estimation.
Our object of interest is a, possibly lower-dimensional, compact set $S
\subset {\mathbb R}^d$.
The general aim is to identify (via stochastic procedures) some qualitative
or quantitative features of $S$, of geometric or topological character. The
available information is just a random sample of points drawn on $S$.
The term ""to identify"" means here to achieve a correct answer almost surely
(a.s.) when the sample size tends to infinity. More specifically the paper aims
at giving some partial answers to the following questions: is $S$ full
dimensional? Is $S$ ""close to a lower dimensional set"" $\mathcal{M}$? If so,
can we estimate $\mathcal{M}$ or some functionals of $\mathcal{M}$ (in
particular, the Minkowski content of $\mathcal{M}$)? As an important auxiliary
tool in the answers of these questions, a denoising procedure is proposed in
order to partially remove the noise in the original data. The theoretical
results are complemented with some simulations and graphical illustrations.
",0,0,1,1,0,0
4997,4998,A Kuroda-style j-translation,"  In topos theory it is well-known that any nucleus j gives rise to a
translation of intuitionistic logic into itself in a way which generalises the
Goedel-Gentzen negative translation. Here we show that there exists a similar
j-translation which is more in the spirit of Kuroda's negative translation. The
key is to apply the nucleus not only to the entire formula and universally
quantified subformulas, but to conclusions of implications as well. The
development is entirely syntactic and no knowledge of topos theory is required
to read this small note.
",0,0,1,0,0,0
18094,18095,The Kontsevich tetrahedral flow in 2D: a toy model,"  In the paper ""Formality conjecture"" (1996) Kontsevich designed a universal
flow $\dot{\mathcal{P}}=\mathcal{Q}_{a:b}(\mathcal{P})=a\Gamma_{1}+b\Gamma_{2}$
on the spaces of Poisson structures $\mathcal{P}$ on all affine manifolds of
dimension $n \geqslant 2$. We prove a claim from $\textit{loc. cit.}$ stating
that if $n=2$, the flow $\mathcal{Q}_{1:0}=\Gamma_{1}(\mathcal{P})$ is
Poisson-cohomology trivial: $\Gamma_{1}(\mathcal{P})$ is the Schouten bracket
of $\mathcal{P}$ with $\mathcal{X}$, for some vector field $\mathcal{X}$; we
examine the structure of the space of solutions $\mathcal{X}$. Both the
construction of differential polynomials $\Gamma_{1}(\mathcal{P})$ and
$\Gamma_{2}(\mathcal{P})$ and the technique to study them remain valid in
higher dimensions $n \geqslant 3$, but neither the trivializing vector field
$\mathcal{X}$ nor the setting $b:=0$ survive at $n\geqslant 3$, where the
balance is $a:b=1:6$.
",0,0,1,0,0,0
11501,11502,A gradient flow approach to linear Boltzmann equations,"  We introduce a gradient flow formulation of linear Boltzmann equations. Under
a diffusive scaling we derive a diffusion equation by using the machinery of
gradient flows.
",0,0,1,0,0,0
2936,2937,R-boundedness Approach to linear third differential equations in a UMD Space,"  The aim of this work is to study the existence of a periodic solutions of
third order differential equations $z'''(t) = Az(t) + f(t)$ with the periodic
condition $x(0) = x(2\pi), x'(0) = x'(2\pi)$ and $x''(0) = x''(2\pi)$. Our
approach is based on the R-boundedness and $L^{p}$-multiplier of linear
operators.
",0,0,1,0,0,0
1100,1101,Self-consistent dynamical model of the Broad Line Region,"  We develope a self-consistent description of the Broad Line Region based on
the concept of the failed wind powered by the radiation pressure acting on
dusty accretion disk atmosphere in Keplerian motion. The material raised high
above the disk is illuminated, dust evaportes, and the matter falls back
towards the disk. This material is the source of emission lines. The model
predicts the inner and outer radius of the region, the cloud dynamics under the
dust radiation pressure and, subsequently, just the gravitational field of the
central black hole, which results in assymetry between the rise and fall.
Knowledge of the dynamics allows to predict the shapes of the emission lines as
functions of the basic parameters of an active nucleus: black hole mass,
accretion rate, black hole spin (or accretion efficiency) and the viewing angle
with respect to the symmetry axis. Here we show preliminary results based on
analytical approximations to the cloud motion.
",0,1,0,0,0,0
9490,9491,Irreducible compositions of degree two polynomials over finite fields have regular structure,"  Let $q$ be an odd prime power and $D$ be the set of monic irreducible
polynomials in $\mathbb F_q[x]$ which can be written as a composition of monic
degree two polynomials. In this paper we prove that $D$ has a natural regular
structure by showing that there exists a finite automaton having $D$ as
accepted language. Our method is constructive.
",1,0,1,0,0,0
13547,13548,Resolving API Mentions in Informal Documents,"  Developer forums contain opinions and information related to the usage of
APIs. API names in forum posts are often not explicitly linked to their
official resources. Automatic linking of an API mention to its official
resources can be challenging for various reasons, such as, name overloading. We
present a technique, ANACE, to automatically resolve API mentions in the
textual contents of forum posts. Given a database of APIs, we first detect all
words in a forum post that are potential references to an API. We then use a
combination of heuristics and machine learning to eliminate false positives and
to link true positives to the actual APIs and their resources.
",1,0,0,0,0,0
8675,8676,Chaos in three coupled rotators: From Anosov dynamics to hyperbolic attractors,"  Starting from Anosov chaotic dynamics of geodesic flow on a surface of
negative curvature, we develop and consider a number of self-oscillatory
systems including those with hinged mechanical coupling of three rotators and a
system of rotators interacting through a potential function. These results are
used to design an electronic circuit for generation of rough (structurally
stable) chaos. Results of numerical integration of the model equations of
different degree of accuracy are presented and discussed. Also, circuit
simulation of the electronic generator is provided using the NI Multisim
environment. Portraits of attractors, waveforms of generated oscillations,
Lyapunov exponents, and spectra are considered and found to be in good
correspondence for the dynamics on the attractive sets of the self-oscillatory
systems and for the original Anosov geodesic flow. The hyperbolic nature of the
dynamics is tested numerically using a criterion based on statistics of angles
of intersection of stable and unstable subspaces of the perturbation vectors at
a reference phase trajectory on the attractor.
",0,1,0,0,0,0
4176,4177,Causal Discovery in the Presence of Measurement Error: Identifiability Conditions,"  Measurement error in the observed values of the variables can greatly change
the output of various causal discovery methods. This problem has received much
attention in multiple fields, but it is not clear to what extent the causal
model for the measurement-error-free variables can be identified in the
presence of measurement error with unknown variance. In this paper, we study
precise sufficient identifiability conditions for the measurement-error-free
causal model and show what information of the causal model can be recovered
from observed data. In particular, we present two different sets of
identifiability conditions, based on the second-order statistics and
higher-order statistics of the data, respectively. The former was inspired by
the relationship between the generating model of the
measurement-error-contaminated data and the factor analysis model, and the
latter makes use of the identifiability result of the over-complete independent
component analysis problem.
",1,0,0,1,0,0
13037,13038,Hilbert Bases and Lecture Hall Partitions,"  In the interest of finding the minimum additive generating set for the set of
$\boldsymbol{s}$-lecture hall partitions, we compute the Hilbert bases for the
$\boldsymbol{s}$-lecture hall cones in certain cases. In particular, we compute
the Hilbert bases for two well-studied families of sequences, namely the $1\mod
k$ sequences and the $\ell$-sequences. Additionally, we provide a
characterization of the Hilbert bases for $\boldsymbol{u}$-generated Gorenstein
$\boldsymbol{s}$-lecture hall cones in low dimensions.
",0,0,1,0,0,0
4528,4529,A Functional Taxonomy of Music Generation Systems,"  Digital advances have transformed the face of automatic music generation
since its beginnings at the dawn of computing. Despite the many breakthroughs,
issues such as the musical tasks targeted by different machines and the degree
to which they succeed remain open questions. We present a functional taxonomy
for music generation systems with reference to existing systems. The taxonomy
organizes systems according to the purposes for which they were designed. It
also reveals the inter-relatedness amongst the systems. This design-centered
approach contrasts with predominant methods-based surveys and facilitates the
identification of grand challenges to set the stage for new breakthroughs.
",1,0,0,0,0,0
9297,9298,Mitigating Confirmation Bias on Twitter by Recommending Opposing Views,"  In this work, we propose a content-based recommendation approach to increase
exposure to opposing beliefs and opinions. Our aim is to help provide users
with more diverse viewpoints on issues, which are discussed in partisan groups
from different perspectives. Since due to the backfire effect, people's
original beliefs tend to strengthen when challenged with counter evidence, we
need to expose them to opposing viewpoints at the right time. The preliminary
work presented here describes our first step into this direction. As
illustrative showcase, we take the political debate on Twitter around the
presidency of Donald Trump.
",1,0,0,0,0,0
6607,6608,The nature and origin of heavy tails in retweet activity,"  Modern social media platforms facilitate the rapid spread of information
online. Modelling phenomena such as social contagion and information diffusion
are contingent upon a detailed understanding of the information-sharing
processes. In Twitter, an important aspect of this occurs with retweets, where
users rebroadcast the tweets of other users. To improve our understanding of
how these distributions arise, we analyse the distribution of retweet times. We
show that a power law with exponential cutoff provides a better fit than the
power laws previously suggested. We explain this fit through the burstiness of
human behaviour and the priorities individuals place on different tasks.
",1,1,0,1,0,0
12115,12116,Multiuser Communication Based on the DFT Eigenstructure,"  The eigenstructure of the discrete Fourier transform (DFT) is examined and
new systematic procedures to generate eigenvectors of the unitary DFT are
proposed. DFT eigenvectors are suggested as user signatures for data
communication over the real adder channel (RAC). The proposed multiuser
communication system over the 2-user RAC is detailed.
",1,0,0,1,0,0
16971,16972,On a direct algorithm for constructing recursion operators and Lax pairs for integrable models,"  We suggested an algorithm for searching the recursion operators for nonlinear
integrable equations. It was observed that the recursion operator $R$ can be
represented as a ratio of the form $R=L_1^{-1}L_2$ where the linear
differential operators $L_1$ and $L_2$ are chosen in such a way that the
ordinary differential equation $(L_2-\lambda L_1)U=0$ is consistent with the
linearization of the given nonlinear integrable equation for any value of the
parameter $\lambda\in \textbf{C}$. For constructing the operator $L_1$ we use
the concept of the invariant manifold which is a generalization of the
symmetry. Then for searching $L_2$ we take an auxiliary linear equation
connected with the linearized equation by the Darboux transformation.
Connection of the invariant manifold with the Lax pairs and the
Dubrovin-Weierstrass equations is discussed.
",0,1,0,0,0,0
20593,20594,Degeneration in VAE: in the Light of Fisher Information Loss,"  While enormous progress has been made to Variational Autoencoder (VAE) in
recent years, similar to other deep networks, VAE with deep networks suffers
from the problem of degeneration, which seriously weakens the correlation
between the input and the corresponding latent codes, deviating from the goal
of the representation learning. To investigate how degeneration affects VAE
from a theoretical perspective, we illustrate the information transmission in
VAE and analyze the intermediate layers of the encoders/decoders. Specifically,
we propose a Fisher Information measure for the layer-wise analysis. With such
measure, we demonstrate that information loss is ineluctable in feed-forward
networks and causes the degeneration in VAE. We show that skip connections in
VAE enable the preservation of information without changing the model
architecture. We call this class of VAE equipped with skip connections as SCVAE
and perform a range of experiments to show its advantages in information
preservation and degeneration mitigation.
",0,0,0,1,0,0
11266,11267,Chordal SLE$_6$ explorations of a quantum disk,"  We consider a particular type of $\sqrt{8/3}$-Liouville quantum gravity
surface called a doubly marked quantum disk (equivalently, a Brownian disk)
decorated by an independent chordal SLE$_6$ curve $\eta$ between its marked
boundary points. We obtain descriptions of the law of the quantum surfaces
parameterized by the complementary connected components of $\eta([0,t])$ for
each time $t \geq 0$ as well as the law of the left/right $\sqrt{8/3}$-quantum
boundary length process for $\eta$.
",0,0,1,0,0,0
8740,8741,"Evolution of Anisotropic Displacement Parameters and Superconductivity with Chemical Pressure in BiS2-Based REO0.5F0.5BiS2 (RE = La, Ce, Pr, and Nd)","  In order to understand the mechanisms behind the emergence of
superconductivity by the chemical pressure effect in REO0.5F0.5BiS2 (RE = La,
Ce, Pr, and Nd), where bulk superconductivity is induced by the substitutions
with a smaller-radius RE, we performed synchrotron powder X-ray diffraction,
and analyzed the crystal structure and anisotropic displacement parameters.
With the decrease of the RE3+ ionic radius, the in-plane disorder of the S1
sites significantly decreased, very similar to the trend observed in the
Se-substituted systems: LaO0.5F0.5BiS2-xSex and Eu0.5La0.5FBiS2-xSex.
Therefore, the emergence of bulk superconductivity upon the suppression of the
in-plane disorder at the chalcogen sites is a universal scenario for the
BiCh2-based superconductors. In addition, we indicated that the amplitude of
vibration along the c-axis of the in-plane chalcogen sites may be related to
the Tc in the BiCh2-based superconductors.
",0,1,0,0,0,0
7471,7472,Structural and electronic properties of germanene on MoS$_2$,"  To date, germanene has only been synthesized on metallic substrates. A
metallic substrate is usually detrimental for the two-dimensional Dirac nature
of germanene because the important electronic states near the Fermi level of
germanene can hybridize with the electronic states of the metallic substrate.
Here we report the successful synthesis of germanene on molybdenum disulfide
(MoS$_2$), a band gap material. Pre-existing defects in the MoS$_2$ surface act
as preferential nucleation sites for the germanene islands. The lattice
constant of the germanene layer (3.8 $\pm$ 0.2 \AA) is about 20\% larger than
the lattice constant of the MoS$_2$ substrate (3.16 \AA). Scanning tunneling
spectroscopy measurements and density functional theory calculations reveal
that there are, besides the linearly dispersing bands at the $K$ points, two
parabolic bands that cross the Fermi level at the $\Gamma$ point.
",0,1,0,0,0,0
17111,17112,A Redshift Survey of the Nearby Galaxy Cluster Abell 2199: Comparison of the Spatial and Kinematic Distributions of Galaxies with the Intracluster Medium,"  We present the results from an extensive spectroscopic survey of the central
region of the nearby galaxy cluster Abell 2199 at $z=0.03$. By combining 775
new redshifts from the MMT/Hectospec observations with the data in the
literature, we construct a large sample of 1624 galaxies with measured
redshifts at $R<30^\prime$, which results in high spectroscopic completeness at
$r_{\rm petro,0}<20.5$ (77%). We use these data to study the kinematics and
clustering of galaxies focusing on the comparison with those of the
intracluster medium (ICM) from Suzaku X-ray observations. We identify 406
member galaxies of A2199 at $R<30^\prime$ using the caustic technique. The
velocity dispersion profile of cluster members appears smoothly connected to
the stellar velocity dispersion profile of the cD galaxy. The luminosity
function is well fitted with a Schechter function at $M_r<-15$. The radial
velocities of cluster galaxies generally agree well with those of the ICM, but
there are some regions where the velocity difference between the two is about a
few hundred kilometer per second. The cluster galaxies show a hint of global
rotation at $R<5^\prime$ with $v_{\rm rot}=300{-}600\,\textrm{km s}^{-1}$, but
the ICM in the same region do not show such rotation. We apply a
friends-of-friends algorithm to the cluster galaxy sample at $R<60^\prime$ and
identify 32 group candidates, and examine the spatial correlation between the
galaxy groups and X-ray emission. This extensive survey in the central region
of A2199 provides an important basis for future studies of interplay among the
galaxies, the ICM and the dark matter in the cluster.
",0,1,0,0,0,0
17564,17565,Types and unitary representations of reductive p-adic groups,"  We prove that for every Bushnell-Kutzko type that satisfies a certain
rigidity assumption, the equivalence of categories between the corresponding
Bernstein component and the category of modules for the Hecke algebra of the
type induces a bijection between irreducible unitary representations in the two
categories. This is a generalization of the unitarity criterion of Barbasch and
Moy for representations with Iwahori fixed vectors.
",0,0,1,0,0,0
13207,13208,The inseparability of sampling and time and its influence on attempts to unify the molecular and fossil records,"  The two major approaches to studying macroevolution in deep time are the
fossil record and reconstructed relationships among extant taxa from molecular
data. Results based on one approach sometimes conflict with those based on the
other, with inconsistencies often attributed to inherent flaws of one (or the
other) data source. What is unquestionable is that both the molecular and
fossil records are limited reflections of the same evolutionary history, and
any contradiction between them represents a failure of our existing models to
explain the patterns we observe. Fortunately, the different limitations of each
record provide an opportunity to test or calibrate the other, and new
methodological developments leverage both records simultaneously. However, we
must reckon with the distinct relationships between sampling and time in the
fossil record and molecular phylogenies. These differences impact our
recognition of baselines, and the analytical incorporation of age estimate
uncertainty. These differences in perspective also influence how different
practitioners view the past and evolutionary time itself, bearing important
implications for the generality of methodological advancements, and differences
in the philosophical approach to macroevolutionary theory across fields.
",0,0,0,0,1,0
19456,19457,Learning General Latent-Variable Graphical Models with Predictive Belief Propagation and Hilbert Space Embeddings,"  In this paper, we propose a new algorithm for learning general
latent-variable probabilistic graphical models using the techniques of
predictive state representation, instrumental variable regression, and
reproducing-kernel Hilbert space embeddings of distributions. Under this new
learning framework, we first convert latent-variable graphical models into
corresponding latent-variable junction trees, and then reduce the hard
parameter learning problem into a pipeline of supervised learning problems,
whose results will then be used to perform predictive belief propagation over
the latent junction tree during the actual inference procedure. We then give
proofs of our algorithm's correctness, and demonstrate its good performance in
experiments on one synthetic dataset and two real-world tasks from
computational biology and computer vision - classifying DNA splice junctions
and recognizing human actions in videos.
",1,0,0,1,0,0
4494,4495,Temporal Markov Processes for Transport in Porous Media: Random Lattice Networks,"  Monte Carlo (MC) simulations of transport in random porous networks indicate
that for high variances of the log-normal permeability distribution, the
transport of a passive tracer is non-Fickian. Here we model this non-Fickian
dispersion in random porous networks using discrete temporal Markov models. We
show that such temporal models capture the spreading behavior accurately. This
is true despite the fact that the slow velocities are strongly correlated in
time, and some studies have suggested that the persistence of low velocities
would render the temporal Markovian model inapplicable. Compared to previously
proposed temporal stochastic differential equations with case specific drift
and diffusion terms, the models presented here require fewer modeling
assumptions. Moreover, we show that discrete temporal Markov models can be used
to represent dispersion in unstructured networks, which are widely used to
model porous media. A new method is proposed to extend the state space of
temporal Markov models to improve the model predictions in the presence of
extremely low velocities in particle trajectories and extend the applicability
of the model to higher temporal resolutions. Finally, it is shown that by
combining multiple transitions, temporal models are more efficient for
computing particle evolution compared to correlated CTRW with spatial
increments that are equal to the lengths of the links in the network.
",1,1,0,0,0,0
20087,20088,Attractor of Cantor Type with Positive Measure,"  We construct an iterated function system consisting of strictly increasing
contractions $f,g\colon [0,1]\to [0,1]$ with $f([0,1])\cap g([0,1])=\emptyset$
and such that its attractor has positive Lebesgue measure.
",0,0,1,0,0,0
7124,7125,"The Gaia-ESO Survey: Dynamical models of flattened, rotating globular clusters","  We present a family of self-consistent axisymmetric rotating globular cluster
models which are fitted to spectroscopic data for NGC 362, NGC 1851, NGC 2808,
NGC 4372, NGC 5927 and NGC 6752 to provide constraints on their physical and
kinematic properties, including their rotation signals. They are constructed by
flattening Modified Plummer profiles, which have the same asymptotic behaviour
as classical Plummer models, but can provide better fits to young clusters due
to a slower turnover in the density profile. The models are in dynamical
equilibrium as they depend solely on the action variables. We employ a fully
Bayesian scheme to investigate the uncertainty in our model parameters
(including mass-to-light ratios and inclination angles) and evaluate the
Bayesian evidence ratio for rotating to non-rotating models. We find convincing
levels of rotation only in NGC 2808. In the other clusters, there is just a
hint of rotation (in particular, NGC 4372 and NGC 5927), as the data quality
does not allow us to draw strong conclusions. Where rotation is present, we
find that it is confined to the central regions, within radii of $R \leq 2
r_h$. As part of this work, we have developed a novel q-Gaussian basis
expansion of the line-of-sight velocity distributions, from which general
models can be constructed via interpolation on the basis coefficients.
",0,1,0,0,0,0
19438,19439,Influence of parameterized small-scale gravity waves on the migrating diurnal tide in Earth's thermosphere,"  Effects of subgrid-scale gravity waves (GWs) on the diurnal migrating tides
are investigated from the mesosphere to the upper thermosphere for September
equinox conditions, using a general circulation model coupled with the extended
spectral nonlinear GW parameterization of Yiğit et al (2008). Simulations
with GW effects cut-off above the turbopause and included in the entire
thermosphere have been conducted. GWs appreciably impact the mean circulation
and cool the thermosphere down by up to 12-18%. GWs significantly affect the
winds modulated by the diurnal migrating tide, in particular in the
low-latitude mesosphere and lower thermosphere and in the high-latitude
thermosphere. These effects depend on the mutual correlation of the diurnal
phases of the GW forcing and tides: GWs can either enhance or reduce the tidal
amplitude. In the low-latitude MLT, the correlation between the direction of
the deposited GW momentum and the tidal phase is positive due to propagation of
a broad spectrum of GW harmonics through the alternating winds. In the Northern
Hemisphere high-latitude thermosphere, GWs act against the tide due to an
anti-correlation of tidal wind and GW momentum, while in the Southern
high-latitudes they weakly enhance the tidal amplitude via a combination of a
partial correlation of phases and GW-induced changes of the circulation. The
variable nature of GW effects on the thermal tide can be captured in GCMs
provided that a GW parameterization (1) considers a broad spectrum of
harmonics, (2) properly describes their propagation, and (3) correctly accounts
for the physics of wave breaking/saturation.
",0,1,0,0,0,0
5928,5929,A Calculus of Truly Concurrent Mobile Processes,"  We make a mixture of Milner's $\pi$-calculus and our previous work on truly
concurrent process algebra, which is called $\pi_{tc}$. We introduce syntax and
semantics of $\pi_{tc}$, its properties based on strongly truly concurrent
bisimilarities. Also, we include an axiomatization of $\pi_{tc}$. $\pi_{tc}$
can be used as a formal tool in verifying mobile systems in a truly concurrent
flavor.
",1,0,0,0,0,0
2918,2919,A data driven trimming procedure for robust classification,"  Classification rules can be severely affected by the presence of disturbing
observations in the training sample. Looking for an optimal classifier with
such data may lead to unnecessarily complex rules. So, simpler effective
classification rules could be achieved if we relax the goal of fitting a good
rule for the whole training sample but only consider a fraction of the data. In
this paper we introduce a new method based on trimming to produce
classification rules with guaranteed performance on a significant fraction of
the data. In particular, we provide an automatic way of determining the right
trimming proportion and obtain in this setting oracle bounds for the
classification error on the new data set.
",0,0,1,1,0,0
2153,2154,Dissolution of topological Fermi arcs in a dirty Weyl semimetal,"  Weyl semimetals (WSMs) have recently attracted a great deal of attention as
they provide condensed matter realization of chiral anomaly, feature
topologically protected Fermi arc surface states and sustain sharp chiral Weyl
quasiparticles up to a critical disorder at which a continuous quantum phase
transition (QPT) drives the system into a metallic phase. We here numerically
demonstrate that with increasing strength of disorder the Fermi arc gradually
looses its sharpness, and close to the WSM-metal QPT it completely dissolves
into the metallic bath of the bulk. Predicted topological nature of the
WSM-metal QPT and the resulting bulk-boundary correspondence across this
transition can directly be observed in
angle-resolved-photo-emmision-spectroscopy (ARPES) and Fourier transformed
scanning-tunneling-microscopy (STM) measurements by following the continuous
deformation of the Fermi arcs with increasing disorder in recently discovered
Weyl materials.
",0,1,0,0,0,0
2866,2867,Modulation of High-Energy Particles and the Heliospheric Current Sheet Tilts throughout 1976-2014,"  Cosmic ray intensities (CRIs) recorded by sixteen neutron monitors have been
used to study its dependence on the tilt angles (TA) of the heliospheric
current sheet (HCS) during period 1976-2014, which covers three solar activity
cycles 21, 22 and 23. The median primary rigidity covers the range 16-33 GV.
Our results have indicated that the CRIs are directly sensitive to, and
organized by, the interplanetary magnetic field (IMF) and its neutral sheet
inclinations. The observed differences in the sensitivity of cosmic ray
intensity to changes in the neutral sheet tilt angles before and after the
reversal of interplanetary magnetic field polarity have been studied. Much
stronger intensity-tilt angle correlation was found when the solar magnetic
field in the North Polar Region was directed inward than it was outward. The
rigidity dependence of sensitivities of cosmic rays differs according to the
IMF polarity, for the periods 1981-1988 and 2001-2008 (qA < 0) it was R-1.00
and R-1.48 respectively, while for the 1991-1998 epoch (qA > 0) it was R-1.35.
Hysteresis loops between TA and CRIs have been examined during three solar
activity cycles 21, 22 and 23. A consider differences in time lags during qA >
0 and qA < 0 polarity states of the heliosphere have been observed. We also
found that the cosmic ray intensity decreases at much faster rate with increase
of tilt angle during qA < 0 than qA > 0, indicating stronger response to the
tilt angle changes during qA < 0. Our results are discussed in the light of 3D
modulation models including the gradient, curvature drifts and the tilt of the
heliospheric current sheet.
",0,1,0,0,0,0
6188,6189,Universal edge transport in interacting Hall systems,"  We study the edge transport properties of $2d$ interacting Hall systems,
displaying single-mode chiral edge currents. For this class of many-body
lattice models, including for instance the interacting Haldane model, we prove
the quantization of the edge charge conductance and the bulk-edge
correspondence. Instead, the edge Drude weight and the edge susceptibility are
interaction-dependent; nevertheless, they satisfy exact universal scaling
relations, in agreement with the chiral Luttinger liquid theory. Moreover,
charge and spin excitations differ in their velocities, giving rise to the
spin-charge separation phenomenon. The analysis is based on exact
renormalization group methods, and on a combination of lattice and emergent
Ward identities. The invariance of the emergent chiral anomaly under the
renormalization group flow plays a crucial role in the proof.
",0,1,0,0,0,0
4638,4639,Achieving non-discrimination in prediction,"  Discrimination-aware classification is receiving an increasing attention in
data science fields. The pre-process methods for constructing a
discrimination-free classifier first remove discrimination from the training
data, and then learn the classifier from the cleaned data. However, they lack a
theoretical guarantee for the potential discrimination when the classifier is
deployed for prediction. In this paper, we fill this gap by mathematically
bounding the probability of the discrimination in prediction being within a
given interval in terms of the training data and classifier. We adopt the
causal model for modeling the data generation mechanism, and formally defining
discrimination in population, in a dataset, and in prediction. We obtain two
important theoretical results: (1) the discrimination in prediction can still
exist even if the discrimination in the training data is completely removed;
and (2) not all pre-process methods can ensure non-discrimination in prediction
even though they can achieve non-discrimination in the modified training data.
Based on the results, we develop a two-phase framework for constructing a
discrimination-free classifier with a theoretical guarantee. The experiments
demonstrate the theoretical results and show the effectiveness of our two-phase
framework.
",1,0,0,1,0,0
8553,8554,An Improved SCFlip Decoder for Polar Codes,"  This paper focuses on the recently introduced Successive Cancellation Flip
(SCFlip) decoder of polar codes. Our contribution is twofold. First, we propose
the use of an optimized metric to determine the flipping positions within the
SCFlip decoder, which improves its ability to find the first error that
occurred during the initial SC decoding attempt. We also show that the proposed
metric allows closely approaching the performance of an ideal SCFlip decoder.
Second, we introduce a generalisation of the SCFlip decoder to a number of
$\omega$ nested flips, denoted by SCFlip-$\omega$, using a similar optimized
metric to determine the positions of the nested flips. We show that the
SCFlip-2 decoder yields significant gains in terms of decoding performance and
competes with the performance of the CRC-aided SC-List decoder with list size
L=4, while having an average decoding complexity similar to that of the
standard SC decoding, at medium to high signal to noise ratio.
",1,0,0,0,0,0
15542,15543,Phase limitations of Zames-Falb multipliers,"  Phase limitations of both continuous-time and discrete-time Zames-Falb
multipliers and their relation with the Kalman conjecture are analysed. A phase
limitation for continuous-time multipliers given by Megretski is generalised
and its applicability is clarified; its relation to the Kalman conjecture is
illustrated with a classical example from the literature. It is demonstrated
that there exist fourth-order plants where the existence of a suitable
Zames-Falb multiplier can be discarded and for which simulations show unstable
behavior. A novel phase-limitation for discrete-time Zames-Falb multipliers is
developed. Its application is demonstrated with a second-order counterexample
to the Kalman conjecture. Finally, the discrete-time limitation is used to show
that there can be no direct counterpart of the off-axis circle criterion in the
discrete-time domain.
",1,0,1,0,0,0
18159,18160,Step Detection Algorithm For Accurate Distance Estimation Using Dynamic Step Length,"  In this paper, a new Smartphone sensor based algorithm is proposed to detect
accurate distance estimation. The algorithm consists of two phases, the first
phase is for detecting the peaks from the Smartphone accelerometer sensor. The
other one is for detecting the step length which varies from step to step. The
proposed algorithm is tested and implemented in real environment and it showed
promising results. Unlike the conventional approaches, the error of the
proposed algorithm is fixed and is not affected by the long distance.
Keywords distance estimation, peaks, step length, accelerometer.
",1,0,0,0,0,0
13902,13903,Compiling Diderot: From Tensor Calculus to C,"  Diderot is a parallel domain-specific language for analysis and visualization
of multidimensional scientific images, such as those produced by CT and MRI
scanners. In particular, it supports algorithms where tensor fields (i.e.,
functions from 3D points to tensor values) are used to represent the underlying
physical objects that were scanned by the imaging device. Diderot supports
higher-order programming where tensor fields are first-class values and where
differential operators and lifted linear-algebra operators can be used to
express mathematical reasoning directly in the language. While such lifted
field operations are central to the definition and computation of many
scientific visualization algorithms, to date they have required extensive
manual derivations and laborious implementation.
The challenge for the Diderot compiler is to effectively translate the
high-level mathematical concepts that are expressible in the surface language
to a low-level and efficient implementation in C. This paper describes our
approach to this challenge, which is based around the careful design of an
intermediate representation (IR), called EIN, and a number of compiler
transformations that lower the program from tensor calculus to C while avoiding
combinatorial explosion in the size of the IR. We describe the challenges in
compiling a language like Diderot, the design of EIN, and the transformation
used by the compiler. We also present an evaluation of EIN with respect to both
compiler efficiency and quality of generated code.
",1,0,0,0,0,0
4385,4386,Don't Look Back: Robustifying Place Categorization for Viewpoint- and Condition-Invariant Place Recognition,"  When a human drives a car along a road for the first time, they later
recognize where they are on the return journey typically without needing to
look in their rear-view mirror or turn around to look back, despite significant
viewpoint and appearance change. Such navigation capabilities are typically
attributed to our semantic visual understanding of the environment [1] beyond
geometry to recognizing the types of places we are passing through such as
""passing a shop on the left"" or ""moving through a forested area"". Humans are in
effect using place categorization [2] to perform specific place recognition
even when the viewpoint is 180 degrees reversed. Recent advances in deep neural
networks have enabled high-performance semantic understanding of visual places
and scenes, opening up the possibility of emulating what humans do. In this
work, we develop a novel methodology for using the semantics-aware higher-order
layers of deep neural networks for recognizing specific places from within a
reference database. To further improve the robustness to appearance change, we
develop a descriptor normalization scheme that builds on the success of
normalization schemes for pure appearance-based techniques such as SeqSLAM [3].
Using two different datasets - one road-based, one pedestrian-based, we
evaluate the performance of the system in performing place recognition on
reverse traversals of a route with a limited field of view camera and no
turn-back-and-look behaviours, and compare to existing state-of-the-art
techniques and vanilla off-the-shelf features. The results demonstrate
significant improvements over the existing state of the art, especially for
extreme perceptual challenges that involve both great viewpoint change and
environmental appearance change. We also provide experimental analyses of the
contributions of the various system components.
",1,0,0,0,0,0
6717,6718,Approximate Kernel PCA Using Random Features: Computational vs. Statistical Trade-off,"  Kernel methods are powerful learning methodologies that provide a simple way
to construct nonlinear algorithms from linear ones. Despite their popularity,
they suffer from poor scalability in big data scenarios. Various approximation
methods, including random feature approximation have been proposed to alleviate
the problem. However, the statistical consistency of most of these approximate
kernel methods is not well understood except for kernel ridge regression
wherein it has been shown that the random feature approximation is not only
computationally efficient but also statistically consistent with a minimax
optimal rate of convergence. In this paper, we investigate the efficacy of
random feature approximation in the context of kernel principal component
analysis (KPCA) by studying the trade-off between computational and statistical
behaviors of approximate KPCA. We show that the approximate KPCA is both
computationally and statistically efficient compared to KPCA in terms of the
error associated with reconstructing a kernel function based on its projection
onto the corresponding eigenspaces. Depending on the eigenvalue decay behavior
of the covariance operator, we show that only $n^{2/3}$ features (polynomial
decay) or $\sqrt{n}$ features (exponential decay) are needed to match the
statistical performance of KPCA. We also investigate their statistical
behaviors in terms of the convergence of corresponding eigenspaces wherein we
show that only $\sqrt{n}$ features are required to match the performance of
KPCA and if fewer than $\sqrt{n}$ features are used, then approximate KPCA has
a worse statistical behavior than that of KPCA.
",0,0,1,1,0,0
12811,12812,A Short and Elementary Proof of the Two-sidedness of the Matrix-Inverse,"  An elementary proof of the two-sidedness of the matrix-inverse is given using
only linear independence and the reduced row-echelon form of a matrix. In
addition, it is shown that a matrix is invertible if and only if it is
row-equivalent to the identity matrix without appealing to elementary matrices.
This proof underscores the importance of a basis and provides a proof of the
invertible matrix theorem.
",0,0,1,0,0,0
10009,10010,Graham-Witten's conformal invariant for closed four dimensional submanifolds,"  It was proved by Graham and Witten in 1999 that conformal invariants of
submanifolds can be obtained via volume renormalization of minimal surfaces in
conformally compact Einstein manifolds. The conformal invariant of a
submanifold $\Sigma$ is contained in the volume expansion of the minimal
surface which is asymptotic to $\Sigma$ when the minimal surface approaches the
conformaly infinity. In the paper we give the explicit expression of
Graham-Witten's conformal invariant for closed four dimensional submanifolds
and find critical points of the conformal invariant in the case of Euclidean
ambient spaces.
",0,0,1,0,0,0
7627,7628,Dynamic anisotropy in MHD turbulence induced by mean magnetic field,"  In this paper, we study the development of anisotropy in strong MHD
turbulence in the presence of a large scale magnetic field B 0 by analyzing the
results of direct numerical simulations. Our results show that the developed
anisotropy among the different components of the velocity and magnetic field is
a direct outcome of the inverse cascade of energy of the perpendicular velocity
components u? and a forward cascade of the energy of the parallel component u k
. The inverse cascade develops for a strong B0, where the flow exhibits a
strong vortical structure by the suppression of fluctuations along the magnetic
field. Both the inverse and the forward cascade are examined in detail by
investigating the anisotropic energy spectra, the energy fluxes, and the shell
to shell energy transfers among different scales.
",0,1,0,0,0,0
15263,15264,"Caveat Emptor, Computational Social Science: Large-Scale Missing Data in a Widely-Published Reddit Corpus","  As researchers use computational methods to study complex social behaviors at
scale, the validity of this computational social science depends on the
integrity of the data. On July 2, 2015, Jason Baumgartner published a dataset
advertised to include ``every publicly available Reddit comment'' which was
quickly shared on Bittorrent and the Internet Archive. This data quickly became
the basis of many academic papers on topics including machine learning, social
behavior, politics, breaking news, and hate speech. We have discovered
substantial gaps and limitations in this dataset which may contribute to bias
in the findings of that research. In this paper, we document the dataset,
substantial missing observations in the dataset, and the risks to research
validity from those gaps. In summary, we identify strong risks to research that
considers user histories or network analysis, moderate risks to research that
compares counts of participation, and lesser risk to machine learning research
that avoids making representative claims about behavior and participation on
Reddit.
",1,0,0,0,0,0
6987,6988,Direct Estimation of Regional Wall Thicknesses via Residual Recurrent Neural Network,"  Accurate estimation of regional wall thicknesses (RWT) of left ventricular
(LV) myocardium from cardiac MR sequences is of significant importance for
identification and diagnosis of cardiac disease. Existing RWT estimation still
relies on segmentation of LV myocardium, which requires strong prior
information and user interaction. No work has been devoted into direct
estimation of RWT from cardiac MR images due to the diverse shapes and
structures for various subjects and cardiac diseases, as well as the complex
regional deformation of LV myocardium during the systole and diastole phases of
the cardiac cycle. In this paper, we present a newly proposed Residual
Recurrent Neural Network (ResRNN) that fully leverages the spatial and temporal
dynamics of LV myocardium to achieve accurate frame-wise RWT estimation. Our
ResRNN comprises two paths: 1) a feed forward convolution neural network (CNN)
for effective and robust CNN embedding learning of various cardiac images and
preliminary estimation of RWT from each frame itself independently, and 2) a
recurrent neural network (RNN) for further improving the estimation by modeling
spatial and temporal dynamics of LV myocardium. For the RNN path, we design for
cardiac sequences a Circle-RNN to eliminate the effect of null hidden input for
the first time-step. Our ResRNN is capable of obtaining accurate estimation of
cardiac RWT with Mean Absolute Error of 1.44mm (less than 1-pixel error) when
validated on cardiac MR sequences of 145 subjects, evidencing its great
potential in clinical cardiac function assessment.
",1,0,0,0,0,0
20918,20919,Disentangling by Factorising,"  We define and address the problem of unsupervised learning of disentangled
representations on data generated from independent factors of variation. We
propose FactorVAE, a method that disentangles by encouraging the distribution
of representations to be factorial and hence independent across the dimensions.
We show that it improves upon $\beta$-VAE by providing a better trade-off
between disentanglement and reconstruction quality. Moreover, we highlight the
problems of a commonly used disentanglement metric and introduce a new metric
that does not suffer from them.
",0,0,0,1,0,0
6670,6671,Semi-tied Units for Efficient Gating in LSTM and Highway Networks,"  Gating is a key technique used for integrating information from multiple
sources by long short-term memory (LSTM) models and has recently also been
applied to other models such as the highway network. Although gating is
powerful, it is rather expensive in terms of both computation and storage as
each gating unit uses a separate full weight matrix. This issue can be severe
since several gates can be used together in e.g. an LSTM cell. This paper
proposes a semi-tied unit (STU) approach to solve this efficiency issue, which
uses one shared weight matrix to replace those in all the units in the same
layer. The approach is termed ""semi-tied"" since extra parameters are used to
separately scale each of the shared output values. These extra scaling factors
are associated with the network activation functions and result in the use of
parameterised sigmoid, hyperbolic tangent, and rectified linear unit functions.
Speech recognition experiments using British English multi-genre broadcast data
showed that using STUs can reduce the calculation and storage cost by a factor
of three for highway networks and four for LSTMs, while giving similar word
error rates to the original models.
",0,0,0,1,0,0
17205,17206,Extreme Event Statistics in a Drifting Markov Chain,"  We analyse extreme event statistics of experimentally realized Markov chains
with various drifts. Our Markov chains are individual trajectories of a single
atom diffusing in a one dimensional periodic potential. Based on more than 500
individual atomic traces we verify the applicability of the Sparre Andersen
theorem to our system despite the presence of a drift. We present detailed
analysis of four different rare event statistics for our system: the
distributions of extreme values, of record values, of extreme value occurrence
in the chain, and of the number of records in the chain. We observe that for
our data the shape of the extreme event distributions is dominated by the
underlying exponential distance distribution extracted from the atomic traces.
Furthermore, we find that even small drifts influence the statistics of extreme
events and record values, which is supported by numerical simulations, and we
identify cases in which the drift can be determined without information about
the underlying random variable distributions. Our results facilitate the use of
extreme event statistics as a signal for small drifts in correlated
trajectories.
",0,1,0,0,0,0
20255,20256,Security Analysis of Cache Replacement Policies,"  Modern computer architectures share physical resources between different
programs in order to increase area-, energy-, and cost-efficiency.
Unfortunately, sharing often gives rise to side channels that can be exploited
for extracting or transmitting sensitive information. We currently lack
techniques for systematic reasoning about this interplay between security and
efficiency. In particular, there is no established way for quantifying security
properties of shared caches.
In this paper, we propose a novel model that enables us to characterize
important security properties of caches. Our model encompasses two aspects: (1)
The amount of information that can be absorbed by a cache, and (2) the amount
of information that can effectively be extracted from the cache by an
adversary. We use our model to compute both quantities for common cache
replacement policies (FIFO, LRU, and PLRU) and to compare their isolation
properties. We further show how our model for information extraction leads to
an algorithm that can be used to improve the bounds delivered by the CacheAudit
static analyzer.
",1,0,0,0,0,0
6837,6838,Blind Demixing and Deconvolution at Near-Optimal Rate,"  We consider simultaneous blind deconvolution of r source signals from their
noisy superposition, a problem also referred to blind demixing and
deconvolution. This signal processing problem occurs in the context of the
Internet of Things where a massive number of sensors sporadically communicate
only short messages over unknown channels. We show that robust recovery of
message and channel vectors can be achieved via convex optimization when random
linear encoding using i.i.d. complex Gaussian matrices is used at the devices
and the number of required measurements at the receiver scales with the degrees
of freedom of the overall estimation problem. Since the scaling is linear in r
our result significantly improves over recent works.
",1,0,0,0,0,0
10358,10359,Deep Rewiring: Training very sparse deep networks,"  Neuromorphic hardware tends to pose limits on the connectivity of deep
networks that one can run on them. But also generic hardware and software
implementations of deep learning run more efficiently for sparse networks.
Several methods exist for pruning connections of a neural network after it was
trained without connectivity constraints. We present an algorithm, DEEP R, that
enables us to train directly a sparsely connected neural network. DEEP R
automatically rewires the network during supervised training so that
connections are there where they are most needed for the task, while its total
number is all the time strictly bounded. We demonstrate that DEEP R can be used
to train very sparse feedforward and recurrent neural networks on standard
benchmark tasks with just a minor loss in performance. DEEP R is based on a
rigorous theoretical foundation that views rewiring as stochastic sampling of
network configurations from a posterior.
",1,0,0,1,0,0
18698,18699,Borel subsets of the real line and continuous reducibility,"  We study classes of Borel subsets of the real line $\mathbb{R}$ such as
levels of the Borel hierarchy and the class of sets that are reducible to the
set $\mathbb{Q}$ of rationals, endowed with the Wadge quasi-order of
reducibility with respect to continuous functions on $\mathbb{R}$. Notably, we
explore several structural properties of Borel subsets of $\mathbb{R}$ that
diverge from those of Polish spaces with dimension zero. Our first main result
is on the existence of embeddings of several posets into the restriction of
this quasi-order to any Borel class that is strictly above the classes of open
and closed sets, for instance the linear order $\omega_1$, its reverse
$\omega_1^\star$ and the poset $\mathcal{P}(\omega)/\mathsf{fin}$ of inclusion
modulo finite error. As a consequence of its proof, it is shown that there are
no complete sets for these classes. We further extend the previous theorem to
targets that are reducible to $\mathbb{Q}$. These non-structure results
motivate the study of further restrictions of the Wadge quasi-order. In our
second main theorem, we introduce a combinatorial property that is shown to
characterize those $F_\sigma$ sets that are reducible to $\mathbb{Q}$. This is
applied to construct a minimal set below $\mathbb{Q}$ and prove its uniqueness
up to Wadge equivalence. We finally prove several results concerning gaps and
cardinal characteristics of the Wadge quasi-order and thereby answer questions
of Brendle and Geschke.
",0,0,1,0,0,0
15009,15010,A Stochastic Formulation of the Resolution of Identity: Application to Second Order Møller-Plesset Perturbation Theory,"  A stochastic orbital approach to the resolution of identity (RI)
approximation for 4-index 2-electron electron repulsion integrals (ERIs) is
presented. The stochastic RI-ERIs are then applied to M\o ller-Plesset
perturbation theory (MP2) utilizing a \textit{multiple stochastic orbital
approach}. The introduction of multiple stochastic orbitals results in an $N^3$
scaling for both the stochastic RI-ERIs and stochastic RI-MP2. We demonstrate
that this method exhibits a small prefactor and an observed scaling of
$N^{2.4}$ for a range of water clusters, already outperforming MP2 for clusters
with as few as 21 water molecules.
",0,1,0,0,0,0
8375,8376,When Can Neural Networks Learn Connected Decision Regions?,"  Previous work has questioned the conditions under which the decision regions
of a neural network are connected and further showed the implications of the
corresponding theory to the problem of adversarial manipulation of classifiers.
It has been proven that for a class of activation functions including leaky
ReLU, neural networks having a pyramidal structure, that is no layer has more
hidden units than the input dimension, produce necessarily connected decision
regions. In this paper, we advance this important result by further developing
the sufficient and necessary conditions under which the decision regions of a
neural network are connected. We then apply our framework to overcome the
limits of existing work and further study the capacity to learn connected
regions of neural networks for a much wider class of activation functions
including those widely used, namely ReLU, sigmoid, tanh, softlus, and
exponential linear function.
",1,0,0,1,0,0
3916,3917,Experimental study of mini-magnetosphere,"  Magnetosphere at ion kinetic scales, or mini-magnetosphere, possesses unusual
features as predicted by numerical simulations. However, there are practically
no data on the subject from space observations and the data which are available
are far too incomplete. In the present work we describe results of laboratory
experiment on interaction of plasma flow with magnetic dipole with parameters
such that ion inertia length is smaller than a size of observed magnetosphere.
A detailed structure of non-coplanar or out-of-plane component of magnetic
field has been obtained in meridian plane. Independence of this component on
dipole moment reversal, as was reported in previous work, has been verified. In
the tail distinct lobes and central current sheet have been observed. It was
found that lobe regions adjacent to boundary layer are dominated by
non-coplanar component of magnetic field. Tail-ward oriented electric current
in plasma associated with that component appears to be equal to ion current in
the frontal part of magnetosphere and in the tail current sheet implying that
electrons are stationary in those regions while ions flow by. Obtained data
strongly support the proposed model of mini-magnetosphere based on two-fluid
effects as described by the Hall term.
",0,1,0,0,0,0
12650,12651,Scalable Greedy Feature Selection via Weak Submodularity,"  Greedy algorithms are widely used for problems in machine learning such as
feature selection and set function optimization. Unfortunately, for large
datasets, the running time of even greedy algorithms can be quite high. This is
because for each greedy step we need to refit a model or calculate a function
using the previously selected choices and the new candidate.
Two algorithms that are faster approximations to the greedy forward selection
were introduced recently ([Mirzasoleiman et al. 2013, 2015]). They achieve
better performance by exploiting distributed computation and stochastic
evaluation respectively. Both algorithms have provable performance guarantees
for submodular functions.
In this paper we show that divergent from previously held opinion,
submodularity is not required to obtain approximation guarantees for these two
algorithms. Specifically, we show that a generalized concept of weak
submodularity suffices to give multiplicative approximation guarantees. Our
result extends the applicability of these algorithms to a larger class of
functions. Furthermore, we show that a bounded submodularity ratio can be used
to provide data dependent bounds that can sometimes be tighter also for
submodular functions. We empirically validate our work by showing superior
performance of fast greedy approximations versus several established baselines
on artificial and real datasets.
",1,0,0,1,0,0
17282,17283,Recent Advances in Neural Program Synthesis,"  In recent years, deep learning has made tremendous progress in a number of
fields that were previously out of reach for artificial intelligence. The
successes in these problems has led researchers to consider the possibilities
for intelligent systems to tackle a problem that humans have only recently
themselves considered: program synthesis. This challenge is unlike others such
as object recognition and speech translation, since its abstract nature and
demand for rigor make it difficult even for human minds to attempt. While it is
still far from being solved or even competitive with most existing methods,
neural program synthesis is a rapidly growing discipline which holds great
promise if completely realized. In this paper, we start with exploring the
problem statement and challenges of program synthesis. Then, we examine the
fascinating evolution of program induction models, along with how they have
succeeded, failed and been reimagined since. Finally, we conclude with a
contrastive look at program synthesis and future research recommendations for
the field.
",1,0,0,0,0,0
4822,4823,Optimizing Prediction Intervals by Tuning Random Forest via Meta-Validation,"  Recent studies have shown that tuning prediction models increases prediction
accuracy and that Random Forest can be used to construct prediction intervals.
However, to our best knowledge, no study has investigated the need to, and the
manner in which one can, tune Random Forest for optimizing prediction intervals
{ this paper aims to fill this gap. We explore a tuning approach that combines
an effectively exhaustive search with a validation technique on a single Random
Forest parameter. This paper investigates which, out of eight validation
techniques, are beneficial for tuning, i.e., which automatically choose a
Random Forest configuration constructing prediction intervals that are reliable
and with a smaller width than the default configuration. Additionally, we
present and validate three meta-validation techniques to determine which are
beneficial, i.e., those which automatically chose a beneficial validation
technique. This study uses data from our industrial partner (Keymind Inc.) and
the Tukutuku Research Project, related to post-release defect prediction and
Web application effort estimation, respectively. Results from our study
indicate that: i) the default configuration is frequently unreliable, ii) most
of the validation techniques, including previously successfully adopted ones
such as 50/50 holdout and bootstrap, are counterproductive in most of the
cases, and iii) the 75/25 holdout meta-validation technique is always
beneficial; i.e., it avoids the likely counterproductive effects of validation
techniques.
",0,0,0,1,0,0
10745,10746,Mixed one-bit compressive sensing with applications to overexposure correction for CT reconstruction,"  When a measurement falls outside the quantization or measurable range, it
becomes saturated and cannot be used in classical reconstruction methods. For
example, in C-arm angiography systems, which provide projection radiography,
fluoroscopy, digital subtraction angiography, and are widely used for medical
diagnoses and interventions, the limited dynamic range of C-arm flat detectors
leads to overexposure in some projections during an acquisition, such as
imaging relatively thin body parts (e.g., the knee). Aiming at overexposure
correction for computed tomography (CT) reconstruction, we in this paper
propose a mixed one-bit compressive sensing (M1bit-CS) to acquire information
from both regular and saturated measurements. This method is inspired by the
recent progress on one-bit compressive sensing, which deals with only sign
observations. Its successful applications imply that information carried by
saturated measurements is useful to improve recovery quality. For the proposed
M1bit-CS model, alternating direction methods of multipliers is developed and
an iterative saturation detection scheme is established. Then we evaluate
M1bit-CS on one-dimensional signal recovery tasks. In some experiments, the
performance of the proposed algorithms on mixed measurements is almost the same
as recovery on unsaturated ones with the same amount of measurements. Finally,
we apply the proposed method to overexposure correction for CT reconstruction
on a phantom and a simulated clinical image. The results are promising, as the
typical streaking artifacts and capping artifacts introduced by saturated
projection data are effectively reduced, yielding significant error reduction
compared with existing algorithms based on extrapolation.
",1,0,1,0,0,0
14927,14928,Consistent estimation of the spectrum of trace class data augmentation algorithms,"  Markov chain Monte Carlo is widely used in a variety of scientific
applications to generate approximate samples from intractable distributions. A
thorough understanding of the convergence and mixing properties of these Markov
chains can be obtained by studying the spectrum of the associated Markov
operator. While several methods to bound/estimate the second largest eigenvalue
are available in the literature, very few general techniques for consistent
estimation of the entire spectrum have been proposed. Existing methods for this
purpose require the Markov transition density to be available in closed form,
which is often not true in practice, especially in modern statistical
applications. In this paper, we propose a novel method to consistently estimate
the entire spectrum of a general class of Markov chains arising from a popular
and widely used statistical approach known as Data Augmentation. The transition
densities of these Markov chains can often only be expressed as intractable
integrals. We illustrate the applicability of our method using real and
simulated data.
",0,0,0,1,0,0
2608,2609,Towards a More Reliable Privacy-preserving Recommender System,"  This paper proposes a privacy-preserving distributed recommendation
framework, Secure Distributed Collaborative Filtering (SDCF), to preserve the
privacy of value, model and existence altogether. That says, not only the
ratings from the users to the items, but also the existence of the ratings as
well as the learned recommendation model are kept private in our framework. Our
solution relies on a distributed client-server architecture and a two-stage
Randomized Response algorithm, along with an implementation on the popular
recommendation model, Matrix Factorization (MF). We further prove SDCF to meet
the guarantee of Differential Privacy so that clients are allowed to specify
arbitrary privacy levels. Experiments conducted on numerical rating prediction
and one-class rating action prediction exhibit that SDCF does not sacrifice too
much accuracy for privacy.
",1,0,0,0,0,0
10939,10940,Deep learning for studies of galaxy morphology,"  Establishing accurate morphological measurements of galaxies in a reasonable
amount of time for future big-data surveys such as EUCLID, the Large Synoptic
Survey Telescope or the Wide Field Infrared Survey Telescope is a challenge.
Because of its high level of abstraction with little human intervention, deep
learning appears to be a promising approach. Deep learning is a rapidly growing
discipline that models high-level patterns in data as complex multilayered
networks. In this work we test the ability of deep convolutional networks to
provide parametric properties of Hubble Space Telescope like galaxies
(half-light radii, Sersic indices, total flux etc..). We simulate a set of
galaxies including point spread function and realistic noise from the CANDELS
survey and try to recover the main galaxy parameters using deep-learning. We
com- pare the results with the ones obtained with the commonly used profile
fitting based software GALFIT. This way showing that with our method we obtain
results at least equally good as the ones obtained with GALFIT but, once
trained, with a factor 5 hundred time faster.
",0,1,0,0,0,0
12050,12051,Pushing STEM-education through a social-media-based contest format - experiences and lessons-learned from the H2020-project SciChallenge,"  Science education is a crucial issue with long-term impacts for Europe as the
low enrolment rates in the STEM-fields, including (natural) science,
technology, engineering and mathematics, will lead to a workforce problem in
research and development. In order to address this challenge, the EU-funded
research project SciChallenge (project.scichallenge.eu) aims to find a new way
for getting young people more interested in STEM. For this purpose, the project
developed and implemented a social-media-based STEM-contest for young people,
which aims at increasing the attractiveness of science education and careers
among young people. In the first two parts, the paper reflects on the problem,
introduces the project and highlights the main steps of the preparation of the
contest. The third section of the paper presents the idea, design and
implementation of the digital contest platform (www.scichallenge.eu), which
serves as the core of the challenge. The fourth part of the paper will provide
a status update on the contest pilot. It will provide a summary of the
experiences that the consortium made with this novel approach as well as the
main obstacles that the consortium was facing. The paper will conclude with a
preliminary reflection on the question if such an approach can help to increase
the interest of young people in STEM-education and careers.
",1,0,0,0,0,0
8570,8571,Personalized Driver Stress Detection with Multi-task Neural Networks using Physiological Signals,"  Stress can be seen as a physiological response to everyday emotional, mental
and physical challenges. A long-term exposure to stressful situations can have
negative health consequences, such as increased risk of cardiovascular diseases
and immune system disorder. Therefore, a timely stress detection can lead to
systems for better management and prevention in future circumstances. In this
paper, we suggest a multi-task learning based neural network approach (with
hard parameter sharing of mutual representation and task-specific layers) for
personalized stress recognition using skin conductance and heart rate from
wearable devices. The proposed method is tested on multi-modal physiological
responses collected during real-world and simulator driving tasks.
",1,0,0,0,0,0
11570,11571,"Geometric Ergodicity of the MUCOGARCH(1,1) process","  For the multivariate COGARCH(1,1) volatility process we show sufficient
conditions for the existence of a unique stationary distribution, for the
geometric ergodicity and for the finiteness of moments of the stationary
distribution. One of the conditions demands a sufficiently fast exponential
decay of the MUCOGARCH(1,1) volatility process. Furthermore, we show easily
applicable sufficient conditions for the needed irreducibility of the
volatility process living in the cone of positive semidefinite matrices, if the
driving Lévy process is a compound Poisson process.
",0,0,1,0,0,0
17919,17920,Leveraging Deep Neural Network Activation Entropy to cope with Unseen Data in Speech Recognition,"  Unseen data conditions can inflict serious performance degradation on systems
relying on supervised machine learning algorithms. Because data can often be
unseen, and because traditional machine learning algorithms are trained in a
supervised manner, unsupervised adaptation techniques must be used to adapt the
model to the unseen data conditions. However, unsupervised adaptation is often
challenging, as one must generate some hypothesis given a model and then use
that hypothesis to bootstrap the model to the unseen data conditions.
Unfortunately, reliability of such hypotheses is often poor, given the mismatch
between the training and testing datasets. In such cases, a model hypothesis
confidence measure enables performing data selection for the model adaptation.
Underlying this approach is the fact that for unseen data conditions, data
variability is introduced to the model, which the model propagates to its
output decision, impacting decision reliability. In a fully connected network,
this data variability is propagated as distortions from one layer to the next.
This work aims to estimate the propagation of such distortion in the form of
network activation entropy, which is measured over a short- time running window
on the activation from each neuron of a given hidden layer, and these
measurements are then used to compute summary entropy. This work demonstrates
that such an entropy measure can help to select data for unsupervised model
adaptation, resulting in performance gains in speech recognition tasks. Results
from standard benchmark speech recognition tasks show that the proposed
approach can alleviate the performance degradation experienced under unseen
data conditions by iteratively adapting the model to the unseen datas acoustic
condition.
",1,0,0,1,0,0
2949,2950,"When Do Birds of a Feather Flock Together? k-Means, Proximity, and Conic Programming","  Given a set of data, one central goal is to group them into clusters based on
some notion of similarity between the individual objects. One of the most
popular and widely-used approaches is k-means despite the computational
hardness to find its global minimum. We study and compare the properties of
different convex relaxations by relating them to corresponding proximity
conditions, an idea originally introduced by Kumar and Kannan. Using conic
duality theory, we present an improved proximity condition under which the
Peng-Wei relaxation of k-means recovers the underlying clusters exactly. Our
proximity condition improves upon Kumar and Kannan, and is comparable to that
of Awashti and Sheffet where proximity conditions are established for
projective k-means. In addition, we provide a necessary proximity condition for
the exactness of the Peng-Wei relaxation. For the special case of equal cluster
sizes, we establish a different and completely localized proximity condition
under which the Amini-Levina relaxation yields exact clustering, thereby having
addressed an open problem by Awasthi and Sheffet in the balanced case. Our
framework is not only deterministic and model-free but also comes with a clear
geometric meaning which allows for further analysis and generalization.
Moreover, it can be conveniently applied to analyzing various data generative
models such as the stochastic ball models and Gaussian mixture models. With
this method, we improve the current minimum separation bound for the stochastic
ball models and achieve the state-of-the-art results of learning Gaussian
mixture models.
",0,0,1,0,0,0
6327,6328,A sharp lower bound for the lifespan of small solutions to the Schrödinger equation with a subcritical power nonlinearity,"  Let $T_{\epsilon}$ be the lifespan for the solution to the Schrödinger
equation on $\mathbb{R}^d$ with a power nonlinearity $\lambda |u|^{2\theta/d}u$
($\lambda \in \mathbb{C}$, $0<\theta<1$) and the initial data in the form
$\epsilon \varphi(x)$. We provide a sharp lower bound estimate for
$T_{\epsilon}$ as $\epsilon \to +0$ which can be written explicitly by
$\lambda$, $d$, $\theta$, $\varphi$ and $\epsilon$. This is an improvement of
the previous result by H.Sasaki [Adv. Diff. Eq. 14 (2009), 1021--1039].
",0,0,1,0,0,0
10193,10194,Convergence of ground state solutions for nonlinear Schrödinger equations on graphs,"  We consider the nonlinear Schrödinger equation $-\Delta u+(\lambda
a(x)+1)u=|u|^{p-1}u$ on a locally finite graph $G=(V,E)$. We prove via the
Nehari method that if $a(x)$ satisfies certain assumptions, for any
$\lambda>1$, the equation admits a ground state solution $u_\lambda$. Moreover,
as $\lambda\rightarrow \infty$, the solution $u_\lambda$ converges to a
solution of the Dirichlet problem $-\Delta u+u=|u|^{p-1}u$ which is defined on
the potential well $\Omega$. We also provide a numerical experiment which
solves the equation on a finite graph to illustrate our results.
",0,0,1,0,0,0
20232,20233,Multi-resolution polymer Brownian dynamics with hydrodynamic interactions,"  A polymer model given in terms of beads, interacting through Hookean springs
and hydrodynamic forces, is studied. Brownian dynamics description of this
bead-spring polymer model is extended to multiple resolutions. Using this
multiscale approach, a modeller can efficiently look at different regions of
the polymer in different spatial and temporal resolutions with scalings given
for the number of beads, statistical segment length and bead radius in order to
maintain macro-scale properties of the polymer filament. The Boltzmann
distribution of a Gaussian chain for differing statistical segment lengths
gives a Langevin equation for the multi-resolution model with a mobility tensor
for different bead sizes. Using the pre-averaging approximation, the
translational diffusion coefficient is obtained as a function of the inverse of
a matrix and then in closed form in the long-chain limit. This is then
confirmed with numerical experiments.
",0,1,0,0,0,0
20968,20969,Uniform diamond coatings on WC-Co hard alloy cutting inserts deposited by a microwave plasma CVD,"  Polycrystalline diamond coatings have been grown on cemented carbide
substrates with different aspect ratios by a microwave plasma CVD in
methane-hydrogen gas mixtures. To protect the edges of the substrates from
non-uniform heating due to the plasma edge effect, a special plateholder with
pockets for group growth has been used. The difference in heights of the
substrates and plateholder, and its influence on the diamond film mean grain
size, growth rate, phase composition and stress was investigated. The substrate
temperature range, within which uniform diamond films are produced with good
adhesion, is determined. The diamond-coated cutting inserts produced at
optimized process exhibited a reduction of cutting force and wear resistance by
a factor of two, and cutting efficiency increase by 4.3 times upon turning A390
Al-Si alloy as compared to performance of uncoated tools.
",0,1,0,0,0,0
11592,11593,Database Engines: Evolution of Greenness,"  Context: Information Technology consumes up to 10\% of the world's
electricity generation, contributing to CO2 emissions and high energy costs.
Data centers, particularly databases, use up to 23% of this energy. Therefore,
building an energy-efficient (green) database engine could reduce energy
consumption and CO2 emissions.
Goal: To understand the factors driving databases' energy consumption and
execution time throughout their evolution.
Method: We conducted an empirical case study of energy consumption by two
MySQL database engines, InnoDB and MyISAM, across 40 releases. We examined the
relationships of four software metrics to energy consumption and execution time
to determine which metrics reflect the greenness and performance of a database.
Results: Our analysis shows that database engines' energy consumption and
execution time increase as databases evolve. Moreover, the Lines of Code metric
is correlated moderately to strongly with energy consumption and execution time
in 88% of cases.
Conclusions: Our findings provide insights to both practitioners and
researchers. Database administrators may use them to select a fast, green
release of the MySQL database engine. MySQL database-engine developers may use
the software metric to assess products' greenness and performance. Researchers
may use our findings to further develop new hypotheses or build models to
predict greenness and performance of databases.
",1,0,0,0,0,0
4774,4775,Efficient Dense Labeling of Human Activity Sequences from Wearables using Fully Convolutional Networks,"  Recognizing human activities in a sequence is a challenging area of research
in ubiquitous computing. Most approaches use a fixed size sliding window over
consecutive samples to extract features---either handcrafted or learned
features---and predict a single label for all samples in the window. Two key
problems emanate from this approach: i) the samples in one window may not
always share the same label. Consequently, using one label for all samples
within a window inevitably lead to loss of information; ii) the testing phase
is constrained by the window size selected during training while the best
window size is difficult to tune in practice. We propose an efficient algorithm
that can predict the label of each sample, which we call dense labeling, in a
sequence of human activities of arbitrary length using a fully convolutional
network. In particular, our approach overcomes the problems posed by the
sliding window step. Additionally, our algorithm learns both the features and
classifier automatically. We release a new daily activity dataset based on a
wearable sensor with hospitalized patients. We conduct extensive experiments
and demonstrate that our proposed approach is able to outperform the
state-of-the-arts in terms of classification and label misalignment measures on
three challenging datasets: Opportunity, Hand Gesture, and our new dataset.
",1,0,0,0,0,0
914,915,Riemannian stochastic variance reduced gradient,"  Stochastic variance reduction algorithms have recently become popular for
minimizing the average of a large but finite number of loss functions. In this
paper, we propose a novel Riemannian extension of the Euclidean stochastic
variance reduced gradient algorithm (R-SVRG) to a manifold search space. The
key challenges of averaging, adding, and subtracting multiple gradients are
addressed with retraction and vector transport. We present a global convergence
analysis of the proposed algorithm with a decay step size and a local
convergence rate analysis under a fixed step size under some natural
assumptions. The proposed algorithm is applied to problems on the Grassmann
manifold, such as principal component analysis, low-rank matrix completion, and
computation of the Karcher mean of subspaces, and outperforms the standard
Riemannian stochastic gradient descent algorithm in each case.
",1,0,1,1,0,0
17815,17816,A finite field analogue for Appell series F_3,"  In this paper we introduce a finite field analogue for the Appell series F_3
and give some reduction formulae and certain generating functions for this
function over finite fields.
",0,0,1,0,0,0
4135,4136,Corrupt Bandits for Preserving Local Privacy,"  We study a variant of the stochastic multi-armed bandit (MAB) problem in
which the rewards are corrupted. In this framework, motivated by privacy
preservation in online recommender systems, the goal is to maximize the sum of
the (unobserved) rewards, based on the observation of transformation of these
rewards through a stochastic corruption process with known parameters. We
provide a lower bound on the expected regret of any bandit algorithm in this
corrupted setting. We devise a frequentist algorithm, KLUCB-CF, and a Bayesian
algorithm, TS-CF and give upper bounds on their regret. We also provide the
appropriate corruption parameters to guarantee a desired level of local privacy
and analyze how this impacts the regret. Finally, we present some experimental
results that confirm our analysis.
",1,0,0,1,0,0
17989,17990,Active Bias: Training More Accurate Neural Networks by Emphasizing High Variance Samples,"  Self-paced learning and hard example mining re-weight training instances to
improve learning accuracy. This paper presents two improved alternatives based
on lightweight estimates of sample uncertainty in stochastic gradient descent
(SGD): the variance in predicted probability of the correct class across
iterations of mini-batch SGD, and the proximity of the correct class
probability to the decision threshold. Extensive experimental results on six
datasets show that our methods reliably improve accuracy in various network
architectures, including additional gains on top of other popular training
techniques, such as residual learning, momentum, ADAM, batch normalization,
dropout, and distillation.
",0,0,0,1,0,0
18713,18714,Clustering with Temporal Constraints on Spatio-Temporal Data of Human Mobility,"  Extracting significant places or places of interest (POIs) using individuals'
spatio-temporal data is of fundamental importance for human mobility analysis.
Classical clustering methods have been used in prior work for detecting POIs,
but without considering temporal constraints. Usually, the involved parameters
for clustering are difficult to determine, e.g., the optimal cluster number in
hierarchical clustering. Currently, researchers either choose heuristic values
or use spatial distance-based optimization to determine an appropriate
parameter set. We argue that existing research does not optimally address
temporal information and thus leaves much room for improvement. Considering
temporal constraints in human mobility, we introduce an effective clustering
approach - namely POI clustering with temporal constraints (PC-TC) - to extract
POIs from spatio-temporal data of human mobility. Following human mobility
nature in modern society, our approach aims to extract both global POIs (e.g.,
workplace or university) and local POIs (e.g., library, lab, and canteen).
Based on two publicly available datasets including 193 individuals, our
evaluation results show that PC-TC has much potential for next place prediction
in terms of granularity (i.e., the number of extracted POIs) and
predictability.
",0,0,0,1,0,0
15360,15361,A Survey of Runtime Monitoring Instrumentation Techniques,"  Runtime Monitoring is a lightweight and dynamic verification technique that
involves observing the internal operations of a software system and/or its
interactions with other external entities, with the aim of determining whether
the system satisfies or violates a correctness specification. Compilation
techniques employed in Runtime Monitoring tools allow monitors to be
automatically derived from high-level correctness specifications (aka.
properties). This allows the same property to be converted into different types
of monitors, which may apply different instrumentation techniques for checking
whether the property was satisfied or not. In this paper we compare and
contrast the various types of monitoring methodologies found in the current
literature, and classify them into a spectrum of monitoring instrumentation
techniques, ranging from completely asynchronous monitoring on the one end and
completely synchronous monitoring on the other.
",1,0,0,0,0,0
18670,18671,Visualization of Constraint Handling Rules: Semantics and Applications,"  The work in the paper presents an animation extension ($CHR^{vis}$) to
Constraint Handling Rules (CHR). Visualizations have always helped programmers
understand data and debug programs. A picture is worth a thousand words. It can
help identify where a problem is or show how something works. It can even
illustrate a relation that was not clear otherwise. $CHR^{vis}$ aims at
embedding animation and visualization features into CHR programs. It thus
enables users, while executing programs, to have such executions animated. The
paper aims at providing the operational semantics for $CHR^{vis}$. The
correctness of $CHR^{vis}$ programs is also discussed. Some applications of the
new extension are also introduced.
",1,0,0,0,0,0
17264,17265,Wave propagation and homogenization in 2D and 3D lattices: a semi-analytical approach,"  Wave motion in two- and three-dimensional periodic lattices of beam members
supporting longitudinal and flexural waves is considered. An analytic method
for solving the Bloch wave spectrum is developed, characterized by a
generalized eigenvalue equation obtained by enforcing the Floquet condition.
The dynamic stiffness matrix is shown to be explicitly Hermitian and to admit
positive eigenvalues. Lattices with hexagonal, rectangular, tetrahedral and
cubic unit cells are analyzed. The semi-analytical method can be asymptotically
expanded for low frequency yielding explicit forms for the Christoffel matrix
describing wave motion in the quasistatic limit.
",0,1,0,0,0,0
6312,6313,An approach to Griffiths conjecture,"  The Griffiths conjecture asserts that every ample vector bundle $E$ over a
compact complex manifold $S$ admits a hermitian metric with positive curvature
in the sense of Griffiths. In this article we give a sufficient condition for a
positive hermitian metric on $\mathcal{O}_{\mathbb{P}(E^*)}(1)$ to induce a
Griffiths positive $L^2$-metric on the vector bundle $E$. This result suggests
to study the relative Kähler-Ricci flow on $\mathcal{O}_{\mathbb{P}(E^*)}(1)$
for the fibration $\mathbb{P}(E^*)\to S$. We define a flow and give arguments
for the convergence.
",0,0,1,0,0,0
7399,7400,PerformanceNet: Score-to-Audio Music Generation with Multi-Band Convolutional Residual Network,"  Music creation is typically composed of two parts: composing the musical
score, and then performing the score with instruments to make sounds. While
recent work has made much progress in automatic music generation in the
symbolic domain, few attempts have been made to build an AI model that can
render realistic music audio from musical scores. Directly synthesizing audio
with sound sample libraries often leads to mechanical and deadpan results,
since musical scores do not contain performance-level information, such as
subtle changes in timing and dynamics. Moreover, while the task may sound like
a text-to-speech synthesis problem, there are fundamental differences since
music audio has rich polyphonic sounds. To build such an AI performer, we
propose in this paper a deep convolutional model that learns in an end-to-end
manner the score-to-audio mapping between a symbolic representation of music
called the piano rolls and an audio representation of music called the
spectrograms. The model consists of two subnets: the ContourNet, which uses a
U-Net structure to learn the correspondence between piano rolls and
spectrograms and to give an initial result; and the TextureNet, which further
uses a multi-band residual network to refine the result by adding the spectral
texture of overtones and timbre. We train the model to generate music clips of
the violin, cello, and flute, with a dataset of moderate size. We also present
the result of a user study that shows our model achieves higher mean opinion
score (MOS) in naturalness and emotional expressivity than a WaveNet-based
model and two commercial sound libraries. We open our source code at
this https URL
",1,0,0,0,0,0
19828,19829,A structure theorem for almost low-degree functions on the slice,"  The Fourier-Walsh expansion of a Boolean function $f \colon \{0,1\}^n
\rightarrow \{0,1\}$ is its unique representation as a multilinear polynomial.
The Kindler-Safra theorem (2002) asserts that if in the expansion of $f$, the
total weight on coefficients beyond degree $k$ is very small, then $f$ can be
approximated by a Boolean-valued function depending on at most $O(2^k)$
variables.
In this paper we prove a similar theorem for Boolean functions whose domain
is the `slice' ${{[n]}\choose{pn}} = \{x \in \{0,1\}^n\colon \sum_i x_i =
pn\}$, where $0 \ll p \ll 1$, with respect to their unique representation as
harmonic multilinear polynomials. We show that if in the representation of
$f\colon {{[n]}\choose{pn}} \rightarrow \{0,1\}$, the total weight beyond
degree $k$ is at most $\epsilon$, where $\epsilon = \min(p, 1-p)^{O(k)}$, then
$f$ can be $O(\epsilon)$-approximated by a degree-$k$ Boolean function on the
slice, which in turn depends on $O(2^{k})$ coordinates. This proves a
conjecture of Filmus, Kindler, Mossel, and Wimmer (2015). Our proof relies on
hypercontractivity, along with a novel kind of a shifting procedure.
In addition, we show that the approximation rate in the Kindler-Safra theorem
can be improved from $\epsilon + \exp(O(k)) \epsilon^{1/4}$ to
$\epsilon+\epsilon^2 (2\ln(1/\epsilon))^k/k!$, which is tight in terms of the
dependence on $\epsilon$ and misses at most a factor of $2^{O(k)}$ in the
lower-order term.
",1,0,0,0,0,0
12649,12650,Quadrics and Scherk towers,"  We investigate the relation between quadrics and their Christoffel duals on
the one hand, and certain zero mean curvature surfaces and their Gauss maps on
the other hand. To study the relation between timelike minimal surfaces and the
Christoffel duals of 1-sheeted hyperboloids we introduce para-holomorphic
elliptic functions. The curves of type change for real isothermic surfaces of
mixed causal type turn out to be aligned with the real curvature line net.
",0,0,1,0,0,0
1242,1243,Analyzing Cloud Optical Properties Using Sky Cameras,"  Clouds play a significant role in the fluctuation of solar radiation received
by the earth's surface. It is important to study the various cloud properties,
as it impacts the total solar irradiance falling on the earth's surface. One of
such important optical properties of the cloud is the Cloud Optical Thickness
(COT). It is defined with the amount of light that can pass through the clouds.
The COT values are generally obtained from satellite images. However, satellite
images have a low temporal- and spatial- resolutions; and are not suitable for
study in applications as solar energy generation and forecasting. Therefore,
ground-based sky cameras are now getting popular in such fields. In this paper,
we analyze the cloud optical thickness value, from the ground-based sky
cameras, and provide future research directions.
",0,1,0,0,0,0
2213,2214,Supermodular Optimization for Redundant Robot Assignment under Travel-Time Uncertainty,"  This paper considers the assignment of multiple mobile robots to goal
locations under uncertain travel time estimates. Our aim is to produce optimal
assignments, such that the average waiting time at destinations is minimized.
Our premise is that time is the most valuable asset in the system. Hence, we
make use of redundant robots to counter the effect of uncertainty. Since
solving the redundant assignment problem is strongly NP-hard, we exploit
structural properties of our problem to propose a polynomial-time, near-optimal
solution. We demonstrate that our problem can be reduced to minimizing a
supermodular cost function subject to a matroid constraint. This allows us to
develop a greedy algorithm, for which we derive sub-optimality bounds. A
comparison with the baseline non-redundant assignment shows that redundant
assignment reduces the waiting time at goals, and that this performance gap
increases as noise increases. Finally, we evaluate our method on a mobility
data set (specifying vehicle availability and passenger requests), recorded in
the area of Manhattan, New York. Our algorithm performs in real-time, and
reduces passenger waiting times when travel times are uncertain.
",1,0,0,0,0,0
1199,1200,Towards a realistic NNLIF model: Analysis and numerical solver for excitatory-inhibitory networks with delay and refractory periods,"  The Network of Noisy Leaky Integrate and Fire (NNLIF) model describes the
behavior of a neural network at mesoscopic level. It is one of the simplest
self-contained mean-field models considered for that purpose. Even so, to study
the mathematical properties of the model some simplifications were necessary
Cáceres-Carrillo-Perthame(2011), Cáceres-Perthame(2014),
Cáceres-Schneider(2017), which disregard crucial phenomena. In this work we
deal with the general NNLIF model without simplifications. It involves a
network with two populations (excitatory and inhibitory), with transmission
delays between the neurons and where the neurons remain in a refractory state
for a certain time. We have studied the number of steady states in terms of the
model parameters, the long time behaviour via the entropy method and
Poincaré's inequality, blow-up phenomena, and the importance of transmission
delays between excitatory neurons to prevent blow-up and to give rise to
synchronous solutions. Besides analytical results, we have presented a
numerical resolutor for this model, based on high order flux-splitting WENO
schemes and an explicit third order TVD Runge-Kutta method, in order to
describe the wide range of phenomena exhibited by the network: blow-up,
asynchronous/synchronous solutions and instability/stability of the steady
states; the solver also allows us to observe the time evolution of the firing
rates, refractory states and the probability distributions of the excitatory
and inhibitory populations.
",0,0,1,0,0,0
1120,1121,Detecting Bot Activity in the Ethereum Blockchain Network,"  The Ethereum blockchain network is a decentralized platform enabling smart
contract execution and transactions of Ether (ETH) [1], its designated
cryptocurrency. Ethereum is the second most popular cryptocurrency with a
market cap of more than 100 billion USD, with hundreds of thousands of
transactions executed daily by hundreds of thousands of unique wallets. Tens of
thousands of those wallets are newly generated each day. The Ethereum platform
enables anyone to freely open multiple new wallets [2] free of charge
(resulting in a large number of wallets that are controlled by the same
entities). This attribute makes the Ethereum network a breeding space for
activity by software robots (bots). The existence of bots is widespread in
different digital technologies and there are various approaches to detect their
activity such as rule-base, clustering, machine learning and more [3,4]. In
this work we demonstrate how bot detection can be implemented using a network
theory approach.
",1,0,0,0,0,0
17376,17377,Taylor coefficients of non-holomorphic Jacobi forms and applications,"  In this paper, we prove modularity results of Taylor coefficients of certain
non-holomorphic Jacobi forms. It is well-known that Taylor coefficients of
holomorphic Jacobi forms are quasimoular forms. However recently there has been
a wide interest for Taylor coefficients of non-holomorphic Jacobi forms for
example arising in combinatorics. In this paper, we show that such coefficients
still inherit modular properties. We then work out the precise spaces in which
these coefficients lie for two examples.
",0,0,1,0,0,0
11166,11167,Least models of second-order set theories,"  The main theorems of this paper are (1) there is no least transitive model of
Kelley--Morse set theory $\mathsf{KM}$ and (2) there is a least
$\beta$-model---that is, a transitive model which is correct about which of its
classes are well-founded---of Gödel--Bernays set theory $\mathsf{GBC}$ +
Elementary Transfinite Recursion. Along the way I characterize when a countable
model of $\mathsf{ZFC}$ has a least $\mathsf{GBC}$-realization and show that no
countable model of $\mathsf{ZFC}$ has a least $\mathsf{KM}$-realization. I also
show that fragments of Elementary Transfinite Recursion have least
$\beta$-models and, for sufficiently weak fragments, least transitive models.
These fragments can be separated from each other and from the full principle of
Elementary Transfinite Recursion by consistency strength. The main question
left unanswered by this article is whether there is a least transitive model of
$\mathsf{GBC}$ + Elementary Transfinite Recursion.
",0,0,1,0,0,0
1915,1916,"Seasonal evolution of $\mathrm{C_2N_2}$, $\mathrm{C_3H_4}$, and $\mathrm{C_4H_2}$ abundances in Titan's lower stratosphere","  We study the seasonal evolution of Titan's lower stratosphere (around
15~mbar) in order to better understand the atmospheric dynamics and chemistry
in this part of the atmosphere. We analysed Cassini/CIRS far-IR observations
from 2006 to 2016 in order to measure the seasonal variations of three
photochemical by-products: $\mathrm{C_4H_2}$, $\mathrm{C_3H_4}$, and
$\mathrm{C_2N_2}$. We show that the abundances of these three gases have
evolved significantly at northern and southern high latitudes since 2006. We
measure a sudden and steep increase of the volume mixing ratios of
$\mathrm{C_4H_2}$, $\mathrm{C_3H_4}$, and $\mathrm{C_2N_2}$ at the south pole
from 2012 to 2013, whereas the abundances of these gases remained approximately
constant at the north pole over the same period. At northern mid-latitudes,
$\mathrm{C_2N_2}$ and $\mathrm{C_4H_2}$ abundances decrease after 2012 while
$\mathrm{C_3H_4}$ abundances stay constant. The comparison of these volume
mixing ratio variations with the predictions of photochemical and dynamical
models provides constraints on the seasonal evolution of atmospheric
circulation and chemical processes at play.
",0,1,0,0,0,0
15588,15589,Antibonding Ground state of Adatom Molecules in Bulk Dirac Semimetals,"  The ground state of the diatomic molecules in nature is inevitably bonding,
and its first excited state is antibonding. We demonstrate theoretically that,
for a pair of distant adatoms placed buried in three-dimensional-Dirac
semimetals, this natural order of the states can be reversed and an antibonding
ground state occurs at the lowest energy of the so-called bound states in the
continuum. We propose an experimental protocol with the use of a scanning
tunneling microscope tip to visualize the topographic map of the local density
of states on the surface of the system to reveal the emerging physics.
",0,1,0,0,0,0
2598,2599,Supervised learning with quantum enhanced feature spaces,"  Machine learning and quantum computing are two technologies each with the
potential for altering how computation is performed to address previously
untenable problems. Kernel methods for machine learning are ubiquitous for
pattern recognition, with support vector machines (SVMs) being the most
well-known method for classification problems. However, there are limitations
to the successful solution to such problems when the feature space becomes
large, and the kernel functions become computationally expensive to estimate. A
core element to computational speed-ups afforded by quantum algorithms is the
exploitation of an exponentially large quantum state space through controllable
entanglement and interference. Here, we propose and experimentally implement
two novel methods on a superconducting processor. Both methods represent the
feature space of a classification problem by a quantum state, taking advantage
of the large dimensionality of quantum Hilbert space to obtain an enhanced
solution. One method, the quantum variational classifier builds on [1,2] and
operates through using a variational quantum circuit to classify a training set
in direct analogy to conventional SVMs. In the second, a quantum kernel
estimator, we estimate the kernel function and optimize the classifier
directly. The two methods present a new class of tools for exploring the
applications of noisy intermediate scale quantum computers [3] to machine
learning.
",0,0,0,1,0,0
653,654,Highly accurate model for prediction of lung nodule malignancy with CT scans,"  Computed tomography (CT) examinations are commonly used to predict lung
nodule malignancy in patients, which are shown to improve noninvasive early
diagnosis of lung cancer. It remains challenging for computational approaches
to achieve performance comparable to experienced radiologists. Here we present
NoduleX, a systematic approach to predict lung nodule malignancy from CT data,
based on deep learning convolutional neural networks (CNN). For training and
validation, we analyze >1000 lung nodules in images from the LIDC/IDRI cohort.
All nodules were identified and classified by four experienced thoracic
radiologists who participated in the LIDC project. NoduleX achieves high
accuracy for nodule malignancy classification, with an AUC of ~0.99. This is
commensurate with the analysis of the dataset by experienced radiologists. Our
approach, NoduleX, provides an effective framework for highly accurate nodule
malignancy prediction with the model trained on a large patient population. Our
results are replicable with software available at
this http URL.
",0,0,0,1,1,0
14718,14719,Transiting Planets with LSST III: Detection Rate per Year of Operation,"  The Large Synoptic Survey Telescope (LSST) will generate light curves for
approximately 1 billion stars. Our previous work has demonstrated that, by the
end of the LSST 10 year mission, large numbers of transiting exoplanetary
systems could be recovered using the LSST ""deep drilling"" cadence. Here we
extend our previous work to examine how the recoverability of transiting
planets over a range of orbital periods and radii evolves per year of LSST
operation. As specific example systems we consider hot Jupiters orbiting
solar-type stars and hot Neptunes orbiting K-Dwarfs at distances from Earth of
several kpc, as well as super-Earths orbiting nearby low-mass M-dwarfs. The
detection of transiting planets increases steadily with the accumulation of
data over time, generally becoming large (greater than 10 percent) after 4 - 6
years of operation. However, we also find that short-period (less than 2 day)
hot Jupiters orbiting G-dwarfs and hot Neptunes orbiting K-dwarfs can already
be discovered within the first 1 - 2 years of LSST operation.
",0,1,0,0,0,0
1112,1113,Improved Algorithms for Computing the Cycle of Minimum Cost-to-Time Ratio in Directed Graphs,"  We study the problem of finding the cycle of minimum cost-to-time ratio in a
directed graph with $ n $ nodes and $ m $ edges. This problem has a long
history in combinatorial optimization and has recently seen interesting
applications in the context of quantitative verification. We focus on strongly
polynomial algorithms to cover the use-case where the weights are relatively
large compared to the size of the graph. Our main result is an algorithm with
running time $ \tilde O (m^{3/4} n^{3/2}) $, which gives the first improvement
over Megiddo's $ \tilde O (n^3) $ algorithm [JACM'83] for sparse graphs. We
further demonstrate how to obtain both an algorithm with running time $ n^3 /
2^{\Omega{(\sqrt{\log n})}} $ on general graphs and an algorithm with running
time $ \tilde O (n) $ on constant treewidth graphs. To obtain our main result,
we develop a parallel algorithm for negative cycle detection and single-source
shortest paths that might be of independent interest.
",1,0,0,0,0,0
14550,14551,Gevrey estimates for one dimensional parabolic invariant manifolds of non-hyperbolic fixed points,"  We study the Gevrey character of a natural parameterization of one
dimensional invariant manifolds associated to a parabolic direction of fixed
points of analytic maps, that is, a direction associated with an eigenvalue
equal to $1$. We show that, under general hypotheses, these invariant manifolds
are Gevrey with type related to some explicit constants. We provide examples of
the optimality of our results as well as some applications to celestial
mechanics, namely, the Sitnikov problem and the restricted planar three body
problem.
",0,0,1,0,0,0
10621,10622,"Performance of an Algorithm for Estimation of Flux, Background and Location on One-Dimensional Signals","  Optimal estimation of signal amplitude, background level, and photocentre
location is crucial to the combined extraction of astrometric and photometric
information from focal plane images, and in particular from the one-dimensional
measurements performed by Gaia on intermediate to faint magnitude stars. Our
goal is to define a convenient maximum likelihood framework, suited to
efficient iterative implementation and to assessment of noise level, bias, and
correlation among variables. The analytical model is investigated numerically
and verified by simulation over a range of magnitude and background values. The
estimates are unbiased, with a well-understood correlation between amplitude
and background, and with a much lower correlation of either of them with
location, further alleviated in case of signal symmetry. Two versions of the
algorithm are implemented and tested against each other, respectively, for
independent and combined parameter estimation. Both are effective and provide
consistent results, but the latter is more efficient because it takes into
account the flux-background estimate correlation.
",0,1,0,0,0,0
9275,9276,Cubic Fields: A Primer,"  We classify all cubic extensions of any field of arbitrary characteristic, up
to isomorphism, via an explicit construction involving three fundamental types
of cubic forms. We deduce a classification of any Galois cubic extension of a
field. The splitting and ramification of places in a separable cubic extension
of any global function field are completely determined, and precise
Riemann-Hurwitz formulae are given. In doing so, we determine the decomposition
of any cubic polynomial over a finite field.
",0,0,1,0,0,0
13503,13504,Warp: a method for neural network interpretability applied to gene expression profiles,"  We show a proof of principle for warping, a method to interpret the inner
working of neural networks in the context of gene expression analysis. Warping
is an efficient way to gain insight to the inner workings of neural nets and
make them more interpretable. We demonstrate the ability of warping to recover
meaningful information for a given class on a samplespecific individual basis.
We found warping works well in both linearly and nonlinearly separable
datasets. These encouraging results show that warping has a potential to be the
answer to neural networks interpretability in computational biology.
",1,0,0,0,0,0
2878,2879,Exponential Moving Average Model in Parallel Speech Recognition Training,"  As training data rapid growth, large-scale parallel training with multi-GPUs
cluster is widely applied in the neural network model learning currently.We
present a new approach that applies exponential moving average method in
large-scale parallel training of neural network model. It is a non-interference
strategy that the exponential moving average model is not broadcasted to
distributed workers to update their local models after model synchronization in
the training process, and it is implemented as the final model of the training
system. Fully-connected feed-forward neural networks (DNNs) and deep
unidirectional Long short-term memory (LSTM) recurrent neural networks (RNNs)
are successfully trained with proposed method for large vocabulary continuous
speech recognition on Shenma voice search data in Mandarin. The character error
rate (CER) of Mandarin speech recognition further degrades than
state-of-the-art approaches of parallel training.
",1,0,0,0,0,0
19564,19565,Energy Dissipation in Hamiltonian Chains of Rotators,"  We discuss, in the context of energy flow in high-dimensional systems and
Kolmogorov-Arnol'd-Moser (KAM) theory, the behavior of a chain of rotators
(rotors) which is purely Hamiltonian, apart from dissipation at just one end.
We derive bounds on the dissipation rate which become arbitrarily small in
certain physical regimes, and we present numerical evidence that these bounds
are sharp. We relate this to the decoupling of non-resonant terms as is known
in KAM problems.
",0,1,1,0,0,0
4735,4736,Controllability and optimal control of the transport equation with a localized vector field,"  We study controllability of a Partial Differential Equation of transport
type, that arises in crowd models. We are interested in controlling such system
with a control being a Lipschitz vector field on a fixed control set $\omega$.
We prove that, for each initial and final configuration, one can steer one to
another with such class of controls only if the uncontrolled dynamics allows to
cross the control set $\omega$. We also prove a minimal time result for such
systems. We show that the minimal time to steer one initial configuration to
another is related to the condition of having enough mass in $\omega$ to feed
the desired final configuration.
",0,0,1,0,0,0
6725,6726,"On the Hilbert coefficients, depth of associated graded rings and reduction numbers","  Let $(R,\mathfrak{m})$ be a $d$-dimensional Cohen-Macaulay local ring, $I$ an
$\mathfrak{m}$-primary ideal of $R$ and $J=(x_1,...,x_d)$ a minimal reduction
of $I$. We show that if $J_{d-1}=(x_1,...,x_{d-1})$ and
$\sum\limits_{n=1}^\infty\lambda{({I^{n+1}\cap J_{d-1}})/({J{I^n} \cap
J_{d-1}})=i}$ where i=0,1, then depth $G(I)\geq{d-i-1}$. Moreover, we prove
that if $e_2(I) = \sum_{n=2}^\infty (n-1) \lambda (I^n/JI^{n-1})-2;$ or if $I$
is integrally closed and $e_2(I) = \sum_{n=2}^\infty
(n-1)\lambda({I^{n}}/JI^{n-1})-i$ where $i=3,4$, then $e_1(I) =
\sum_{n=1}^\infty \lambda(I^n / JI^{n-1})-1.$ In addition, we show that $r(I)$
is independent. Furthermore, we study the independence of $r(I)$ with some
other conditions.
",0,0,1,0,0,0
11376,11377,Learning Rare Word Representations using Semantic Bridging,"  We propose a methodology that adapts graph embedding techniques (DeepWalk
(Perozzi et al., 2014) and node2vec (Grover and Leskovec, 2016)) as well as
cross-lingual vector space mapping approaches (Least Squares and Canonical
Correlation Analysis) in order to merge the corpus and ontological sources of
lexical knowledge. We also perform comparative analysis of the used algorithms
in order to identify the best combination for the proposed system. We then
apply this to the task of enhancing the coverage of an existing word
embedding's vocabulary with rare and unseen words. We show that our technique
can provide considerable extra coverage (over 99%), leading to consistent
performance gain (around 10% absolute gain is achieved with w2v-gn-500K cf.§
3.3) on the Rare Word Similarity dataset.
",1,0,0,0,0,0
17251,17252,Agent based simulation of the evolution of society as an alternate maximization problem,"  Understanding the evolution of human society, as a complex adaptive system,
is a task that has been looked upon from various angles. In this paper, we
simulate an agent-based model with a high enough population tractably. To do
this, we characterize an entity called \textit{society}, which helps us reduce
the complexity of each step from $\mathcal{O}(n^2)$ to $\mathcal{O}(n)$. We
propose a very realistic setting, where we design a joint alternate
maximization step algorithm to maximize a certain \textit{fitness} function,
which we believe simulates the way societies develop. Our key contributions
include (i) proposing a novel protocol for simulating the evolution of a
society with cheap, non-optimal joint alternate maximization steps (ii)
providing a framework for carrying out experiments that adhere to this
joint-optimization simulation framework (iii) carrying out experiments to show
that it makes sense empirically (iv) providing an alternate justification for
the use of \textit{society} in the simulations.
",1,0,0,1,0,0
6272,6273,A Trio Neural Model for Dynamic Entity Relatedness Ranking,"  Measuring entity relatedness is a fundamental task for many natural language
processing and information retrieval applications. Prior work often studies
entity relatedness in static settings and an unsupervised manner. However,
entities in real-world are often involved in many different relationships,
consequently entity-relations are very dynamic over time. In this work, we
propose a neural networkbased approach for dynamic entity relatedness,
leveraging the collective attention as supervision. Our model is capable of
learning rich and different entity representations in a joint framework.
Through extensive experiments on large-scale datasets, we demonstrate that our
method achieves better results than competitive baselines.
",0,0,0,1,0,0
800,801,Tangle-tree duality in abstract separation systems,"  We prove a general width duality theorem for combinatorial structures with
well-defined notions of cohesion and separation. These might be graphs and
matroids, but can be much more general or quite different. The theorem asserts
a duality between the existence of high cohesiveness somewhere local and a
global overall tree structure.
We describe cohesive substructures in a unified way in the format of tangles:
as orientations of low-order separations satisfying certain consistency axioms.
These axioms can be expressed without reference to the underlying structure,
such as a graph or matroid, but just in terms of the poset of the separations
themselves. This makes it possible to identify tangles, and apply our
tangle-tree duality theorem, in very diverse settings.
Our result implies all the classical duality theorems for width parameters in
graph minor theory, such as path-width, tree-width, branch-width or rank-width.
It yields new, tangle-type, duality theorems for tree-width and path-width. It
implies the existence of width parameters dual to cohesive substructures such
as $k$-blocks, edge-tangles, or given subsets of tangles, for which no width
duality theorems were previously known.
Abstract separation systems can be found also in structures quite unlike
graphs and matroids. For example, our theorem can be applied to image analysis
by capturing the regions of an image as tangles of separations defined as
natural partitions of its set of pixels. It can be applied in big data contexts
by capturing clusters as tangles. It can be applied in the social sciences,
e.g. by capturing as tangles the few typical mindsets of individuals found by a
survey. It could also be applied in pure mathematics, e.g. to separations of
compact manifolds.
",0,0,1,0,0,0
9462,9463,Towards a Generic Diver-Following Algorithm: Balancing Robustness and Efficiency in Deep Visual Detection,"  This paper explores the design and development of a class of robust
diver-following algorithms for autonomous underwater robots. By considering the
operational challenges for underwater visual tracking in diverse real-world
settings, we formulate a set of desired features of a generic diver following
algorithm. We attempt to accommodate these features and maximize general
tracking performance by exploiting the state-of-the-art deep object detection
models. We fine-tune the building blocks of these models with a goal of
balancing the trade-off between robustness and efficiency in an onboard setting
under real-time constraints. Subsequently, we design an architecturally simple
Convolutional Neural Network (CNN)-based diver-detection model that is much
faster than the state-of-the-art deep models yet provides comparable detection
performances. In addition, we validate the performance and effectiveness of the
proposed diver-following modules through a number of field experiments in
closed-water and open-water environments.
",1,0,0,0,0,0
277,278,Cross-layer optimized routing with low duty cycle TDMA across multiple wireless body area networks,"  In this paper, we study the performance of two cross-layer optimized dynamic
routing techniques for radio interference mitigation across multiple coexisting
wireless body area networks (BANs), based on real-life measurements. At the
network layer, the best route is selected according to channel state
information from the physical layer, associated with low duty cycle TDMA at the
MAC layer. The routing techniques (i.e., shortest path routing (SPR), and novel
cooperative multi-path routing (CMR) incorporating 3-branch selection
combining) perform real-time and reliable data transfer across BANs operating
near the 2.4 GHz ISM band. An open-access experimental data set of 'everyday'
mixed-activities is used for analyzing the proposed cross-layer optimization.
We show that CMR gains up to 14 dB improvement with 8.3% TDMA duty cycle, and
even 10 dB improvement with 0.2% TDMA duty cycle over SPR, at 10% outage
probability at a realistic signal-to-interference-plus-noise ratio (SINR).
Acceptable packet delivery ratios (PDR) and spectral efficiencies are obtained
from SPR and CMR with reasonably sensitive receivers across a range of TDMA low
duty cycles, with up to 9 dB improvement of CMR over SPR at 90% PDR. The
distribution fits for received SINR through routing are also derived and
validated with theoretical analysis.
",1,0,0,0,0,0
13396,13397,Composite Rational Functions and Arithmetic Progressions,"  In this paper we deal with composite rational functions having zeros and
poles forming consecutive elements of an arithmetic progression. We also
correct a result published earlier related to composite rational functions
having a fixed number of zeros and poles.
",0,0,1,0,0,0
2119,2120,On the universality of anomalous scaling exponents of structure functions in turbulent flows,"  All previous experiments in open turbulent flows (e.g. downstream of grids,
jet and atmospheric boundary layer) have produced quantitatively consistent
values for the scaling exponents of velocity structure functions. The only
measurement in closed turbulent flow (von Kármán swirling flow) using
Taylor-hypothesis, however, produced scaling exponents that are significantly
smaller, suggesting that the universality of these exponents are broken with
respect to change of large scale geometry of the flow. Here, we report
measurements of longitudinal structure functions of velocity in a von
Kármán setup without the use of Taylor-hypothesis. The measurements are
made using Stereo Particle Image Velocimetry at 4 different ranges of spatial
scales, in order to observe a combined inertial subrange spanning roughly one
and a half order of magnitude. We found scaling exponents (up to 9th order)
that are consistent with values from open turbulent flows, suggesting that they
might be in fact universal.
",0,1,0,0,0,0
17219,17220,A training process for improving the quality of software projects developed by a practitioner,"  Background: The quality of a software product depends on the quality of the
software process followed in developing the product. Therefore, many higher
education institutions (HEI) and software organizations have implemented
software process improvement (SPI) training courses to improve the software
quality. Objective: Because the duration of a course is a concern for HEI and
software organizations, we investigate whether the quality of software projects
will be improved by reorganizing the activities of the ten assignments of the
original personal software process (PSP) course into a modified PSP having
fewer assignments (i.e., seven assignments). Method: The assignments were
developed by following a modified PSP with fewer assignments but including the
phases, forms, standards, and logs suggested in the original PSP. The
measurement of the quality of the software assignments was based on defect
density. Results: When the activities in the original PSP were reordered into
fewer assignments, as practitioners progress through the PSP training, the
defect density improved with statistical significance. Conclusions: Our
modified PSP could be applied in academy and industrial environments which are
concerned in the sense of reducing the PSP training time
",1,0,0,0,0,0
3481,3482,Symmetric calorons and the rotation map,"  We study $SU(2)$ calorons, also known as periodic instantons, and consider
invariance under isometries of $S^1\times\mathbb{R}^3$ coupled with a
non-spatial isometry called the rotation map. In particular, we investigate the
fixed points under various cyclic symmetry groups. Our approach utilises a
construction akin to the ADHM construction of instantons -- what we call the
monad matrix data for calorons -- derived from the work of Charbonneau and
Hurtubise. To conclude, we present an example of how investigating these
symmetry groups can help to construct new calorons by deriving Nahm data in the
case of charge $2$.
",0,0,1,0,0,0
8484,8485,A Concave Optimization Algorithm for Matching Partially Overlapping Point Sets,"  Point matching refers to the process of finding spatial transformation and
correspondences between two sets of points. In this paper, we focus on the case
that there is only partial overlap between two point sets. Following the
approach of the robust point matching method, we model point matching as a
mixed linear assignment-least square problem and show that after eliminating
the transformation variable, the resulting problem of minimization with respect
to point correspondence is a concave optimization problem. Furthermore, this
problem has the property that the objective function can be converted into a
form with few nonlinear terms via a linear transformation. Based on these
properties, we employ the branch-and-bound (BnB) algorithm to optimize the
resulting problem where the dimension of the search space is small. To further
improve efficiency of the BnB algorithm where computation of the lower bound is
the bottleneck, we propose a new lower bounding scheme which has a
k-cardinality linear assignment formulation and can be efficiently solved.
Experimental results show that the proposed algorithm outperforms
state-of-the-art methods in terms of robustness to disturbances and point
matching accuracy.
",1,0,0,0,0,0
12013,12014,Structured low-rank matrix learning: algorithms and applications,"  We consider the problem of learning a low-rank matrix, constrained to lie in
a linear subspace, and introduce a novel factorization for modeling such
matrices. A salient feature of the proposed factorization scheme is it
decouples the low-rank and the structural constraints onto separate factors. We
formulate the optimization problem on the Riemannian spectrahedron manifold,
where the Riemannian framework allows to develop computationally efficient
conjugate gradient and trust-region algorithms. Experiments on problems such as
standard/robust/non-negative matrix completion, Hankel matrix learning and
multi-task learning demonstrate the efficacy of our approach. A shorter version
of this work has been published in ICML'18.
",0,0,0,1,0,0
145,146,Evolution of the Kondo lattice electronic structure above the transport coherence temperature,"  The temperature-dependent evolution of the Kondo lattice is a long-standing
topic of theoretical and experimental investigation and yet it lacks a truly
microscopic description of the relation of the basic $f$-$d$ hybridization
processes to the fundamental temperature scales of Kondo screening and
Fermi-liquid lattice coherence. Here, the temperature-dependence of $f$-$d$
hybridized band dispersions and Fermi-energy $f$ spectral weight in the Kondo
lattice system CeCoIn$_5$ is investigated using $f$-resonant angle-resolved
photoemission (ARPES) with sufficient detail to allow direct comparison to
first principles dynamical mean field theory (DMFT) calculations containing
full realism of crystalline electric field states. The ARPES results, for two
orthogonal (001) and (100) cleaved surfaces and three different $f$-$d$
hybridization scenarios, with additional microscopic insight provided by DMFT,
reveal $f$ participation in the Fermi surface at temperatures much higher than
the lattice coherence temperature, $T^*\approx$ 45 K, commonly believed to be
the onset for such behavior. The identification of a $T$-dependent crystalline
electric field degeneracy crossover in the DMFT theory $below$ $T^*$ is
specifically highlighted.
",0,1,0,0,0,0
7915,7916,On rumour propagation among sceptics,"  Junior, Machado and Zuluaga (2011) studied a model to understand the spread
of a rumour. Their model consists of individuals situated at the integer points
of the line $\N$. An individual at the origin $0$ starts a rumour and passes it
to all individuals in the interval $[0,R_0]$, where $R_0$ is a non-negative
random variable. An individual located at $i$ in this interval receives the
rumour and transmits it further among individuals in $[i, i+R_i]$ where $R_0$
and $R_i$ are i.i.d. random variables. The rumour spreads in this manner. An
alternate model considers individuals seeking to find the rumour from
individuals who have already heard it. For this s/he asks individuals to the
left of her/him and lying in an interval of a random size. We study these two
models, when the individuals are more sceptical and they transmit or accept the
rumour only if they receive it from at least two different sources.
In stochastic geometry the equivalent of this rumour process is the study of
coverage of the space $\N^d$ by random sets. Our study here extends the study
of coverage of space and considers the case when each vertex of $\N^d$ is
covered by at least two distinct random sets.
",0,0,1,1,0,0
18932,18933,Asymptotic Analysis of Plausible Tree Hash Modes for SHA-3,"  Discussions about the choice of a tree hash mode of operation for a
standardization have recently been undertaken. It appears that a single tree
mode cannot address adequately all possible uses and specifications of a
system. In this paper, we review the tree modes which have been proposed, we
discuss their problems and propose remedies. We make the reasonable assumption
that communicating systems have different specifications and that software
applications are of different types (securing stored content or live-streamed
content). Finally, we propose new modes of operation that address the resource
usage problem for the three most representative categories of devices and we
analyse their asymptotic behavior.
",1,0,0,0,0,0
6665,6666,A Measurement of CMB Cluster Lensing with SPT and DES Year 1 Data,"  Clusters of galaxies gravitationally lens the cosmic microwave background
(CMB) radiation, resulting in a distinct imprint in the CMB on arcminute
scales. Measurement of this effect offers a promising way to constrain the
masses of galaxy clusters, particularly those at high redshift. We use CMB maps
from the South Pole Telescope Sunyaev-Zel'dovich (SZ) survey to measure the CMB
lensing signal around galaxy clusters identified in optical imaging from first
year observations of the Dark Energy Survey. The cluster catalog used in this
analysis contains 3697 members with mean redshift of $\bar{z} = 0.45$. We
detect lensing of the CMB by the galaxy clusters at $8.1\sigma$ significance.
Using the measured lensing signal, we constrain the amplitude of the relation
between cluster mass and optical richness to roughly $17\%$ precision, finding
good agreement with recent constraints obtained with galaxy lensing. The error
budget is dominated by statistical noise but includes significant contributions
from systematic biases due to the thermal SZ effect and cluster miscentering.
",0,1,0,0,0,0
11593,11594,Parseval Networks: Improving Robustness to Adversarial Examples,"  We introduce Parseval networks, a form of deep neural networks in which the
Lipschitz constant of linear, convolutional and aggregation layers is
constrained to be smaller than 1. Parseval networks are empirically and
theoretically motivated by an analysis of the robustness of the predictions
made by deep neural networks when their input is subject to an adversarial
perturbation. The most important feature of Parseval networks is to maintain
weight matrices of linear and convolutional layers to be (approximately)
Parseval tight frames, which are extensions of orthogonal matrices to
non-square matrices. We describe how these constraints can be maintained
efficiently during SGD. We show that Parseval networks match the
state-of-the-art in terms of accuracy on CIFAR-10/100 and Street View House
Numbers (SVHN) while being more robust than their vanilla counterpart against
adversarial examples. Incidentally, Parseval networks also tend to train faster
and make a better usage of the full capacity of the networks.
",1,0,0,1,0,0
7382,7383,No Need for a Lexicon? Evaluating the Value of the Pronunciation Lexica in End-to-End Models,"  For decades, context-dependent phonemes have been the dominant sub-word unit
for conventional acoustic modeling systems. This status quo has begun to be
challenged recently by end-to-end models which seek to combine acoustic,
pronunciation, and language model components into a single neural network. Such
systems, which typically predict graphemes or words, simplify the recognition
process since they remove the need for a separate expert-curated pronunciation
lexicon to map from phoneme-based units to words. However, there has been
little previous work comparing phoneme-based versus grapheme-based sub-word
units in the end-to-end modeling framework, to determine whether the gains from
such approaches are primarily due to the new probabilistic model, or from the
joint learning of the various components with grapheme-based units.
In this work, we conduct detailed experiments which are aimed at quantifying
the value of phoneme-based pronunciation lexica in the context of end-to-end
models. We examine phoneme-based end-to-end models, which are contrasted
against grapheme-based ones on a large vocabulary English Voice-search task,
where we find that graphemes do indeed outperform phonemes. We also compare
grapheme and phoneme-based approaches on a multi-dialect English task, which
once again confirm the superiority of graphemes, greatly simplifying the system
for recognizing multiple dialects.
",1,0,0,1,0,0
13124,13125,Solving internal covariate shift in deep learning with linked neurons,"  This work proposes a novel solution to the problem of internal covariate
shift and dying neurons using the concept of linked neurons. We define the
neuron linkage in terms of two constraints: first, all neuron activations in
the linkage must have the same operating point. That is to say, all of them
share input weights. Secondly, a set of neurons is linked if and only if there
is at least one member of the linkage that has a non-zero gradient in regard to
the input of the activation function. This means that for any input in the
activation function, there is at least one member of the linkage that operates
in a non-flat and non-zero area. This simple change has profound implications
in the network learning dynamics. In this article we explore the consequences
of this proposal and show that by using this kind of units, internal covariate
shift is implicitly solved. As a result of this, the use of linked neurons
allows to train arbitrarily large networks without any architectural or
algorithmic trick, effectively removing the need of using re-normalization
schemes such as Batch Normalization, which leads to halving the required
training time. It also solves the problem of the need for standarized input
data. Results show that the units using the linkage not only do effectively
solve the aforementioned problems, but are also a competitive alternative with
respect to state-of-the-art with very promising results.
",1,0,0,1,0,0
11062,11063,Chiral and Topological Orbital Magnetism of Spin Textures,"  Using a semiclassical Green's function formalism, we discover the emergence
of chiral and topological orbital magnetism in two-dimensional chiral spin
textures by explicitly finding the corrections to the orbital magnetization,
proportional to the powers of the gradients of the texture. We show that in the
absence of spin-orbit coupling, the resulting orbital moment can be understood
as the electronic response to the emergent magnetic field associated with the
real-space Berry curvature. By referring to the Rashba model, we demonstrate
that by tuning the parameters of surface systems the engineering of emergent
orbital magnetism in spin textures can pave the way to novel concepts in
orbitronics.
",0,1,0,0,0,0
16086,16087,Joint Regression and Ranking for Image Enhancement,"  Research on automated image enhancement has gained momentum in recent years,
partially due to the need for easy-to-use tools for enhancing pictures captured
by ubiquitous cameras on mobile devices. Many of the existing leading methods
employ machine-learning-based techniques, by which some enhancement parameters
for a given image are found by relating the image to the training images with
known enhancement parameters. While knowing the structure of the parameter
space can facilitate search for the optimal solution, none of the existing
methods has explicitly modeled and learned that structure. This paper presents
an end-to-end, novel joint regression and ranking approach to model the
interaction between desired enhancement parameters and images to be processed,
employing a Gaussian process (GP). GP allows searching for ideal parameters
using only the image features. The model naturally leads to a ranking technique
for comparing images in the induced feature space. Comparative evaluation using
the ground-truth based on the MIT-Adobe FiveK dataset plus subjective tests on
an additional data-set were used to demonstrate the effectiveness of the
proposed approach.
",1,0,0,0,0,0
219,220,GENFIRE: A generalized Fourier iterative reconstruction algorithm for high-resolution 3D imaging,"  Tomography has made a radical impact on diverse fields ranging from the study
of 3D atomic arrangements in matter to the study of human health in medicine.
Despite its very diverse applications, the core of tomography remains the same,
that is, a mathematical method must be implemented to reconstruct the 3D
structure of an object from a number of 2D projections. In many scientific
applications, however, the number of projections that can be measured is
limited due to geometric constraints, tolerable radiation dose and/or
acquisition speed. Thus it becomes an important problem to obtain the
best-possible reconstruction from a limited number of projections. Here, we
present the mathematical implementation of a tomographic algorithm, termed
GENeralized Fourier Iterative REconstruction (GENFIRE). By iterating between
real and reciprocal space, GENFIRE searches for a global solution that is
concurrently consistent with the measured data and general physical
constraints. The algorithm requires minimal human intervention and also
incorporates angular refinement to reduce the tilt angle error. We demonstrate
that GENFIRE can produce superior results relative to several other popular
tomographic reconstruction techniques by numerical simulations, and by
experimentally by reconstructing the 3D structure of a porous material and a
frozen-hydrated marine cyanobacterium. Equipped with a graphical user
interface, GENFIRE is freely available from our website and is expected to find
broad applications across different disciplines.
",0,1,0,0,0,0
14126,14127,MM2RTB: Bringing Multimedia Metrics to Real-Time Bidding,"  In display advertising, users' online ad experiences are important for the
advertising effectiveness. However, users have not been well accommodated in
real-time bidding (RTB). This further influences their site visits and
perception of the displayed banner ads. In this paper, we propose a novel
computational framework which brings multimedia metrics, like the contextual
relevance, the visual saliency and the ad memorability into RTB to improve the
users' ad experiences as well as maintain the benefits of the publisher and the
advertiser. We aim at developing a vigorous ecosystem by optimizing the
trade-offs among all stakeholders. The framework considers the scenario of a
webpage with multiple ad slots. Our experimental results show that the benefits
of the advertiser and the user can be significantly improved if the publisher
would slightly sacrifice his short-term revenue. The improved benefits will
increase the advertising requests (demand) and the site visits (supply), which
can further boost the publisher's revenue in the long run.
",1,0,0,0,0,0
14083,14084,Minimum edge cuts of distance-regular and strongly regular digraphs,"  In this paper, we show that the edge connectivity of a distance-regular
digraph $\Gamma$ with valency $k$ is $k$ and for $k>2$, any minimum edge cut of
$\Gamma$ is the set of all edges going into (or coming out of) a single vertex.
Moreover we show that the same result holds for strongly regular digraphs.
These results extend the same known results for undirected case with quite
different proofs.
",0,0,1,0,0,0
7548,7549,Joint Modeling of Event Sequence and Time Series with Attentional Twin Recurrent Neural Networks,"  A variety of real-world processes (over networks) produce sequences of data
whose complex temporal dynamics need to be studied. More especially, the event
timestamps can carry important information about the underlying network
dynamics, which otherwise are not available from the time-series evenly sampled
from continuous signals. Moreover, in most complex processes, event sequences
and evenly-sampled times series data can interact with each other, which
renders joint modeling of those two sources of data necessary. To tackle the
above problems, in this paper, we utilize the rich framework of (temporal)
point processes to model event data and timely update its intensity function by
the synergic twin Recurrent Neural Networks (RNNs). In the proposed
architecture, the intensity function is synergistically modulated by one RNN
with asynchronous events as input and another RNN with time series as input.
Furthermore, to enhance the interpretability of the model, the attention
mechanism for the neural point process is introduced. The whole model with
event type and timestamp prediction output layers can be trained end-to-end and
allows a black-box treatment for modeling the intensity. We substantiate the
superiority of our model in synthetic data and three real-world benchmark
datasets.
",1,0,0,0,0,0
17255,17256,Dimensions of equilibrium measures on a class of planar self-affine sets,"  We study equilibrium measures (Käenmäki measures) supported on
self-affine sets generated by a finite collection of diagonal and anti-diagonal
matrices acting on the plane and satisfying the strong separation property. Our
main result is that such measures are exact dimensional and the dimension
satisfies the Ledrappier-Young formula, which gives an explicit expression for
the dimension in terms of the entropy and Lyapunov exponents as well as the
dimension of the important coordinate projection of the measure. In particular,
we do this by showing that the Käenmäki measure is equal to the sum of (the
pushforwards) of two Gibbs measures on an associated subshift of finite type.
",0,0,1,0,0,0
8763,8764,Fast evaluation of solid harmonic Gaussian integrals for local resolution-of-the-identity methods and range-separated hybrid functionals,"  An integral scheme for the efficient evaluation of two-center integrals over
contracted solid harmonic Gaussian functions is presented. Integral expressions
are derived for local operators that depend on the position vector of one of
the two Gaussian centers. These expressions are then used to derive the formula
for three-index overlap integrals where two of the three Gaussians are located
at the same center. The efficient evaluation of the latter is essential for
local resolution-of-the-identity techniques that employ an overlap metric. We
compare the performance of our integral scheme to the widely used Cartesian
Gaussian-based method of Obara and Saika (OS). Non-local interaction potentials
such as standard Coulomb, modified Coulomb and Gaussian-type operators, that
occur in range-separated hybrid functionals, are also included in the
performance tests. The speed-up with respect to the OS scheme is up to three
orders of magnitude for both, integrals and their derivatives. In particular,
our method is increasingly efficient for large angular momenta and highly
contracted basis sets.
",0,1,0,0,0,0
3473,3474,An Incremental Slicing Method for Functional Programs,"  Several applications of slicing require a program to be sliced with respect
to more than one slicing criterion. Program specialization, parallelization and
cohesion measurement are examples of such applications. These applications can
benefit from an incremental static slicing method in which a significant extent
of the computations for slicing with respect to one criterion could be reused
for another. In this paper, we consider the problem of incremental slicing of
functional programs. We first present a non-incremental version of the slicing
algorithm which does a polyvariant analysis 1 of functions. Since polyvariant
analyses tend to be costly, we compute a compact context-independent summary of
each function and then use this summary at the call sites of the function. The
construction of the function summary is non-trivial and helps in the
development of the incremental version. The incremental method, on the other
hand, consists of a one-time pre-computation step that uses the non-incremental
version to slice the program with respect to a fixed default slicing criterion
and processes the results further to a canonical form. Presented with an actual
slicing criterion, the incremental step involves a low-cost computation that
uses the results of the pre-computation to obtain the slice. We have
implemented a prototype of the slicer for a pure subset of Scheme, with pairs
and lists as the only algebraic data types. Our experiments show that the
incremental step of the slicer runs orders of magnitude faster than the
non-incremental version. We have also proved the correctness of our incremental
algorithm with respect to the non-incremental version.
",1,0,0,0,0,0
10354,10355,Almost sure scattering for the energy-critical NLS with radial data below $H^1(\mathbb{R}^4)$,"  We prove almost sure global existence and scattering for the energy-critical
nonlinear Schrödinger equation with randomized spherically symmetric initial
data in $H^s(\mathbb{R}^4)$ with $\frac56<s<1$. We were inspired to consider
this problem by the recent work of Dodson--Lührmann--Mendelson, which treated
the analogous problem for the energy-critical wave equation.
",0,0,1,0,0,0
8419,8420,A semianalytical approach for determining the nonclassical mechanical properties of materials,"  In this article, a semianalytical approach for demonstrating elastic waves
propagation in nanostructures has been presented based on the modified
couple-stress theory including acceleration gradients. Using the experimental
results and atomic simulations, the static and dynamic length scales were
calculated for several materials, zinc oxide (ZnO), silicon (Si), silicon
carbide (SiC), indium antimonide (InSb), and diamond. To evaluate the predicted
static and dynamic length scales as well as the presented model, the natural
frequencies of a beam in addition to the phase velocity and group velocity of
Si were studied and compared with the available static length scales, estimated
using strain-gradient theory without considering acceleration gradients. These
three criteria, natural frequency, phase velocity, and group velocity, show
that the presented model is dynamically stable even for larger wavevector
values. Furthermore, it is explained why the previous works, which all are
based on the strain-gradient theory without acceleration gradients, predicted
very small values for the static length scale in the longitudinal direction
rather than the static length scale in the transverse directions.
",0,1,0,0,0,0
7182,7183,Confidence Intervals for Stochastic Arithmetic,"  Quantifying errors and losses due to the use of Floating-Point (FP)
calculations in industrial scientific computing codes is an important part of
the Verification, Validation and Uncertainty Quantification (VVUQ) process.
Stochastic Arithmetic is one way to model and estimate FP losses of accuracy,
which scales well to large, industrial codes. It exists in different flavors,
such as CESTAC or MCA, implemented in various tools such as CADNA, Verificarlo
or Verrou. These methodologies and tools are based on the idea that FP losses
of accuracy can be modeled via randomness. Therefore, they share the same need
to perform a statistical analysis of programs results in order to estimate the
significance of the results. In this paper, we propose a framework to perform a
solid statistical analysis of Stochastic Arithmetic. This framework unifies all
existing definitions of the number of significant digits (CESTAC and MCA), and
also proposes a new quantity of interest: the number of digits contributing to
the accuracy of the results. Sound confidence intervals are provided for all
estimators, both in the case of normally distributed results, and in the
general case. The use of this framework is demonstrated by two case studies of
large, industrial codes: Europlexus and code\_aster.
",1,0,0,1,0,0
14057,14058,"Anomalous Thermal Expansion, Negative Linear Compressibility and High-Pressure Phase Transition in ZnAu2(CN)4: Neutron Inelastic Scattering and Lattice Dynamics Studies","  We present temperature dependent inelastic neutron scattering measurments,
accompanied byab-initio calculations of phonon spectra and elastic properties
as a function of pressure to understand anharmonicity of phonons and to study
the mechanism of negative thermal expansion and negative linear compressibility
behaviour of ZnAu2(CN)4. The mechanism is identified in terms of specific
anharmonic modes that involve bending of the Zn(CN)4-Au- Zn(CN)4 linkage. The
high-pressure phase transition at about 2 GPa is also investigated and found to
be related to softening of a phonon mode at the L-point at the Brillouin zone
boundary and its coupling with a zone-centre phonon and an M-point phonon in
the ambient pressure phase. Although the phase transition is primarily driven
by a L-point soft phonon mode, which usually leads to a second order transition
with a 2 x 2 x 2 supercell, in the present case the structure is close to an
elastic instability that leads to a weakly first order transition.
",0,1,0,0,0,0
12714,12715,A Controlled Set-Up Experiment to Establish Personalized Baselines for Real-Life Emotion Recognition,"  We design, conduct and present the results of a highly personalized baseline
emotion recognition experiment, which aims to set reliable ground-truth
estimates for the subject's emotional state for real-life prediction under
similar conditions using a small number of physiological sensors. We also
propose an adaptive stimuli-selection mechanism that would use the user's
feedback as guide for future stimuli selection in the controlled-setup
experiment and generate optimal ground-truth personalized sessions
systematically. Initial results are very promising (85% accuracy) and variable
importance analysis shows that only a few features, which are easy-to-implement
in portable devices, would suffice to predict the subject's emotional state.
",1,0,0,1,0,0
16172,16173,On the Upward/Downward Closures of Petri Nets,"  We study the size and the complexity of computing finite state automata (FSA)
representing and approximating the downward and the upward closure of Petri net
languages with coverability as the acceptance condition. We show how to
construct an FSA recognizing the upward closure of a Petri net language in
doubly-exponential time, and therefore the size is at most doubly exponential.
For downward closures, we prove that the size of the minimal automata can be
non-primitive recursive. In the case of BPP nets, a well-known subclass of
Petri nets, we show that an FSA accepting the downward/upward closure can be
constructed in exponential time. Furthermore, we consider the problem of
checking whether a simple regular language is included in the downward/upward
closure of a Petri net/BPP net language. We show that this problem is
EXPSPACE-complete (resp. NP-complete) in the case of Petri nets (resp. BPP
nets). Finally, we show that it is decidable whether a Petri net language is
upward/downward closed. To this end, we prove that one can decide whether a
given regular language is a subset of a Petri net coverability language.
",1,0,0,0,0,0
14114,14115,Introduction to the declination function for gerrymanders,"  The declination is a quantitative method for identifying possible partisan
gerrymanders by analyzing vote distributions. In this expository note we
explain and motivate the definition of the declination. The minimal computer
code required for computing the declination is included. We end by computing
its value on several recent elections.
",0,0,0,1,0,0
10156,10157,Accelerating equilibrium isotope effect calculations: I. Stochastic thermodynamic integration with respect to mass,"  Accurate path integral Monte Carlo or molecular dynamics calculations of
isotope effects have until recently been expensive because of the necessity to
reduce three types of errors present in such calculations: statistical errors
due to sampling, path integral discretization errors, and thermodynamic
integration errors. While the statistical errors can be reduced with virial
estimators and path integral discretization errors with high-order
factorization of the Boltzmann operator, here we propose a method for
accelerating isotope effect calculations by eliminating the integration error.
We show that the integration error can be removed entirely by changing particle
masses stochastically during the calculation and by using a piecewise linear
umbrella biasing potential. Moreover, we demonstrate numerically that this
approach does not increase the statistical error. The resulting acceleration of
isotope effect calculations is demonstrated on a model harmonic system and on
deuterated species of methane.
",0,1,0,0,0,0
2552,2553,An inexact subsampled proximal Newton-type method for large-scale machine learning,"  We propose a fast proximal Newton-type algorithm for minimizing regularized
finite sums that returns an $\epsilon$-suboptimal point in
$\tilde{\mathcal{O}}(d(n + \sqrt{\kappa d})\log(\frac{1}{\epsilon}))$ FLOPS,
where $n$ is number of samples, $d$ is feature dimension, and $\kappa$ is the
condition number. As long as $n > d$, the proposed method is more efficient
than state-of-the-art accelerated stochastic first-order methods for non-smooth
regularizers which requires $\tilde{\mathcal{O}}(d(n + \sqrt{\kappa
n})\log(\frac{1}{\epsilon}))$ FLOPS. The key idea is to form the subsampled
Newton subproblem in a way that preserves the finite sum structure of the
objective, thereby allowing us to leverage recent developments in stochastic
first-order methods to solve the subproblem. Experimental results verify that
the proposed algorithm outperforms previous algorithms for $\ell_1$-regularized
logistic regression on real datasets.
",1,0,0,1,0,0
3094,3095,Random Perturbations of Matrix Polynomials,"  A sum of a large-dimensional random matrix polynomial and a fixed low-rank
matrix polynomial is considered. The main assumption is that the resolvent of
the random polynomial converges to some deterministic limit. A formula for the
limit of the resolvent of the sum is derived and the eigenvalues are localised.
Three instances are considered: a low-rank matrix perturbed by the Wigner
matrix, a product $HX$ of a fixed diagonal matrix $H$ and the Wigner matrix $X$
and a special matrix polynomial. The results are illustrated with various
examples and numerical simulations.
",0,0,1,0,0,0
6640,6641,An energy-based equilibrium contact angle boundary condition on jagged surfaces for phase-field methods,"  We consider an energy-based boundary condition to impose an equilibrium
wetting angle for the Cahn-Hilliard-Navier-Stokes phase-field model on
voxel-set-type computational domains. These domains typically stem from the
micro-CT imaging of porous rock and approximate a (on {\mu}m scale) smooth
domain with a certain resolution. Planar surfaces that are perpendicular to the
main axes are naturally approximated by a layer of voxels. However, planar
surfaces in any other directions and curved surfaces yield a jagged/rough
surface approximation by voxels. For the standard Cahn-Hilliard formulation,
where the contact angle between the diffuse interface and the domain boundary
(fluid-solid interface/wall) is 90 degrees, jagged surfaces have no impact on
the contact angle. However, a prescribed contact angle smaller or larger than
90 degrees on jagged voxel surfaces is amplified in either direction. As a
remedy, we propose the introduction of surface energy correction factors for
each fluid-solid voxel face that counterbalance the difference of the voxel-set
surface area with the underlying smooth one. The discretization of the model
equations is performed with the discontinuous Galerkin method, however, the
presented semi-analytical approach of correcting the surface energy is equally
applicable to other direct numerical methods such as finite elements, finite
volumes, or finite differences, since the correction factors appear in the
strong formulation of the model.
",0,1,0,0,0,0
16980,16981,Leveraging Pre-Trained 3D Object Detection Models For Fast Ground Truth Generation,"  Training 3D object detectors for autonomous driving has been limited to small
datasets due to the effort required to generate annotations. Reducing both task
complexity and the amount of task switching done by annotators is key to
reducing the effort and time required to generate 3D bounding box annotations.
This paper introduces a novel ground truth generation method that combines
human supervision with pretrained neural networks to generate per-instance 3D
point cloud segmentation, 3D bounding boxes, and class annotations. The
annotators provide object anchor clicks which behave as a seed to generate
instance segmentation results in 3D. The points belonging to each instance are
then used to regress object centroids, bounding box dimensions, and object
orientation. Our proposed annotation scheme requires 30x lower human annotation
time. We use the KITTI 3D object detection dataset to evaluate the efficiency
and the quality of our annotation scheme. We also test the the proposed scheme
on previously unseen data from the Autonomoose self-driving vehicle to
demonstrate generalization capabilities of the network.
",0,0,0,1,0,0
1237,1238,"Fractional quantum Hall systems near nematicity: bimetric theory, composite fermions, and Dirac brackets","  We perform a detailed comparison of the Dirac composite fermion and the
recently proposed bimetric theory for a quantum Hall Jain states near half
filling. By tuning the composite Fermi liquid to the vicinity of a nematic
phase transition, we find that the two theories are equivalent to each other.
We verify that the single mode approximation for the response functions and the
static structure factor becomes reliable near the phase transition. We show
that the dispersion relation of the nematic mode near the phase transition can
be obtained from the Dirac brackets between the components of the nematic order
parameter. The dispersion is quadratic at low momenta and has a magnetoroton
minimum at a finite momentum, which is not related to any nearby inhomogeneous
phase.
",0,1,0,0,0,0
8250,8251,Channel Feedback Based on AoD-Adaptive Subspace Codebook in FDD Massive MIMO Systems,"  Channel feedback is essential in frequency division duplexing (FDD) massive
multiple-input multiple-output (MIMO) systems. Unfortunately, previous work on
multiuser MIMO has shown that the codebook size for channel feedback should
scale exponentially with the number of base station (BS) antennas, which is
greatly increased in massive MIMO systems. To reduce the codebook size and
feedback overhead, we propose an angle-of-departure (AoD)-adaptive subspace
codebook for channel feedback in FDD massive MIMO systems. Our key insight is
to leverage the observation that path AoDs vary more slowly than the path
gains. Within the angle coherence time, by utilizing the constant AoD
information, the proposed AoD-adaptive subspace codebook is able to quantize
the channel vector in a more accurate way. We also provide performance analysis
of the proposed codebook in the large-dimensional regime, where we prove that
to limit the capacity degradation within an acceptable level, the required
number of feedback bits only scales linearly with the number of resolvable
(path) AoDs, which is much smaller than the number of BS antennas. Moreover, we
compare quantized channel feedback using the proposed AoD-adaptive subspace
codebook with analog channel feedback. Extensive simulations that verify the
analytical results are provided.
",1,0,0,0,0,0
1645,1646,A Topological Perspective on Interacting Algebraic Theories,"  Techniques from higher categories and higher-dimensional rewriting are
becoming increasingly important for understanding the finer, computational
properties of higher algebraic theories that arise, among other fields, in
quantum computation. These theories have often the property of containing
simpler sub-theories, whose interaction is regulated in a limited number of
ways, which reveals a topological substrate when pictured by string diagrams.
By exploring the double nature of computads as presentations of higher
algebraic theories, and combinatorial descriptions of ""directed spaces"", we
develop a basic language of directed topology for the compositional study of
algebraic theories. We present constructions of computads, all with clear
analogues in standard topology, that capture in great generality such notions
as homomorphisms and actions, and the interactions of monoids and comonoids
that lead to the theory of Frobenius algebras and of bialgebras. After a number
of examples, we describe how a fragment of the ZX calculus can be reconstructed
in this framework.
",1,0,1,0,0,0
11152,11153,A Branch-and-Bound Algorithm for Checkerboard Extraction in Camera-Laser Calibration,"  We address the problem of camera-to-laser-scanner calibration using a
checkerboard and multiple image-laser scan pairs. Distinguishing which laser
points measure the checkerboard and which lie on the background is essential to
any such system. We formulate the checkerboard extraction as a combinatorial
optimization problem with a clear cut objective function. We propose a
branch-and-bound technique that deterministically and globally optimizes the
objective. Unlike what is available in the literature, the proposed method is
not heuristic and does not require assumptions such as constraints on the
background or relying on discontinuity of the range measurements to partition
the data into line segments. The proposed approach is generic and can be
applied to both 3D or 2D laser scanners as well as the cases where multiple
checkerboards are present. We demonstrate the effectiveness of the proposed
approach by providing numerical simulations as well as experimental results.
",1,0,0,0,0,0
1586,1587,International crop trade networks: The impact of shocks and cascades,"  Analyzing available FAO data from 176 countries over 21 years, we observe an
increase of complexity in the international trade of maize, rice, soy, and
wheat. A larger number of countries play a role as producers or intermediaries,
either for trade or food processing. In consequence, we find that the trade
networks become more prone to failure cascades caused by exogenous shocks. In
our model, countries compensate for demand deficits by imposing export
restrictions. To capture these, we construct higher-order trade dependency
networks for the different crops and years. These networks reveal hidden
dependencies between countries and allow to discuss policy implications.
",0,0,0,0,0,1
6826,6827,Thermalizing sterile neutrino dark matter,"  Sterile neutrinos produced through oscillations are a well motivated dark
matter candidate, but recent constraints from observations have ruled out most
of the parameter space. We analyze the impact of new interactions on the
evolution of keV sterile neutrino dark matter in the early Universe. Based on
general considerations we find a mechanism which thermalizes the sterile
neutrinos after an initial production by oscillations. The thermalization of
sterile neutrinos is accompanied by dark entropy production which increases the
yield of dark matter and leads to a lower characteristic momentum. This
resolves the growing tensions with structure formation and X-ray observations
and even revives simple non-resonant production as a viable way to produce
sterile neutrino dark matter. We investigate the parameters required for the
realization of the thermalization mechanism in a representative model and find
that a simple estimate based on energy- and entropy conservation describes the
mechanism well.
",0,1,0,0,0,0
10907,10908,SLAMBooster: An Application-aware Controller for Approximation in SLAM,"  Simultaneous Localization and Mapping (SLAM) is the problem of constructing a
map of an agent's environment while localizing or tracking the mobile agent's
position and orientation within the map. Algorithms for SLAM have high
computational requirements, which has hindered their use on embedded devices.
Approximation can be used to reduce the time and energy requirements of SLAM
implementations as long as the approximations do not prevent the agent from
navigating correctly through the environment. Previous studies of approximation
in SLAM have assumed that the entire trajectory of the agent is known before
the agent starts to move, and they have focused on offline controllers that use
features of the trajectory to set approximation knobs at the start of the
trajectory. In practice, the trajectory is not usually known ahead of time, and
allowing knob settings to change dynamically opens up more opportunities for
reducing computation time and energy.
We describe SLAMBooster, an application-aware online control system for SLAM
that adaptively controls approximation knobs during the motion of the agent.
SLAMBooster is based on a control technique called hierarchical proportional
control but our experiments showed this application-agnostic control led to an
unacceptable reduction in the quality of localization. To address this problem,
SLAMBooster exploits domain knowledge: it uses features extracted from input
frames and from the estimated motion of the agent in its algorithm for
controlling approximation.
We implemented SLAMBooster in the open-source SLAMBench framework. Our
experiments show that SLAMBooster reduces the computation time and energy
consumption by around half on the average on an embedded platform, while
maintaining the accuracy of the localization within reasonable bounds. These
improvements make it feasible to deploy SLAM on a wider range of devices.
",1,0,0,0,0,0
18604,18605,Multi-Labelled Value Networks for Computer Go,"  This paper proposes a new approach to a novel value network architecture for
the game Go, called a multi-labelled (ML) value network. In the ML value
network, different values (win rates) are trained simultaneously for different
settings of komi, a compensation given to balance the initiative of playing
first. The ML value network has three advantages, (a) it outputs values for
different komi, (b) it supports dynamic komi, and (c) it lowers the mean
squared error (MSE). This paper also proposes a new dynamic komi method to
improve game-playing strength. This paper also performs experiments to
demonstrate the merits of the architecture. First, the MSE of the ML value
network is generally lower than the value network alone. Second, the program
based on the ML value network wins by a rate of 67.6% against the program based
on the value network alone. Third, the program with the proposed dynamic komi
method significantly improves the playing strength over the baseline that does
not use dynamic komi, especially for handicap games. To our knowledge, up to
date, no handicap games have been played openly by programs using value
networks. This paper provides these programs with a useful approach to playing
handicap games.
",1,0,0,0,0,0
19112,19113,Resilience of Core-Periphery Networks in the Case of Rich-Club,"  Core-periphery networks are structures that present a set of central and
densely connected nodes, namely the core, and a set of non-central and sparsely
connected nodes, namely the periphery. The rich-club refers to a set in which
the highest degree nodes show a high density of connections. Thus, a network
that displays a rich-club can be interpreted as a core-periphery network in
which the core is made up by a number of hubs. In this paper, we test the
resilience of networks showing a progressively denser rich-club and we observe
how this structure is able to affect the network measures in terms of both
cohesion and efficiency in information flow. Additionally, we consider the case
in which, instead of making the core denser, we add links to the periphery.
These two procedures of core and periphery thickening delineate a decision
process in the placement of new links and allow us to conduct a scenario
analysis that can be helpful in the comprehension and supervision of complex
networks under the resilience perspective. The advantages of the two
procedures, as well as their implications, are discussed in relation to both
network effciency and node heterogeneity.
",1,0,0,0,0,0
16567,16568,Proper efficiency and cone efficiency,"  In this report, two general concepts for proper efficiency in vector
optimization are studied. Properly efficient elements can be defined as
minimizers of functionals with certain monotonicity properties or as weakly
efficient elements with respect to sets that contain the domination set.
Interdependencies between both concepts are proved in topological vector spaces
by means of Gerstewitz functionals. The investigation includes proper
efficiency notions introduced by Henig and by Nehse and Iwanow. In contrary to
Henig's notion, proper efficiency by Nehse and Iwanow is defined as efficiency
with respect to certain convex sets which are not necessarily cones. For the
finite-dimensional case, we turn to Geoffrion's proper efficiency as a special
case of Henig's proper efficiency. It is characterized as efficiency with
regard to subclasses of the set of polyhedral cones. Conditions for the
existence of Geoffrion's properly efficient points are proved. For closed
feasible point sets, Geoffrion's properly efficient point set is empty or
coincides with that of Nehse and Iwanow. Properly efficient elements by Nehse
and Iwanow are the minimizers of continuous convex functionals with certain
monotonicity properties. Henig's proper efficiency can be described by means of
minimizers of continuous sublinear functionals with certain monotonicity
properties.
",0,0,1,0,0,0
16738,16739,The complexity of recognizing minimally tough graphs,"  Let $t$ be a positive real number. A graph is called $t$-tough, if the
removal of any cutset $S$ leaves at most $|S|/t$ components. The toughness of a
graph is the largest $t$ for which the graph is $t$-tough. A graph is minimally
$t$-tough, if the toughness of the graph is $t$ and the deletion of any edge
from the graph decreases the toughness. The complexity class DP is the set of
all languages that can be expressed as the intersection of a language in NP and
a language in coNP. We prove that recognizing minimally $t$-tough graphs is
DP-complete for any positive integer $t$ and for any positive rational number
$t \leq 1/2$.
",1,0,0,0,0,0
20306,20307,Model-Based Policy Search for Automatic Tuning of Multivariate PID Controllers,"  PID control architectures are widely used in industrial applications. Despite
their low number of open parameters, tuning multiple, coupled PID controllers
can become tedious in practice. In this paper, we extend PILCO, a model-based
policy search framework, to automatically tune multivariate PID controllers
purely based on data observed on an otherwise unknown system. The system's
state is extended appropriately to frame the PID policy as a static state
feedback policy. This renders PID tuning possible as the solution of a finite
horizon optimal control problem without further a priori knowledge. The
framework is applied to the task of balancing an inverted pendulum on a seven
degree-of-freedom robotic arm, thereby demonstrating its capabilities of fast
and data-efficient policy learning, even on complex real world problems.
",1,0,0,1,0,0
17763,17764,Probabilistic Multigraph Modeling for Improving the Quality of Crowdsourced Affective Data,"  We proposed a probabilistic approach to joint modeling of participants'
reliability and humans' regularity in crowdsourced affective studies.
Reliability measures how likely a subject will respond to a question seriously;
and regularity measures how often a human will agree with other
seriously-entered responses coming from a targeted population.
Crowdsourcing-based studies or experiments, which rely on human self-reported
affect, pose additional challenges as compared with typical crowdsourcing
studies that attempt to acquire concrete non-affective labels of objects. The
reliability of participants has been massively pursued for typical
non-affective crowdsourcing studies, whereas the regularity of humans in an
affective experiment in its own right has not been thoroughly considered. It
has been often observed that different individuals exhibit different feelings
on the same test question, which does not have a sole correct response in the
first place. High reliability of responses from one individual thus cannot
conclusively result in high consensus across individuals. Instead, globally
testing consensus of a population is of interest to investigators. Built upon
the agreement multigraph among tasks and workers, our probabilistic model
differentiates subject regularity from population reliability. We demonstrate
the method's effectiveness for in-depth robust analysis of large-scale
crowdsourced affective data, including emotion and aesthetic assessments
collected by presenting visual stimuli to human subjects.
",1,0,0,1,0,0
20726,20727,Spin-wave propagation in cubic anisotropic materials,"  The information carrier of modern technologies is the electron charge whose
transport inevitably generates Joule heating. Spin-waves, the collective
precessional motion of electron spins, do not involve moving charges and thus
avoid Joule heating. In this respect, magnonic devices in which the information
is carried by spin-waves attract interest for low-power computing. However
implementation of magnonic devices for practical use suffers from low spin-wave
signal and on/off ratio. Here we demonstrate that cubic anisotropic materials
can enhance spin-wave signals by improving spin-wave amplitude as well as group
velocity and attenuation length. Furthermore, cubic anisotropic material shows
an enhanced on/off ratio through a laterally localized edge mode, which closely
mimics the gate-controlled conducting channel in traditional field-effect
transistors. These attractive features of cubic anisotropic materials will
invigorate magnonics research towards wave-based functional devices.
",0,1,0,0,0,0
9036,9037,Preserving Order of Data When Validating Defect Prediction Models,"  [Context] The use of defect prediction models, such as classifiers, can
support testing resource allocations by using data of the previous releases of
the same project for predicting which software components are likely to be
defective. A validation technique, hereinafter technique defines a specific way
to split available data in training and test sets to measure a classifier
accuracy. Time-series techniques have the unique ability to preserve the
temporal order of data; i.e., preventing the testing set to have data
antecedent to the training set. [Aim] The aim of this paper is twofold: first
we check if there is a difference in the classifiers accuracy measured by
time-series versus non-time-series techniques. Afterward, we check for a
possible reason for this difference, i.e., if defect rates change across
releases of a project. [Method] Our method consists of measuring the accuracy,
i.e., AUC, of 10 classifiers on 13 open and two closed projects by using three
validation techniques, namely cross validation, bootstrap, and walk-forward,
where only the latter is a time-series technique. [Results] We find that the
AUC of the same classifier used on the same project and measured by 10-fold
varies compared to when measured by walk-forward in the range [-0.20, 0.22],
and it is statistically different in 45% of the cases. Similarly, the AUC
measured by bootstrap varies compared to when measured by walk-forward in the
range [-0.17, 0.43], and it is statistically different in 56% of the cases.
[Conclusions] We recommend choosing the technique to be used by carefully
considering the conclusions to draw, the property of the available datasets,
and the level of realism with the classifier usage scenario.
",1,0,0,0,0,0
9822,9823,Connections between transport of intensity equation and two-dimensional phase unwrapping,"  In a recent publication [Appl. Opt. 55, 2418 (2016)], a method for
two-dimensional phase unwrapping based on the transport of intensity equation
(TIE) was studied. We wish to show that this approach is associated with the
standard least squares phase unwrapping algorithm, but with additional
numerical errors.
",0,1,0,0,0,0
12721,12722,On a Fractional Stochastic Hodgkin-Huxley Model,"  The model studied in this paper is a stochastic extension of the so-called
neuron model introduced by Hodgkin and Huxley. In the sense of rough paths, the
model is perturbed by a multiplicative noise driven by a fractional Brownian
motion, with a vector field satisfying the viability condition of Coutin and
Marie for $\mathbb R\times [0,1]^3$. An application to the modeling of the
membrane potential of nerve fibers damaged by a neuropathy is provided.
",0,0,1,0,0,0
4116,4117,Spectral curves for the rogue waves,"  Here we find the spectral curves, corresponding to the known rational or
quasi-rational solutions of AKNS hierarchy equations, ultimately connected with
the modeling of the rogue waves events in the optical waveguides and in
hydrodynamics. We also determine spectral curves for the multi-phase
trigonometric, hyperbolic and elliptic solutions for the same hierarchy. It
seams that the nature of the related spectral curves was not sufficiently
discussed in existing literature.
",0,1,1,0,0,0
17071,17072,The short-term price impact of trades is universal,"  We analyze a proprietary dataset of trades by a single asset manager,
comparing their price impact with that of the trades of the rest of the market.
In the context of a linear propagator model we find no significant difference
between the two, suggesting that both the magnitude and time dependence of
impact are universal in anonymous, electronic markets. This result is important
as optimal execution policies often rely on propagators calibrated on anonymous
data. We also find evidence that in the wake of a trade the order flow of other
market participants first adds further copy-cat trades enhancing price impact
on very short time scales. The induced order flow then quickly inverts, thereby
contributing to impact decay.
",0,1,0,0,0,0
15809,15810,A variety of elastic anomalies in orbital-active nearly-itinerant cobalt vanadate spinel,"  We perform ultrasound velocity measurements on a single crystal of
nearly-metallic spinel Co$_{1.21}$V$_{1.79}$O$_4$ which exhibits a
ferrimagnetic phase transition at $T_C \sim$ 165 K. The experiments reveal a
variety of elastic anomalies in not only the paramagnetic phase above $T_C$ but
also the ferrimagnetic phase below $T_C$, which should be driven by the
nearly-itinerant character of the orbitally-degenerate V 3$d$ electrons. In the
paramagnetic phase above $T_C$, the elastic moduli exhibit
elastic-mode-dependent unusual temperature variations, suggesting the existence
of a dynamic spin-cluster state. Furthermore, above $T_C$, the sensitive
magnetic-field response of the elastic moduli suggests that, with the negative
magnetoresistance, the magnetic-field-enhanced nearly-itinerant character of
the V 3$d$ electrons emerges from the spin-cluster state. This should be
triggered by the inter-V-site interactions acting on the orbitally-degenerate
3$d$ electrons. In the ferrimagnetic phase below $T_C$, the elastic moduli
exhibit distinct anomalies at $T_1\sim$ 95 K and $T_2\sim$ 50 K, with a sign
change of the magnetoresistance at $T_1$ (positive below $T_1$) and an
enhancement of the positive magnetoresistance below $T_2$, respectively. These
observations below $T_C$ suggest the successive occurrence of an orbital glassy
order at $T_1$ and a structural phase transition at $T_2$, where the rather
localized character of the V 3$d$ electrons evolves below $T_1$ and is further
enhanced below $T_2$.
",0,1,0,0,0,0
3714,3715,Results of measurements of the flux of albedo muons with NEVOD-DECOR experimental complex,"  Results of investigations of the near-horizontal muons in the range of zenith
angles of 85-95 degrees are presented. In this range, so-called ""albedo"" muons
(atmospheric muons scattered in the ground into the upper hemisphere) are
detected. Albedo muons are one of the main sources of the background in
neutrino experiments. Experimental data of two series of measurements conducted
at the experimental complex NEVOD-DECOR with the duration of about 30 thousand
hours ""live"" time are analyzed. The results of measurements of the muon flux
intensity are compared with simulation results using Monte-Carlo on the basis
of two multiple Coulomb scattering models: model of point-like nuclei and model
taking into account finite size of nuclei.
",0,1,0,0,0,0
10723,10724,Phase-tunable Josephson thermal router,"  Since the the first studies of thermodynamics, heat transport has been a
crucial element for the understanding of any thermal system. Quantum mechanics
has introduced new appealing ingredients for the manipulation of heat currents,
such as the long-range coherence of the superconducting condensate. The latter
has been exploited by phase-coherent caloritronics, a young field of
nanoscience, to realize Josephson heat interferometers, which can control
electronic thermal currents as a function of the external magnetic flux. So
far, only one output temperature has been modulated, while multi-terminal
devices that allow to distribute the heat flux among different reservoirs are
still missing. Here, we report the experimental realization of a phase-tunable
thermal router able to control the heat transferred between two terminals
residing at different temperatures. Thanks to the Josephson effect, our
structure allows to regulate the thermal gradient between the output electrodes
until reaching its inversion. Together with interferometers, heat diodes and
thermal memories, the thermal router represents a fundamental step towards the
thermal conversion of non-linear electronic devices, and the realization of
caloritronic logic components.
",0,1,0,0,0,0
16871,16872,The Steinberg linkage class for a reductive algebraic group,"  Let G be a reductive algebraic group over a field of positive characteristic
and denote by C(G) the category of rational G-modules. In this note we
investigate the subcategory of C(G) consisting of those modules whose
composition factors all have highest weights linked to the Steinberg weight.
This subcategory is denoted ST and called the Steinberg component. We give an
explicit equivalence between ST and C(G) and we derive some consequences. In
particular, our result allows us to relate the Frobenius contracting functor to
the projection functor from C(G) onto ST .
",0,0,1,0,0,0
4807,4808,Energy spectrum of cascade showers generated by cosmic ray muons in water,"  The spatial distribution of Cherenkov radiation from cascade showers
generated by muons in water has been measured with Cherenkov water calorimeter
(CWC) NEVOD. This result allowed to improve the techniques of treating cascade
showers with unknown axes by means of CWC response analysis. The techniques of
selecting the events with high energy cascade showers and reconstructing their
parameters are discussed. Preliminary results of measurements of the spectrum
of cascade showers in the energy range 100 GeV - 20 TeV generated by cosmic ray
muons at large zenith angles and their comparison with expectation are
presented.
",0,1,0,0,0,0
19880,19881,Adding educational funcionalities to classic board games,"  In this paper we revisit some classic board games like Pachisi or the Game of
Gosse. The main contribution of the paper is to design and add some
functionalities to the games in order to transform them in serious games, that
is, in games with learning and educational purposes. To do that, at the
beginning of the game, players choose one or several topics and during the
game, players have to anwers questions on these topics in order to move their
markers. We choose classic board games because a lot of people are familiar
with them so it is very easy to start to play without wasting time learning
game rules and, we think that this is an important element to make the game
more attractive to people. To enlarge the number of potential users we have
implement the games just using html and javascript and the games can be used in
any web browser, in any computer (including tablets) , in any computer
arquitecture (Windows, Mac, Linux) and no internet/server conexion is required.
Associated software is distributed under Creative Commons
Attribution-NonCommercial-ShareAlike 3.0 licence and can be obtained at
this http URL
",1,0,0,0,0,0
9156,9157,"Study of secondary neutron interactions with $^{232}$Th, $^{129}$I, and $^{127}$I nuclei with the uranium assembly ""QUINTA"" at 2, 4, and 8 GeV deuteron beams of the JINR Nuclotron accelerator","  The natural uranium assembly, ""QUINTA"", was irradiated with 2, 4, and 8 GeV
deuterons. The $^{232}$Th, $^{127}$I, and $^{129}$I samples have been exposed
to secondary neutrons produced in the assembly at a 20-cm radial distance from
the deuteron beam axis. The spectra of gamma rays emitted by the activated
$^{232}$Th, $^{127}$I, and $^{129}$I samples have been analyzed and several
tens of product nuclei have been identified. For each of those products,
neutron-induced reaction rates have been determined. The transmutation power
for the $^{129}$I samples is estimated. Experimental results were compared to
those calculated with well-known stochastic and deterministic codes.
",0,1,0,0,0,0
9382,9383,"On the Distribution, Model Selection Properties and Uniqueness of the Lasso Estimator in Low and High Dimensions","  We derive expressions for the finite-sample distribution of the Lasso
estimator in the context of a linear regression model with normally distributed
errors in low as well as in high dimensions by exploiting the structure of the
optimization problem defining the estimator. In low dimensions we assume full
rank of the regressor matrix and present expressions for the cumulative
distribution function as well as the densities of the absolutely continuous
parts of the estimator. Additionally, we establish an explicit formula for the
correspondence between the Lasso and the least-squares estimator. We derive
analogous results for the distribution in less explicit form in high dimensions
where we make no assumptions on the regressor matrix at all. In this setting,
we also investigate the model selection properties of the Lasso and show that
possibly only a subset of models might be selected by the estimator, completely
independently of the observed response vector. Finally, we present a condition
for uniqueness of the estimator that is necessary as well as sufficient.
",0,0,1,1,0,0
5225,5226,Parallelized Kendall's Tau Coefficient Computation via SIMD Vectorized Sorting On Many-Integrated-Core Processors,"  Pairwise association measure is an important operation in data analytics.
Kendall's tau coefficient is one widely used correlation coefficient
identifying non-linear relationships between ordinal variables. In this paper,
we investigated a parallel algorithm accelerating all-pairs Kendall's tau
coefficient computation via single instruction multiple data (SIMD) vectorized
sorting on Intel Xeon Phis by taking advantage of many processing cores and
512-bit SIMD vector instructions. To facilitate workload balancing and overcome
on-chip memory limitation, we proposed a generic framework for symmetric
all-pairs computation by building provable bijective functions between job
identifier and coordinate space. Performance evaluation demonstrated that our
algorithm on one 5110P Phi achieves two orders-of-magnitude speedups over
16-threaded MATLAB and three orders-of-magnitude speedups over sequential R,
both running on high-end CPUs. Besides, our algorithm exhibited rather good
distributed computing scalability with respect to number of Phis. Source code
and datasets are publicly available at this http URL.
",1,0,0,0,0,0
17567,17568,Learning to update Auto-associative Memory in Recurrent Neural Networks for Improving Sequence Memorization,"  Learning to remember long sequences remains a challenging task for recurrent
neural networks. Register memory and attention mechanisms were both proposed to
resolve the issue with either high computational cost to retain memory
differentiability, or by discounting the RNN representation learning towards
encoding shorter local contexts than encouraging long sequence encoding.
Associative memory, which studies the compression of multiple patterns in a
fixed size memory, were rarely considered in recent years. Although some recent
work tries to introduce associative memory in RNN and mimic the energy decay
process in Hopfield nets, it inherits the shortcoming of rule-based memory
updates, and the memory capacity is limited. This paper proposes a method to
learn the memory update rule jointly with task objective to improve memory
capacity for remembering long sequences. Also, we propose an architecture that
uses multiple such associative memory for more complex input encoding. We
observed some interesting facts when compared to other RNN architectures on
some well-studied sequence learning tasks.
",1,0,0,1,0,0
5619,5620,Scalable Gaussian Process Computations Using Hierarchical Matrices,"  We present a kernel-independent method that applies hierarchical matrices to
the problem of maximum likelihood estimation for Gaussian processes. The
proposed approximation provides natural and scalable stochastic estimators for
its gradient and Hessian, as well as the expected Fisher information matrix,
that are computable in quasilinear $O(n \log^2 n)$ complexity for a large range
of models. To accomplish this, we (i) choose a specific hierarchical
approximation for covariance matrices that enables the computation of their
exact derivatives and (ii) use a stabilized form of the Hutchinson stochastic
trace estimator. Since both the observed and expected information matrices can
be computed in quasilinear complexity, covariance matrices for MLEs can also be
estimated efficiently. After discussing the associated mathematics, we
demonstrate the scalability of the method, discuss details of its
implementation, and validate that the resulting MLEs and confidence intervals
based on the inverse Fisher information matrix faithfully approach those
obtained by the exact likelihood.
",0,0,0,1,0,0
3098,3099,Parametrised second-order complexity theory with applications to the study of interval computation,"  We extend the framework for complexity of operators in analysis devised by
Kawamura and Cook (2012) to allow for the treatment of a wider class of
representations. The main novelty is to endow represented spaces of interest
with an additional function on names, called a parameter, which measures the
complexity of a given name. This parameter generalises the size function which
is usually used in second-order complexity theory and therefore also central to
the framework of Kawamura and Cook. The complexity of an algorithm is measured
in terms of its running time as a second-order function in the parameter, as
well as in terms of how much it increases the complexity of a given name, as
measured by the parameters on the input and output side.
As an application we develop a rigorous computational complexity theory for
interval computation. In the framework of Kawamura and Cook the representation
of real numbers based on nested interval enclosures does not yield a reasonable
complexity theory. In our new framework this representation is polytime
equivalent to the usual Cauchy representation based on dyadic rational
approximation. By contrast, the representation of continuous real functions
based on interval enclosures is strictly smaller in the polytime reducibility
lattice than the usual representation, which encodes a modulus of continuity.
Furthermore, the function space representation based on interval enclosures is
optimal in the sense that it contains the minimal amount of information amongst
those representations which render evaluation polytime computable.
",1,0,0,0,0,0
5999,6000,Gate-error analysis in simulations of quantum computers with transmon qubits,"  In the model of gate-based quantum computation, the qubits are controlled by
a sequence of quantum gates. In superconducting qubit systems, these gates can
be implemented by voltage pulses. The success of implementing a particular gate
can be expressed by various metrics such as the average gate fidelity, the
diamond distance, and the unitarity. We analyze these metrics of gate pulses
for a system of two superconducting transmon qubits coupled by a resonator, a
system inspired by the architecture of the IBM Quantum Experience. The metrics
are obtained by numerical solution of the time-dependent Schrödinger equation
of the transmon system. We find that the metrics reflect systematic errors that
are most pronounced for echoed cross-resonance gates, but that none of the
studied metrics can reliably predict the performance of a gate when used
repeatedly in a quantum algorithm.
",0,1,0,0,0,0
10437,10438,Solving a non-linear model of HIV infection for CD4+T cells by combining Laplace transformation and Homotopy analysis,"  The aim of this paper is to find the approximate solution of HIV infection
model of CD4+T cells. For this reason, the homotopy analysis transform method
(HATM) is applied. The presented method is combination of traditional homotopy
analysis method (HAM) and the Laplace transformation. The convergence of
presented method is discussed by preparing a theorem which shows the
capabilities of method. The numerical results are shown for different values of
iterations. Also, the regions of convergence are demonstrated by plotting
several h-curves. Furthermore in order to show the efficiency and accuracy of
method, the residual error for different iterations are presented.
",0,0,0,0,1,0
14408,14409,Cell-to-cell variation sets a tissue-rheology-dependent bound on collective gradient sensing,"  When a single cell senses a chemical gradient and chemotaxes, stochastic
receptor-ligand binding can be a fundamental limit to the cell's accuracy. For
clusters of cells responding to gradients, however, there is a critical
difference: even genetically identical cells have differing responses to
chemical signals. With theory and simulation, we show collective chemotaxis is
limited by cell-to-cell variation in signaling. We find that when different
cells cooperate the resulting bias can be much larger than the effects of
ligand-receptor binding. Specifically, when a strongly-responding cell is at
one end of a cell cluster, cluster motion is biased toward that cell. These
errors are mitigated if clusters average measurements over times long enough
for cells to rearrange. In consequence, fluid clusters are better able to sense
gradients: we derive a link between cluster accuracy, cell-to-cell variation,
and the cluster rheology. Because of this connection, increasing the noisiness
of individual cell motion can actually increase the collective accuracy of a
cluster by improving fluidity.
",0,1,0,0,0,0
7017,7018,"Macro-molecular data storage with petabyte/cm^3 density, highly parallel read/write operations, and genuine 3D storage capability","  Digital information can be encoded in the building-block sequence of
macro-molecules, such as RNA and single-stranded DNA. Methods of ""writing"" and
""reading"" macromolecular strands are currently available, but they are slow and
expensive. In an ideal molecular data storage system, routine operations such
as write, read, erase, store, and transfer must be done reliably and at high
speed within an integrated chip. As a first step toward demonstrating the
feasibility of this concept, we report preliminary results of DNA readout
experiments conducted in miniaturized chambers that are scalable to even
smaller dimensions. We show that translocation of a single-stranded DNA
molecule (consisting of 50 adenosine bases followed by 100 cytosine bases)
through an ion-channel yields a characteristic signal that is attributable to
the 2-segment structure of the molecule. We also examine the dependence of the
rate and speed of molecular translocation on the adjustable parameters of the
experiment.
",1,1,0,0,0,0
2113,2114,All-but-the-Top: Simple and Effective Postprocessing for Word Representations,"  Real-valued word representations have transformed NLP applications; popular
examples are word2vec and GloVe, recognized for their ability to capture
linguistic regularities. In this paper, we demonstrate a {\em very simple}, and
yet counter-intuitive, postprocessing technique -- eliminate the common mean
vector and a few top dominating directions from the word vectors -- that
renders off-the-shelf representations {\em even stronger}. The postprocessing
is empirically validated on a variety of lexical-level intrinsic tasks (word
similarity, concept categorization, word analogy) and sentence-level tasks
(semantic textural similarity and { text classification}) on multiple datasets
and with a variety of representation methods and hyperparameter choices in
multiple languages; in each case, the processed representations are
consistently better than the original ones.
",1,0,0,1,0,0
7998,7999,Periodic solution for strongly nonlinear oscillators by He's new amplitude-frequency relationship,"  This paper applies He's new amplitude-frequency relationship recently
established by Ji-Huan He (Int J Appl Comput Math 3 1557-1560, 2017) to study
periodic solutions of strongly nonlinear systems with odd nonlinearities. Some
examples are given to illustrate the effectiveness, ease and convenience of the
method. In general, the results are valid for small as well as large
oscillation amplitude. The method can be easily extended to other nonlinear
systems with odd nonlinearities and can therefore be found widely applicable in
engineering and other science. The method used in this paper can be applied
directly to highly nonlinear problems without any discretization, linearization
or additional requirements.
",0,1,0,0,0,0
19118,19119,Kernelized Hashcode Representations for Relation Extraction,"  Kernel methods have produced state-of-the-art results for a number of NLP
tasks such as relation extraction, but suffer from poor scalability due to the
high cost of computing kernel similarities between natural language structures.
A recently proposed technique, kernelized locality-sensitive hashing (KLSH),
can significantly reduce the computational cost, but is only applicable to
classifiers operating on kNN graphs. Here we propose to use random subspaces of
KLSH codes for efficiently constructing an explicit representation of NLP
structures suitable for general classification methods. Further, we propose an
approach for optimizing the KLSH model for classification problems by
maximizing an approximation of mutual information between the KLSH codes
(feature vectors) and the class labels. We evaluate the proposed approach on
biomedical relation extraction datasets, and observe significant and robust
improvements in accuracy w.r.t. state-of-the-art classifiers, along with
drastic (orders-of-magnitude) speedup compared to conventional kernel methods.
",1,0,0,1,0,0
1622,1623,A Tidy Data Model for Natural Language Processing using cleanNLP,"  The package cleanNLP provides a set of fast tools for converting a textual
corpus into a set of normalized tables. The underlying natural language
processing pipeline utilizes Stanford's CoreNLP library, exposing a number of
annotation tasks for text written in English, French, German, and Spanish.
Annotators include tokenization, part of speech tagging, named entity
recognition, entity linking, sentiment analysis, dependency parsing,
coreference resolution, and information extraction.
",1,0,0,1,0,0
5344,5345,Gated Recurrent Networks for Seizure Detection,"  Recurrent Neural Networks (RNNs) with sophisticated units that implement a
gating mechanism have emerged as powerful technique for modeling sequential
signals such as speech or electroencephalography (EEG). The latter is the focus
on this paper. A significant big data resource, known as the TUH EEG Corpus
(TUEEG), has recently become available for EEG research, creating a unique
opportunity to evaluate these recurrent units on the task of seizure detection.
In this study, we compare two types of recurrent units: long short-term memory
units (LSTM) and gated recurrent units (GRU). These are evaluated using a state
of the art hybrid architecture that integrates Convolutional Neural Networks
(CNNs) with RNNs. We also investigate a variety of initialization methods and
show that initialization is crucial since poorly initialized networks cannot be
trained. Furthermore, we explore regularization of these convolutional gated
recurrent networks to address the problem of overfitting. Our experiments
revealed that convolutional LSTM networks can achieve significantly better
performance than convolutional GRU networks. The convolutional LSTM
architecture with proper initialization and regularization delivers 30%
sensitivity at 6 false alarms per 24 hours.
",0,0,0,1,0,0
4324,4325,Enhancing Multi-Class Classification of Random Forest using Random Vector Functional Neural Network and Oblique Decision Surfaces,"  Both neural networks and decision trees are popular machine learning methods
and are widely used to solve problems from diverse domains. These two
classifiers are commonly used base classifiers in an ensemble framework. In
this paper, we first present a new variant of oblique decision tree based on a
linear classifier, then construct an ensemble classifier based on the fusion of
a fast neural network, random vector functional link network and oblique
decision trees. Random Vector Functional Link Network has an elegant closed
form solution with extremely short training time. The neural network partitions
each training bag (obtained using bagging) at the root level into C subsets
where C is the number of classes in the dataset and subsequently, C oblique
decision trees are trained on such partitions. The proposed method provides a
rich insight into the data by grouping the confusing or hard to classify
samples for each class and thus, provides an opportunity to employ fine-grained
classification rule over the data. The performance of the ensemble classifier
is evaluated on several multi-class datasets where it demonstrates a superior
performance compared to other state-of- the-art classifiers.
",0,0,0,1,0,0
523,524,Static Free Space Detection with Laser Scanner using Occupancy Grid Maps,"  Drivable free space information is vital for autonomous vehicles that have to
plan evasive maneuvers in real-time. In this paper, we present a new efficient
method for environmental free space detection with laser scanner based on 2D
occupancy grid maps (OGM) to be used for Advanced Driving Assistance Systems
(ADAS) and Collision Avoidance Systems (CAS). Firstly, we introduce an enhanced
inverse sensor model tailored for high-resolution laser scanners for building
OGM. It compensates the unreflected beams and deals with the ray casting to
grid cells accuracy and computational effort problems. Secondly, we introduce
the 'vehicle on a circle for grid maps' map alignment algorithm that allows
building more accurate local maps by avoiding the computationally expensive
inaccurate operations of image sub-pixel shifting and rotation. The resulted
grid map is more convenient for ADAS features than existing methods, as it
allows using less memory sizes, and hence, results into a better real-time
performance. Thirdly, we present an algorithm to detect what we call the
'in-sight edges'. These edges guarantee modeling the free space area with a
single polygon of a fixed number of vertices regardless the driving situation
and map complexity. The results from real world experiments show the
effectiveness of our approach.
",1,0,0,0,0,0
2567,2568,Human experts vs. machines in taxa recognition,"  The step of expert taxa recognition currently slows down the response time of
many bioassessments. Shifting to quicker and cheaper state-of-the-art machine
learning approaches is still met with expert scepticism towards the ability and
logic of machines. In our study, we investigate both the differences in
accuracy and in the identification logic of taxonomic experts and machines. We
propose a systematic approach utilizing deep Convolutional Neural Nets with the
transfer learning paradigm and extensively evaluate it over a multi-label and
multi-pose taxonomic dataset specifically created for this comparison. We also
study the prediction accuracy on different ranks of taxonomic hierarchy in
detail. Our results revealed that human experts using actual specimens yield
the lowest classification error. However, our proposed, much faster, automated
approach using deep Convolutional Neural Nets comes very close to human
accuracy. Contrary to previous findings in the literature, we find that
machines following a typical flat classification approach commonly used in
machine learning performs better than forcing machines to adopt a hierarchical,
local per parent node approach used by human taxonomic experts. Finally, we
publicly share our unique dataset to serve as a public benchmark dataset in
this field.
",1,0,0,1,0,0
2564,2565,Accelerated Dual Learning by Homotopic Initialization,"  Gradient descent and coordinate descent are well understood in terms of their
asymptotic behavior, but less so in a transient regime often used for
approximations in machine learning. We investigate how proper initialization
can have a profound effect on finding near-optimal solutions quickly. We show
that a certain property of a data set, namely the boundedness of the
correlations between eigenfeatures and the response variable, can lead to
faster initial progress than expected by commonplace analysis. Convex
optimization problems can tacitly benefit from that, but this automatism does
not apply to their dual formulation. We analyze this phenomenon and devise
provably good initialization strategies for dual optimization as well as
heuristics for the non-convex case, relevant for deep learning. We find our
predictions and methods to be experimentally well-supported.
",1,0,0,0,0,0
10303,10304,Resonance-Free Light Recycling,"  The inability to efficiently tune the optical properties of waveguiding
structures has been one of the major hurdles for the future scalability of
integrated photonic systems. In silicon photonics, although dynamic tuning has
been achieved with various mechanisms, even the most effective thermo-optic
effect offers a refractive index change of only $1.86 \times 10^{-4} K^{-1}$.
To enhance this small change, light recycling based on resonators has been
employed in order to realize efficient modulators, phase shifters, and optical
switches. However, the resonant enhancement comes at a great cost of optical
bandwidth, fabrication tolerance and system scalability. Here we demonstrate a
scalable light recycling approach based on spatial-mode multiplexing. Our
approach offers a fabrication tolerance of ${\pm}$ 15 nm, in stark contrast to
the non-scalable subnanometer tolerance in typical silicon resonators. We
experimentally demonstrate light recycling up to 7 passes with an optical
bandwidth greater than 100 nm. We realize power-efficient thermo-optic phase
shifters that require only 1.7 mW per ${\pi}$, representing more than an 8-fold
reduction in the power consumption.
",0,1,0,0,0,0
1826,1827,Lightweight Multilingual Software Analysis,"  Developer preferences, language capabilities and the persistence of older
languages contribute to the trend that large software codebases are often
multilingual, that is, written in more than one computer language. While
developers can leverage monolingual software development tools to build
software components, companies are faced with the problem of managing the
resultant large, multilingual codebases to address issues with security,
efficiency, and quality metrics. The key challenge is to address the opaque
nature of the language interoperability interface: one language calling
procedures in a second (which may call a third, or even back to the first),
resulting in a potentially tangled, inefficient and insecure codebase. An
architecture is proposed for lightweight static analysis of large multilingual
codebases: the MLSA architecture. Its modular and table-oriented structure
addresses the open-ended nature of multiple languages and language
interoperability APIs. We focus here as an application on the construction of
call-graphs that capture both inter-language and intra-language calls. The
algorithms for extracting multilingual call-graphs from codebases are
presented, and several examples of multilingual software engineering analysis
are discussed. The state of the implementation and testing of MLSA is
presented, and the implications for future work are discussed.
",1,0,0,0,0,0
3862,3863,Efficient and principled score estimation with Nyström kernel exponential families,"  We propose a fast method with statistical guarantees for learning an
exponential family density model where the natural parameter is in a
reproducing kernel Hilbert space, and may be infinite-dimensional. The model is
learned by fitting the derivative of the log density, the score, thus avoiding
the need to compute a normalization constant. Our approach improves the
computational efficiency of an earlier solution by using a low-rank,
Nyström-like solution. The new solution retains the consistency and
convergence rates of the full-rank solution (exactly in Fisher distance, and
nearly in other distances), with guarantees on the degree of cost and storage
reduction. We evaluate the method in experiments on density estimation and in
the construction of an adaptive Hamiltonian Monte Carlo sampler. Compared to an
existing score learning approach using a denoising autoencoder, our estimator
is empirically more data-efficient when estimating the score, runs faster, and
has fewer parameters (which can be tuned in a principled and interpretable
way), in addition to providing statistical guarantees.
",1,0,0,1,0,0
9981,9982,MPC meets SNA: A Privacy Preserving Analysis of Distributed Sensitive Social Networks,"  In this paper, we formalize the notion of distributed sensitive social
networks (DSSNs), which encompasses networks like enmity networks, financial
transaction networks, supply chain networks and sexual relationship networks.
Compared to the well studied traditional social networks, DSSNs are often more
challenging to study, given the privacy concerns of the individuals on whom the
network is knit. In the current work, we envision the use of secure multiparty
tools and techniques for performing privacy preserving social network analysis
over DSSNs. As a step towards realizing this, we design efficient
data-oblivious algorithms for computing the K-shell decomposition and the
PageRank centrality measure for a given DSSN. The designed data-oblivious
algorithms can be translated into equivalent secure computation protocols. We
also list a string of challenges that are needed to be addressed, for employing
secure computation protocols as a practical solution for studying DSSNs.
",1,0,0,0,0,0
7484,7485,Optimization by a quantum reinforcement algorithm,"  A reinforcement algorithm solves a classical optimization problem by
introducing a feedback to the system which slowly changes the energy landscape
and converges the algorithm to an optimal solution in the configuration space.
Here, we use this strategy to concentrate (localize) preferentially the wave
function of a quantum particle, which explores the configuration space of the
problem, on an optimal configuration. We examine the method by solving
numerically the equations governing the evolution of the system, which are
similar to the nonlinear Schrödinger equations, for small problem sizes. In
particular, we observe that reinforcement increases the minimal energy gap of
the system in a quantum annealing algorithm. Our numerical simulations and the
latter observation show that such kind of quantum feedbacks might be helpful in
solving a computationally hard optimization problem by a quantum reinforcement
algorithm.
",1,1,0,0,0,0
13552,13553,The effect of an offset polar cap dipolar magnetic field on the modeling of the Vela pulsar's $γ$-ray light curves,"  We performed geometric pulsar light curve modeling using static, retarded
vacuum, and offset polar cap (PC) dipole $B$-fields (the latter is
characterized by a parameter $\epsilon$), in conjunction with standard two-pole
caustic (TPC) and outer gap (OG) emission geometries. The offset-PC dipole
$B$-field mimics deviations from the static dipole (which corresponds to
$\epsilon=0$). In addition to constant-emissivity geometric models, we also
considered a slot gap (SG) $E$-field associated with the offset-PC dipole
$B$-field and found that its inclusion leads to qualitatively different light
curves. Solving the particle transport equation shows that the particle energy
only becomes large enough to yield significant curvature radiation at large
altitudes above the stellar surface, given this relatively low $E$-field.
Therefore, particles do not always attain the radiation-reaction limit. Our
overall optimal light curve fit is for the retarded vacuum dipole field and OG
model, at an inclination angle $\alpha=78{_{-1}^{+1}}^{\circ}$ and observer
angle $\zeta=69{_{-1}^{+2}}^{\circ}$. For this $B$-field, the TPC model is
statistically disfavored compared to the OG model. For the static dipole field,
neither model is significantly preferred. We found that smaller values of
$\epsilon$ are favored for the offset-PC dipole field when assuming constant
emissivity, and larger $\epsilon$ values favored for variable emissivity, but
not significantly so. When multiplying the SG $E$-field by a factor of 100, we
found improved light curve fits, with $\alpha$ and $\zeta$ being closer to best
fits from independent studies, as well as curvature radiation reaction at lower
altitudes.
",0,1,0,0,0,0
20467,20468,Coarse fundamental groups and box spaces,"  We use a coarse version of the fundamental group first introduced by Barcelo,
Kramer, Laubenbacher and Weaver to show that box spaces of finitely presented
groups detect the normal subgroups used to construct the box space, up to
isomorphism. As a consequence we have that two finitely presented groups admit
coarsely equivalent box spaces if and only if they are commensurable via normal
subgroups. We also provide an example of two filtrations $(N_i)$ and $(M_i)$ of
a free group $F$ such that $M_i>N_i$ for all $i$ with $[M_i:N_i]$ uniformly
bounded, but with $\Box_{(N_i)}F$ not coarsely equivalent to $\Box_{(M_i)}F$.
Finally, we give some applications of the main theorem for rank gradient and
the first $\ell^2$ Betti number, and show that the main theorem can be used to
construct infinitely many coarse equivalence classes of box spaces with various
properties.
",0,0,1,0,0,0
8347,8348,A PCA-based approach for subtracting thermal background emission in high-contrast imaging data,"  Ground-based observations at thermal infrared wavelengths suffer from large
background radiation due to the sky, telescope and warm surfaces in the
instrument. This significantly limits the sensitivity of ground-based
observations at wavelengths longer than 3 microns. We analyzed this background
emission in infrared high contrast imaging data, show how it can be modelled
and subtracted and demonstrate that it can improve the detection of faint
sources, such as exoplanets. We applied principal component analysis to model
and subtract the thermal background emission in three archival high contrast
angular differential imaging datasets in the M and L filter. We describe how
the algorithm works and explain how it can be applied. The results of the
background subtraction are compared to the results from a conventional mean
background subtraction scheme. Finally, both methods for background subtraction
are also compared by performing complete data reductions. We analyze the
results from the M dataset of HD100546 qualitatively. For the M band dataset of
beta Pic and the L band dataset of HD169142, which was obtained with an annular
groove phase mask vortex vector coronagraph, we also calculate and analyze the
achieved signal to noise (S/N). We show that applying PCA is an effective way
to remove spatially and temporarily varying thermal background emission down to
close to the background limit. The procedure also proves to be very successful
at reconstructing the background that is hidden behind the PSF. In the complete
data reductions, we find at least qualitative improvements for HD100546 and
HD169142, however, we fail to find a significant increase in S/N of beta Pic b.
We discuss these findings and argue that in particular datasets with strongly
varying observing conditions or infrequently sampled sky background will
benefit from the new approach.
",0,1,0,0,0,0
13539,13540,Local incompressibility estimates for the Laughlin phase,"  We prove sharp density upper bounds on optimal length-scales for the ground
states of classical 2D Coulomb systems and generalizations thereof. Our method
is new, based on an auxiliary Thomas-Fermi-like variational model. Moreover, we
deduce density upper bounds for the related low-temperature Gibbs states. Our
motivation comes from fractional quantum Hall physics, more precisely, the
perturbation of the Laughlin state by external potentials or impurities. These
give rise to a class of many-body wave-functions that have the form of a
product of the Laughlin state and an analytic function of many variables. This
class is related via Laughlin's plasma analogy to Gibbs states of the
generalized classical Coulomb systems we consider. Our main result shows that
the perturbation of the Laughlin state cannot increase the particle density
anywhere, with implications for the response of FQHE systems to external
perturbations.
",0,1,1,0,0,0
19958,19959,Wind Shear and Turbulence on Titan : Huygens Analysis,"  Wind shear measured by Doppler tracking of the Huygens probe is evaluated,
and found to be within the range anticipated by pre-flight assessments (namely
less than two times the Brunt-Vaisala frequency). The strongest large-scale
shear encountered was ~5 m/s/km, a level associated with 'Light' turbulence in
terrestrial aviation. Near-surface winds (below 4km) have small-scale
fluctuations of ~0.2 m/s , indicated both by probe tilt and Doppler tracking,
and the characteristics of the fluctuation, of interest for future missions to
Titan, can be reproduced with a simple autoregressive (AR(1)) model. The
turbulent dissipation rate at an altitude of ~500m is found to be 16 cm2/sec3,
which may be a useful benchmark for atmospheric circulation models.
",0,1,0,0,0,0
2365,2366,Entrywise Eigenvector Analysis of Random Matrices with Low Expected Rank,"  Recovering low-rank structures via eigenvector perturbation analysis is a
common problem in statistical machine learning, such as in factor analysis,
community detection, ranking, matrix completion, among others. While a large
variety of results provide tight bounds on the average errors between empirical
and population statistics of eigenvectors, fewer results are tight for
entrywise analyses, which are critical for a number of problems such as
community detection and ranking.
This paper investigates the entrywise perturbation analysis for a large class
of random matrices whose expectations are low-rank, including community
detection, synchronization ($\mathbb{Z}_2$-spiked Wigner model) and matrix
completion models. Denoting by $\{u_k\}$, respectively $\{u_k^*\}$, the
eigenvectors of a random matrix $A$, respectively $\mathbb{E} A$, the paper
characterizes cases for which $$u_k \approx \frac{A u_k^*}{\lambda_k^*}$$
serves as a first-order approximation under the $\ell_\infty$ norm. The fact
that the approximation is both tight and linear in the random matrix $A$ allows
for sharp comparisons of $u_k$ and $u_k^*$. In particular, it allows to compare
the signs of $u_k$ and $u_k^*$ even when $\| u_k - u_k^*\|_{\infty}$ is large,
which in turn allows to settle the conjecture in Abbe et al. (2016) that the
spectral algorithm achieves exact recovery in the stochastic block model
without any trimming or cleaning steps. The results are further extended to the
perturbation of eigenspaces, providing new bounds for $\ell_\infty$-type errors
in noisy matrix completion.
",0,0,1,1,0,0
10023,10024,On the Whittaker Plancherel Theorem for Real Reductive Groups,"  The main purpose of this article is to fix several aspects aspects of the
proof of the Whittaker Plancherel Theorem in Real Reductive Groups II that are
affected by recently observed errors or gaps . In the process of completing the
proof of the theorem the paper also gives an exposition of its structure, and
adds some clarifying new results. It also outlines the steps in the proof of
the Harish-Chandra Plancherel theorem as they are needed in our proof of the
Whittaker version.
",0,0,1,0,0,0
9459,9460,Towards Adaptive Resilience in High Performance Computing,"  Failure rates in high performance computers rapidly increase due to the
growth in system size and complexity. Hence, failures became the norm rather
than the exception. Different approaches on high performance computing (HPC)
systems have been introduced, to prevent failures (e. g., redundancy) or at
least minimize their impacts (e. g., checkpoint and restart). In most cases,
when these approaches are employed to increase the resilience of certain parts
of a system, energy consumption rapidly increases, or performance significantly
degrades. To address this challenge, we propose on-demand resilience as an
approach to achieve adaptive resilience in HPC systems. In this work, the HPC
system is considered in its entirety and resilience mechanisms such as
checkpointing, isolation, and migration, are activated on-demand. Using the
proposed approach, the unavoidable increase in total energy consumption and
system performance degradation is decreased compared to the typical
checkpoint/restart and redundant resilience mechanisms. Our work aims to
mitigate a large number of failures occurring at various layers in the system,
to prevent their propagation, and to minimize their impact, all of this in an
energy-saving manner. In the case of failures that are estimated to occur but
cannot be mitigated using the proposed on-demand resilience approach, the
system administrators will be notified in view of performing further
investigations into the causes of these failures and their impacts.
",1,0,0,0,0,0
17894,17895,Evolution and Recent Developments of the Gaseous Photon Detectors Technologies,"  The evolution and the present status of the gaseous photon detectors
technologies are reviewed. The most recent developments in several branches of
the field are described, in particular the installation and commissioning of
the first large area MPGD-based detectors of single photons on COMPASS RICH-1.
Investigation of novel detector architectures, different materials and various
applications are reported, and the quest for visible light gaseous photon
detectors is discussed. The progress on the use of gaseous photon detector
related techniques in the field of cryogenic applications and gaseous or liquid
scintillation imaging are presented.
",0,1,0,0,0,0
15198,15199,Simple Policy Evaluation for Data-Rich Iterative Tasks,"  A data-based policy for iterative control task is presented. The proposed
strategy is model-free and can be applied whenever safe input and state
trajectories of a system performing an iterative task are available. These
trajectories, together with a user-defined cost function, are exploited to
construct a piecewise affine approximation to the value function. Approximated
value functions are then used to evaluate the control policy by solving a
linear program. We show that for linear system subject to convex cost and
constraints, the proposed strategy guarantees closed-loop constraint
satisfaction and performance bounds on the closed-loop trajectory. We evaluate
the proposed strategy in simulations and experiments, the latter carried out on
the Berkeley Autonomous Race Car (BARC) platform. We show that the proposed
strategy is able to reduce the computation time by one order of magnitude while
achieving the same performance as our model-based control algorithm.
",1,0,0,0,0,0
8027,8028,Scaling relations in the diffusive infiltration in fractals,"  In a recent work on fluid infiltration in a Hele-Shaw cell with the
pore-block geometry of Sierpinski carpets (SCs), the area filled by the
invading fluid was shown to scale as F~t^n, with n<1/2, thus providing a
macroscopic realization of anomalous diffusion [Filipovitch et al, Water
Resour. Res. 52 5167 (2016)]. The results agree with simulations of a diffusion
equation with constant pressure at one of the borders of those fractals, but
the exponent n is very different from the anomalous exponent nu=1/D_W of single
particle diffusion in the same fractals (D_W is the random walk dimension).
Here we use a scaling approach to show that those exponents are related as
n=nu(D_F-D_B), where D_F and D_B are the fractal dimensions of the bulk and of
the border from which diffusing particles come, respectively. This relation is
supported by accurate numerical estimates in two SCs and in two generalized
Menger sponges (MSs), in which we performed simulations of single particle
random walks (RWs) with a rigid impermeable border and of a diffusive
infiltration model in which that border is permanently filled with diffusing
particles. This study includes one MS whose external border is also fractal.
The exponent relation is also consistent with the recent simulational and
experimental results on fluid infiltration in SCs, and explains the approximate
quadratic dependence of n on D_F in these fractals. We also show that the
mean-square displacement of single particle RWs has log-periodic oscillations,
whose periods are similar for fractals with the same scaling factor in the
generator (even with different embedding dimensions), which is consistent with
the discrete scale invariance scenario. The roughness of a diffusion front
defined in the infiltration problem also shows this type of oscillation, which
is enhanced in fractals with narrow channels between large lacunas.
",0,1,0,0,0,0
5311,5312,Lancaster A at SemEval-2017 Task 5: Evaluation metrics matter: predicting sentiment from financial news headlines,"  This paper describes our participation in Task 5 track 2 of SemEval 2017 to
predict the sentiment of financial news headlines for a specific company on a
continuous scale between -1 and 1. We tackled the problem using a number of
approaches, utilising a Support Vector Regression (SVR) and a Bidirectional
Long Short-Term Memory (BLSTM). We found an improvement of 4-6% using the LSTM
model over the SVR and came fourth in the track. We report a number of
different evaluations using a finance specific word embedding model and reflect
on the effects of using different evaluation metrics.
",1,0,0,0,0,0
1532,1533,Integrating a Global Induction Mechanism into a Sequent Calculus,"  Most interesting proofs in mathematics contain an inductive argument which
requires an extension of the LK-calculus to formalize. The most commonly used
calculi for induction contain a separate rule or axiom which reduces the valid
proof theoretic properties of the calculus. To the best of our knowledge, there
are no such calculi which allow cut-elimination to a normal form with the
subformula property, i.e. every formula occurring in the proof is a subformula
of the end sequent. Proof schemata are a variant of LK-proofs able to simulate
induction by linking proofs together. There exists a schematic normal form
which has comparable proof theoretic behaviour to normal forms with the
subformula property. However, a calculus for the construction of proof schemata
does not exist. In this paper, we introduce a calculus for proof schemata and
prove soundness and completeness with respect to a fragment of the inductive
arguments formalizable in Peano arithmetic.
",1,0,0,0,0,0
14899,14900,Long term availability of raw experimental data in experimental fracture mechanics,"  Experimental data availability is a cornerstone for reproducibility in
experimental fracture mechanics, which is crucial to the scientific method.
This short communication focuses on the accessibility and long term
availability of raw experimental data. The corresponding authors of the eleven
most cited papers, related to experimental fracture mechanics, for every year
from 2000 up to 2016, were kindly asked about the availability of the raw
experimental data associated with each publication. For the 187 e-mails sent:
22.46% resulted in outdated contact information, 57.75% of the authors did
received our request and did not reply, and 19.79 replied to our request. The
availability of data is generally low with only $11$ available data sets
(5.9%). The authors identified two main issues for the lacking availability of
raw experimental data. First, the ability to retrieve data is strongly attached
to the the possibility to contact the corresponding author. This study suggests
that institutional e-mail addresses are insufficient means for obtaining
experimental data sets. Second, lack of experimental data is also due that
submission and publication does not require to make the raw experimental data
available. The following solutions are proposed: (1) Requirement of unique
identifiers, like ORCID or ResearcherID, to detach the author(s) from their
institutional e-mail address, (2) Provide DOIs, like Zenodo or Dataverse, to
make raw experimental data citable, and (3) grant providing organizations
should ensure that experimental data by public funded projects is available to
the public.
",0,0,0,1,0,0
11021,11022,Random non-Abelian G-circulant matrices. Spectrum of random convolution operators on large finite groups,"  We analyse the limiting behavior of the eigenvalue and singular value
distribution for random convolution operators on large (not necessarily
Abelian) groups, extending the results by M. Meckes for the Abelian case. We
show that for regular sequences of groups the limiting distribution of
eigenvalues (resp. singular values) is a mixture of eigenvalue (resp. singular
value) distributions of Ginibre matrices with the directing measure being
related to the limiting behavior of the Plancherel measure of the sequence of
groups. In particular for the sequence of symmetric groups, the limiting
distributions are just the circular and quarter circular laws, whereas e.g. for
the dihedral groups the limiting distributions have unbounded supports but are
different than in the Abelian case.
We also prove that under additional assumptions on the sequence of groups (in
particular for symmetric groups of increasing order) families of stochastically
independent random projection operators converge in moments to free circular
elements.
Finally, in the Gaussian case we provide Central Limit Theorems for linear
eigenvalue statistics.
",0,0,1,0,0,0
5093,5094,Solitons and breathers for nonisospectral mKdV equation with Darboux transformation,"  Under investigation in this paper is the nonisospectral and variable
coefficients modified Kortweg-de Vries (vc-mKdV) equation, which manifests in
diverse areas of physics such as fluid dynamics, ion acoustic solitons and
plasma mechanics. With the degrees of restriction reduced, a simplified
constraint is introduced, under which the vc-mKdV equation is an integrable
system and the spectral flow is time-varying. The Darboux transformation for
such equation is constructed, which gives rise to the generation of variable
kinds of solutions including the double-breather coherent structure, periodical
soliton-breather and localized solitons and breathers. In addition, the effect
of variable coefficients and initial phases is discussed in terms of the
soliton amplitude, polarity, velocity and width, which might provide feasible
soliton management with certain conditions taken into account.
",0,1,0,0,0,0
20032,20033,Identifying Critical Risks of Cascading Failures in Power Systems,"  Potential critical risks of cascading failures in power systems can be
identified by exposing those critical electrical elements on which certain
initial disturbances may cause maximum disruption to power transmission
networks. In this work, we investigate cascading failures in power systems
described by the direct current (DC) power flow equations, while initial
disturbances take the form of altering admittance of elements. The disruption
is quantified with the remaining transmission power at the end of cascading
process. In particular, identifying the critical elements and the corresponding
initial disturbances causing the worst-case cascading blackout is formulated as
a dynamic optimization problem (DOP) in the framework of optimal control
theory, where the entire propagation process of cascading failures is put under
consideration. An Identifying Critical Risk Algorithm (ICRA) based on the
maximum principle is proposed to solve the DOP. Simulation results on the IEEE
9-Bus and the IEEE 14-Bus test systems are presented to demonstrate the
effectiveness of the algorithm.
",1,0,0,0,0,0
5681,5682,Combining Information from Multiple Forecasters: Inefficiency of Central Tendency,"  Even though the forecasting literature agrees that aggregating multiple
predictions of some future outcome typically outperforms the individual
predictions, there is no general consensus about the right way to do this. Most
common aggregators are means, defined loosely as aggregators that always remain
between the smallest and largest predictions. Examples include the arithmetic
mean, trimmed means, median, mid-range, and many other measures of central
tendency. If the forecasters use different information, the aggregator ideally
combines their information into a consensus without losing or distorting any of
it. An aggregator that achieves this is considered efficient. Unfortunately,
our results show that if the forecasters use their information accurately, an
aggregator that always remains strictly between the smallest and largest
predictions is never efficient in practice. A similar result holds even if the
ideal predictions are distorted with random error that is centered at zero. If
these noisy predictions are aggregated with a similar notion of centrality,
then, under some mild conditions, the aggregator is asymptotically inefficient.
",0,0,1,1,0,0
13777,13778,A Continuous Beam Steering Slotted Waveguide Antenna Using Rotating Dielectric Slabs,"  The design, simulation and measurement of a beam steerable slotted waveguide
antenna operating in X band are presented. The proposed beam steerable antenna
consists of a standard rectangular waveguide (RWG) section with longitudinal
slots in the broad wall. The beam steering in this configuration is achieved by
rotating two dielectric slabs inside the waveguide and consequently changing
the phase of the slots excitations. In order to confirm the usefulness of this
concept, a non-resonant 20-slot waveguide array antenna with an element spacing
of d = 0.58{\lambda}0 has been designed, built and measured. A 14 deg beam
scanning from near broadside ({\theta} = 4 deg) toward end-fire ({\theta} = 18
deg) direction is observed. The gain varies from 18.33 dB to 19.11 dB which
corresponds to the radiation efficiencies between 95% and 79%. The side-lobe
level is -14 dB at the design frequency of 9.35 GHz. The simulated co-polarized
realized gain closely matches the fabricated prototype patterns.
",0,1,0,0,0,0
8892,8893,Ten Simple Rules for Reproducible Research in Jupyter Notebooks,"  Reproducibility of computational studies is a hallmark of scientific
methodology. It enables researchers to build with confidence on the methods and
findings of others, reuse and extend computational pipelines, and thereby drive
scientific progress. Since many experimental studies rely on computational
analyses, biologists need guidance on how to set up and document reproducible
data analyses or simulations.
In this paper, we address several questions about reproducibility. For
example, what are the technical and non-technical barriers to reproducible
computational studies? What opportunities and challenges do computational
notebooks offer to overcome some of these barriers? What tools are available
and how can they be used effectively?
We have developed a set of rules to serve as a guide to scientists with a
specific focus on computational notebook systems, such as Jupyter Notebooks,
which have become a tool of choice for many applications. Notebooks combine
detailed workflows with narrative text and visualization of results. Combined
with software repositories and open source licensing, notebooks are powerful
tools for transparent, collaborative, reproducible, and reusable data analyses.
",1,0,0,0,0,0
20544,20545,Learning to Remember Rare Events,"  Despite recent advances, memory-augmented deep neural networks are still
limited when it comes to life-long and one-shot learning, especially in
remembering rare events. We present a large-scale life-long memory module for
use in deep learning. The module exploits fast nearest-neighbor algorithms for
efficiency and thus scales to large memory sizes. Except for the
nearest-neighbor query, the module is fully differentiable and trained
end-to-end with no extra supervision. It operates in a life-long manner, i.e.,
without the need to reset it during training.
Our memory module can be easily added to any part of a supervised neural
network. To show its versatility we add it to a number of networks, from simple
convolutional ones tested on image classification to deep sequence-to-sequence
and recurrent-convolutional models. In all cases, the enhanced network gains
the ability to remember and do life-long one-shot learning. Our module
remembers training examples shown many thousands of steps in the past and it
can successfully generalize from them. We set new state-of-the-art for one-shot
learning on the Omniglot dataset and demonstrate, for the first time, life-long
one-shot learning in recurrent neural networks on a large-scale machine
translation task.
",1,0,0,0,0,0
4200,4201,Metric-Optimized Example Weights,"  Real-world machine learning applications often have complex test metrics, and
may have training and test data that follow different distributions. We propose
addressing these issues by using a weighted loss function with a standard
convex loss, but with weights on the training examples that are learned to
optimize the test metric of interest on the validation set. These
metric-optimized example weights can be learned for any test metric, including
black box losses and customized metrics for specific applications. We
illustrate the performance of our proposal with public benchmark datasets and
real-world applications with domain shift and custom loss functions that
balance multiple objectives, impose fairness policies, and are non-convex and
non-decomposable.
",0,0,0,1,0,0
1499,1500,Learning RBM with a DC programming Approach,"  By exploiting the property that the RBM log-likelihood function is the
difference of convex functions, we formulate a stochastic variant of the
difference of convex functions (DC) programming to minimize the negative
log-likelihood. Interestingly, the traditional contrastive divergence algorithm
is a special case of the above formulation and the hyperparameters of the two
algorithms can be chosen such that the amount of computation per mini-batch is
identical. We show that for a given computational budget the proposed algorithm
almost always reaches a higher log-likelihood more rapidly, compared to the
standard contrastive divergence algorithm. Further, we modify this algorithm to
use the centered gradients and show that it is more efficient and effective
compared to the standard centered gradient algorithm on benchmark datasets.
",1,0,0,1,0,0
12423,12424,Azumaya algebras and canonical components,"  Let $M$ be a compact 3-manifold and $\Gamma=\pi_1(M)$. The work of Thurston
and Culler--Shalen established the $\mathrm{SL}_2(\mathbb{C})$ character
variety $X(\Gamma)$ as fundamental tool in the study of the geometry and
topology of $M$. This is particularly so in the case when $M$ is the exterior
of a hyperbolic knot $K$ in $S^3$. The main goals of this paper are to bring to
bear tools from algebraic and arithmetic geometry to understand algebraic and
number theoretic properties of the so-called canonical component of $X(\Gamma)$
as well as distinguished points on the canonical component when $\Gamma$ is a
knot group. In particular, we study how the theory of quaternion Azumaya
algebras can be used to obtain algebraic and arithmetic information about Dehn
surgeries, and perhaps of most interest, to construct new knot invariants that
lie in the Brauer groups of curves over number fields.
",0,0,1,0,0,0
18977,18978,Variegation and space weathering on asteroid 21 Lutetia,"  During the flyby in 2010, the OSIRIS camera on-board Rosetta acquired
hundreds of high-resolution images of asteroid Lutetia's surface through a
range of narrow-band filters. While Lutetia appears very bland in the visible
wavelength range, Magrin et al. (2012) tentatively identified UV color
variations in the Baetica cluster, a group of relatively young craters close to
the north pole. As Lutetia remains a poorly understood asteroid, such color
variations may provide clues to the nature of its surface. We take the color
analysis one step further. First we orthorectify the images using a shape model
and improved camera pointing, then apply a variety of techniques (photometric
correction, principal component analysis) to the resulting color cubes. We
characterize variegation in the Baetica crater cluster at high spatial
resolution, identifying crater rays and small, fresh impact craters. We argue
that at least some of the color variation is due to space weathering, which
makes Lutetia's regolith redder and brighter.
",0,1,0,0,0,0
13981,13982,Versality of the relative Fukaya category,"  Seidel introduced the notion of a Fukaya category `relative to an ample
divisor', explained that it is a deformation of the Fukaya category of the
affine variety that is the complement of the divisor, and showed how the
relevant deformation theory is controlled by the symplectic cohomology of the
complement. We elaborate on Seidel's definition of the relative Fukaya
category, and give a criterion under which the deformation is versal.
",0,0,1,0,0,0
8021,8022,Convex Relaxations for Pose Graph Optimization with Outliers,"  Pose Graph Optimization involves the estimation of a set of poses from
pairwise measurements and provides a formalization for many problems arising in
mobile robotics and geometric computer vision. In this paper, we consider the
case in which a subset of the measurements fed to pose graph optimization is
spurious. Our first contribution is to develop robust estimators that can cope
with heavy-tailed measurement noise, hence increasing robustness to the
presence of outliers. Since the resulting estimators require solving nonconvex
optimization problems, we further develop convex relaxations that approximately
solve those problems via semidefinite programming. We then provide conditions
under which the proposed relaxations are exact. Contrarily to existing
approaches, our convex relaxations do not rely on the availability of an
initial guess for the unknown poses, hence they are more suitable for setups in
which such guess is not available (e.g., multi-robot localization, recovery
after localization failure). We tested the proposed techniques in extensive
simulations, and we show that some of the proposed relaxations are indeed tight
(i.e., they solve the original nonconvex problem 10 exactly) and ensure
accurate estimation in the face of a large number of outliers.
",1,0,0,0,0,0
13485,13486,From parabolic-trough to metasurface-concentrator,"  Metasurfaces are promising tools towards novel designs for flat optics
applications. As such their quality and tolerance to fabrication imperfections
need to be evaluated with specific tools. However, most such tools rely on the
geometrical optics approximation and are not straightforwardly applicable to
metasurfaces. In this Letter, we introduce and evaluate, for metasurfaces,
parameters such as the intercept factor and the slope error usually defined for
solar concentrators in the realm of ray-optics. After proposing definitions
valid in physical optics, we put forward an approach to calculate them. As
examples, we design three different concentrators based on three specific unit
cells and assess them numerically. The concept allows for the comparison of the
efficiency of the metasurfaces, their sensitivities to fabrication
imperfections and will be critical for practical systems.
",0,1,0,0,0,0
12768,12769,Gaussian curvature directs the distribution of spontaneous curvature on bilayer membrane necks,"  Formation of membrane necks is crucial for fission and fusion in lipid
bilayers. In this work, we seek to answer the following fundamental question:
what is the relationship between protein-induced spontaneous mean curvature and
the Gaussian curvature at a membrane neck? Using an augmented Helfrich model
for lipid bilayers to include membrane-protein interaction, we solve the shape
equation on catenoids to find the field of spontaneous curvature that satisfies
mechanical equilibrium of membrane necks. In this case, the shape equation
reduces to a variable coefficient Helmholtz equation for spontaneous curvature,
where the source term is proportional to the Gaussian curvature. We show how
this latter quantity is responsible for non-uniform distribution of spontaneous
curvature in minimal surfaces. We then explore the energetics of catenoids with
different spontaneous curvature boundary conditions and geometric asymmetries
to show how heterogeneities in spontaneous curvature distribution can couple
with Gaussian curvature to result in membrane necks of different geometries.
",0,1,0,0,0,0
1540,1541,Faster integer and polynomial multiplication using cyclotomic coefficient rings,"  We present an algorithm that computes the product of two n-bit integers in
O(n log n (4\sqrt 2)^{log^* n}) bit operations. Previously, the best known
bound was O(n log n 6^{log^* n}). We also prove that for a fixed prime p,
polynomials in F_p[X] of degree n may be multiplied in O(n log n 4^{log^* n})
bit operations; the previous best bound was O(n log n 8^{log^* n}).
",1,0,0,0,0,0
529,530,Searching for the Transit of the Earth--mass exoplanet Proxima~Centauri~b in Antarctica: Preliminary Result,"  Proxima Centauri is known as the closest star from the Sun. Recently, radial
velocity observations revealed the existence of an Earth-mass planet around it.
With an orbital period of ~11 days, the surface of Proxima Centauri b is
temperate and might be habitable. We took a photometric monitoring campaign to
search for its transit, using the Bright Star Survey Telescope at the Zhongshan
Station in Antarctica. A transit-like signal appearing on 2016 September 8th,
is identified tentatively. Its midtime, $T_{C}=2,457,640.1990\pm0.0017$ HJD, is
consistent with the predicted ephemeris based on RV orbit in a 1$\sigma$
confidence interval. Time-correlated noise is pronounced in the light curve of
Proxima Centauri, affecting detection of transits. We develop a technique, in a
Gaussian process framework, to gauge the statistical significance of potential
transit detection. The tentative transit signal reported here, has a confidence
level of $2.5\sigma$. Further detection of its periodic signals is necessary to
confirm the planetary transit of Proxima Centauri b. We plan to monitor Proxima
Centauri in next Polar night at Dome A in Antarctica, taking the advantage of
continuous darkness. \citet{Kipping17} reported two tentative transit-like
signals of Proxima Centauri b, observed by the Microvariability and Oscillation
of Stars space Telescope in 2014 and 2015, respectively. The midtransit time of
our detection is 138 minutes later than that predicted by their transit
ephemeris. If all the signals are real transits, the misalignment of the epochs
plausibly suggests transit timing variations of Proxima Centauri b induced by
an outer planet in this system.
",0,1,0,0,0,0
10271,10272,"Deep learning Approach for Classifying, Detecting and Predicting Photometric Redshifts of Quasars in the Sloan Digital Sky Survey Stripe 82","  We apply a convolutional neural network (CNN) to classify and detect quasars
in the Sloan Digital Sky Survey Stripe 82 and also to predict the photometric
redshifts of quasars. The network takes the variability of objects into account
by converting light curves into images. The width of the images, noted w,
corresponds to the five magnitudes ugriz and the height of the images, noted h,
represents the date of the observation. The CNN provides good results since its
precision is 0.988 for a recall of 0.90, compared to a precision of 0.985 for
the same recall with a random forest classifier. Moreover 175 new quasar
candidates are found with the CNN considering a fixed recall of 0.97. The
combination of probabilities given by the CNN and the random forest makes good
performance even better with a precision of 0.99 for a recall of 0.90.
For the redshift predictions, the CNN presents excellent results which are
higher than those obtained with a feature extraction step and different
classifiers (a K-nearest-neighbors, a support vector machine, a random forest
and a gaussian process classifier). Indeed, the accuracy of the CNN within
|\Delta z|<0.1 can reach 78.09%, within |\Delta z|<0.2 reaches 86.15%, within
|\Delta z|<0.3 reaches 91.2% and the value of rms is 0.359. The performance of
the KNN decreases for the three |\Delta z| regions, since within the accuracy
of |\Delta z|<0.1, |\Delta z|<0.2 and |\Delta z|<0.3 is 73.72%, 82.46% and
90.09% respectively, and the value of rms amounts to 0.395. So the CNN
successfully reduces the dispersion and the catastrophic redshifts of quasars.
This new method is very promising for the future of big databases like the
Large Synoptic Survey Telescope.
",0,1,0,0,0,0
5209,5210,The Trees of Hanoi,"  The game of the Towers of Hanoi is generalized to binary trees. First, a
straightforward solution of the game is discussed. Second, a shorter solution
is presented, which is then shown to be optimal.
",1,0,1,0,0,0
19899,19900,A Gaussian Process Regression Model for Distribution Inputs,"  Monge-Kantorovich distances, otherwise known as Wasserstein distances, have
received a growing attention in statistics and machine learning as a powerful
discrepancy measure for probability distributions. In this paper, we focus on
forecasting a Gaussian process indexed by probability distributions. For this,
we provide a family of positive definite kernels built using transportation
based distances. We provide a probabilistic understanding of these kernels and
characterize the corresponding stochastic processes. We prove that the Gaussian
processes indexed by distributions corresponding to these kernels can be
efficiently forecast, opening new perspectives in Gaussian process modeling.
",0,0,1,1,0,0
10251,10252,Regulous vector bundles,"  Among recently introduced new notions in real algebraic geometry is that of
regulous functions. Such functions form a foundation for the development of
regulous geometry. Several interesting results on regulous varieties and
regulous sheaves are already available. In this paper, we define and
investigate regulous vector bundles. We establish algebraic and geometric
properties of such vector bundles, and identify them with stratified-algebraic
vector bundles. Furthermore, using new results on curve-rational functions, we
characterize regulous vector bundles among families of vector spaces
parametrized by an affine regulous variety. We also study relationships between
regulous and topological vector bundles.
",0,0,1,0,0,0
5733,5734,Topological strings linking with quasi-particle exchange in superconducting Dirac semimetals,"  We demonstrate a topological classification of vortices in three dimensional
time-reversal invariant topological superconductors based on superconducting
Dirac semimetals with an s-wave superconducting order parameter by means of a
pair of numbers $(N_\Phi,N)$, accounting how many units $N_\Phi$ of magnetic
fluxes $hc/4e$ and how many $N$ chiral Majorana modes the vortex carries. From
these quantities, we introduce a topological invariant which further classifies
the properties of such vortices under linking processes. While such processes
are known to be related to instanton processes in a field theoretic
description, we demonstrate here that they are, in fact, also equivalent to the
fractional Josephson effect on junctions based at the edges of quantum spin
Hall systems. This allows one to consider microscopically the effects of
interactions in the linking problem. We therefore demonstrate that associated
to links between vortices, one has the exchange of quasi-particles, either
Majorana zero-modes or $e/2$ quasi-particles, which allows for a topological
classification of vortices in these systems, seen to be $\mathbb{Z}_8$
classified. While $N_\Phi$ and $N$ are shown to be both even or odd in the
weakly-interacting limit, in the strongly interacting scenario one loosens this
constraint. In this case, one may have further fractionalization possibilities
for the vortices, whose excitations are described by $SO(3)_3$-like conformal
field theories with quasi-particle exchanges of more exotic types.
",0,1,0,0,0,0
10954,10955,A framework for quantitative modeling and analysis of highly (re)configurable systems,"  This paper presents our approach to the quantitative modeling and analysis of
highly (re)configurable systems, such as software product lines. Different
combinations of the optional features of such a system give rise to
combinatorially many individual system variants. We use a formal modeling
language that allows us to model systems with probabilistic behavior, possibly
subject to quantitative feature constraints, and able to dynamically install,
remove or replace features. More precisely, our models are defined in the
probabilistic feature-oriented language QFLAN, a rich domain specific language
(DSL) for systems with variability defined in terms of features. QFLAN
specifications are automatically encoded in terms of a process algebra whose
operational behavior interacts with a store of constraints, and hence allows to
separate system configuration from system behavior. The resulting probabilistic
configurations and behavior converge seamlessly in a semantics based on
discrete-time Markov chains, thus enabling quantitative analysis. Our analysis
is based on statistical model checking techniques, which allow us to scale to
larger models with respect to precise probabilistic analysis techniques. The
analyses we can conduct range from the likelihood of specific behavior to the
expected average cost, in terms of feature attributes, of specific system
variants. Our approach is supported by a novel Eclipse-based tool which
includes state-of-the-art DSL utilities for QFLAN based on the Xtext framework
as well as analysis plug-ins to seamlessly run statistical model checking
analyses. We provide a number of case studies that have driven and validated
the development of our framework.
",1,0,0,0,0,0
20640,20641,A semiparametric approach for bivariate extreme exceedances,"  Inference over tails is performed by applying only the results of extreme
value theory. Whilst such theory is well defined and flexible enough in the
univariate case, multivariate inferential methods often require the imposition
of arbitrary constraints not fully justifed by the underlying theory. In
contrast, our approach uses only the constraints imposed by theory. We build on
previous, theoretically justified work for marginal exceedances over a high,
unknown threshold, by combining it with flexible, semiparametric copulae
specifications to investigate extreme dependence. Whilst giving probabilistic
judgements about the extreme regime of all marginal variables, our approach
formally uses the full dataset and allows for a variety of patterns of
dependence, be them extremal or not. A new probabilistic criterion quantifying
the possibility that the data exhibits asymptotic independence is introduced
and its robustness empirically studied. Estimation of functions of interest in
extreme value analyses is performed via MCMC algorithms. Attention is also
devoted to the prediction of new extreme observations. Our approach is
evaluated through a series of simulations, applied to real data sets and
assessed against competing approaches. Evidence demonstrates that the bulk of
the data does not bias and improves the inferential process for the extremal
dependence.
",0,0,0,1,0,0
4238,4239,Towards Open Data for the Citation Content Analysis,"  The paper presents first results of the CitEcCyr project funded by RANEPA.
The project aims to create a source of open citation data for research papers
written in Russian. Compared to existing sources of citation data, CitEcCyr is
working to provide the following added values: a) a transparent and distributed
architecture of a technology that generates the citation data; b) an openness
of all built/used software and created citation data; c) an extended set of
citation data sufficient for the citation content analysis; d) services for
public control over a quality of the citation data and a citing activity of
researchers.
",1,0,0,0,0,0
9778,9779,Spectroscopic evidence of odd frequency superconducting order,"  Spin filter superconducting S/I/N tunnel junctions (NbN/GdN/TiN) show a
robust and pronounced zero bias conductance peak at low temperatures, the
magnitude of which is several times the normal state conductance of the
junction. Such a conductance anomaly is representative of unconventional
superconductivity and is interpreted as a direct signature of an odd frequency
superconducting order.
",0,1,0,0,0,0
20068,20069,Far-from-equilibrium energy flow and entanglement entropy,"  The time evolution of the energy transport triggered in a strongly coupled
system by a temperature gradient is holographically related to the evolution of
an asymptotically AdS black brane. We study the far-from-equilibrium properties
of such a system by using the AdS/CFT correspondence. In particular, we
describe the appearance of a steady state, and study the information flow by
computing the time evolution of the holographic entanglement entropy. Some
universal properties of the quenching process are presented.
",0,1,0,0,0,0
10426,10427,Thick-medium model of transverse pattern formation in optically excited cold two-level atoms with a feedback mirror,"  We study a pattern forming instability in a laser driven optically thick
cloud of cold two-level atoms with a planar feedback mirror. A theoretical
model is developed, enabling a full analysis of transverse patterns in a medium
with saturable nonlinearity, taking into account diffraction within the medium,
and both the transmission and reflection gratings. Focus of the analysis is on
combined treatment of nonlinear propagation in a diffractively- and
optically-thick medium and the boundary condition given by feedback. We
demonstrate explicitly how diffraction within the medium breaks the degeneracy
of Talbot modes inherent in thin slice models. Existence of envelope curves
bounding all possible pattern formation thresholds is predicted. The importance
of envelope curves and their interaction with threshold curves is illustrated
by experimental observation of a sudden transition between length scales as
mirror displacement is varied.
",0,1,0,0,0,0
5038,5039,Deep Learning with Permutation-invariant Operator for Multi-instance Histopathology Classification,"  The computer-aided analysis of medical scans is a longstanding goal in the
medical imaging field. Currently, deep learning has became a dominant
methodology for supporting pathologists and radiologist. Deep learning
algorithms have been successfully applied to digital pathology and radiology,
nevertheless, there are still practical issues that prevent these tools to be
widely used in practice. The main obstacles are low number of available cases
and large size of images (a.k.a. the small n, large p problem in machine
learning), and a very limited access to annotation at a pixel level that can
lead to severe overfitting and large computational requirements. We propose to
handle these issues by introducing a framework that processes a medical image
as a collection of small patches using a single, shared neural network. The
final diagnosis is provided by combining scores of individual patches using a
permutation-invariant operator (combination). In machine learning community
such approach is called a multi-instance learning (MIL).
",1,0,0,1,0,0
15470,15471,An Optimal Control Formulation of Pulse-Based Control Using Koopman Operator,"  In many applications, and in systems/synthetic biology, in particular, it is
desirable to compute control policies that force the trajectory of a bistable
system from one equilibrium (the initial point) to another equilibrium (the
target point), or in other words to solve the switching problem. It was
recently shown that, for monotone bistable systems, this problem admits
easy-to-implement open-loop solutions in terms of temporal pulses (i.e., step
functions of fixed length and fixed magnitude). In this paper, we develop this
idea further and formulate a problem of convergence to an equilibrium from an
arbitrary initial point. We show that this problem can be solved using a static
optimization problem in the case of monotone systems. Changing the initial
point to an arbitrary state allows to build closed-loop, event-based or
open-loop policies for the switching/convergence problems. In our derivations
we exploit the Koopman operator, which offers a linear infinite-dimensional
representation of an autonomous nonlinear system. One of the main advantages of
using the Koopman operator is the powerful computational tools developed for
this framework. Besides the presence of numerical solutions, the
switching/convergence problem can also serve as a building block for solving
more complicated control problems and can potentially be applied to
non-monotone systems. We illustrate this argument on the problem of
synchronizing cardiac cells by defibrillation. Potentially, our approach can be
extended to problems with different parametrizations of control signals since
the only fundamental limitation is the finite time application of the control
signal.
",1,0,1,0,0,0
13421,13422,"The asymptotic coarse-graining formulation of slender-rods, bio-filaments and flagella","  The inertialess fluid-structure interactions of active and passive
inextensible filaments and slender- rods are ubiquitous in nature, from the
dynamics of semi-flexible polymers and cytoskeletal filaments to cellular
mechanics and flagella. The coupling between the geometry of deformation and
the phys- ical interaction governing the dynamics of bio-filaments is complex.
Governing equations negotiate elastohydrodynamical interactions with
non-holonomic constraints arising from the filament inex- tensibility. Such
elastohydrodynamic systems are structurally convoluted, prone to numerical
erros, thus requiring penalization methods and high-order spatiotemporal
propagators. The asymptotic coarse-graining formulation presented here exploits
the momentum balance in the asymptotic limit of small rod-like elements which
are integrated semi-analytically. This greatly simplifies the elas-
tohydrodynamic interactions and overcomes previous numerical instability. The
resulting matricial system is straightforward and intuitive to implement, and
allows for a fast and efficient computation, over than a hundred times faster
than previous schemes. Only basic knowledge of systems of linear equations is
required, and implementation achieved with any solver of choice.
Generalisations for complex interaction of multiple rods, Brownian polymer
dynamics, active filaments and non-local hydrodynamics are also
straightforward. We demonstrate these in four examples commonly found in
biological systems, including the dynamics of filaments and flagella. Three of
these systems are novel in the literature. We additionally provide a Matlab
code that can be used as a basis for further generalisations.
",0,1,0,0,0,0
10525,10526,On the metastable Mabillard-Wagner conjecture,"  The purpose of this note is to attract attention to the following conjecture
(metastable $r$-fold Whitney trick) by clarifying its status as not having a
complete proof, in the sense described in the paper.
Assume that $D=D_1\sqcup\ldots\sqcup D_r$ is disjoint union of $r$ disks of
dimension $s$, $f:D\to B^d$ a proper PL map such that $f\partial
D_1\cap\ldots\cap f\partial D_r=\emptyset$, $rd\ge (r+1)s+3$ and $d\ge s+3$. If
the map $$f^r:\partial(D_1\times\ldots\times D_r)\to
(B^d)^r-\{(x,x,\ldots,x)\in(B^d)^r\ |\ x\in B^d\}$$ extends to
$D_1\times\ldots\times D_r$, then there is a PL map $\overline f:D\to B^d$ such
that $$\overline f=f \quad\text{on}\quad D_r\cup\partial D\quad\text{and}\quad
\overline fD_1\cap\ldots\cap \overline fD_r=\emptyset.$$
",1,0,1,0,0,0
6573,6574,Sparse covariance matrix estimation in high-dimensional deconvolution,"  We study the estimation of the covariance matrix $\Sigma$ of a
$p$-dimensional normal random vector based on $n$ independent observations
corrupted by additive noise. Only a general nonparametric assumption is imposed
on the distribution of the noise without any sparsity constraint on its
covariance matrix. In this high-dimensional semiparametric deconvolution
problem, we propose spectral thresholding estimators that are adaptive to the
sparsity of $\Sigma$. We establish an oracle inequality for these estimators
under model miss-specification and derive non-asymptotic minimax convergence
rates that are shown to be logarithmic in $n/\log p$. We also discuss the
estimation of low-rank matrices based on indirect observations as well as the
generalization to elliptical distributions. The finite sample performance of
the threshold estimators is illustrated in a numerical example.
",0,0,1,0,0,0
270,271,Multilevel maximum likelihood estimation with application to covariance matrices,"  The asymptotic variance of the maximum likelihood estimate is proved to
decrease when the maximization is restricted to a subspace that contains the
true parameter value. Maximum likelihood estimation allows a systematic fitting
of covariance models to the sample, which is important in data assimilation.
The hierarchical maximum likelihood approach is applied to the spectral
diagonal covariance model with different parameterizations of eigenvalue decay,
and to the sparse inverse covariance model with specified parameter values on
different sets of nonzero entries. It is shown computationally that using
smaller sets of parameters can decrease the sampling noise in high dimension
substantially.
",0,0,1,1,0,0
11214,11215,A Minimal Closed-Form Solution for Multi-Perspective Pose Estimation using Points and Lines,"  We propose a minimal solution for pose estimation using both points and lines
for a multi-perspective camera. In this paper, we treat the multi-perspective
camera as a collection of rigidly attached perspective cameras. These type of
imaging devices are useful for several computer vision applications that
require a large coverage such as surveillance, self-driving cars, and
motion-capture studios. While prior methods have considered the cases using
solely points or lines, the hybrid case involving both points and lines has not
been solved for multi-perspective cameras. We present the solutions for two
cases. In the first case, we are given 2D to 3D correspondences for two points
and one line. In the later case, we are given 2D to 3D correspondences for one
point and two lines. We show that the solution for the case of two points and
one line can be formulated as a fourth degree equation. This is interesting
because we can get a closed-form solution and thereby achieve high
computational efficiency. The later case involving two lines and one point can
be mapped to an eighth degree equation. We show simulations and real
experiments to demonstrate the advantages and benefits over existing methods.
",1,0,0,0,0,0
20657,20658,Defending Against Adversarial Attacks by Leveraging an Entire GAN,"  Recent work has shown that state-of-the-art models are highly vulnerable to
adversarial perturbations of the input. We propose cowboy, an approach to
detecting and defending against adversarial attacks by using both the
discriminator and generator of a GAN trained on the same dataset. We show that
the discriminator consistently scores the adversarial samples lower than the
real samples across multiple attacks and datasets. We provide empirical
evidence that adversarial samples lie outside of the data manifold learned by
the GAN. Based on this, we propose a cleaning method which uses both the
discriminator and generator of the GAN to project the samples back onto the
data manifold. This cleaning procedure is independent of the classifier and
type of attack and thus can be deployed in existing systems.
",0,0,0,1,0,0
12682,12683,Nonlinear Sequential Accepts and Rejects for Identification of Top Arms in Stochastic Bandits,"  We address the M-best-arm identification problem in multi-armed bandits. A
player has a limited budget to explore K arms (M<K), and once pulled, each arm
yields a reward drawn (independently) from a fixed, unknown distribution. The
goal is to find the top M arms in the sense of expected reward. We develop an
algorithm which proceeds in rounds to deactivate arms iteratively. At each
round, the budget is divided by a nonlinear function of remaining arms, and the
arms are pulled correspondingly. Based on a decision rule, the deactivated arm
at each round may be accepted or rejected. The algorithm outputs the accepted
arms that should ideally be the top M arms. We characterize the decay rate of
the misidentification probability and establish that the nonlinear budget
allocation proves to be useful for different problem environments (described by
the number of competitive arms). We provide comprehensive numerical experiments
showing that our algorithm outperforms the state-of-the-art using suitable
nonlinearity.
",1,0,0,1,0,0
1339,1340,Protein Folding and Machine Learning: Fundamentals,"  In spite of decades of research, much remains to be discovered about folding:
the detailed structure of the initial (unfolded) state, vestigial folding
instructions remaining only in the unfolded state, the interaction of the
molecule with the solvent, instantaneous power at each point within the
molecule during folding, the fact that the process is stable in spite of myriad
possible disturbances, potential stabilization of trajectory by chaos, and, of
course, the exact physical mechanism (code or instructions) by which the
folding process is specified in the amino acid sequence. Simulations based upon
microscopic physics have had some spectacular successes and continue to
improve, particularly as super-computer capabilities increase. The simulations,
exciting as they are, are still too slow and expensive to deal with the
enormous number of molecules of interest. In this paper, we introduce an
approximate model based upon physics, empirics, and information science which
is proposed for use in machine learning applications in which very large
numbers of sub-simulations must be made. In particular, we focus upon machine
learning applications in the learning phase and argue that our model is
sufficiently close to the physics that, in spite of its approximate nature, can
facilitate stepping through machine learning solutions to explore the mechanics
of folding mentioned above. We particularly emphasize the exploration of energy
flow (power) within the molecule during folding, the possibility of energy
scale invariance (above a threshold), vestigial information in the unfolded
state as attractive targets for such machine language analysis, and statistical
analysis of an ensemble of folding micro-steps.
",0,0,0,0,1,0
4386,4387,Ultra-broadband On-chip Twisted Light Emitter,"  On-chip twisted light emitters are essential components for orbital angular
momentum (OAM) communication devices, which could address the growing demand
for high-capacity communication systems by providing an additional degree of
freedom for wavelength/frequency division multiplexing (WDM/FDM). Although
whispering gallery mode enabled OAM emitters have been shown to possess some
advantages, such as being compact and phase accurate, their inherent narrow
bandwidth prevents them from being compatible with WDM/FDM techniques. Here, we
demonstrate an ultra-broadband multiplexed OAM emitter that utilizes a novel
joint path-resonance phase control concept. The emitter has a micron sized
radius and nanometer sized features. Coaxial OAM beams are emitted across the
entire telecommunication band from 1450 to 1650 nm. We applied the emitter for
OAM communication with a data rate of 1.2 Tbit/s assisted by 30-channel optical
frequency combs (OFC). The emitter provides a new solution to further increase
of the capacity in the OFC communication scenario.
",0,1,0,0,0,0
20252,20253,Inequalities related to Symmetrized Harmonic Convex Functions,"  In this paper, we extend the Hermite-Hadamard type $\dot{I}$scan inequality
to the class of symmetrized harmonic convex functions. The corresponding
version for harmonic h-convex functions is also investigated. Furthermore, we
establish Hermite-Hadamard type inequalites for the product of a harmonic
convex function with a symmetrized harmonic convex function.
",0,0,1,0,0,0
10552,10553,An Optimal Combination of Proportional and Stop-Loss Reinsurance Contracts From Insurer's and Reinsurer's Viewpoints,"  A reinsurance contract should address the conflicting interests of the
insurer and reinsurer. Most of existing optimal reinsurance contracts only
considers the interests of one party. This article combines the proportional
and stop-loss reinsurance contracts and introduces a new reinsurance contract
called proportional-stop-loss reinsurance. Using the balanced loss function,
unknown parameters of the proportional-stop-loss reinsurance have been
estimated such that the expected surplus for both the insurer and reinsurer are
maximized. Several characteristics for the new reinsurance are provided.
",0,0,0,1,0,0
2622,2623,Multirole Logic (Extended Abstract),"  We identify multirole logic as a new form of logic in which
conjunction/disjunction is interpreted as an ultrafilter on the power set of
some underlying set (of roles) and the notion of negation is generalized to
endomorphisms on this underlying set. We formalize both multirole logic (MRL)
and linear multirole logic (LMRL) as natural generalizations of classical logic
(CL) and classical linear logic (CLL), respectively, and also present a
filter-based interpretation for intuitionism in multirole logic. Among various
meta-properties established for MRL and LMRL, we obtain one named multiparty
cut-elimination stating that every cut involving one or more sequents (as a
generalization of a (binary) cut involving exactly two sequents) can be
eliminated, thus extending the celebrated result of cut-elimination by Gentzen.
",1,0,1,0,0,0
9633,9634,Steklov problem on differential forms,"  In this paper we study spectral properties of Dirichlet-to-Neumann map on
differential forms obtained by a slight modification of the definition due to
Belishev and Sharafutdinov. The resulting operator $\Lambda$ is shown to be
self-adjoint on the subspace of coclosed forms and to have purely discrete
spectrum there.We investigate properies of eigenvalues of $\Lambda$ and prove a
Hersch-Payne-Schiffer type inequality relating products of those eigenvalues to
eigenvalues of Hodge Laplacian on the boundary. Moreover, non-trivial
eigenvalues of $\Lambda$ are always at least as large as eigenvalues of
Dirichlet-to-Neumann map defined by Raulot and Savo. Finally, we remark that a
particular case of $p$-forms on the boundary of $2p+2$-dimensional manifold
shares a lot of important properties with the classical Steklov eigenvalue
problem on surfaces.
",0,0,1,0,0,0
729,730,Algebraic models of the Euclidean plane,"  We introduce a new invariant, the real (logarithmic)-Kodaira dimension, that
allows to distinguish smooth real algebraic surfaces up to birational
diffeomorphism. As an application, we construct infinite families of smooth
rational real algebraic surfaces with trivial homology groups, whose real loci
are diffeomorphic to $\mathbb{R}^2$, but which are pairwise not birationally
diffeomorphic. There are thus infinitely many non-trivial models of the
euclidean plane, contrary to the compact case.
",0,0,1,0,0,0
13228,13229,Spatially Adaptive Colocalization Analysis in Dual-Color Fluorescence Microscopy,"  Colocalization analysis aims to study complex spatial associations between
bio-molecules via optical imaging techniques. However, existing colocalization
analysis workflows only assess an average degree of colocalization within a
certain region of interest and ignore the unique and valuable spatial
information offered by microscopy. In the current work, we introduce a new
framework for colocalization analysis that allows us to quantify colocalization
levels at each individual location and automatically identify pixels or regions
where colocalization occurs. The framework, referred to as spatially adaptive
colocalization analysis (SACA), integrates a pixel-wise local kernel model for
colocalization quantification and a multi-scale adaptive propagation-separation
strategy for utilizing spatial information to detect colocalization in a
spatially adaptive fashion. Applications to simulated and real biological
datasets demonstrate the practical merits of SACA in what we hope to be an
easily applicable and robust colocalization analysis method. In addition,
theoretical properties of SACA are investigated to provide rigorous statistical
justification.
",0,0,0,1,0,0
18794,18795,Autonomous Electric Race Car Design,"  Autonomous driving and electric vehicles are nowadays very active research
and development areas. In this paper we present the conversion of a standard
Kyburz eRod into an autonomous vehicle that can be operated in challenging
environments such as Swiss mountain passes. The overall hardware and software
architectures are described in detail with a special emphasis on the sensor
requirements for autonomous vehicles operating in partially structured
environments. Furthermore, the design process itself and the finalized system
architecture are presented. The work shows state of the art results in
localization and controls for self-driving high-performance electric vehicles.
Test results of the overall system are presented, which show the importance of
generalizable state estimation algorithms to handle a plethora of conditions.
",1,0,0,0,0,0
17089,17090,Converging expansions for Lipschitz self-similar perforations of a plane sector,"  In contrast with the well-known methods of matching asymptotics and
multiscale (or compound) asymptotics, the "" functional analytic approach "" of
Lanza de Cristoforis (Analysis 28, 2008) allows to prove convergence of
expansions around interior small holes of size $\epsilon$ for solutions of
elliptic boundary value problems. Using the method of layer potentials, the
asymptotic behavior of the solution as $\epsilon$ tends to zero is described
not only by asymptotic series in powers of $\epsilon$, but by convergent power
series. Here we use this method to investigate the Dirichlet problem for the
Laplace operator where holes are collapsing at a polygonal corner of opening
$\omega$. Then in addition to the scale $\epsilon$ there appears the scale
$\eta = \epsilon^{\pi/\omega}$. We prove that when $\pi/\omega$ is irrational,
the solution of the Dirichlet problem is given by convergent series in powers
of these two small parameters. Due to interference of the two scales, this
convergence is obtained, in full generality, by grouping together integer
powers of the two scales that are very close to each other. Nevertheless, there
exists a dense subset of openings $\omega$ (characterized by Diophantine
approximation properties), for which real analyticity in the two variables
$\epsilon$ and $\eta$ holds and the power series converge unconditionally. When
$\pi/\omega$ is rational, the series are unconditionally convergent, but
contain terms in log $\epsilon$.
",0,0,1,0,0,0
19315,19316,Deforming 3-manifolds of bounded geometry and uniformly positive scalar curvature,"  We prove that the moduli space of complete Riemannian metrics of bounded
geometry and uniformly positive scalar curvature on an orientable 3-manifold is
path-connected. This generalizes the main result of the fourth author [Mar12]
in the compact case. The proof uses Ricci flow with surgery as well as
arguments involving performing infinite connected sums with control on the
geometry.
",0,0,1,0,0,0
11920,11921,"Small-scale Effects of Thermal Inflation on Halo Abundance at High-$z$, Galaxy Substructure Abundance and 21-cm Power Spectrum","  We study the impact of thermal inflation on the formation of cosmological
structures and present astrophysical observables which can be used to constrain
and possibly probe the thermal inflation scenario. These are dark matter halo
abundance at high redshifts, satellite galaxy abundance in the Milky Way, and
fluctuation in the 21-cm radiation background before the epoch of reionization.
The thermal inflation scenario leaves a characteristic signature on the matter
power spectrum by boosting the amplitude at a specific wavenumber determined by
the number of e-foldings during thermal inflation ($N_{\rm bc}$), and strongly
suppressing the amplitude for modes at smaller scales. For a reasonable range
of parameter space, one of the consequences is the suppression of minihalo
formation at high redshifts and that of satellite galaxies in the Milky Way.
While this effect is substantial, it is degenerate with other cosmological or
astrophysical effects. The power spectrum of the 21-cm background probes this
impact more directly, and its observation may be the best way to constrain the
thermal inflation scenario due to the characteristic signature in the power
spectrum. The Square Kilometre Array (SKA) in phase 1 (SKA1) has sensitivity
large enough to achieve this goal for models with $N_{\rm bc}\gtrsim 26$ if a
10000-hr observation is performed. The final phase SKA, with anticipated
sensitivity about an order of magnitude higher, seems more promising and will
cover a wider parameter space.
",0,1,0,0,0,0
17460,17461,Localization Algorithm with Circular Representation in 2D and its Similarity to Mammalian Brains,"  Extended Kalman filter (EKF) does not guarantee consistent mean and
covariance under linearization, even though it is the main framework for
robotic localization. While Lie group improves the modeling of the state space
in localization, the EKF on Lie group still relies on the arbitrary Gaussian
assumption in face of nonlinear models. We instead use von Mises filter for
orientation estimation together with the conventional Kalman filter for
position estimation, and thus we are able to characterize the first two moments
of the state estimates. Since the proposed algorithm holds a solid
probabilistic basis, it is fundamentally relieved from the inconsistency
problem. Furthermore, we extend the localization algorithm to fully circular
representation even for position, which is similar to grid patterns found in
mammalian brains and in recurrent neural networks. The applicability of the
proposed algorithms is substantiated not only by strong mathematical foundation
but also by the comparison against other common localization methods.
",1,0,0,0,1,0
20169,20170,Thicket Density,"  Thicket density is a new measure of the complexity of a set system, having
the same relationship to stable formulas that VC density has to NIP formulas.
It satisfies a Sauer-Shelah type dichotomy that has applications in both model
theory and the theory of algorithms
",0,0,1,0,0,0
4305,4306,Understanding news story chains using information retrieval and network clustering techniques,"  Content analysis of news stories (whether manual or automatic) is a
cornerstone of the communication studies field. However, much research is
conducted at the level of individual news articles, despite the fact that news
events (especially significant ones) are frequently presented as ""stories"" by
news outlets: chains of connected articles covering the same event from
different angles. These stories are theoretically highly important in terms of
increasing public recall of news items and enhancing the agenda-setting power
of the press. Yet thus far, the field has lacked an efficient method for
detecting groups of articles which form stories in a way that enables their
analysis.
In this work, we present a novel, automated method for identifying linked
news stories from within a corpus of articles. This method makes use of
techniques drawn from the field of information retrieval to identify textual
closeness of pairs of articles, and then clustering techniques taken from the
field of network analysis to group these articles into stories. We demonstrate
the application of the method to a corpus of 61,864 articles, and show how it
can efficiently identify valid story clusters within the corpus. We use the
results to make observations about the prevalence and dynamics of stories
within the UK news media, showing that more than 50% of news production takes
place within stories.
",1,0,0,0,0,0
4314,4315,A Divergence Bound for Hybrids of MCMC and Variational Inference and an Application to Langevin Dynamics and SGVI,"  Two popular classes of methods for approximate inference are Markov chain
Monte Carlo (MCMC) and variational inference. MCMC tends to be accurate if run
for a long enough time, while variational inference tends to give better
approximations at shorter time horizons. However, the amount of time needed for
MCMC to exceed the performance of variational methods can be quite high,
motivating more fine-grained tradeoffs. This paper derives a distribution over
variational parameters, designed to minimize a bound on the divergence between
the resulting marginal distribution and the target, and gives an example of how
to sample from this distribution in a way that interpolates between the
behavior of existing methods based on Langevin dynamics and stochastic gradient
variational inference (SGVI).
",1,0,0,1,0,0
9135,9136,Light in Power: A General and Parameter-free Algorithm for Caustic Design,"  We present in this paper a generic and parameter-free algorithm to
efficiently build a wide variety of optical components, such as mirrors or
lenses, that satisfy some light energy constraints. In all of our problems, one
is given a collimated or point light source and a desired illumination after
reflection or refraction and the goal is to design the geometry of a mirror or
lens which transports exactly the light emitted by the source onto the target.
We first propose a general framework and show that eight different optical
component design problems amount to solving a light energy conservation
equation that involves the computation of visibility diagrams. We then show
that these diagrams all have the same structure and can be obtained by
intersecting a 3D Power diagram with a planar or spherical domain. This allows
us to propose an efficient and fully generic algorithm capable to solve these
eight optical component design problems. The support of the prescribed target
illumination can be a set of directions or a set of points located at a finite
distance. Our solutions satisfy design constraints such as convexity or
concavity. We show the effectiveness of our algorithm on simulated and
fabricated examples.
",1,0,0,0,0,0
9378,9379,A simple and efficient feedback control strategy for wastewater denitrification,"  Due to severe mathematical modeling and calibration difficulties open-loop
feedforward control is mainly employed today for wastewater denitrification,
which is a key ecological issue. In order to improve the resulting poor
performances a new model-free control setting and its corresponding
""intelligent"" controller are introduced. The pitfall of regulating two output
variables via a single input variable is overcome by introducing also an
open-loop knowledge-based control deduced from the plant behavior. Several
convincing computer simulations are presented and discussed.
",1,0,1,0,0,0
10166,10167,Signal propagation in sensing and reciprocating cellular systems with spatial and structural heterogeneity,"  Sensing and reciprocating cellular systems (SARs) are important for the
operation of many biological systems. Production in interferon (IFN) SARs is
achieved through activation of the Jak-Stat pathway, and downstream
upregulation of IFN regulatory factor (IRF)-3 and IFN transcription, but the
role that high and low affinity IFNs play in this process remains unclear. We
present a comparative between a minimal spatio-temporal partial differential
equation (PDE) model and a novel spatio-structural-temporal (SST) model for the
consideration of receptor, binding, and metabolic aspects of SAR behaviour.
Using the SST framework, we simulate single- and multi-cluster paradigms of IFN
communication. Simulations reveal a cyclic process between the binding of IFN
to the receptor, and the consequent increase in metabolism, decreasing the
propensity for binding due to the internal feed-back mechanism. One observes
the effect of heterogeneity between cellular clusters, allowing them to
individualise and increase local production, and within clusters, where we
observe `sub popular quiescence'; a process whereby intra-cluster
subpopulations reduce their binding and metabolism such that other such
subpopulations may augment their production. Finally, we observe the ability
for low affinity IFN to communicate a long range signal, where high affinity
cannot, and the breakdown of this relationship through the introduction of cell
motility. Biological systems may utilise cell motility where environments are
unrestrictive and may use fixed system, with low affinity communication, where
a localised response is desirable.
",0,0,0,0,1,0
1346,1347,Automated Assistants to Identify and Prompt Action on Visual News Bias,"  Bias is a common problem in today's media, appearing frequently in text and
in visual imagery. Users on social media websites such as Twitter need better
methods for identifying bias. Additionally, activists --those who are motivated
to effect change related to some topic, need better methods to identify and
counteract bias that is contrary to their mission. With both of these use cases
in mind, in this paper we propose a novel tool called UnbiasedCrowd that
supports identification of, and action on bias in visual news media. In
particular, it addresses the following key challenges (1) identification of
bias; (2) aggregation and presentation of evidence to users; (3) enabling
activists to inform the public of bias and take action by engaging people in
conversation with bots. We describe a preliminary study on the Twitter platform
that explores the impressions that activists had of our tool, and how people
reacted and engaged with online bots that exposed visual bias. We conclude by
discussing design and implication of our findings for creating future systems
to identify and counteract the effects of news bias.
",1,0,0,0,0,0
3436,3437,Interpretable Low-Dimensional Regression via Data-Adaptive Smoothing,"  We consider the problem of estimating a regression function in the common
situation where the number of features is small, where interpretability of the
model is a high priority, and where simple linear or additive models fail to
provide adequate performance. To address this problem, we present Maximum
Variance Total Variation denoising (MVTV), an approach that is conceptually
related both to CART and to the more recent CRISP algorithm, a state-of-the-art
alternative method for interpretable nonlinear regression. MVTV divides the
feature space into blocks of constant value and fits the value of all blocks
jointly via a convex optimization routine. Our method is fully data-adaptive,
in that it incorporates highly robust routines for tuning all hyperparameters
automatically. We compare our approach against CART and CRISP via both a
complexity-accuracy tradeoff metric and a human study, demonstrating that that
MVTV is a more powerful and interpretable method.
",0,0,0,1,0,0
8551,8552,Electrical control of metallic heavy-metal/ferromagnet interfacial states,"  Voltage control effects provide an energy-efficient means of tailoring
material properties, especially in highly integrated nanoscale devices.
However, only insulating and semiconducting systems can be controlled so far.
In metallic systems, there is no electric field due to electron screening
effects and thus no such control effect exists. Here we demonstrate that
metallic systems can also be controlled electrically through ionic not
electronic effects. In a Pt/Co structure, the control of the metallic Pt/Co
interface can lead to unprecedented control effects on the magnetic properties
of the entire structure. Consequently, the magnetization and perpendicular
magnetic anisotropy of the Co layer can be independently manipulated to any
desired state, the efficient spin toques can be enhanced about 3.5 times, and
the switching current can be reduced about one order of magnitude. This ability
to control a metallic system may be extended to control other physical
phenomena.
",0,1,0,0,0,0
14118,14119,Distributive Minimization Comprehensions and the Polynomial Hierarchy,"  A categorical point of view about minimization in subrecursive classes is
presented by extending the concept of Symmetric Monoidal Comprehension to that
of Distributive Minimization Comprehension. This is achieved by endowing the
former with coproducts and a finality condition for coalgebras over the
endofunctor sending X to ${1}\oplus{X}$ to perform a safe minimization
operator. By relying on the characterization given by Bellantoni, a tiered
structure is presented from which one can obtain the levels of the Polytime
Hierarchy as those classes of partial functions obtained after a certain number
of minimizations.
",1,0,1,0,0,0
1084,1085,"Fast, Better Training Trick -- Random Gradient","  In this paper, we will show an unprecedented method to accelerate training
and improve performance, which called random gradient (RG). This method can be
easier to the training of any model without extra calculation cost, we use
Image classification, Semantic segmentation, and GANs to confirm this method
can improve speed which is training model in computer vision. The central idea
is using the loss multiplied by a random number to random reduce the
back-propagation gradient. We can use this method to produce a better result in
Pascal VOC, Cifar, Cityscapes datasets.
",0,0,0,1,0,0
382,383,Eigenvalues of symmetric tridiagonal interval matrices revisited,"  In this short note, we present a novel method for computing exact lower and
upper bounds of eigenvalues of a symmetric tridiagonal interval matrix.
Compared to the known methods, our approach is fast, simple to present and to
implement, and avoids any assumptions. Our construction explicitly yields those
matrices for which particular lower and upper bounds are attained.
",1,0,0,0,0,0
6817,6818,New Abilities and Limitations of Spectral Graph Bisection,"  Spectral based heuristics belong to well-known commonly used methods which
determines provably minimal graph bisection or outputs ""fail"" when the
optimality cannot be certified. In this paper we focus on Boppana's algorithm
which belongs to one of the most prominent methods of this type. It is well
known that the algorithm works well in the random \emph{planted bisection
model} -- the standard class of graphs for analysis minimum bisection and
relevant problems. In 2001 Feige and Kilian posed the question if Boppana's
algorithm works well in the semirandom model by Blum and Spencer. In our paper
we answer this question affirmatively. We show also that the algorithm achieves
similar performance on graph classes which extend the semirandom model.
Since the behavior of Boppana's algorithm on the semirandom graphs remained
unknown, Feige and Kilian proposed a new semidefinite programming (SDP) based
approach and proved that it works on this model. The relationship between the
performance of the SDP based algorithm and Boppana's approach was left as an
open problem. In this paper we solve the problem in a complete way by proving
that the bisection algorithm of Feige and Kilian provides exactly the same
results as Boppana's algorithm. As a consequence we get that Boppana's
algorithm achieves the optimal threshold for exact cluster recovery in the
\emph{stochastic block model}. On the other hand we prove some limitations of
Boppana's approach: we show that if the density difference on the parameters of
the planted bisection model is too small then the algorithm fails with high
probability in the model.
",1,0,0,0,0,0
12723,12724,Integrated analysis of energy transfers in elastic-wave turbulence,"  In elastic-wave turbulence, strong turbulence appears in small wave numbers
while weak turbulence does in large wave numbers. Energy transfers in the
coexistence of these turbulent states are numerically investigated in both of
the Fourier space and the real space. An analytical expression of a detailed
energy balance reveals from which mode to which mode energy is transferred in
the triad interaction. Stretching energy excited by external force is
transferred nonlocally and intermittently to large wave numbers as the kinetic
energy in the strong turbulence. In the weak turbulence, the resonant
interactions according to the weak turbulence theory produces cascading net
energy transfer to large wave numbers. Because the system's nonlinearity shows
strong temporal intermittency, the energy transfers are investigated at active
and moderate phases separately. The nonlocal interactions in the Fourier space
are characterized by the intermittent bundles of fibrous structures in the real
space.
",0,1,0,0,0,0
5270,5271,Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations,"  Neural networks are among the most accurate supervised learning methods in
use today, but their opacity makes them difficult to trust in critical
applications, especially when conditions in training differ from those in test.
Recent work on explanations for black-box models has produced tools (e.g. LIME)
to show the implicit rules behind predictions, which can help us identify when
models are right for the wrong reasons. However, these methods do not scale to
explaining entire datasets and cannot correct the problems they reveal. We
introduce a method for efficiently explaining and regularizing differentiable
models by examining and selectively penalizing their input gradients, which
provide a normal to the decision boundary. We apply these penalties both based
on expert annotation and in an unsupervised fashion that encourages diverse
models with qualitatively different decision boundaries for the same
classification problem. On multiple datasets, we show our approach generates
faithful explanations and models that generalize much better when conditions
differ between training and test.
",1,0,0,1,0,0
10388,10389,Nonclassical Light Generation from III-V and Group-IV Solid-State Cavity Quantum Systems,"  In this chapter, we present the state-of-the-art in the generation of
nonclassical states of light using semiconductor cavity quantum electrodynamics
(QED) platforms. Our focus is on the photon blockade effects that enable the
generation of indistinguishable photon streams with high purity and efficiency.
Starting with the leading platform of InGaAs quantum dots in optical
nanocavities, we review the physics of a single quantum emitter strongly
coupled to a cavity. Furthermore, we propose a complete model for photon
blockade and tunneling in III-V quantum dot cavity QED systems. Turning toward
quantum emitters with small inhomogeneous broadening, we propose a direction
for novel experiments for nonclassical light generation based on group-IV
color-center systems. We present a model of a multi-emitter cavity QED
platform, which features richer dressed-states ladder structures, and show how
it can offer opportunities for studying new regimes of high-quality photon
blockade.
",0,1,0,0,0,0
2895,2896,Interpreting and Explaining Deep Neural Networks for Classification of Audio Signals,"  Interpretability of deep neural networks is a recently emerging area of
machine learning research targeting a better understanding of how models
perform feature selection and derive their classification decisions. In this
paper, two neural network architectures are trained on spectrogram and raw
waveform data for audio classification tasks on a newly created audio dataset
and layer-wise relevance propagation (LRP), a previously proposed
interpretability method, is applied to investigate the models' feature
selection and decision making. It is demonstrated that the networks are highly
reliant on feature marked as relevant by LRP through systematic manipulation of
the input data. Our results show that by making deep audio classifiers
interpretable, one can analyze and compare the properties and strategies of
different models beyond classification accuracy, which potentially opens up new
ways for model improvements.
",1,0,0,0,0,0
12720,12721,Deformation theory of the blown-up Seiberg-Witten equation in dimension three,"  Associated with every quaternionic representation of a compact, connected Lie
group there is a Seiberg-Witten equation in dimension three. The moduli spaces
of solutions to these equations are typically non-compact. We construct
Kuranishi models around boundary points of a partially compactified moduli
space. The Haydys correspondence identifies such boundary points with Fueter
sections - solutions of a non-linear Dirac equation - of the bundle of
hyperkähler quotients associated with the quaternionic representation. We
discuss when such a Fueter section can be deformed to a solution of the
Seiberg-Witten equation.
",0,0,1,0,0,0
12608,12609,On inverse and right inverse ordered semigroups,"  A regular ordered semigroup $S$ is called right inverse if every principal
left ideal of $S$ is generated by an $\mathcal{R}$-unique ordered idempotent.
Here we explore the theory of right inverse ordered semigroups. We show that a
regular ordered semigroup is right inverse if and only if any two right
inverses of an element $a\in S$ are $\mathcal{R}$-related. Furthermore,
different characterizations of right Clifford, right group-like, group like
ordered semigroups are done by right inverse ordered semigroups. Thus a
foundation of right inverse semigroups has been developed.
",0,0,1,0,0,0
19555,19556,"The paradigm-shift of social spambots: Evidence, theories, and tools for the arms race","  Recent studies in social media spam and automation provide anecdotal
argumentation of the rise of a new generation of spambots, so-called social
spambots. Here, for the first time, we extensively study this novel phenomenon
on Twitter and we provide quantitative evidence that a paradigm-shift exists in
spambot design. First, we measure current Twitter's capabilities of detecting
the new social spambots. Later, we assess the human performance in
discriminating between genuine accounts, social spambots, and traditional
spambots. Then, we benchmark several state-of-the-art techniques proposed by
the academic literature. Results show that neither Twitter, nor humans, nor
cutting-edge applications are currently capable of accurately detecting the new
social spambots. Our results call for new approaches capable of turning the
tide in the fight against this raising phenomenon. We conclude by reviewing the
latest literature on spambots detection and we highlight an emerging common
research trend based on the analysis of collective behaviors. Insights derived
from both our extensive experimental campaign and survey shed light on the most
promising directions of research and lay the foundations for the arms race
against the novel social spambots. Finally, to foster research on this novel
phenomenon, we make publicly available to the scientific community all the
datasets used in this study.
",1,0,0,0,0,0
9613,9614,Gaussian Process Regression for Arctic Coastal Erosion Forecasting,"  Arctic coastal morphology is governed by multiple factors, many of which are
affected by climatological changes. As the season length for shorefast ice
decreases and temperatures warm permafrost soils, coastlines are more
susceptible to erosion from storm waves. Such coastal erosion is a concern,
since the majority of the population centers and infrastructure in the Arctic
are located near the coasts. Stakeholders and decision makers increasingly need
models capable of scenario-based predictions to assess and mitigate the effects
of coastal morphology on infrastructure and land use. Our research uses
Gaussian process models to forecast Arctic coastal erosion along the Beaufort
Sea near Drew Point, AK. Gaussian process regression is a data-driven modeling
methodology capable of extracting patterns and trends from data-sparse
environments such as remote Arctic coastlines. To train our model, we use
annual coastline positions and near-shore summer temperature averages from
existing datasets and extend these data by extracting additional coastlines
from satellite imagery. We combine our calibrated models with future climate
models to generate a range of plausible future erosion scenarios. Our results
show that the Gaussian process methodology substantially improves yearly
predictions compared to linear and nonlinear least squares methods, and is
capable of generating detailed forecasts suitable for use by decision makers.
",0,1,0,1,0,0
13915,13916,Prediction and Generation of Binary Markov Processes: Can a Finite-State Fox Catch a Markov Mouse?,"  Understanding the generative mechanism of a natural system is a vital
component of the scientific method. Here, we investigate one of the fundamental
steps toward this goal by presenting the minimal generator of an arbitrary
binary Markov process. This is a class of processes whose predictive model is
well known. Surprisingly, the generative model requires three distinct
topologies for different regions of parameter space. We show that a previously
proposed generator for a particular set of binary Markov processes is, in fact,
not minimal. Our results shed the first quantitative light on the relative
(minimal) costs of prediction and generation. We find, for instance, that the
difference between prediction and generation is maximized when the process is
approximately independently, identically distributed.
",1,1,0,0,0,0
10124,10125,A Holistic Approach to Forecasting Wholesale Energy Market Prices,"  Electricity market price predictions enable energy market participants to
shape their consumption or supply while meeting their economic and
environmental objectives. By utilizing the basic properties of the
supply-demand matching process performed by grid operators, we develop a method
to recover energy market's structure and predict the resulting nodal prices as
a function of generation mix and system load on the grid. Our methodology uses
the latest advancements in compressed sensing and statistics to cope with the
high-dimensional and sparse power grid topologies, underlying physical laws, as
well as scarce, public market data. Rigorous validations using Southwest Power
Pool (SPP) market data demonstrate significantly higher accuracy of the
proposed approach when compared to the state-of-the-art industry benchmark.
",1,0,0,1,0,0
17910,17911,The Statistical Recurrent Unit,"  Sophisticated gated recurrent neural network architectures like LSTMs and
GRUs have been shown to be highly effective in a myriad of applications. We
develop an un-gated unit, the statistical recurrent unit (SRU), that is able to
learn long term dependencies in data by only keeping moving averages of
statistics. The SRU's architecture is simple, un-gated, and contains a
comparable number of parameters to LSTMs; yet, SRUs perform favorably to more
sophisticated LSTM and GRU alternatives, often outperforming one or both in
various tasks. We show the efficacy of SRUs as compared to LSTMs and GRUs in an
unbiased manner by optimizing respective architectures' hyperparameters in a
Bayesian optimization scheme for both synthetic and real-world tasks.
",1,0,0,1,0,0
4595,4596,Nano-optical imaging of monolayer MoSe2 using tip-enhanced photoluminescence,"  Band gap tuning in two-dimensional transitional metal dichalcogenides (TMDs)
is crucial in fabricating new optoelectronic devices. High resolution
photoluminescence (PL) microscopy is needed for accurate band gap
characterization. We performed tip-enhanced photoluminescence (TEPL)
measurements of monolayer MoSe2 with nanoscale spatial resolution, providing an
improved characterization of the band gap correlated with the topography
compared with the conventional far field spectroscopy. We also observed PL
shifts at the edges and investigated the spatial dependence of the TEPL
enhancement factors.
",0,1,0,0,0,0
2868,2869,The Hamiltonian Dynamics of Magnetic Confinement in Toroidal Domains,"  We consider a class of magnetic fields defined over the interior of a
manifold $M$ which go to infinity at its boundary and whose direction near the
boundary of $M$ is controlled by a closed 1-form $\sigma_\infty \in
\Gamma(T^*\partial M)$. We are able to show that charged particles in the
interior of $M$ under the influence of such fields can only escape the manifold
through the zero locus of $\sigma_\infty$. In particular in the case where the
1-form is nowhere vanishing we conclude that the particles become confined to
its interior for all time.
",0,0,1,0,0,0
8178,8179,Deep Reinforcement Learning for Swarm Systems,"  Recently, deep reinforcement learning (RL) methods have been applied
successfully to multi-agent scenarios. Typically, these methods rely on a
concatenation of agent states to represent the information content required for
decentralized decision making. However, concatenation scales poorly to swarm
systems with a large number of homogeneous agents as it does not exploit the
fundamental properties inherent to these systems: (i) the agents in the swarm
are interchangeable and (ii) the exact number of agents in the swarm is
irrelevant. Therefore, we propose a new state representation for deep
multi-agent RL based on mean embeddings of distributions. We treat the agents
as samples of a distribution and use the empirical mean embedding as input for
a decentralized policy. We define different feature spaces of the mean
embedding using histograms, radial basis functions and a neural network learned
end-to-end. We evaluate the representation on two well known problems from the
swarm literature (rendezvous and pursuit evasion), in a globally and locally
observable setup. For the local setup we furthermore introduce simple
communication protocols. Of all approaches, the mean embedding representation
using neural network features enables the richest information exchange between
neighboring agents facilitating the development of more complex collective
strategies.
",1,0,0,1,0,0
19075,19076,Prime geodesic theorem of Gallagher type,"  We reduce the exponent in the error term of the prime geodesic theorem for
compact Riemann surfaces from $\frac{3}{4}$ to $\frac{7}{10}$ outside a set of
finite logarithmic measure.
",0,0,1,0,0,0
409,410,J-MOD$^{2}$: Joint Monocular Obstacle Detection and Depth Estimation,"  In this work, we propose an end-to-end deep architecture that jointly learns
to detect obstacles and estimate their depth for MAV flight applications. Most
of the existing approaches either rely on Visual SLAM systems or on depth
estimation models to build 3D maps and detect obstacles. However, for the task
of avoiding obstacles this level of complexity is not required. Recent works
have proposed multi task architectures to both perform scene understanding and
depth estimation. We follow their track and propose a specific architecture to
jointly estimate depth and obstacles, without the need to compute a global map,
but maintaining compatibility with a global SLAM system if needed. The network
architecture is devised to exploit the joint information of the obstacle
detection task, that produces more reliable bounding boxes, with the depth
estimation one, increasing the robustness of both to scenario changes. We call
this architecture J-MOD$^{2}$. We test the effectiveness of our approach with
experiments on sequences with different appearance and focal lengths and
compare it to SotA multi task methods that jointly perform semantic
segmentation and depth estimation. In addition, we show the integration in a
full system using a set of simulated navigation experiments where a MAV
explores an unknown scenario and plans safe trajectories by using our detection
model.
",1,0,0,0,0,0
13071,13072,On Approximation for Fractional Stochastic Partial Differential Equations on the Sphere,"  This paper gives the exact solution in terms of the Karhunen-Loève
expansion to a fractional stochastic partial differential equation on the unit
sphere $\mathbb{S}^{2}\subset \mathbb{R}^{3}$ with fractional Brownian motion
as driving noise and with random initial condition given by a fractional
stochastic Cauchy problem. A numerical approximation to the solution is given
by truncating the Karhunen-Loève expansion. We show the convergence rates
of the truncation errors in degree and the mean square approximation errors in
time. Numerical examples using an isotropic Gaussian random field as initial
condition and simulations of evolution of cosmic microwave background (CMB) are
given to illustrate the theoretical results.
",0,0,1,1,0,0
4048,4049,Household poverty classification in data-scarce environments: a machine learning approach,"  We describe a method to identify poor households in data-scarce countries by
leveraging information contained in nationally representative household
surveys. It employs standard statistical learning techniques---cross-validation
and parameter regularization---which together reduce the extent to which the
model is over-fitted to match the idiosyncracies of observed survey data. The
automated framework satisfies three important constraints of this development
setting: i) The prediction model uses at most ten questions, which limits the
costs of data collection; ii) No computation beyond simple arithmetic is needed
to calculate the probability that a given household is poor, immediately after
data on the ten indicators is collected; and iii) One specification of the
model (i.e. one scorecard) is used to predict poverty throughout a country that
may be characterized by significant sub-national differences. Using survey data
from Zambia, the model's out-of-sample predictions distinguish poor households
from non-poor households using information contained in ten questions.
",0,0,0,1,0,0
14356,14357,Predicting radio emission from the newborn hot Jupiter V830 Tau and its host star,"  Magnetised exoplanets are expected to emit at radio frequencies analogously
to the radio auroral emission of Earth and Jupiter. We predict the radio
emission from V830 Tau b, the youngest (2 Myr) detected exoplanet to date. We
model the host star wind using 3DMHD simulations that take into account its
surface magnetism. With this, we constrain the local conditions around V830 Tau
b that we use to then compute its radio emission. We estimate average radio
flux densities of 6 to 24mJy, depending on the assumed radius of the planet
(one or two Rjupiter). These radio fluxes are present peaks that are up to
twice the average values. We show here that these fluxes are weakly dependent
(a factor of 1.8) on the assumed polar planetary magnetic field (10 to 100G),
opposed to the maximum frequency of the emission, which ranges from 18 to
240MHz. We also estimate the thermal radio emission from the stellar wind. By
comparing our results with VLA and VLBA observations of the system, we
constrain the stellar mass-loss rate to be <3e-9 Msun/yr, with likely values
between ~1e-12 and 1e-10 Msun/yr. The frequency-dependent extension of the
radio-emitting wind is around ~ 3 to 30 Rstar for frequencies in the range of
275 to 50MHz, implying that V830 Tau b, at an orbital distance of 6.1 Rstar,
could be embedded in the regions of the host star's wind that are optically
thick to radio wavelengths, but not deeply so. Planetary emission can only
propagate in the stellar wind plasma if the frequency of the cyclotron emission
exceeds the stellar wind plasma frequency. For that, we find that for planetary
radio emission to propagate through the host star wind, planetary magnetic
field strengths larger than ~1.3 to 13 G are required. The V830 Tau system is a
very interesting system for conducting radio observations from both the
perspective of radio emission from the planet as well as from the host star's
wind.
",0,1,0,0,0,0
2952,2953,Metastability and avalanche dynamics in strongly-correlated gases with long-range interactions,"  We experimentally study the stability of a bosonic Mott-insulator against the
formation of a density wave induced by long-range interactions, and
characterize the intrinsic dynamics between these two states. The
Mott-insulator is created in a quantum degenerate gas of 87-Rubidium atoms,
trapped in a three-dimensional optical lattice. The gas is located inside and
globally coupled to an optical cavity. This causes interactions of global
range, mediated by photons dispersively scattered between a transverse lattice
and the cavity. The scattering comes with an atomic density modulation, which
is measured by the photon flux leaking from the cavity. We initialize the
system in a Mott-insulating state and then rapidly increase the global coupling
strength. We observe that the system falls into either of two distinct final
states. One is characterized by a low photon flux, signaling a Mott insulator,
and the other is characterized by a high photon flux, which we associate with a
density wave. Ramping the global coupling slowly, we observe a hysteresis loop
between the two states - a further signature of metastability. A comparison
with a theoretical model confirms that the metastability originates in the
competition between short- and global-range interactions. From the increasing
photon flux monitored during the switching process, we find that several
thousand atoms tunnel to a neighboring site on the time scale of the single
particle dynamics. We argue that a density modulation, initially forming in the
compressible surface of the trapped gas, triggers an avalanche tunneling
process in the Mott-insulating region.
",0,1,0,0,0,0
3390,3391,Analysis of Footnote Chasing and Citation Searching in an Academic Search Engine,"  In interactive information retrieval, researchers consider the user behavior
towards systems and search tasks in order to adapt search results by analyzing
their past interactions. In this paper, we analyze the user behavior towards
Marcia Bates' search stratagems such as 'footnote chasing' and 'citation
search' in an academic search engine. We performed a preliminary analysis of
their frequency and stage of use in the social sciences search engine sowiport.
In addition, we explored the impact of these stratagems on the whole search
process performance. We can conclude that the appearance of these two search
features in real retrieval sessions lead to an improvement of the precision in
terms of positive interactions with 16% when using footnote chasing and 17% for
the citation search stratagem.
",1,0,0,0,0,0
11926,11927,Application of Convolutional Neural Network to Predict Airfoil Lift Coefficient,"  The adaptability of the convolutional neural network (CNN) technique for
aerodynamic meta-modeling tasks is probed in this work. The primary objective
is to develop suitable CNN architecture for variable flow conditions and object
geometry, in addition to identifying a sufficient data preparation process.
Multiple CNN structures were trained to learn the lift coefficients of the
airfoils with a variety of shapes in multiple flow Mach numbers, Reynolds
numbers, and diverse angles of attack. This is conducted to illustrate the
concept of the technique. A multi-layered perceptron (MLP) is also used for the
training sets. The MLP results are compared with that of the CNN results. The
newly proposed meta-modeling concept has been found to be comparable with the
MLP in learning capability; and more importantly, our CNN model exhibits a
competitive prediction accuracy with minimal constraints in a geometric
representation.
",1,0,0,1,0,0
9058,9059,A Deep Generative Framework for Paraphrase Generation,"  Paraphrase generation is an important problem in NLP, especially in question
answering, information retrieval, information extraction, conversation systems,
to name a few. In this paper, we address the problem of generating paraphrases
automatically. Our proposed method is based on a combination of deep generative
models (VAE) with sequence-to-sequence models (LSTM) to generate paraphrases,
given an input sentence. Traditional VAEs when combined with recurrent neural
networks can generate free text but they are not suitable for paraphrase
generation for a given sentence. We address this problem by conditioning the
both, encoder and decoder sides of VAE, on the original sentence, so that it
can generate the given sentence's paraphrases. Unlike most existing models, our
model is simple, modular and can generate multiple paraphrases, for a given
sentence. Quantitative evaluation of the proposed method on a benchmark
paraphrase dataset demonstrates its efficacy, and its performance improvement
over the state-of-the-art methods by a significant margin, whereas qualitative
human evaluation indicate that the generated paraphrases are well-formed,
grammatically correct, and are relevant to the input sentence. Furthermore, we
evaluate our method on a newly released question paraphrase dataset, and
establish a new baseline for future research.
",1,0,0,0,0,0
3602,3603,Virtual Network Migration on the GENI Wide-Area SDN-Enabled Infrastructure,"  A virtual network (VN) contains a collection of virtual nodes and links
assigned to underlying physical resources in a network substrate. VN migration
is the process of remapping a VN's logical topology to a new set of physical
resources to provide failure recovery, energy savings, or defense against
attack. Providing VN migration that is transparent to running applications is a
significant challenge. Efficient migration mechanisms are highly dependent on
the technology deployed in the physical substrate. Prior work has considered
migration in data centers and in the PlanetLab infrastructure. However, there
has been little effort targeting an SDN-enabled wide-area networking
environment - an important building block of future networking infrastructure.
In this work, we are interested in the design, implementation and evaluation of
VN migration in GENI as a working example of such a future network. We identify
and propose techniques to address key challenges: the dynamic allocation of
resources during migration, managing hosts connected to the VN, and flow table
migration sequences to minimize packet loss. We find that GENI's virtualization
architecture makes transparent and efficient migration challenging. We suggest
alternatives that might be adopted in GENI and are worthy of adoption by
virtual network providers to facilitate migration.
",1,0,0,0,0,0
9315,9316,A yield-cost tradeoff governs Escherichia coli's decision between fermentation and respiration in carbon-limited growth,"  Many microbial systems are known to actively reshape their proteomes in
response to changes in growth conditions induced e.g. by nutritional stress or
antibiotics. Part of the re-allocation accounts for the fact that, as the
growth rate is limited by targeting specific metabolic activities, cells simply
respond by fine-tuning their proteome to invest more resources into the
limiting activity (i.e. by synthesizing more proteins devoted to it). However,
this is often accompanied by an overall re-organization of metabolism, aimed at
improving the growth yield under limitation by re-wiring resource through
different pathways. While both effects impact proteome composition, the latter
underlies a more complex systemic response to stress. By focusing on E. coli's
`acetate switch', we use mathematical modeling and a re-analysis of empirical
data to show that the transition from a predominantly fermentative to a
predominantly respirative metabolism in carbon-limited growth results from the
trade-off between maximizing the growth yield and minimizing its costs in terms
of required the proteome share. In particular, E. coli's metabolic phenotypes
appear to be Pareto-optimal for these objective functions over a broad range of
dilutions.
",0,1,0,0,0,0
4508,4509,Generalized Hölder's inequality on Morrey spaces,"  The aim of this paper is to present necessary and sufficient conditions for
generalized Hölder's inequality on generalized Morrey spaces. We also
obtain similar results on weak Morrey spaces and on generalized weak Morrey
spaces. The necessary and sufficient conditions for the generalized
Hölder's inequality on these spaces are obtained through estimates for
characteristic functions of balls in $\mathbb{R}^d$.
",0,0,1,0,0,0
15283,15284,The homology class of a Poisson transversal,"  This note is devoted to the study of the homology class of a compact Poisson
transversal in a Poisson manifold. For specific classes of Poisson structures,
such as unimodular Poisson structures and Poisson manifolds with closed leaves,
we prove that all their compact Poisson transversals represent non-trivial
homology classes, generalizing the symplectic case. We discuss several examples
in which this property does not hold, as well as a weaker version of this
property, which holds for log-symplectic structures. Finally, we extend our
results to Dirac geometry.
",0,0,1,0,0,0
4302,4303,A model for random fire induced tree-grass coexistence in savannas,"  Tree-grass coexistence in savanna ecosystems depends strongly on
environmental disturbances out of which crucial is fire. Most modeling attempts
in the literature lack stochastic approach to fire occurrences which is
essential to reflect their unpredictability. Existing models that actually
include stochasticity of fire are usually analyzed only numerically. We
introduce new minimalistic model of tree-grass coexistence where fires occur
according to stochastic process. We use the tools of linear semigroup theory to
provide more careful mathematical analysis of the model. Essentially we show
that there exists a unique stationary distribution of tree and grass biomasses.
",0,0,0,0,1,0
5455,5456,Flatness of Minima in Random Inflationary Landscapes,"  We study the likelihood which relative minima of random polynomial potentials
support the slow-roll conditions for inflation. Consistent with
renormalizability and boundedness, the coefficients that appear in the
potential are chosen to be order one with respect to the energy scale at which
inflation transpires. Investigation of the single field case illustrates a
window in which the potentials satisfy the slow-roll conditions. When there are
two scalar fields, we find that the probability depends on the choice of
distribution for the coefficients. A uniform distribution yields a $0.05\%$
probability of finding a suitable minimum in the random potential whereas a
maximum entropy distribution yields a $0.1\%$ probability.
",0,1,0,0,0,0
14845,14846,On Polymorphic Sessions and Functions: A Tale of Two (Fully Abstract) Encodings,"  This work exploits the logical foundation of session types to determine what
kind of type discipline for the pi-calculus can exactly capture, and is
captured by, lambda-calculus behaviours. Leveraging the proof theoretic content
of the soundness and completeness of sequent calculus and natural deduction
presentations of linear logic, we develop the first mutually inverse and fully
abstract processes-as-functions and functions-as-processes encodings between a
polymorphic session pi-calculus and a linear formulation of System F. We are
then able to derive results of the session calculus from the theory of the
lambda-calculus: (1) we obtain a characterisation of inductive and coinductive
session types via their algebraic representations in System F; and (2) we
extend our results to account for value and process passing, entailing strong
normalisation.
",1,0,0,0,0,0
19657,19658,A direct method to compute the galaxy count angular correlation function including redshift-space distortions,"  In the near future, cosmology will enter the wide and deep galaxy survey area
allowing high-precision studies of the large scale structure of the universe in
three dimensions. To test cosmological models and determine their parameters
accurately, it is natural to confront data with exact theoretical expectations
expressed in the observational parameter space (angles and redshift). The
data-driven galaxy number count fluctuations on redshift shells, can be used to
build correlation functions $C(\theta; z_1, z_2)$ on and between shells which
can probe the baryonic acoustic oscillations, the distance-redshift distortions
as well as gravitational lensing and other relativistic effects. Transforming
the model to the data space usually requires the computation of the angular
power spectrum $C_\ell(z_1, z_2)$ but this appears as an artificial and
inefficient step plagued by apodization issues. In this article we show that it
is not necessary and present a compact expression for $C(\theta; z_1, z_2)$
that includes directly the leading density and redshift space distortions terms
from the full linear theory. It can be evaluated using a fast integration
method based on Clenshaw-Curtis quadrature and Chebyshev polynomial series.
This new method to compute the correlation functions without any Limber
approximation, allows us to produce and discuss maps of the correlation
function directly in the observable space and is a significant step towards
disentangling the data from the tested models.
",0,1,0,0,0,0
20755,20756,An integration of fast alignment and maximum-likelihood methods for electron subtomogram averaging and classification,"  Motivation: Cellular Electron CryoTomography (CECT) is an emerging 3D imaging
technique that visualizes subcellular organization of single cells at
submolecular resolution and in near-native state. CECT captures large numbers
of macromolecular complexes of highly diverse structures and abundances.
However, the structural complexity and imaging limits complicate the systematic
de novo structural recovery and recognition of these macromolecular complexes.
Efficient and accurate reference-free subtomogram averaging and classification
represent the most critical tasks for such analysis. Existing subtomogram
alignment based methods are prone to the missing wedge effects and low
signal-to-noise ratio (SNR). Moreover, existing maximum-likelihood based
methods rely on integration operations, which are in principle computationally
infeasible for accurate calculation.
Results: Built on existing works, we propose an integrated method, Fast
Alignment Maximum Likelihood method (FAML), which uses fast subtomogram
alignment to sample sub-optimal rigid transformations. The transformations are
then used to approximate integrals for maximum-likelihood update of subtomogram
averages through expectation-maximization algorithm. Our tests on simulated and
experimental subtomograms showed that, compared to our previously developed
fast alignment method (FA), FAML is significantly more robust to noise and
missing wedge effects with moderate increases of computation cost.Besides, FAML
performs well with significantly fewer input subtomograms when the FA method
fails. Therefore, FAML can serve as a key component for improved construction
of initial structural models from macromolecules captured by CECT.
",0,0,0,1,1,0
1073,1074,Identities and congruences involving the Fubini polynomials,"  In this paper, we investigate the umbral representation of the Fubini
polynomials $F_{x}^{n}:=F_{n}(x)$ to derive some properties involving these
polynomials. For any prime number $p$ and any polynomial $f$ with integer
coefficients, we show $(f(F_{x}))^{p}\equiv f(F_{x})$ and we give other curious
congruences.
",0,0,1,0,0,0
8234,8235,Comparing Different Models for Investigating Cascading Failures in Power Systems,"  This paper centers on the comparison of three different models that describe
cascading failures of power systems. Specifically, these models are different
in characterizing the physical properties of power networks and computing the
branch power flow. Optimal control approach is applied on these models to
identify the critical disturbances that result in the worst-case cascading
failures of power networks. Then we compare these models by analyzing the
critical disturbances and cascading processes. Significantly, comparison
results on IEEE 9 bus system demonstrate that physical and electrical
properties of power networks play a crucial role in the evolution of cascading
failures, and it is necessary to take into account these properties
appropriately while applying the model in the analysis of cascading blackout.
",0,0,1,0,0,0
4701,4702,802.11 Wireless Simulation and Anomaly Detection using HMM and UBM,"  Despite the growing popularity of 802.11 wireless networks, users often
suffer from connectivity problems and performance issues due to unstable radio
conditions and dynamic user behavior among other reasons. Anomaly detection and
distinction are in the thick of major challenges that network managers
encounter. Complication of monitoring the broaden and complex WLANs, that often
requires heavy instrumentation of the user devices, makes the anomaly detection
analysis even harder. In this paper we exploit 802.11 access point usage data
and propose an anomaly detection technique based on Hidden Markov Model (HMM)
and Universal Background Model (UBM) on data that is inexpensive to obtain. We
then generate a number of network anomalous scenarios in OMNeT++/INET network
simulator and compare the detection outcomes with those in baseline approaches
(RawData and PCA). The experimental results show the superiority of HMM and
HMM-UBM models in detection precision and sensitivity.
",1,0,0,0,0,0
19911,19912,Towards integrated superconducting detectors on lithium niobate waveguides,"  Superconducting detectors are now well-established tools for low-light
optics, and in particular quantum optics, boasting high-efficiency, fast
response and low noise. Similarly, lithium niobate is an important platform for
integrated optics given its high second-order nonlinearity, used for high-speed
electro-optic modulation and polarization conversion, as well as frequency
conversion and sources of quantum light. Combining these technologies addresses
the requirements for a single platform capable of generating, manipulating and
measuring quantum light in many degrees of freedom, in a compact and
potentially scalable manner. We will report on progress integrating tungsten
transition-edge sensors (TESs) and amorphous tungsten silicide superconducting
nanowire single-photon detectors (SNSPDs) on titanium in-diffused lithium
niobate waveguides. The travelling-wave design couples the evanescent field
from the waveguides into the superconducting absorber. We will report on
simulations and measurements of the absorption, which we can characterize at
room temperature prior to cooling down the devices. Independently, we show how
the detectors respond to flood illumination, normally incident on the devices,
demonstrating their functionality.
",0,1,0,0,0,0
11038,11039,Recovery Guarantees for One-hidden-layer Neural Networks,"  In this paper, we consider regression problems with one-hidden-layer neural
networks (1NNs). We distill some properties of activation functions that lead
to $\mathit{local~strong~convexity}$ in the neighborhood of the ground-truth
parameters for the 1NN squared-loss objective. Most popular nonlinear
activation functions satisfy the distilled properties, including rectified
linear units (ReLUs), leaky ReLUs, squared ReLUs and sigmoids. For activation
functions that are also smooth, we show $\mathit{local~linear~convergence}$
guarantees of gradient descent under a resampling rule. For homogeneous
activations, we show tensor methods are able to initialize the parameters to
fall into the local strong convexity region. As a result, tensor initialization
followed by gradient descent is guaranteed to recover the ground truth with
sample complexity $ d \cdot \log(1/\epsilon) \cdot \mathrm{poly}(k,\lambda )$
and computational complexity $n\cdot d \cdot \mathrm{poly}(k,\lambda) $ for
smooth homogeneous activations with high probability, where $d$ is the
dimension of the input, $k$ ($k\leq d$) is the number of hidden nodes,
$\lambda$ is a conditioning property of the ground-truth parameter matrix
between the input layer and the hidden layer, $\epsilon$ is the targeted
precision and $n$ is the number of samples. To the best of our knowledge, this
is the first work that provides recovery guarantees for 1NNs with both sample
complexity and computational complexity $\mathit{linear}$ in the input
dimension and $\mathit{logarithmic}$ in the precision.
",1,0,0,1,0,0
10886,10887,Towards Provably Safe Mixed Transportation Systems with Human-driven and Automated Vehicles,"  Currently, we are in an environment where the fraction of automated vehicles
is negligibly small. We anticipate that this fraction will increase in coming
decades before if ever, we have a fully automated transportation system.
Motivated by this we address the problem of provable safety of mixed traffic
consisting of both intelligent vehicles (IVs) as well as human-driven vehicles
(HVs). An important issue that arises is that such mixed systems may well have
lesser throughput than all human traffic systems if the automated vehicles are
expected to remain provably safe with respect to human traffic. This
necessitates the consideration of strategies such as platooning of automated
vehicles in order to increase the throughput. In this paper, we address the
design of provably safe systems consisting of a mix of automated and
human-driven vehicles including the use of platooning by automated vehicles.
We design motion planing policies and coordination rules for participants in
this novel mixed system. HVs are considered as nearsighted and modeled with
relatively loose constraints, while IVs are considered as capable of following
much tighter constraints. HVs are expected to follow reasonable and simple
rules. IVs are designed to move under a model predictive control (MPC) based
motion plans and coordination protocols. Our contribution of this paper is in
showing how to integrate these two types of models safely into a mixed system.
System safety is proved in single lane scenarios, as well as in multi-lane
situations allowing lane changes.
",1,0,0,0,0,0
7626,7627,"Rotational spectroscopy, tentative interstellar detection, and chemical modelling of N-methylformamide","  N-methylformamide, CH3NHCHO, may be an important molecule for interstellar
pre-biotic chemistry because it contains a peptide bond. The rotational
spectrum of the most stable trans conformer of CH3NHCHO is complicated by
strong torsion-rotation interaction due to the low barrier of the methyl
torsion. We use two absorption spectrometers in Kharkiv and Lille to measure
the rotational spectra over 45--630 GHz. The analysis is carried out using the
Rho-axis method and the RAM36 code. We search for N-methylformamide toward the
hot molecular core Sgr B2(N2) using a spectral line survey carried out with
ALMA. The astronomical results are put into a broader astrochemical context
with the help of a gas-grain chemical kinetics model. The laboratory data set
for the trans conformer of CH3NHCHO consists of 9469 line frequencies with J <=
62, including the first assignment of the rotational spectra of the first and
second excited torsional states. All these lines are fitted within experimental
accuracy. We report the tentative detection of CH3NHCHO towards Sgr B2(N2). We
find CH3NHCHO to be more than one order of magnitude less abundant than NH2CHO,
a factor of two less abundant than CH3NCO, but only slightly less abundant than
CH3CONH2. The chemical models indicate that the efficient formation of HNCO via
NH + CO on grains is a necessary step in the achievement of the observed
gas-phase abundance of CH3NCO. Production of CH3NHCHO may plausibly occur on
grains either through the direct addition of functional-group radicals or
through the hydrogenation of CH3NCO. Provided the detection of CH3NHCHO is
confirmed, the only slight underabundance of this molecule compared to its more
stable structural isomer acetamide and the sensitivity of the model abundances
to the chemical kinetics parameters suggest that the formation of these two
molecules is controlled by kinetics rather than thermal equilibrium.
",0,1,0,0,0,0
17351,17352,Gaussian approximation of maxima of Wiener functionals and its application to high-frequency data,"  This paper establishes an upper bound for the Kolmogorov distance between the
maximum of a high-dimensional vector of smooth Wiener functionals and the
maximum of a Gaussian random vector. As a special case, we show that the
maximum of multiple Wiener-Itô integrals with common orders is
well-approximated by its Gaussian analog in terms of the Kolmogorov distance if
their covariance matrices are close to each other and the maximum of the fourth
cumulants of the multiple Wiener-Itô integrals is close to zero. This may be
viewed as a new kind of fourth moment phenomenon, which has attracted
considerable attention in the recent studies of probability. This type of
Gaussian approximation result has many potential applications to statistics. To
illustrate this point, we present two statistical applications in
high-frequency financial econometrics: One is the hypothesis testing problem
for the absence of lead-lag effects and the other is the construction of
uniform confidence bands for spot volatility.
",0,0,1,1,0,0
15696,15697,Competition between disorder and interaction effects in 3D Weyl semimetals,"  We investigate the low-energy scaling behavior of an interacting 3D Weyl
semimetal in the presence of disorder. In order to achieve a renormalization
group analysis of the theory, we focus on the effects of a
short-ranged-correlated disorder potential, checking nevertheless that this
choice is not essential to locate the different phases of the Weyl semimetal.
We show that there is a line of fixed-points in the renormalization group flow
of the interacting theory, corresponding to the disorder-driven transition to a
diffusive metal phase. Along that boundary, the critical disorder strength
undergoes a strong increase with respect to the noninteracting theory, as a
consequence of the unconventional screening of the Coulomb and disorder-induced
interactions. A complementary resolution of the Schwinger-Dyson equations
allows us to determine the full phase diagram of the system, showing the
prevalence of a renormalized semimetallic phase in the regime of intermediate
interaction strength, and adjacent to the non-Fermi liquid phase characteristic
of the strong interaction regime of 3D Weyl semimetals.
",0,1,0,0,0,0
208,209,Clamped seismic metamaterials: Ultra-low broad frequency stop-bands,"  The regularity of earthquakes, their destructive power, and the nuisance of
ground vibration in urban environments, all motivate designs of defence
structures to lessen the impact of seismic and ground vibration waves on
buildings. Low frequency waves, in the range $1$ to $10$ Hz for earthquakes and
up to a few tens of Hz for vibrations generated by human activities, cause a
large amount of damage, or inconvenience, depending on the geological
conditions they can travel considerable distances and may match the resonant
fundamental frequency of buildings. The ultimate aim of any seismic
metamaterial, or any other seismic shield, is to protect over this entire range
of frequencies, the long wavelengths involved, and low frequency, have meant
this has been unachievable to date.
Elastic flexural waves, applicable in the mechanical vibrations of thin
elastic plates, can be designed to have a broad zero-frequency stop-band using
a periodic array of very small clamped circles. Inspired by this experimental
and theoretical observation, all be it in a situation far removed from seismic
waves, we demonstrate that it is possible to achieve elastic surface (Rayleigh)
and body (pressure P and shear S) wave reflectors at very large wavelengths in
structured soils modelled as a fully elastic layer periodically clamped to
bedrock.
We identify zero frequency stop-bands that only exist in the limit of columns
of concrete clamped at their base to the bedrock. In a realistic configuration
of a sedimentary basin 15 meters deep we observe a zero frequency stop-band
covering a broad frequency range of $0$ to $30$ Hz.
",0,1,0,0,0,0
13443,13444,Intersubband polarons in oxides,"  Intersubband (ISB) polarons result from the interaction of an ISB transition
and the longitudinal optical (LO) phonons in a semiconductor quantum well (QW).
Their observation requires a very dense two dimensional electron gas (2DEG) in
the QW and a polar or highly ionic semiconductor. Here we show that in
ZnO/MgZnO QWs the strength of such a coupling can be as high as 1.5 times the
LO-phonon frequency due to the very dense 2DEG achieved and the large
difference between the static and high-frequency dielectric constants in ZnO.
The ISB polaron is observed optically in multiple QW structures with 2DEG
densities ranging from $5\times 10^{12}$ to $5\times 10^{13}$ cm$^{-2}$, where
an unprecedented regime is reached in which the frequency of the upper ISB
polaron branch is three times larger than that of the bare ISB transition. This
study opens new prospects to the exploitation of oxides in phenomena happening
in the ultrastrong coupling regime.
",0,1,0,0,0,0
14506,14507,Galactic Dark Matter Halos and Globular Cluster Populations. III: Extension to Extreme Environments,"  The total mass M_GCS in the globular cluster (GC) system of a galaxy is
empirically a near-constant fraction of the total mass M_h = M_bary + M_dark of
the galaxy, across a range of 10^5 in galaxy mass. This trend is radically
unlike the strongly nonlinear behavior of total stellar mass M_star versus M_h.
We discuss extensions of this trend to two more extreme situations: (a) entire
clusters of galaxies, and (b) the Ultra-Diffuse Galaxies (UDGs) recently
discovered in Coma and elsewhere. Our calibration of the ratio \eta_M = M_GCS /
M_h from normal galaxies, accounting for new revisions in the adopted
mass-to-light ratio for GCs, now gives \eta_M = 2.9 \times 10^{-5} as the mean
absolute mass fraction. We find that the same ratio appears valid for galaxy
clusters and UDGs. Estimates of \eta_M in the four clusters we examine tend to
be slightly higher than for individual galaxies, butmore data and better
constraints on the mean GC mass in such systems are needed to determine if this
difference is significant. We use the constancy of \eta_M to estimate total
masses for several individual cases; for example, the total mass of the Milky
Way is calculated to be M_h = 1.1 \times 10^{12} M_sun. Physical explanations
for the uniformity of \eta_M are still descriptive, but point to a picture in
which massive, dense star clusters in their formation stages were relatively
immune to the feedback that more strongly influenced lower-density regions
where most stars form.
",0,1,0,0,0,0
8622,8623,Modelling the Milky Way's globular cluster system,"  We construct a model for the Galactic globular cluster system based on a
realistic gravitational potential and a distribution function (DF) analytic in
the action integrals. The DF comprises disc and halo components whose
functional forms resemble those recently used to describe the stellar discs and
stellar halo. We determine the posterior distribution of our model parameters
using a Bayesian approach. This gives us an understanding of how well the
globular cluster data constrain our model. The favoured parameter values of the
disc and halo DFs are similar to values previously obtained from fits to the
stellar disc and halo, although the cluster halo system shows clearer rotation
than does the stellar halo. Our model reproduces the generic features of the
globular cluster system, namely the density profile, the mean rotation
velocity. The fraction of disc clusters coincides with the observed fraction of
metal-rich clusters. However, the data indicate either incompatibility between
catalogued cluster distances and current estimates of distance to the Galactic
Centre, or failure to identify clusters behind the bulge. As the data for our
Galaxy's components increase in volume and precision over the next few years,
it will be rewarding to revisit the present analysis.
",0,1,0,0,0,0
6237,6238,Bifurcation of solutions to Hamiltonian boundary value problems,"  A bifurcation is a qualitative change in a family of solutions to an equation
produced by varying parameters. In contrast to the local bifurcations of
dynamical systems that are often related to a change in the number or stability
of equilibria, bifurcations of boundary value problems are global in nature and
may not be related to any obvious change in dynamical behaviour. Catastrophe
theory is a well-developed framework which studies the bifurcations of critical
points of functions. In this paper we study the bifurcations of solutions of
boundary-value problems for symplectic maps, using the language of
(finite-dimensional) singularity theory. We associate certain such problems
with a geometric picture involving the intersection of Lagrangian submanifolds,
and hence with the critical points of a suitable generating function. Within
this framework, we then study the effect of three special cases: (i) some
common boundary conditions, such as Dirichlet boundary conditions for
second-order systems, restrict the possible types of bifurcations (for example,
in generic planar systems only the A-series beginning with folds and cusps can
occur); (ii) integrable systems, such as planar Hamiltonian systems, can
exhibit a novel periodic pitchfork bifurcation; and (iii) systems with
Hamiltonian symmetries or reversing symmetries can exhibit restricted
bifurcations associated with the symmetry. This approach offers an alternative
to the analysis of critical points in function spaces, typically used in the
study of bifurcation of variational problems, and opens the way to the
detection of more exotic bifurcations than the simple folds and cusps that are
often found in examples.
",0,0,1,0,0,0
16128,16129,Ambient noise correlation-based imaging with moving sensors,"  Waves can be used to probe and image an unknown medium. Passive imaging uses
ambient noise sources to illuminate the medium. This paper considers passive
imaging with moving sensors. The motivation is to generate large synthetic
apertures, which should result in enhanced resolution. However Doppler effects
and lack of reciprocity significantly affect the imaging process. This paper
discusses the consequences in terms of resolution and it shows how to design
appropriate imaging functions depending on the sensor trajectory and velocity.
",0,1,0,0,0,0
7581,7582,Integrating sentiment and social structure to determine preference alignments: The Irish Marriage Referendum,"  We examine the relationship between social structure and sentiment through
the analysis of a large collection of tweets about the Irish Marriage
Referendum of 2015. We obtain the sentiment of every tweet with the hashtags
#marref and #marriageref that was posted in the days leading to the referendum,
and construct networks to aggregate sentiment and use it to study the
interactions among users. Our results show that the sentiment of mention tweets
posted by users is correlated with the sentiment of received mentions, and
there are significantly more connections between users with similar sentiment
scores than among users with opposite scores in the mention and follower
networks. We combine the community structure of the two networks with the
activity level of the users and sentiment scores to find groups of users who
support voting `yes' or `no' in the referendum. There were numerous
conversations between users on opposing sides of the debate in the absence of
follower connections, which suggests that there were efforts by some users to
establish dialogue and debate across ideological divisions. Our analysis shows
that social structure can be integrated successfully with sentiment to analyse
and understand the disposition of social media users. These results have
potential applications in the integration of data and meta-data to study
opinion dynamics, public opinion modelling, and polling.
",1,1,0,0,0,0
20230,20231,A Stress/Displacement Virtual Element Method for Plane Elasticity Problems,"  The numerical approximation of 2D elasticity problems is considered, in the
framework of the small strain theory and in connection with the mixed
Hellinger-Reissner variational formulation. A low-order Virtual Element Method
(VEM) with a-priori symmetric stresses is proposed. Several numerical tests are
provided, along with a rigorous stability and convergence analysis.
",0,0,1,0,0,0
13535,13536,Characteristic functions as bounded multipliers on anisotropic spaces,"  We show that characteristic functions of domains with boundaries transversal
to stable cones are bounded multipliers on a recently introduced scale
$U^{t,s}_p$ of anisotropic Banach spaces, under the conditions -1+1/p<s<-t<0
and -(r-1)+t<s, with 1<p<infty. (Amended after comments from the referee and M.
Jézéquel, January 10, 2018)
",0,1,0,0,0,0
19935,19936,Modelling the descent of nitric oxide during the elevated stratopause event of January 2013,"  Using simulations with a whole-atmosphere chemistry-climate model nudged by
meteorological analyses, global satellite observations of nitrogen oxide (NO)
and water vapour by the Sub-Millimetre Radiometer instrument (SMR), of
temperature by the Microwave Limb Sounder (MLS), as well as local radar
observations, this study examines the recent major stratospheric sudden warming
accompanied by an elevated stratopause event (ESE) that occurred in January
2013. We examine dynamical processes during the ESE, including the role of
planetary wave, gravity wave and tidal forcing on the initiation of the descent
in the mesosphere-lower thermosphere (MLT) and its continuation throughout the
mesosphere and stratosphere, as well as the impact of model eddy diffusion. We
analyse the transport of NO and find the model underestimates the large descent
of NO compared to SMR observations. We demonstrate that the discrepancy arises
abruptly in the MLT region at a time when the resolved wave forcing and the
planetary wave activity increase, just before the elevated stratopause reforms.
The discrepancy persists despite doubling the model eddy diffusion. While the
simulations reproduce an enhancement of the semi-diurnal tide following the
onset of the 2013 SSW, corroborating new meteor radar observations at high
northern latitudes over Trondheim (63.4$^{\circ}$N), the modelled tidal
contribution to the forcing of the mean meridional circulation and to the
descent is a small portion of the resolved wave forcing, and lags it by about
ten days.
",0,1,0,0,0,0
10463,10464,Near-infrared spectroscopy of 5 ultra-massive galaxies at 1.7 < z < 2.7,"  We present the results of a pilot near-infrared (NIR) spectroscopic campaign
of five very massive galaxies ($\log(\text{M}_\star/\text{M}_\odot)>11.45$) in
the range of $1.7<z<2.7$. We measure an absorption feature redshift for one
galaxy at $z_\text{spec}=2.000\pm0.006$. For the remaining galaxies, we combine
the photometry with the continuum from the spectra to estimate continuum
redshifts and stellar population properties. We define a continuum redshift
($z_{\rm cont}$ ) as one in which the redshift is estimated probabilistically
using EAZY from the combination of catalog photometry and the observed
spectrum. We derive the uncertainties on the stellar population synthesis
properties using a Monte Carlo simulation and examine the correlations between
the parameters with and without the use of the spectrum in the modeling of the
spectral energy distributions (SEDs). The spectroscopic constraints confirm the
extreme stellar masses of the galaxies in our sample. We find that three out of
five galaxies are quiescent (star formation rate of $\lesssim 1
M_\odot~yr^{-1}$) with low levels of dust obscuration ($A_{\rm V} < 1$) , that
one galaxy displays both high levels of star formation and dust obscuration
(${\rm SFR} \approx 300 M_\odot~{\rm yr}^{-1}$, $A_{\rm V} \approx 1.7$~mag),
and that the remaining galaxy has properties that are intermediate between the
quiescent and star-forming populations.
",0,1,0,0,0,0
14793,14794,Bio-Inspired Local Information-Based Control for Probabilistic Swarm Distribution Guidance,"  This paper addresses a task allocation problem for a large-scale robotic
swarm, namely swarm distribution guidance problem. Unlike most of the existing
frameworks handling this problem, the proposed framework suggests utilising
local information available to generate its time-varying stochastic policies.
As each agent requires only local consistency on information with neighbouring
agents, rather than the global consistency, the proposed framework offers
various advantages, e.g., a shorter timescale for using new information and
potential to incorporate an asynchronous decision-making process. We perform
theoretical analysis on the properties of the proposed framework. From the
analysis, it is proved that the framework can guarantee the convergence to the
desired density distribution even using local information while maintaining
advantages of global-information-based approaches. The design requirements for
these advantages are explicitly listed in this paper. This paper also provides
specific examples of how to implement the framework developed. The results of
numerical experiments confirm the effectiveness and comparability of the
proposed framework, compared with the global-information-based framework.
",0,0,1,1,0,0
9022,9023,Thermodynamic and kinetic fragility of Freon113: the most fragile plastic crystal,"  We present a dynamic and thermodynamic study of the orientational glass
former Freon113 (CCl2F-CClF2) in order to analyze its kinetic and thermodynamic
fragilities. Freon113 displays internal molecular degrees of freedom which
promote a complex energy landscape. Experimental specific heat and its
microscopic origin, the vibrational density of states from inelastic neutron
scattering, together with the orientational dynamics obtained by means of
dielectric spectroscopy have revealed the highest fragility value, both
thermodynamic and kinetic, found for this orientational glass former. The
excess in both Debye-reduced specific heat and density of states (boson peak)
evidences the existence of glassy low-energy excitations. We demonstrate that
early proposed correlations between the boson peak and the Debye specific heat
value are elusive as revealed by the clear counterexample of the studied case.
",0,1,0,0,0,0
7520,7521,On the Optimization Landscape of Tensor Decompositions,"  Non-convex optimization with local search heuristics has been widely used in
machine learning, achieving many state-of-art results. It becomes increasingly
important to understand why they can work for these NP-hard problems on typical
data. The landscape of many objective functions in learning has been
conjectured to have the geometric property that ""all local optima are
(approximately) global optima"", and thus they can be solved efficiently by
local search algorithms. However, establishing such property can be very
difficult.
In this paper, we analyze the optimization landscape of the random
over-complete tensor decomposition problem, which has many applications in
unsupervised learning, especially in learning latent variable models. In
practice, it can be efficiently solved by gradient ascent on a non-convex
objective. We show that for any small constant $\epsilon > 0$, among the set of
points with function values $(1+\epsilon)$-factor larger than the expectation
of the function, all the local maxima are approximate global maxima.
Previously, the best-known result only characterizes the geometry in small
neighborhoods around the true components. Our result implies that even with an
initialization that is barely better than the random guess, the gradient ascent
algorithm is guaranteed to solve this problem.
Our main technique uses Kac-Rice formula and random matrix theory. To our
best knowledge, this is the first time when Kac-Rice formula is successfully
applied to counting the number of local minima of a highly-structured random
polynomial with dependent coefficients.
",1,0,1,1,0,0
7163,7164,Deep Asymmetric Multi-task Feature Learning,"  We propose Deep Asymmetric Multitask Feature Learning (Deep-AMTFL) which can
learn deep representations shared across multiple tasks while effectively
preventing negative transfer that may happen in the feature sharing process.
Specifically, we introduce an asymmetric autoencoder term that allows reliable
predictors for the easy tasks to have high contribution to the feature learning
while suppressing the influences of unreliable predictors for more difficult
tasks. This allows the learning of less noisy representations, and enables
unreliable predictors to exploit knowledge from the reliable predictors via the
shared latent features. Such asymmetric knowledge transfer through shared
features is also more scalable and efficient than inter-task asymmetric
transfer. We validate our Deep-AMTFL model on multiple benchmark datasets for
multitask learning and image classification, on which it significantly
outperforms existing symmetric and asymmetric multitask learning models, by
effectively preventing negative transfer in deep feature learning.
",1,0,0,1,0,0
13160,13161,Workload Analysis of Blue Waters,"  Blue Waters is a Petascale-level supercomputer whose mission is to enable the
national scientific and research community to solve ""grand challenge"" problems
that are orders of magnitude more complex than can be carried out on other high
performance computing systems. Given the important and unique role that Blue
Waters plays in the U.S. research portfolio, it is important to have a detailed
understanding of its workload in order to guide performance optimization both
at the software and system configuration level as well as inform architectural
balance tradeoffs. Furthermore, understanding the computing requirements of the
Blue Water's workload (memory access, IO, communication, etc.), which is
comprised of some of the most computationally demanding scientific problems,
will help drive changes in future computing architectures, especially at the
leading edge. With this objective in mind, the project team carried out a
detailed workload analysis of Blue Waters.
",1,0,0,0,0,0
18654,18655,Perturbations of self-adjoint operators in semifinite von Neumann algebras: Kato-Rosenblum theorem,"  In the paper, we prove an analogue of the Kato-Rosenblum theorem in a
semifinite von Neumann algebra. Let $\mathcal{M}$ be a countably decomposable,
properly infinite, semifinite von Neumann algebra acting on a Hilbert space
$\mathcal{H}$ and let $\tau$ be a faithful normal semifinite tracial weight of
$\mathcal M$. Suppose that $H$ and $H_1$ are self-adjoint operators affiliated
with $\mathcal{M}$. We show that if $H-H_1$ is in $\mathcal{M}\cap
L^{1}\left(\mathcal{M},\tau\right)$, then the ${norm}$ absolutely continuous
parts of $H$ and $H_1$ are unitarily equivalent. This implies that the real
part of a non-normal hyponormal operator in $\mathcal M$ is not a perturbation
by $\mathcal{M}\cap L^{1}\left(\mathcal{M},\tau\right)$ of a diagonal operator.
Meanwhile, for $n\ge 2$ and $1\leq p<n$, by modifying Voiculescu's invariant we
give examples of commuting $n$-tuples of self-adjoint operators in
$\mathcal{M}$ that are not arbitrarily small perturbations of commuting
diagonal operators modulo $\mathcal{M}\cap L^{p}\left(\mathcal{M},\tau\right)$.
",0,0,1,0,0,0
18190,18191,Affine processes under parameter uncertainty,"  We develop a one-dimensional notion of affine processes under parameter
uncertainty, which we call non-linear affine processes. This is done as
follows: given a set of parameters for the process, we construct a
corresponding non-linear expectation on the path space of continuous processes.
By a general dynamic programming principle we link this non-linear expectation
to a variational form of the Kolmogorov equation, where the generator of a
single affine process is replaced by the supremum over all corresponding
generators of affine processes with parameters in the parameter set. This
non-linear affine process yields a tractable model for Knightian uncertainty,
especially for modelling interest rates under ambiguity.
We then develop an appropriate Ito-formula, the respective term-structure
equations and study the non-linear versions of the Vasicek and the
Cox-Ingersoll-Ross (CIR) model. Thereafter we introduce the non-linear
Vasicek-CIR model. This model is particularly suitable for modelling interest
rates when one does not want to restrict the state space a priori and hence the
approach solves this modelling issue arising with negative interest rates.
",0,0,0,0,0,1
4935,4936,XES Tensorflow - Process Prediction using the Tensorflow Deep-Learning Framework,"  Predicting the next activity of a running process is an important aspect of
process management. Recently, artificial neural networks, so called
deep-learning approaches, have been proposed to address this challenge. This
demo paper describes a software application that applies the Tensorflow
deep-learning framework to process prediction. The software application reads
industry-standard XES files for training and presents the user with an
easy-to-use graphical user interface for both training and prediction. The
system provides several improvements over earlier work. This demo paper focuses
on the software implementation and describes the architecture and user
interface.
",1,0,0,0,0,0
7753,7754,Goldstone and Higgs Hydrodynamics in the BCS-BEC Crossover,"  We discuss the derivation of a low-energy effective field theory of phase
(Goldstone) and amplitude (Higgs) modes of the pairing field from a microscopic
theory of attractive fermions. The coupled equations for Goldstone and Higgs
fields are critically analyzed in the Bardeen-Cooper-Schrieffer (BCS) to
Bose-Einstein condensate (BEC) crossover both in three spatial dimensions and
in two spatial dimensions. The crucial role of pair fluctuations is
investigated, and the beyond-mean-field Gaussian theory of the BCS-BEC
crossover is compared with available experimental data of the two-dimensional
ultracold Fermi superfluid.
",0,1,0,0,0,0
1615,1616,Identification of Near-Infrared [Se III] and [Kr VI] Emission Lines in Planetary Nebulae,"  We identify [Se III] 1.0994 micron in the planetary nebula (PN) NGC 5315 and
[Kr VI] 1.2330 micron in three PNe, from spectra obtained with the FIRE
spectrometer on the 6.5-m Baade Telescope. Se and Kr are the two most
widely-detected neutron-capture elements in astrophysical nebulae, and can be
enriched by s-process nucleosynthesis in PN progenitor stars. The detection of
[Se III] 1.0994 micron is particularly valuable when paired with observations
of [Se IV] 2.2858 micron, as it can be used to improve the accuracy of nebular
Se abundance determinations, and allows Se ionization correction factor (ICF)
schemes to be empirically tested for the first time. We present new effective
collision strength calculations for Se^{2+} and Kr^{5+}, which we use to
compute ionic abundances. In NGC 5315, we find that the Se abundance computed
from Se^{3+}/H^+ is lower than that determined with ICFs that incorporate
Se^{2+}/H^+. We compute new Kr ICFs that take Kr^{5+}/H^+ into account, by
fitting correlations found in grids of Cloudy models between Kr ionic fractions
and those of more abundant elements, and use these to derive Kr abundances in
four PNe. Observations of [Se III] and [Kr VI] in a larger sample of PNe, with
a range of excitation levels, are needed to rigorously test the ICF
prescriptions for Se and our new Kr ICFs.
",0,1,0,0,0,0
15585,15586,Connectivity-Driven Brain Parcellation via Consensus Clustering,"  We present two related methods for deriving connectivity-based brain atlases
from individual connectomes. The proposed methods exploit a previously proposed
dense connectivity representation, termed continuous connectivity, by first
performing graph-based hierarchical clustering of individual brains, and
subsequently aggregating the individual parcellations into a consensus
parcellation. The search for consensus minimizes the sum of cluster membership
distances, effectively estimating a pseudo-Karcher mean of individual
parcellations. We assess the quality of our parcellations using (1)
Kullback-Liebler and Jensen-Shannon divergence with respect to the dense
connectome representation, (2) inter-hemispheric symmetry, and (3) performance
of the simplified connectome in a biological sex classification task. We find
that the parcellation based-atlas computed using a greedy search at a
hierarchical depth 3 outperforms all other parcellation-based atlases as well
as the standard Dessikan-Killiany anatomical atlas in all three assessments.
",0,0,0,1,1,0
12857,12858,Teacher Improves Learning by Selecting a Training Subset,"  We call a learner super-teachable if a teacher can trim down an iid training
set while making the learner learn even better. We provide sharp super-teaching
guarantees on two learners: the maximum likelihood estimator for the mean of a
Gaussian, and the large margin classifier in 1D. For general learners, we
provide a mixed-integer nonlinear programming-based algorithm to find a super
teaching set. Empirical experiments show that our algorithm is able to find
good super-teaching sets for both regression and classification problems.
",0,0,0,1,0,0
12647,12648,Elliptic supersymmetric integrable model and multivariable elliptic functions,"  We investigate the elliptic integrable model introduced by Deguchi and
Martin, which is an elliptic extension of the Perk-Schultz model. We introduce
and study a class of partition functions of the elliptic model by using the
Izergin-Korepin analysis. We show that the partition functions are expressed as
a product of elliptic factors and elliptic Schur-type symmetric functions. This
result resembles the recent works by number theorists in which the
correspondence between the partition functions of trigonometric models and the
product of the deformed Vandermonde determinant and Schur functions were
established.
",0,1,0,0,0,0
4951,4952,Asymptotic Properties of Recursive Maximum Likelihood Estimation in Non-Linear State-Space Models,"  Using stochastic gradient search and the optimal filter derivative, it is
possible to perform recursive (i.e., online) maximum likelihood estimation in a
non-linear state-space model. As the optimal filter and its derivative are
analytically intractable for such a model, they need to be approximated
numerically. In [Poyiadjis, Doucet and Singh, Biometrika 2018], a recursive
maximum likelihood algorithm based on a particle approximation to the optimal
filter derivative has been proposed and studied through numerical simulations.
Here, this algorithm and its asymptotic behavior are analyzed theoretically. We
show that the algorithm accurately estimates maxima to the underlying (average)
log-likelihood when the number of particles is sufficiently large. We also
derive (relatively) tight bounds on the estimation error. The obtained results
hold under (relatively) mild conditions and cover several classes of non-linear
state-space models met in practice.
",0,0,0,1,0,0
4785,4786,Inference for Differential Equation Models using Relaxation via Dynamical Systems,"  Statistical regression models whose mean functions are represented by
ordinary differential equations (ODEs) can be used to describe phenomenons
dynamical in nature, which are abundant in areas such as biology, climatology
and genetics. The estimation of parameters of ODE based models is essential for
understanding its dynamics, but the lack of an analytical solution of the ODE
makes the parameter estimation challenging. The aim of this paper is to propose
a general and fast framework of statistical inference for ODE based models by
relaxation of the underlying ODE system. Relaxation is achieved by a properly
chosen numerical procedure, such as the Runge-Kutta, and by introducing
additive Gaussian noises with small variances. Consequently, filtering methods
can be applied to obtain the posterior distribution of the parameters in the
Bayesian framework. The main advantage of the proposed method is computation
speed. In a simulation study, the proposed method was at least 14 times faster
than the other methods. Theoretical results which guarantee the convergence of
the posterior of the approximated dynamical system to the posterior of true
model are presented. Explicit expressions are given that relate the order and
the mesh size of the Runge-Kutta procedure to the rate of convergence of the
approximated posterior as a function of sample size.
",0,0,0,1,0,0
10122,10123,Attacking Binarized Neural Networks,"  Neural networks with low-precision weights and activations offer compelling
efficiency advantages over their full-precision equivalents. The two most
frequently discussed benefits of quantization are reduced memory consumption,
and a faster forward pass when implemented with efficient bitwise operations.
We propose a third benefit of very low-precision neural networks: improved
robustness against some adversarial attacks, and in the worst case, performance
that is on par with full-precision models. We focus on the very low-precision
case where weights and activations are both quantized to $\pm$1, and note that
stochastically quantizing weights in just one layer can sharply reduce the
impact of iterative attacks. We observe that non-scaled binary neural networks
exhibit a similar effect to the original defensive distillation procedure that
led to gradient masking, and a false notion of security. We address this by
conducting both black-box and white-box experiments with binary models that do
not artificially mask gradients.
",1,0,0,1,0,0
18809,18810,Time Series Cube Data Model,"  The purpose of this document is to create a data model and its serialization
for expressing generic time series data. Already existing IVOA data models are
reused as much as possible. The model is also made as generic as possible to be
open to new extensions but at the same time closed for modifications. This
enables maintaining interoperability throughout different versions of the data
model. We define the necessary building blocks for metadata discovery,
serialization of time series data and understanding it by clients. We present
several categories of time series science cases with examples of
implementation. We also take into account the most pressing topics for time
series providers like tracking original images for every individual point of a
light curve or time-derived axes like frequency for gravitational wave
analysis. The main motivation for the creation of a new model is to provide a
unified time series data publishing standard - not only for light curves but
also more generic time series data, e.g., radial velocity curves, power
spectra, hardness ratio, provenance linkage, etc. The flexibility is the most
crucial part of our model - we are not dependent on any physical domain or
frame models. While images or spectra are already stable and standardized
products, the time series related domains are still not completely evolved and
new ones will likely emerge in near future. That is why we need to keep models
like Time Series Cube DM independent of any underlying physical models. In our
opinion, this is the only correct and sustainable way for future development of
IVOA standards.
",1,1,0,0,0,0
19286,19287,Kinodynamic Planning on Constraint Manifolds,"  This paper presents a motion planner for systems subject to kinematic and
dynamic constraints. The former appear when kinematic loops are present in the
system, such as in parallel manipulators, in robots that cooperate to achieve a
given task, or in situations involving contacts with the environment. The
latter are necessary to obtain realistic trajectories, taking into account the
forces acting on the system. The kinematic constraints make the state space
become an implicitly-defined manifold, which complicates the application of
common motion planning techniques. To address this issue, the planner
constructs an atlas of the state space manifold incrementally, and uses this
atlas both to generate random states and to dynamically simulate the steering
of the system towards such states. The resulting tools are then exploited to
construct a rapidly-exploring random tree (RRT) over the state space. To the
best of our knowledge, this is the first randomized kinodynamic planner for
implicitly-defined state spaces. The test cases presented in this paper
validate the approach in significantly-complex systems.
",1,0,0,0,0,0
12293,12294,Geometric Matrix Completion with Recurrent Multi-Graph Neural Networks,"  Matrix completion models are among the most common formulations of
recommender systems. Recent works have showed a boost of performance of these
techniques when introducing the pairwise relationships between users/items in
the form of graphs, and imposing smoothness priors on these graphs. However,
such techniques do not fully exploit the local stationarity structures of
user/item graphs, and the number of parameters to learn is linear w.r.t. the
number of users and items. We propose a novel approach to overcome these
limitations by using geometric deep learning on graphs. Our matrix completion
architecture combines graph convolutional neural networks and recurrent neural
networks to learn meaningful statistical graph-structured patterns and the
non-linear diffusion process that generates the known ratings. This neural
network system requires a constant number of parameters independent of the
matrix size. We apply our method on both synthetic and real datasets, showing
that it outperforms state-of-the-art techniques.
",1,0,0,1,0,0
17499,17500,Speaker identification from the sound of the human breath,"  This paper examines the speaker identification potential of breath sounds in
continuous speech. Speech is largely produced during exhalation. In order to
replenish air in the lungs, speakers must periodically inhale. When inhalation
occurs in the midst of continuous speech, it is generally through the mouth.
Intra-speech breathing behavior has been the subject of much study, including
the patterns, cadence, and variations in energy levels. However, an often
ignored characteristic is the {\em sound} produced during the inhalation phase
of this cycle. Intra-speech inhalation is rapid and energetic, performed with
open mouth and glottis, effectively exposing the entire vocal tract to enable
maximum intake of air. This results in vocal tract resonances evoked by
turbulence that are characteristic of the speaker's speech-producing apparatus.
Consequently, the sounds of inhalation are expected to carry information about
the speaker's identity. Moreover, unlike other spoken sounds which are subject
to active control, inhalation sounds are generally more natural and less
affected by voluntary influences. The goal of this paper is to demonstrate that
breath sounds are indeed bio-signatures that can be used to identify speakers.
We show that these sounds by themselves can yield remarkably accurate speaker
recognition with appropriate feature representations and classification
frameworks.
",1,0,0,1,0,0
12339,12340,EmbedInsight: Automated Grading of Embedded Systems Assignments,"  Grading in embedded systems courses typically requires a face-to-face
appointment between the student and the instructor because of experimental
setups that are only available in laboratory facilities. Such a manual grading
process is an impediment to both students and instructors. Students have to
wait for several days to get feedback, and instructors may spend valuable time
evaluating trivial aspects of the assignment. As seen with software courses, an
automated grading system can significantly improve the insights available to
the instructor and encourage students to learn quickly with iterative testing.
We have designed and implemented EmbedInsight, an automated grading system for
embedded system courses that accommodates a wide variety of experimental setups
and is scalable to MOOC-style courses. EmbedInsight employs a modular web
services design that separates the user interface and the experimental setup
that evaluates student assignments. We deployed and evaluated EmbedInsight for
our university embedded systems course. We show that our system scales well to
a large number of submissions, and students are satisfied with their overall
experience.
",1,0,0,0,0,0
1578,1579,Aggregation of Classifiers: A Justifiable Information Granularity Approach,"  In this study, we introduce a new approach to combine multi-classifiers in an
ensemble system. Instead of using numeric membership values encountered in
fixed combining rules, we construct interval membership values associated with
each class prediction at the level of meta-data of observation by using
concepts of information granules. In the proposed method, uncertainty
(diversity) of findings produced by the base classifiers is quantified by
interval-based information granules. The discriminative decision model is
generated by considering both the bounds and the length of the obtained
intervals. We select ten and then fifteen learning algorithms to build a
heterogeneous ensemble system and then conducted the experiment on a number of
UCI datasets. The experimental results demonstrate that the proposed approach
performs better than the benchmark algorithms including six fixed combining
methods, one trainable combining method, AdaBoost, Bagging, and Random
Subspace.
",1,0,0,1,0,0
1213,1214,Optimal Gossip Algorithms for Exact and Approximate Quantile Computations,"  This paper gives drastically faster gossip algorithms to compute exact and
approximate quantiles.
Gossip algorithms, which allow each node to contact a uniformly random other
node in each round, have been intensely studied and been adopted in many
applications due to their fast convergence and their robustness to failures.
Kempe et al. [FOCS'03] gave gossip algorithms to compute important aggregate
statistics if every node is given a value. In particular, they gave a beautiful
$O(\log n + \log \frac{1}{\epsilon})$ round algorithm to $\epsilon$-approximate
the sum of all values and an $O(\log^2 n)$ round algorithm to compute the exact
$\phi$-quantile, i.e., the the $\lceil \phi n \rceil$ smallest value.
We give an quadratically faster and in fact optimal gossip algorithm for the
exact $\phi$-quantile problem which runs in $O(\log n)$ rounds. We furthermore
show that one can achieve an exponential speedup if one allows for an
$\epsilon$-approximation. We give an $O(\log \log n + \log \frac{1}{\epsilon})$
round gossip algorithm which computes a value of rank between $\phi n$ and
$(\phi+\epsilon)n$ at every node.% for any $0 \leq \phi \leq 1$ and $0 <
\epsilon < 1$. Our algorithms are extremely simple and very robust - they can
be operated with the same running times even if every transmission fails with
a, potentially different, constant probability. We also give a matching
$\Omega(\log \log n + \log \frac{1}{\epsilon})$ lower bound which shows that
our algorithm is optimal for all values of $\epsilon$.
",1,0,0,0,0,0
694,695,A parallel orbital-updating based plane-wave basis method for electronic structure calculations,"  Motivated by the recently proposed parallel orbital-updating approach in real
space method, we propose a parallel orbital-updating based plane-wave basis
method for electronic structure calculations, for solving the corresponding
eigenvalue problems. In addition, we propose two new modified parallel
orbital-updating methods. Compared to the traditional plane-wave methods, our
methods allow for two-level parallelization, which is particularly interesting
for large scale parallelization. Numerical experiments show that these new
methods are more reliable and efficient for large scale calculations on modern
supercomputers
",0,1,1,0,0,0
19764,19765,Towards parallelizable sampling-based Nonlinear Model Predictive Control,"  This paper proposes a new sampling-based nonlinear model predictive control
(MPC) algorithm, with a bound on complexity quadratic in the prediction horizon
N and linear in the number of samples. The idea of the proposed algorithm is to
use the sequence of predicted inputs from the previous time step as a warm
start, and to iteratively update this sequence by changing its elements one by
one, starting from the last predicted input and ending with the first predicted
input. This strategy, which resembles the dynamic programming principle, allows
for parallelization up to a certain level and yields a suboptimal nonlinear MPC
algorithm with guaranteed recursive feasibility, stability and improved cost
function at every iteration, which is suitable for real-time implementation.
The complexity of the algorithm per each time step in the prediction horizon
depends only on the horizon, the number of samples and parallel threads, and it
is independent of the measured system state. Comparisons with the fmincon
nonlinear optimization solver on benchmark examples indicate that as the
simulation time progresses, the proposed algorithm converges rapidly to the
""optimal"" solution, even when using a small number of samples.
",1,0,0,0,0,0
2000,2001,Autonomy in the interactive music system VIVO,"  Interactive Music Systems (IMS) have introduced a new world of music-making
modalities. But can we really say that they create music, as in true autonomous
creation? Here we discuss Video Interactive VST Orchestra (VIVO), an IMS that
considers extra-musical information by adopting a simple salience based model
of user-system interaction when simulating intentionality in automatic music
generation. Key features of the theoretical framework, a brief overview of
pilot research, and a case study providing validation of the model are
presented. This research demonstrates that a meaningful user/system interplay
is established in what we define as reflexive multidominance.
",1,0,0,0,0,0
7219,7220,Generating global network structures by triad types,"  This paper addresses the question of whether it is possible to generate
networks with a given global structure (defined by selected blockmodels, i.e.,
cohesive, core-periphery, hierarchical and transitivity), considering only
different types of triads. Two methods are used to generate networks: (i) the
method of relocating links; and (ii) the Monte Carlo Multi Chain algorithm
implemented in the ""ergm"" package implemented in R. Although all types of
triads can generate networks with the selected blockmodel types, the selection
of only a subset of triads improves the generated networks' blockmodel
structure. However, in the case of a hierarchical blockmodel without complete
blocks on the diagonal, additional local structures are needed to achieve the
desired global structure of generated networks. This shows that blockmodels can
emerge based on only local processes that do not take attributes into account.
",0,0,1,1,0,0
12858,12859,Photospheric Emission of Gamma-Ray Bursts,"  We review the physics of GRB production by relativistic jets that start
highly opaque near the central source and then expand to transparency. We
discuss dissipative and radiative processes in the jet and how radiative
transfer shapes the observed nonthermal spectrum released at the photosphere. A
comparison of recent detailed models with observations gives estimates for
important parameters of GRB jets, such as the Lorentz factor and magnetization.
We also discuss predictions for GRB polarization and neutrino emission.
",0,1,0,0,0,0
12589,12590,Embedding Feature Selection for Large-scale Hierarchical Classification,"  Large-scale Hierarchical Classification (HC) involves datasets consisting of
thousands of classes and millions of training instances with high-dimensional
features posing several big data challenges. Feature selection that aims to
select the subset of discriminant features is an effective strategy to deal
with large-scale HC problem. It speeds up the training process, reduces the
prediction time and minimizes the memory requirements by compressing the total
size of learned model weight vectors. Majority of the studies have also shown
feature selection to be competent and successful in improving the
classification accuracy by removing irrelevant features. In this work, we
investigate various filter-based feature selection methods for dimensionality
reduction to solve the large-scale HC problem. Our experimental evaluation on
text and image datasets with varying distribution of features, classes and
instances shows upto 3x order of speed-up on massive datasets and upto 45% less
memory requirements for storing the weight vectors of learned model without any
significant loss (improvement for some datasets) in the classification
accuracy. Source Code: this https URL.
",1,0,0,1,0,0
3546,3547,The effect of the spatial domain in FANOVA models with ARH(1) error term,"  Functional Analysis of Variance (FANOVA) from Hilbert-valued correlated data
with spatial rectangular or circular supports is analyzed, when Dirichlet
conditions are assumed on the boundary. Specifically, a Hilbert-valued fixed
effect model with error term defined from an Autoregressive Hilbertian process
of order one (ARH(1) process) is considered, extending the formulation given in
Ruiz-Medina (2016). A new statistical test is also derived to contrast the
significance of the functional fixed effect parameters. The Dirichlet
conditions established at the boundary affect the dependence range of the
correlated error term. While the rate of convergence to zero of the eigenvalues
of the covariance kernels, characterizing the Gaussian functional error
components, directly affects the stability of the generalized least-squares
parameter estimation problem. A simulation study and a real-data application
related to fMRI analysis are undertaken to illustrate the performance of the
parameter estimator and statistical test derived.
",0,0,1,1,0,0
6729,6730,"Cross-Sectional Variation of Intraday Liquidity, Cross-Impact, and their Effect on Portfolio Execution","  The composition of natural liquidity has been changing over time. An analysis
of intraday volumes for the S&P500 constituent stocks illustrates that (i)
volume surprises, i.e., deviations from their respective forecasts, are
correlated across stocks, and (ii) this correlation increases during the last
few hours of the trading session. These observations could be attributed, in
part, to the prevalence of portfolio trading activity that is implicit in the
growth of ETF, passive and systematic investment strategies; and, to the
increased trading intensity of such strategies towards the end of the trading
session, e.g., due to execution of mutual fund inflows/outflows that are
benchmarked to the closing price on each day. In this paper, we investigate the
consequences of such portfolio liquidity on price impact and portfolio
execution. We derive a linear cross-asset market impact from a stylized model
that explicitly captures the fact that a certain fraction of natural liquidity
providers only trade portfolios of stocks whenever they choose to execute. We
find that due to cross-impact and its intraday variation, it is optimal for a
risk-neutral, cost minimizing liquidator to execute a portfolio of orders in a
coupled manner, as opposed to a separable VWAP-like execution that is often
assumed. The optimal schedule couples the execution of the various orders so as
to be able to take advantage of increased portfolio liquidity towards the end
of the day. A worst case analysis shows that the potential cost reduction from
this optimized execution schedule over the separable approach can be as high as
6% for plausible model parameters. Finally, we discuss how to estimate
cross-sectional price impact if one had a dataset of realized portfolio
transaction records that exploits the low-rank structure of its coefficient
matrix suggested by our analysis.
",0,0,0,0,0,1
10165,10166,Neumann Optimizer: A Practical Optimization Algorithm for Deep Neural Networks,"  Progress in deep learning is slowed by the days or weeks it takes to train
large models. The natural solution of using more hardware is limited by
diminishing returns, and leads to inefficient use of additional resources. In
this paper, we present a large batch, stochastic optimization algorithm that is
both faster than widely used algorithms for fixed amounts of computation, and
also scales up substantially better as more computational resources become
available. Our algorithm implicitly computes the inverse Hessian of each
mini-batch to produce descent directions; we do so without either an explicit
approximation to the Hessian or Hessian-vector products. We demonstrate the
effectiveness of our algorithm by successfully training large ImageNet models
(Inception-V3, Resnet-50, Resnet-101 and Inception-Resnet-V2) with mini-batch
sizes of up to 32000 with no loss in validation error relative to current
baselines, and no increase in the total number of steps. At smaller mini-batch
sizes, our optimizer improves the validation error in these models by 0.8-0.9%.
Alternatively, we can trade off this accuracy to reduce the number of training
steps needed by roughly 10-30%. Our work is practical and easily usable by
others -- only one hyperparameter (learning rate) needs tuning, and
furthermore, the algorithm is as computationally cheap as the commonly used
Adam optimizer.
",1,0,0,1,0,0
18489,18490,Structure theorems for star-commuting power partial isometries,"  We give a new formulation and proof of a theorem of Halmos and Wallen on the
structure of power partial isometries on Hilbert space. We then use this
theorem to give a structure theorem for a finite set of partial isometries
which star-commute: each operator commutes with the others and with their
adjoints.
",0,0,1,0,0,0
7324,7325,Discrete Cycloids from Convex Symmetric Polygons,"  Cycloids, hipocycloids and epicycloids have an often forgotten common
property: they are homothetic to their evolutes. But what if use convex
symmetric polygons as unit balls, can we define evolutes and cycloids which are
genuinely discrete? Indeed, we can! We define discrete cycloids as eigenvectors
of a discrete double evolute transform which can be seen as a linear operator
on a vector space we call curvature radius space. We are also able to classify
such cycloids according to the eigenvalues of that transform, and show that the
number of cusps of each cycloid is well determined by the ordering of those
eigenvalues. As an elegant application, we easily establish a version of the
four-vertex theorem for closed convex polygons. The whole theory is developed
using only linear algebra, and concrete examples are given.
",0,0,1,0,0,0
11623,11624,Small Moving Window Calibration Models for Soft Sensing Processes with Limited History,"  Five simple soft sensor methodologies with two update conditions were
compared on two experimentally-obtained datasets and one simulated dataset. The
soft sensors investigated were moving window partial least squares regression
(and a recursive variant), moving window random forest regression, the mean
moving window of $y$, and a novel random forest partial least squares
regression ensemble (RF-PLS), all of which can be used with small sample sizes
so that they can be rapidly placed online. It was found that, on two of the
datasets studied, small window sizes led to the lowest prediction errors for
all of the moving window methods studied. On the majority of datasets studied,
the RF-PLS calibration method offered the lowest one-step-ahead prediction
errors compared to those of the other methods, and it demonstrated greater
predictive stability at larger time delays than moving window PLS alone. It was
found that both the random forest and RF-PLS methods most adequately modeled
the datasets that did not feature purely monotonic increases in property
values, but that both methods performed more poorly than moving window PLS
models on one dataset with purely monotonic property values. Other data
dependent findings are presented and discussed.
",1,0,0,1,0,0
11760,11761,"Re-entrant charge order in overdoped (Bi,Pb)$_{2.12}$Sr$_{1.88}$CuO$_{6+δ}$ outside the pseudogap regime","  Charge modulations are considered as a leading competitor of high-temperature
superconductivity in the underdoped cuprates, and their relationship to Fermi
surface reconstructions and to the pseudogap state is an important subject of
current research. Overdoped cuprates, on the other hand, are widely regarded as
conventional Fermi liquids without collective electronic order. For the
overdoped (Bi,Pb)2.12Sr1.88CuO6+{\delta} (Bi2201) high-temperature
superconductor, here we report resonant x-ray scattering measurements revealing
incommensurate charge order reflections, with correlation lengths of 40-60
lattice units, that persist up to at least 250K. Charge order is markedly more
robust in the overdoped than underdoped regime but the incommensurate wave
vectors follow a common trend; moreover it coexists with a single,
unreconstructed Fermi surface, without pseudogap or nesting features, as
determined from angle-resolved photoemission spectroscopy. This re-entrant
charge order is reproduced by model calculations that consider a strong van
Hove singularity within a Fermi liquid framework.
",0,1,0,0,0,0
8908,8909,Mask R-CNN,"  We present a conceptually simple, flexible, and general framework for object
instance segmentation. Our approach efficiently detects objects in an image
while simultaneously generating a high-quality segmentation mask for each
instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a
branch for predicting an object mask in parallel with the existing branch for
bounding box recognition. Mask R-CNN is simple to train and adds only a small
overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to
generalize to other tasks, e.g., allowing us to estimate human poses in the
same framework. We show top results in all three tracks of the COCO suite of
challenges, including instance segmentation, bounding-box object detection, and
person keypoint detection. Without bells and whistles, Mask R-CNN outperforms
all existing, single-model entries on every task, including the COCO 2016
challenge winners. We hope our simple and effective approach will serve as a
solid baseline and help ease future research in instance-level recognition.
Code has been made available at: this https URL
",1,0,0,0,0,0
18180,18181,Fast Trajectory Optimization for Legged Robots using Vertex-based ZMP Constraints,"  This paper combines the fast Zero-Moment-Point (ZMP) approaches that work
well in practice with the broader range of capabilities of a Trajectory
Optimization formulation, by optimizing over body motion, footholds and Center
of Pressure simultaneously. We introduce a vertex-based representation of the
support-area constraint, which can treat arbitrarily oriented point-, line-,
and area-contacts uniformly. This generalization allows us to create motions
such quadrupedal walking, trotting, bounding, pacing, combinations and
transitions between these, limping, bipedal walking and push-recovery all with
the same approach. This formulation constitutes a minimal representation of the
physical laws (unilateral contact forces) and kinematic restrictions (range of
motion) in legged locomotion, which allows us to generate various motion in
less than a second. We demonstrate the feasibility of the generated motions on
a real quadruped robot.
",1,0,1,0,0,0
10556,10557,Regularity of Kleinian limit sets and Patterson-Sullivan measures,"  We consider several (related) notions of geometric regularity in the context
of limit sets of geometrically finite Kleinian groups and associated
Patterson-Sullivan measures. We begin by computing the upper and lower
regularity dimensions of the Patterson-Sullivan measure, which involves
controlling the relative measure of concentric balls. We then compute the
Assouad and lower dimensions of the limit set, which involves controlling local
doubling properties. Unlike the Hausdorff, packing, and box-counting
dimensions, we show that the Assouad and lower dimensions are not necessarily
given by the Poincaré exponent.
",0,0,1,0,0,0
11233,11234,Quantum gravity corrections to the thermodynamics and phase transition of Schwarzschild black hole,"  In this work, we derive a new kind of rainbow functions, which has
generalized uncertainty principle parameter. Then, we investigate modified
thermodynamic quantities and phase transition of rainbow Schwarzschild black
hole by employing this new kind of rainbow functions. Our results demonstrate
that the effect of rainbow gravity and generalized uncertainty principle have a
great effect on the picture of Hawking radiation. It prevents black holes from
total evaporation and causes the remnant. In addition, after analyzing the the
modified local thermodynamic quantities, we find that effect of rainbow gravity
and generalized uncertainty principle lead to one first-order phase transition,
two second-order phase transitions, and two Hawking-Page-type phase transitions
in the thermodynamic system of rainbow Schwarzschild black hole.
",0,1,0,0,0,0
18667,18668,Learning to Play with Intrinsically-Motivated Self-Aware Agents,"  Infants are experts at playing, with an amazing ability to generate novel
structured behaviors in unstructured environments that lack clear extrinsic
reward signals. We seek to mathematically formalize these abilities using a
neural network that implements curiosity-driven intrinsic motivation. Using a
simple but ecologically naturalistic simulated environment in which an agent
can move and interact with objects it sees, we propose a ""world-model"" network
that learns to predict the dynamic consequences of the agent's actions.
Simultaneously, we train a separate explicit ""self-model"" that allows the agent
to track the error map of its own world-model, and then uses the self-model to
adversarially challenge the developing world-model. We demonstrate that this
policy causes the agent to explore novel and informative interactions with its
environment, leading to the generation of a spectrum of complex behaviors,
including ego-motion prediction, object attention, and object gathering.
Moreover, the world-model that the agent learns supports improved performance
on object dynamics prediction, detection, localization and recognition tasks.
Taken together, our results are initial steps toward creating flexible
autonomous agents that self-supervise in complex novel physical environments.
",0,0,0,1,0,0
10801,10802,Couple microscale periodic patches to simulate macroscale emergent dynamics,"  This article proposes a new way to construct computationally efficient
`wrappers' around fine scale, microscopic, detailed descriptions of dynamical
systems, such as molecular dynamics, to make predictions at the macroscale
`continuum' level. It is often significantly easier to code a microscale
simulator with periodicity: so the challenge addressed here is to develop a
scheme that uses only a given periodic microscale simulator; specifically, one
for atomistic dynamics. Numerical simulations show that applying a suitable
proportional controller within `action regions' of a patch of atomistic
simulation effectively predicts the macroscale transport of heat. Theoretical
analysis establishes that such an approach will generally be effective and
efficient, and also determines good values for the strength of the proportional
controller. This work has the potential to empower systematic analysis and
understanding at a macroscopic system level when only a given microscale
simulator is available.
",0,0,1,0,0,0
3278,3279,The Effects of Superheating Treatment on Distribution of Eutectic Silicon Particles in A357-Continuous Stainless Steel Composite,"  In the present study, superheating treatment has been applied on A357
reinforced with 0.5 wt. % (Composite 1) and 1.0 wt.% (Composite 2) continuous
stainless steel composite. In Composite 1 the microstructure displayed poor
bonding between matrix and reinforcement interface. Poor bonding associated
with large voids also can be seen in Composite 1. The results also showed that
coarser eutectic silicon (Si) particles were less intensified around the
matrix-reinforcement interface. From energy dispersive spectrometry (EDS)
elemental mapping, it was clearly shown that the distribution of eutectic Si
particles were less concentrated at poor bonding regions associated with large
voids. Meanwhile in Composite 2, the microstructure displayed good bonding
combined with more concentrated finer eutectic Si particles around the
matrix-reinforcement interface. From EDS elemental mapping, it was clearly
showed more concentrated of eutectic Si particles were distributed at the good
bonding area. The superheating prior to casting has influenced the
microstructure and tends to produce finer, rounded and preferred oriented
{\alpha}-Al dendritic structures.
",0,1,0,0,0,0
2024,2025,Optimal designs for enzyme inhibition kinetic models,"  In this paper we present a new method for determining optimal designs for
enzyme inhibition kinetic models, which are used to model the influence of the
concentration of a substrate and an inhibition on the velocity of a reaction.
The approach uses a nonlinear transformation of the vector of predictors such
that the model in the new coordinates is given by an incomplete response
surface model. Although there exist no explicit solutions of the optimal design
problem for incomplete response surface models so far, the corresponding design
problem in the new coordinates is substantially more transparent, such that
explicit or numerical solutions can be determined more easily. The designs for
the original problem can finally be found by an inverse transformation of the
optimal designs determined for the response surface model. We illustrate the
method determining explicit solutions for the $D$-optimal design and for the
optimal design problem for estimating the individual coefficients in a
non-competitive enzyme inhibition kinetic model.
",0,0,1,1,0,0
13115,13116,"Logically Isolated, Actually Unpredictable? Measuring Hypervisor Performance in Multi-Tenant SDNs","  Ideally, by enabling multi-tenancy, network virtualization allows to improve
resource utilization, while providing performance isolation: although the
underlying resources are shared, the virtual network appears as a dedicated
network to the tenant. However, providing such an illusion is challenging in
practice, and over the last years, many expedient approaches have been proposed
to provide performance isolation in virtual networks, by enforcing bandwidth
reservations. We in this paper study another source for overheads and
unpredictable performance in virtual networks: the hypervisor.
The hypervisor is a critical component in multi-tenant environments, but its
overhead and influence on performance are hardly understood today. In
particular, we focus on OpenFlow-based virtualized Software Defined Networks
(vSDNs). Network virtualization is considered a killer application for SDNs: a
vSDN allows each tenant to flexibly manage its network from a logically
centralized perspective, via a simple API. For the purpose of our study, we
developed a new benchmarking tool for OpenFlow control and data planes,
enabling high and consistent OpenFlow message rates. Using our tool, we
identify and measure controllable and uncontrollable effects on performance and
overhead, including the hypervisor technology, the number of tenants as well as
the tenant type, as well as the type of OpenFlow messages.
",1,0,0,0,0,0
9626,9627,Policy Evaluation and Optimization with Continuous Treatments,"  We study the problem of policy evaluation and learning from batched
contextual bandit data when treatments are continuous, going beyond previous
work on discrete treatments. Previous work for discrete treatment/action spaces
focuses on inverse probability weighting (IPW) and doubly robust (DR) methods
that use a rejection sampling approach for evaluation and the equivalent
weighted classification problem for learning. In the continuous setting, this
reduction fails as we would almost surely reject all observations. To tackle
the case of continuous treatments, we extend the IPW and DR approaches to the
continuous setting using a kernel function that leverages treatment proximity
to attenuate discrete rejection. Our policy estimator is consistent and we
characterize the optimal bandwidth. The resulting continuous policy optimizer
(CPO) approach using our estimator achieves convergent regret and approaches
the best-in-class policy for learnable policy classes. We demonstrate that the
estimator performs well and, in particular, outperforms a discretization-based
benchmark. We further study the performance of our policy optimizer in a case
study on personalized dosing based on a dataset of Warfarin patients, their
covariates, and final therapeutic doses. Our learned policy outperforms
benchmarks and nears the oracle-best linear policy.
",0,0,0,1,0,0
17096,17097,"The effect of the environment on the structure, morphology and star-formation history of intermediate-redshift galaxies","  With the aim of understanding the effect of the environment on the star
formation history and morphological transformation of galaxies, we present a
detailed analysis of the colour, morphology and internal structure of cluster
and field galaxies at $0.4 \le z \le 0.8$. We use {\em HST} data for over 500
galaxies from the ESO Distant Cluster Survey (EDisCS) to quantify how the
galaxies' light distribution deviate from symmetric smooth profiles. We
visually inspect the galaxies' images to identify the likely causes for such
deviations. We find that the residual flux fraction ($RFF$), which measures the
fractional contribution to the galaxy light of the residuals left after
subtracting a symmetric and smooth model, is very sensitive to the degree of
structural disturbance but not the causes of such disturbance. On the other
hand, the asymmetry of these residuals ($A_{\rm res}$) is more sensitive to the
causes of the disturbance, with merging galaxies having the highest values of
$A_{\rm res}$. Using these quantitative parameters we find that, at a fixed
morphology, cluster and field galaxies show statistically similar degrees of
disturbance. However, there is a higher fraction of symmetric and passive
spirals in the cluster than in the field. These galaxies have smoother light
distributions than their star-forming counterparts. We also find that while
almost all field and cluster S0s appear undisturbed, there is a relatively
small population of star-forming S0s in clusters but not in the field. These
findings are consistent with relatively gentle environmental processes acting
on galaxies infalling onto clusters.
",0,1,0,0,0,0
5087,5088,Continuous CM-regularity of semihomogeneous vector bundles,"  We show that if $X$ is an abelian variety of dimension $g \geq 1$ and
${\mathcal E}$ is an M-regular coherent sheaf on $X$, the Castelnuovo-Mumford
regularity of ${\mathcal E}$ with respect to an ample and globally generated
line bundle ${\mathcal O}(1)$ on $X$ is at most $g$, and that equality is
obtained when ${\mathcal E}^{\vee}(1)$ is continuously globally generated. As
an application, we give a numerical characterization of ample semihomogeneous
vector bundles for which this bound is attained.
",0,0,1,0,0,0
15292,15293,The mapping class groups of reducible Heegaard splittings of genus two,"  The manifold which admits a genus-$2$ reducible Heegaard splitting is one of
the $3$-sphere, $\mathbb{S}^2 \times \mathbb{S}^1$, lens spaces and their
connected sums. For each of those manifolds except most lens spaces, the
mapping class group of the genus-$2$ splitting was shown to be finitely
presented. In this work, we study the remaining generic lens spaces, and show
that the mapping class group of the genus-$2$ Heegaard splitting is finitely
presented for any lens space by giving its explicit presentation. As an
application, we show that the fundamental groups of the spaces of the genus-$2$
Heegaard splittings of lens spaces are all finitely presented.
",0,0,1,0,0,0
15829,15830,Average sampling and average splines on combinatorial graphs,"  In the setting of a weighted combinatorial finite or infinite countable graph
$G$ we introduce functional Paley-Wiener spaces $PW_{\omega}(L),\>\omega>0,$
defined in terms of the spectral resolution of the combinatorial Laplace
operator $L$ in the space $L_{2}(G)$. It is shown that functions in certain
$PW_{\omega}(L),\>\omega>0,$ are uniquely defined by their averages over some
families of ""small"" subgraphs which form a cover of $G$. Reconstruction methods
for reconstruction of an $f\in PW_{\omega}(L)$ from appropriate set of its
averages are introduced. One method is using language of Hilbert frames.
Another one is using average variational interpolating splines which are
constructed in the setting of combinatorial graphs.
",1,0,0,0,0,0
8741,8742,A Proximal Block Coordinate Descent Algorithm for Deep Neural Network Training,"  Training deep neural networks (DNNs) efficiently is a challenge due to the
associated highly nonconvex optimization. The backpropagation (backprop)
algorithm has long been the most widely used algorithm for gradient computation
of parameters of DNNs and is used along with gradient descent-type algorithms
for this optimization task. Recent work have shown the efficiency of block
coordinate descent (BCD) type methods empirically for training DNNs. In view of
this, we propose a novel algorithm based on the BCD method for training DNNs
and provide its global convergence results built upon the powerful framework of
the Kurdyka-Lojasiewicz (KL) property. Numerical experiments on standard
datasets demonstrate its competitive efficiency against standard optimizers
with backprop.
",0,0,0,1,0,0
8738,8739,Detecting Recycled Commodity SoCs: Exploiting Aging-Induced SRAM PUF Unreliability,"  A physical unclonable function (PUF), analogous to a human fingerprint, has
gained an enormous amount of attention from both academia and industry. SRAM
PUF is among one of the popular silicon PUF constructions that exploits random
initial power-up states from SRAM cells to extract hardware intrinsic secrets
for identification and key generation applications. The advantage of SRAM PUFs
is that they are widely embedded into commodity devices, thus such a PUF is
obtained without a custom design and virtually free of implementation costs. A
phenomenon known as `aging' alters the consistent
reproducibility---reliability---of responses that can be extracted from a
readout of a set of SRAM PUF cells. Similar to how a PUF exploits undesirable
manufacturing randomness for generating a hardware intrinsic fingerprint, SRAM
PUF unreliability induced by aging can be exploited to detect recycled
commodity devices requiring no additional cost to the device. In this context,
the SRAM PUF itself acts as an aging sensor by exploiting responses sensitive
to aging. We use SRAMs available in pervasively deployed commercial
off-the-shelf micro-controllers for experimental validations, which complements
recent work demonstrated in FPGA platforms, and we present a simplified
detection methodology along experimental results. We show that less than 1,000
SRAM responses are adequate to guarantee that both false acceptance rate and
false rejection rate are no more than 0.001.
",1,0,0,0,0,0
645,646,Data-Driven Sparse Structure Selection for Deep Neural Networks,"  Deep convolutional neural networks have liberated its extraordinary power on
various tasks. However, it is still very challenging to deploy state-of-the-art
models into real-world applications due to their high computational complexity.
How can we design a compact and effective network without massive experiments
and expert knowledge? In this paper, we propose a simple and effective
framework to learn and prune deep models in an end-to-end manner. In our
framework, a new type of parameter -- scaling factor is first introduced to
scale the outputs of specific structures, such as neurons, groups or residual
blocks. Then we add sparsity regularizations on these factors, and solve this
optimization problem by a modified stochastic Accelerated Proximal Gradient
(APG) method. By forcing some of the factors to zero, we can safely remove the
corresponding structures, thus prune the unimportant parts of a CNN. Comparing
with other structure selection methods that may need thousands of trials or
iterative fine-tuning, our method is trained fully end-to-end in one training
pass without bells and whistles. We evaluate our method, Sparse Structure
Selection with several state-of-the-art CNNs, and demonstrate very promising
results with adaptive depth and width selection.
",1,0,0,0,0,0
1687,1688,"""Noiseless"" thermal noise measurement of atomic force microscopy cantilevers","  When measuring quadratic values representative of random fluctuations, such
as the thermal noise of Atomic Force Microscopy (AFM) cantilevers, the
background measurement noise cannot be averaged to zero. We present a signal
processing method that allows to get rid of this limitation using the
ubiquitous optical beam deflection sensor of standard AFMs. We demonstrate a
two orders of magnitude enhancement of the signal to noise ratio in our
experiment, allowing the calibration of stiff cantilevers or easy
identification of higher order modes from thermal noise measurements.
",0,1,0,0,0,0
17713,17714,Light propagation in Extreme Conditions - The role of optically clear tissues and scattering layers in optical biomedical imaging,"  The field of biomedical imaging has undergone a rapid growth in recent years,
mostly due to the implementation of ad-hoc designed experimental setups,
theoretical support methods and numerical reconstructions. Especially for
biological samples, the high number of scattering events occurring during the
photon propagation process limit the penetration depth and the possibility to
outperform direct imaging in thicker and not transparent samples. In this
thesis, we will examine theoretically and experimentally the scattering process
from two opposite points of view, focusing also on the continuous stimulus
offered by the will to tackle some specific challenges in the emerging optical
imaging science. Firstly, we will discuss the light propagation in diffusive
biological tissues considering the particular case of the presence of optically
transparent regions enclosed in a highly scattering environment. The correct
inclusion of this information, can ultimately lead to higher resolution
reconstruction, especially in neuroimaging. On the other hand, we will examine
the extreme case of the three-dimensional imaging of a totally hidden sample,
in which the phase has been scrambled by a random scattering layer. By making
use of appropriate numerical methods, we will prove how it is possible to
outperform such hidden reconstruction in a very efficient way, opening the path
toward the unexplored field of three-dimensional hidden imaging. Finally, we
will present how, the properties noticed while addressing these problems,
leaded us to the development of a novel alignment-free three-dimensional
tomographic technique that we refer to as Phase-Retrieved Tomography.
Ultimately, we used this technique for the study of the fluorescence
distribution in a three-dimensional spherical tumor model, the cancer cell
spheroid, one of the most important biological model for the study of such
disease.
",0,1,0,0,0,0
13286,13287,A Compressive Sensing Approach to Community Detection with Applications,"  The community detection problem for graphs asks one to partition the n
vertices V of a graph G into k communities, or clusters, such that there are
many intracluster edges and few intercluster edges. Of course this is
equivalent to finding a permutation matrix P such that, if A denotes the
adjacency matrix of G, then PAP^T is approximately block diagonal. As there are
k^n possible partitions of n vertices into k subsets, directly determining the
optimal clustering is clearly infeasible. Instead one seeks to solve a more
tractable approximation to the clustering problem. In this paper we reformulate
the community detection problem via sparse solution of a linear system
associated with the Laplacian of a graph G and then develop a two-stage
approach based on a thresholding technique and a compressive sensing algorithm
to find a sparse solution which corresponds to the community containing a
vertex of interest in G. Crucially, our approach results in an algorithm which
is able to find a single cluster of size n_0 in O(nlog(n)n_0) operations and
all k clusters in fewer than O(n^2ln(n)) operations. This is a marked
improvement over the classic spectral clustering algorithm, which is unable to
find a single cluster at a time and takes approximately O(n^3) operations to
find all k clusters. Moreover, we are able to provide robust guarantees of
success for the case where G is drawn at random from the Stochastic Block
Model, a popular model for graphs with clusters. Extensive numerical results
are also provided, showing the efficacy of our algorithm on both synthetic and
real-world data sets.
",1,0,0,1,0,0
20934,20935,Statistical study on propagation characteristics of Omega signals (VLF) in magnetosphere detected by the Akebono satellite,"  This paper shows a statistical analysis of 10.2 kHz Omega broadcasts of an
artificial signal broadcast from ground stations, propagated in the
plasmasphere, and detected using an automatic detection method we developed. We
study the propagation patterns of the Omega signals to understand the
propagation characteristics that are strongly affected by plasmaspheric
electron density and the ambient magnetic field. We show the unique propagation
patterns of the Omega 10.2 kHz signal when it was broadcast from two
high-middle-latitude stations. We use about eight years of data captured by the
Poynting flux analyzer subsystem on board the Akebono satellite from October
1989 to September 1997. We demonstrate that the signals broadcast from almost
the same latitude (in geomagnetic coordinates) propagated differently depending
on the geographic latitude. We also study propagation characteristics as a
function of local time, season, and solar activity. The Omega signal tended to
propagate farther on the nightside than on the dayside and was more widely
distributed during winter than during summer. When solar activity was at
maximum, the Omega signal propagated at a lower intensity level. In contrast,
when solar activity was at minimum, the Omega signal propagated at a higher
intensity and farther from the transmitter station.
",0,1,0,0,0,0
3578,3579,Conjoined constraints on modified gravity from the expansion history and cosmic growth,"  In this paper we present conjoined constraints on several cosmological models
from the expansion history $H(z)$ and cosmic growth $f\sigma_8(z)$. The models
we study include the CPL $w_0w_a$ parametrization, the Holographic Dark Energy
(HDE) model, the Time varying vacuum ($\Lambda_t$CDM) model, the Dvali,
Gabadadze and Porrati (DGP) and Finsler-Randers (FRDE) models, a power law
$f(T)$ model and finally the Hu-Sawicki $f(R)$ model. In all cases we perform a
simultaneous fit to the SnIa, CMB, BAO, $H(z)$ and growth data, while also
following the conjoined visualization of $H(z)$ and $f\sigma_8(z)$ as in Linder
(2017). Furthermore, we introduce the Figure of Merit (FoM) in the
$H(z)-f\sigma_8(z)$ parameter space as a way to constrain models that jointly
fit both probes well. We use both the latest $H(z)$ and $f\sigma_8(z)$ data,
but also LSST-like mocks with $1\%$ measurements and we find that the conjoined
method of constraining the expansion history and cosmic growth simultaneously
is able not only to place stringent constraints on these parameters but also to
provide an easy visual way to discriminate cosmological models. Finally, we
confirm the existence of a tension between the growth rate and Planck CMB data
and we find that the FoM in the conjoined parameter space of
$H(z)-f\sigma_8(z)$ can be used to discriminate between the $\Lambda$CDM model
and certain classes of modified gravity models, namely the DGP and $f(T)$.
",0,1,0,0,0,0
1151,1152,Injectivity almost everywhere and mappings with finite distortion in nonlinear elasticity,"  We show that a sufficient condition for the weak limit of a sequence of
$W^1_q$-homeomorphisms with finite distortion to be almost everywhere injective
for $q \geq n-1$, can be stated by means of composition operators. Applying
this result, we study nonlinear elasticity problems with respect to these new
classes of mappings. Furthermore, we impose loose growth conditions on the
stored-energy function for the class of $W^1_n$-homeomorphisms with finite
distortion and integrable inner as well as outer distortion coefficients.
",0,0,1,0,0,0
16894,16895,Demonstration of the Relationship between Sensitivity and Identifiability for Inverse Uncertainty Quantification,"  Inverse Uncertainty Quantification (UQ), or Bayesian calibration, is the
process to quantify the uncertainties of random input parameters based on
experimental data. The introduction of model discrepancy term is significant
because ""over-fitting"" can theoretically be avoided. But it also poses
challenges in the practical applications. One of the mostly concerned and
unresolved problem is the ""lack of identifiability"" issue. With the presence of
model discrepancy, inverse UQ becomes ""non-identifiable"" in the sense that it
is difficult to precisely distinguish between the parameter uncertainties and
model discrepancy when estimating the calibration parameters. Previous research
to alleviate the non-identifiability issue focused on using informative priors
for the calibration parameters and the model discrepancy, which is usually not
a viable solution because one rarely has such accurate and informative prior
knowledge. In this work, we show that identifiability is largely related to the
sensitivity of the calibration parameters with regards to the chosen responses.
We adopted an improved modular Bayesian approach for inverse UQ that does not
require priors for the model discrepancy term. The relationship between
sensitivity and identifiability was demonstrated with a practical example in
nuclear engineering. It was shown that, in order for a certain calibration
parameter to be statistically identifiable, it should be significant to at
least one of the responses whose data are used for inverse UQ. Good
identifiability cannot be achieved for a certain calibration parameter if it is
not significant to any of the responses. It is also demonstrated that ""fake
identifiability"" is possible if model responses are not appropriately chosen,
or inaccurate but informative priors are specified.
",0,0,0,1,0,0
573,574,Improved Quantile Regression Estimators when the Errors are Independently and Non-identically Distributed,"  In a classical regression model, it is usually assumed that the explanatory
variables are independent of each other and error terms are normally
distributed. But when these assumptions are not met, situations like the error
terms are not independent or they are not identically distributed or both of
these, LSE will not be robust. Hence, quantile regression has been used to
complement this deficiency of classical regression analysis and to improve the
least square estimation (LSE). In this study, we consider preliminary test and
shrinkage estimation strategies for quantile regression models with
independently and non-identically distributed (i.ni.d.) errors. A Monte Carlo
simulation study is conducted to assess the relative performance of the
estimators. Also, we numerically compare their performance with Ridge, Lasso,
Elastic Net penalty estimation strategies. A real data example is presented to
illustrate the usefulness of the suggested methods. Finally, we obtain the
asymptotic results of suggested estimators
",0,0,1,1,0,0
11256,11257,The curtain remains open: NGC 2617 continues in a high state,"  Optical and near-infrared photometry, optical spectroscopy, and soft X-ray
and UV monitoring of the changing look active galactic nucleus NGC 2617 show
that it continues to have the appearance of a type-1 Seyfert galaxy. An optical
light curve for 2010-2016 indicates that the change of type probably occurred
between 2010 October and 2012 February and was not related to the brightening
in 2013. In 2016 NGC 2617 brightened again to a level of activity close to that
in 2013 April. We find variations in all passbands and in both the intensities
and profiles of the broad Balmer lines. A new displaced emission peak has
appeared in H$\beta$. X-ray variations are well correlated with UV-optical
variability and possibly lead by $\sim$ 2-3 d. The $K$ band lags the $J$ band
by about 21.5 $\pm$ 2.5 d. and lags the combined $B+J$ filters by $\sim$ 25 d.
$J$ lags $B$ by about 3 d. This could be because $J$-band variability arises
from the outer part of the accretion disc, while $K$-band variability comes
from thermal re-emission by dust. We propose that spectral-type changes are a
result of increasing central luminosity causing sublimation of the innermost
dust in the hollow biconical outflow. We briefly discuss various other possible
reasons that might explain the dramatic changes in NGC 2617.
",0,1,0,0,0,0
19059,19060,Deep Learning Approximation: Zero-Shot Neural Network Speedup,"  Neural networks offer high-accuracy solutions to a range of problems, but are
costly to run in production systems because of computational and memory
requirements during a forward pass. Given a trained network, we propose a
techique called Deep Learning Approximation to build a faster network in a tiny
fraction of the time required for training by only manipulating the network
structure and coefficients without requiring re-training or access to the
training data. Speedup is achieved by by applying a sequential series of
independent optimizations that reduce the floating-point operations (FLOPs)
required to perform a forward pass. First, lossless optimizations are applied,
followed by lossy approximations using singular value decomposition (SVD) and
low-rank matrix decomposition. The optimal approximation is chosen by weighing
the relative accuracy loss and FLOP reduction according to a single parameter
specified by the user. On PASCAL VOC 2007 with the YOLO network, we show an
end-to-end 2x speedup in a network forward pass with a 5% drop in mAP that can
be re-gained by finetuning.
",0,0,0,1,0,0
15878,15879,DeepVisage: Making face recognition simple yet with powerful generalization skills,"  Face recognition (FR) methods report significant performance by adopting the
convolutional neural network (CNN) based learning methods. Although CNNs are
mostly trained by optimizing the softmax loss, the recent trend shows an
improvement of accuracy with different strategies, such as task-specific CNN
learning with different loss functions, fine-tuning on target dataset, metric
learning and concatenating features from multiple CNNs. Incorporating these
tasks obviously requires additional efforts. Moreover, it demotivates the
discovery of efficient CNN models for FR which are trained only with identity
labels. We focus on this fact and propose an easily trainable and single CNN
based FR method. Our CNN model exploits the residual learning framework.
Additionally, it uses normalized features to compute the loss. Our extensive
experiments show excellent generalization on different datasets. We obtain very
competitive and state-of-the-art results on the LFW, IJB-A, YouTube faces and
CACD datasets.
",1,0,0,0,0,0
18910,18911,Autonomous Extracting a Hierarchical Structure of Tasks in Reinforcement Learning and Multi-task Reinforcement Learning,"  Reinforcement learning (RL), while often powerful, can suffer from slow
learning speeds, particularly in high dimensional spaces. The autonomous
decomposition of tasks and use of hierarchical methods hold the potential to
significantly speed up learning in such domains. This paper proposes a novel
practical method that can autonomously decompose tasks, by leveraging
association rule mining, which discovers hidden relationship among entities in
data mining. We introduce a novel method called ARM-HSTRL (Association Rule
Mining to extract Hierarchical Structure of Tasks in Reinforcement Learning).
It extracts temporal and structural relationships of sub-goals in RL, and
multi-task RL. In particular,it finds sub-goals and relationship among them. It
is shown the significant efficiency and performance of the proposed method in
two main topics of RL.
",1,0,0,0,0,0
17260,17261,Superzone gap formation and low lying crystal electric field levels in PrPd$_2$Ge$_2$ single crystal,"  The magnetocrystalline anisotropy exhibited in PrPd$_2$Ge$_2$ single crystal
has been investigated by measuring the magnetization, magnetic susceptibility,
electrical resistivity and heat capacity. PrPd$_2$Ge$_2$ crystallizes in the
well known ThCr$_2$Si$_2$\--type tetragonal structure. The antiferromagnetic
ordering is confirmed as 5.1~K with the [001]-axis as the easy axis of
magnetization. A superzone gap formation is observed from the electrical
resistivity measurement when the current is passed along the [001] direction.
The crystal electric field (CEF) analysis on the magnetic susceptibility,
magnetization and the heat capacity measurements confirms a doublet ground
state with a relatively low over all CEF level splitting. The CEF level
spacings and the Zeeman splitting at high fields become comparable and lead to
metamagnetic transition at 34~T due to the CEF level crossing.
",0,1,0,0,0,0
4856,4857,Variable selection for clustering with Gaussian mixture models: state of the art,"  The mixture models have become widely used in clustering, given its
probabilistic framework in which its based, however, for modern databases that
are characterized by their large size, these models behave disappointingly in
setting out the model, making essential the selection of relevant variables for
this type of clustering. After recalling the basics of clustering based on a
model, this article will examine the variable selection methods for model-based
clustering, as well as presenting opportunities for improvement of these
methods.
",1,0,0,1,0,0
19618,19619,Explicit equations for two-dimensional water waves with constant vorticity,"  Governing equations for two-dimensional inviscid free-surface flows with
constant vorticity over arbitrary non-uniform bottom profile are presented in
exact and compact form using conformal variables. An efficient and very
accurate numerical method for this problem is developed.
",0,1,0,0,0,0
5228,5229,Rational motivic path spaces and Kim's relative unipotent section conjecture,"  We initiate a study of path spaces in the nascent context of ""motivic dga's"",
under development in doctoral work by Gabriella Guzman. This enables us to
reconstruct the unipotent fundamental group of a pointed scheme from the
associated augmented motivic dga, and provides us with a factorization of Kim's
relative unipotent section conjecture into several smaller conjectures with a
homotopical flavor. Based on a conversation with Joseph Ayoub, we prove that
the path spaces of the punctured projective line over a number field are
concentrated in degree zero with respect to Levine's t-structure for mixed Tate
motives. This constitutes a step in the direction of Kim's conjecture.
",0,0,1,0,0,0
19003,19004,Rigorous Analysis for Efficient Statistically Accurate Algorithms for Solving Fokker-Planck Equations in Large Dimensions,"  This article presents a rigorous analysis for efficient statistically
accurate algorithms for solving the Fokker-Planck equations associated with
high-dimensional nonlinear turbulent dynamical systems with conditional
Gaussian structures. Despite the conditional Gaussianity, these nonlinear
systems contain many strong non-Gaussian features such as intermittency and
fat-tailed probability density functions (PDFs). The algorithms involve a
hybrid strategy that requires only a small number of samples $L$ to capture
both the transient and the equilibrium non-Gaussian PDFs with high accuracy.
Here, a conditional Gaussian mixture in a high-dimensional subspace via an
extremely efficient parametric method is combined with a judicious Gaussian
kernel density estimation in the remaining low-dimensional subspace. Rigorous
analysis shows that the mean integrated squared error in the recovered PDFs in
the high-dimensional subspace is bounded by the inverse square root of the
determinant of the conditional covariance, where the conditional covariance is
completely determined by the underlying dynamics and is independent of $L$.
This is fundamentally different from a direct application of kernel methods to
solve the full PDF, where $L$ needs to increase exponentially with the
dimension of the system and the bandwidth shrinks. A detailed comparison
between different methods justifies that the efficient statistically accurate
algorithms are able to overcome the curse of dimensionality. It is also shown
with mathematical rigour that these algorithms are robust in long time provided
that the system is controllable and stochastically stable. Particularly,
dynamical systems with energy-conserving quadratic nonlinearity as in many
geophysical and engineering turbulence are proved to have these properties.
",0,0,1,1,0,0
19947,19948,Eckart ro-vibrational Hamiltonians via the gateway Hamilton operator: theory and practice,"  Recently, a general expression for Eckart-frame Hamilton operators has been
obtained by the gateway Hamiltonian method ({\it J. Chem. Phys.} {\bf 142},
174107 (2015); {\it ibid.} {\bf 143}, 064104 (2015)). The kinetic energy
operator in this general Hamiltonian is nearly identical with that of the
Eckart-Watson operator even when curvilinear vibrational coordinates are
employed. Its different realizations correspond to different methods of
calculating Eckart displacements. There are at least two different methods for
calculating such displacements: rotation and projection. In this communication
the application of Eckart Hamiltonian operators constructed by rotation and
projection, respectively, is numerically demonstrated in calculating
vibrational energy levels. The numerical examples confirm that there is no need
for rotation to construct an Eckart ro-vibrational Hamiltonian. The application
of the gateway method is advantageous even when rotation is used, since it
obviates the need for differentiation of the matrix rotating into the Eckart
frame. Simple geometrical arguments explain that there are infinitely many
different methods for calculating Eckart displacements. The geometrical picture
also suggests that a unique Eckart displacement vector may be defined as the
shortest (mass-weighted) Eckart displacement vector among Eckart displacement
vectors corresponding to configurations related by rotation. Its length, as
shown analytically and demonstrated by way of numerical examples, is equal to
or less than that of the Eckart displacement vector one can obtain by rotation
to the Eckart frame.
",0,1,0,0,0,0
525,526,Deuterium fractionation and H2D+ evolution in turbulent and magnetized cloud cores,"  High-mass stars are expected to form from dense prestellar cores. Their
precise formation conditions are widely discussed, including their virial
condition, which results in slow collapse for super-virial cores with strong
support by turbulence or magnetic fields, or fast collapse for sub-virial
sources. To disentangle their formation processes, measurements of the
deuterium fractions are frequently employed to approximately estimate the ages
of these cores and to obtain constraints on their dynamical evolution. We here
present 3D magneto-hydrodynamical simulations including for the first time an
accurate non-equilibrium chemical network with 21 gas-phase species plus dust
grains and 213 reactions. With this network we model the deuteration process in
fully depleted prestellar cores in great detail and determine its response to
variations in the initial conditions. We explore the dependence on the initial
gas column density, the turbulent Mach number, the mass-to-magnetic flux ratio
and the distribution of the magnetic field, as well as the initial
ortho-to-para ratio of H2. We find excellent agreement with recent observations
of deuterium fractions in quiescent sources. Our results show that deuteration
is rather efficient, even when assuming a conservative ortho-to-para ratio of 3
and highly sub-virial initial conditions, leading to large deuterium fractions
already within roughly a free-fall time. We discuss the implications of our
results and give an outlook to relevant future investigations.
",0,1,0,0,0,0
1082,1083,Dispersive Regimes of the Dicke Model,"  We study two dispersive regimes in the dynamics of $N$ two-level atoms
interacting with a bosonic mode for long interaction times. Firstly, we analyze
the dispersive multiqubit quantum Rabi model for the regime in which the qubit
frequencies are equal and smaller than the mode frequency, and for values of
the coupling strength similar or larger than the mode frequency, namely, the
deep strong coupling regime. Secondly, we address an interaction that is
dependent on the photon number, where the coupling strength is comparable to
the geometric mean of the qubit and mode frequencies. We show that the
associated dynamics is analytically tractable and provide useful frameworks
with which to analyze the system behavior. In the deep strong coupling regime,
we unveil the structure of unexpected resonances for specific values of the
coupling, present for $N\ge2$, and in the photon-number-dependent regime we
demonstrate that all the nontrivial dynamical behavior occurs in the atomic
degrees of freedom for a given Fock state. We verify these assertions with
numerical simulations of the qubit population and photon-statistic dynamics.
",0,1,0,0,0,0
845,846,Value added or misattributed? A multi-institution study on the educational benefit of labs for reinforcing physics content,"  Instructional labs are widely seen as a unique, albeit expensive, way to
teach scientific content. We measured the effectiveness of introductory lab
courses at achieving this educational goal across nine different lab courses at
three very different institutions. These institutions and courses encompassed a
broad range of student populations and instructional styles. The nine courses
studied had two key things in common: the labs aimed to reinforce the content
presented in lectures, and the labs were optional. By comparing the performance
of students who did and did not take the labs (with careful normalization for
selection effects), we found universally and precisely no added value to
learning from taking the labs as measured by course exam performance. This work
should motivate institutions and departments to reexamine the goals and conduct
of their lab courses, given their resource-intensive nature. We show why these
results make sense when looking at the comparative mental processes of students
involved in research and instructional labs, and offer alternative goals and
instructional approaches that would make lab courses more educationally
valuable.
",0,1,0,0,0,0
14813,14814,Pressure Induced Superconductivity in the New Compound ScZrCo1-$δ$,"  It is widely perceived that the correlation effect may play an important role
in several unconventional superconducting families, such as cuprate, iron-based
and heavy-fermion superconductors. The application of high pressure can tune
the ground state properties and balance the localization and itineracy of
electrons in correlated systems, which may trigger unconventional
superconductivity. Moreover, non-centrosymmetric structure may induce the spin
triplet pairing which is very rare in nature. Here, we report a new compound
ScZrCo1-${\delta}$ crystallizing in the Ti2Ni structure with the space group of
FD3-MS without a spatial inversion center. The resistivity of the material at
ambient pressure shows a bad metal and weak semiconducting behavior.
Furthermore, specific heat and magnetic susceptibility measurements yield a
rather large value of Wilson ratio ~4.47. Both suggest a ground state with
correlation effect. By applying pressure, the up-going behavior of resistivity
in lowering temperature at ambient pressure is suppressed and gradually it
becomes metallic. At a pressure of about 19.5 GPa superconductivity emerges. Up
to 36.05 GPa, a superconducting transition at about 3.6 K with a quite high
upper critical field is observed. Our discovery here provides a new platform
for investigating the relationship between correlation effect and
superconductivity.
",0,1,0,0,0,0
5508,5509,An adelic arithmeticity theorem for lattices in products,"  We prove that, under mild assumptions, a lattice in a product of semi-simple
Lie group and a totally disconnected locally compact group is, in a certain
sense, arithmetic. We do not assume the lattice to be finitely generated or the
ambient group to be compactly generated.
",0,0,1,0,0,0
4956,4957,The transition matrix between the Specht and web bases is unipotent with additional vanishing entries,"  We compare two important bases of an irreducible representation of the
symmetric group: the web basis and the Specht basis. The web basis has its
roots in the Temperley-Lieb algebra and knot-theoretic considerations. The
Specht basis is a classic algebraic and combinatorial construction of symmetric
group representations which arises in this context through the geometry of
varieties called Springer fibers. We describe a graph that encapsulates
combinatorial relations between each of these bases, prove that there is a
unique way (up to scaling) to map the Specht basis into the web representation,
and use this to recover a result of Garsia-McLarnan that the transition matrix
between the Specht and web bases is upper-triangular with ones along the
diagonal. We then strengthen their result to prove vanishing of certain
additional entries unless a nesting condition on webs is satisfied. In fact we
conjecture that the entries of the transition matrix are nonnegative and are
nonzero precisely when certain directed paths exist in the web graph.
",0,0,1,0,0,0
17257,17258,One Model To Learn Them All,"  Deep learning yields great results across many fields, from speech
recognition, image classification, to translation. But for each problem,
getting a deep model to work well involves research into the architecture and a
long period of tuning. We present a single model that yields good results on a
number of problems spanning multiple domains. In particular, this single model
is trained concurrently on ImageNet, multiple translation tasks, image
captioning (COCO dataset), a speech recognition corpus, and an English parsing
task. Our model architecture incorporates building blocks from multiple
domains. It contains convolutional layers, an attention mechanism, and
sparsely-gated layers. Each of these computational blocks is crucial for a
subset of the tasks we train on. Interestingly, even if a block is not crucial
for a task, we observe that adding it never hurts performance and in most cases
improves it on all tasks. We also show that tasks with less data benefit
largely from joint training with other tasks, while performance on large tasks
degrades only slightly if at all.
",1,0,0,1,0,0
13097,13098,SIFM: A network architecture for seamless flow mobility between LTE and WiFi networks - Analysis and Testbed Implementation,"  This paper deals with cellular (e.g. LTE) networks that selectively offload
the mobile data traffic onto WiFi (IEEE 802.11) networks to improve network
performance. We propose the Seamless Internetwork Flow Mobility (SIFM)
architecture that provides seamless flow-mobility support using concepts of
Software Defined Networking (SDN). The SDN paradigm decouples the control and
data plane, leading to a centralized network intelligence and state. The SIFM
architecture utilizes this aspect of SDN and moves the mobility decisions to a
centralized Flow Controller (FC). This provides a global network view while
making mobility decisions and also reduces the complexity at the PGW. We
implement and evaluate both basic PMIPv6 and the SIFM architectures by
incorporating salient LTE and WiFi network features in the ns-3 simulator.
Performance experiments validate that seamless mobility is achieved. Also, the
SIFM architecture shows an improved network performance when compared to the
base PMIPv6 architecture. A proof-of-concept prototype of the SIFM architecture
has been implemented on an experimental testbed. The LTE network is emulated by
integrating USRP B210x with the OpenLTE eNodeB and OpenLTE EPC. The WiFi
network is emulated using hostapd and dnsmasq daemons running on Ubuntu 12.04.
An off-the-shelf LG G2 mobile phone running Android 4.2.2 is used as the user
equipment. We demonstrate seamless mobility between the LTE network and the
WiFi network with the help of ICMP ping and a TCP chat application.
",1,0,0,0,0,0
14176,14177,Profile Estimation for Partial Functional Partially Linear Single-Index Model,"  This paper studies a \textit{partial functional partially linear single-index
model} that consists of a functional linear component as well as a linear
single-index component. This model generalizes many well-known existing models
and is suitable for more complicated data structures. However, its estimation
inherits the difficulties and complexities from both components and makes it a
challenging problem, which calls for new methodology. We propose a novel
profile B-spline method to estimate the parameters by approximating the unknown
nonparametric link function in the single-index component part with B-spline,
while the linear slope function in the functional component part is estimated
by the functional principal component basis. The consistency and asymptotic
normality of the parametric estimators are derived, and the global convergence
of the proposed estimator of the linear slope function is also established.
More excitingly, the latter convergence is optimal in the minimax sense. A
two-stage procedure is implemented to estimate the nonparametric link function,
and the resulting estimator possesses the optimal global rate of convergence.
Furthermore, the convergence rate of the mean squared prediction error for a
predictor is also obtained. Empirical properties of the proposed procedures are
studied through Monte Carlo simulations. A real data example is also analyzed
to illustrate the power and flexibility of the proposed methodology.
",0,0,1,1,0,0
4255,4256,Counterfactual Reasoning with Disjunctive Knowledge in a Linear Structural Equation Model,"  We consider the problem of estimating counterfactual quantities when prior
knowledge is available in the form of disjunctive statements. These include
disjunction of conditions (e.g., ""the patient is more than 60 years of age"") as
well as disjuction of antecedants (e.g., ""had the patient taken either drug A
or drug B""). Focusing on linear structural equation models (SEM) and imperfect
control plans, we extend the counterfactual framework of Balke and Pearl (1995)
, Chen and Pearl (2015), and Pearl (2009, pp. 389-391) from unconditional to
conditional plans, from a univariate treatment to a set of treatments, and from
point type knowledge to disjunctive knowledge. Finally, we provide improved
matrix representations of the resulting counterfactual parameters, and improved
computational methods of their evaluation.
",0,0,0,1,0,0
16131,16132,Incorporating genuine prior information about between-study heterogeneity in random effects pairwise and network meta-analyses,"  Background: Pairwise and network meta-analyses using fixed effect and random
effects models are commonly applied to synthesise evidence from randomised
controlled trials. The models differ in their assumptions and the
interpretation of the results. The model choice depends on the objective of the
analysis and knowledge of the included studies. Fixed effect models are often
used because there are too few studies with which to estimate the between-study
standard deviation from the data alone. Objectives: The aim is to propose a
framework for eliciting an informative prior distribution for the between-study
standard deviation in a Bayesian random effects meta-analysis model to
genuinely represent heterogeneity when data are sparse. Methods: We developed
an elicitation method using external information such as empirical evidence and
experts' beliefs on the 'range' of treatment effects in order to infer the
prior distribution for the between-study standard deviation. We also developed
the method to be implemented in R. Results: The three-stage elicitation
approach allows uncertainty to be represented by a genuine prior distribution
to avoid making misleading inferences. It is flexible to what judgments an
expert can provide, and is applicable to all types of outcome measure for which
a treatment effect can be constructed on an additive scale. Conclusions: The
choice between using a fixed effect or random effects meta-analysis model
depends on the inferences required and not on the number of available studies.
Our elicitation framework captures external evidence about heterogeneity and
overcomes the often implausible assumption that studies are estimating the same
treatment effect, thereby improving the quality of inferences in decision
making.
",0,0,0,1,0,0
18545,18546,ROCKER: A Refinement Operator for Key Discovery,"  The Linked Data principles provide a decentral approach for publishing
structured data in the RDF format on the Web. In contrast to structured data
published in relational databases where a key is often provided explicitly,
finding a set of properties that allows identifying a resource uniquely is a
non-trivial task. Still, finding keys is of central importance for manifold
applications such as resource deduplication, link discovery, logical data
compression and data integration. In this paper, we address this research gap
by specifying a refinement operator, dubbed ROCKER, which we prove to be
finite, proper and non-redundant. We combine the theoretical characteristics of
this operator with two monotonicities of keys to obtain a time-efficient
approach for detecting keys, i.e., sets of properties that describe resources
uniquely. We then utilize a hash index to compute the discriminability score
efficiently. Therewith, we ensure that our approach can scale to very large
knowledge bases. Results show that ROCKER yields more accurate results, has a
comparable runtime, and consumes less memory w.r.t. existing state-of-the-art
techniques.
",1,0,0,0,0,0
20613,20614,Multi-Player Bandits: A Trekking Approach,"  We study stochastic multi-armed bandits with many players. The players do not
know the number of players, cannot communicate with each other and if multiple
players select a common arm they collide and none of them receive any reward.
We consider the static scenario, where the number of players remains fixed, and
the dynamic scenario, where the players enter and leave at any time. We provide
algorithms based on a novel `trekking approach' that guarantees constant regret
for the static case and sub-linear regret for the dynamic case with high
probability. The trekking approach eliminates the need to estimate the number
of players resulting in fewer collisions and improved regret performance
compared to the state-of-the-art algorithms. We also develop an epoch-less
algorithm that eliminates any requirement of time synchronization across the
players provided each player can detect the presence of other players on an
arm. We validate our theoretical guarantees using simulation based and real
test-bed based experiments.
",0,0,0,1,0,0
9262,9263,Iterative PET Image Reconstruction Using Convolutional Neural Network Representation,"  PET image reconstruction is challenging due to the ill-poseness of the
inverse problem and limited number of detected photons. Recently deep neural
networks have been widely and successfully used in computer vision tasks and
attracted growing interests in medical imaging. In this work, we trained a deep
residual convolutional neural network to improve PET image quality by using the
existing inter-patient information. An innovative feature of the proposed
method is that we embed the neural network in the iterative reconstruction
framework for image representation, rather than using it as a post-processing
tool. We formulate the objective function as a constraint optimization problem
and solve it using the alternating direction method of multipliers (ADMM)
algorithm. Both simulation data and hybrid real data are used to evaluate the
proposed method. Quantification results show that our proposed iterative neural
network method can outperform the neural network denoising and conventional
penalized maximum likelihood methods.
",0,1,0,1,0,0
1362,1363,Domain Generalization by Marginal Transfer Learning,"  Domain generalization is the problem of assigning class labels to an
unlabeled test data set, given several labeled training data sets drawn from
similar distributions. This problem arises in several applications where data
distributions fluctuate because of biological, technical, or other sources of
variation. We develop a distribution-free, kernel-based approach that predicts
a classifier from the marginal distribution of features, by leveraging the
trends present in related classification tasks. This approach involves
identifying an appropriate reproducing kernel Hilbert space and optimizing a
regularized empirical risk over the space. We present generalization error
analysis, describe universal kernels, and establish universal consistency of
the proposed methodology. Experimental results on synthetic data and three real
data applications demonstrate the superiority of the method with respect to a
pooling strategy.
",0,0,0,1,0,0
4259,4260,Spinless hourglass nodal-line semimetals,"  Nodal-line semimetals, one of the topological semimetals, have degeneracy
along nodal lines where the band gap is closed. In many cases, the nodal lines
appear accidentally, and in such cases it is impossible to determine whether
the nodal lines appear or not, only from the crystal symmetry and the electron
filling. In this paper, for spinless systems, we show that in specific space
groups at $4N+2$ fillings ($8N+4$ fillings including the spin degree of
freedom), presence of the nodal lines is required regardless of the details of
the systems. Here, the spinless systems refer to crystals where the spin-orbit
coupling is negligible and the spin degree of freedom can be omitted because of
the SU(2) spin degeneracy. In this case the shape of the band structure around
these nodal lines is like an hourglass, and we call this a spinless hourglass
nodal-line semimetal. We construct a model Hamiltonian as an example and we
show that it is always in the spinless hourglass nodal-line semimetal phase
even when the model parameters are changed without changing the symmetries of
the system. We also establish a list of all the centrosymmetric space groups,
under which spinless systems always have hourglass nodal lines, and illustrate
where the nodal lines are located. We propose that Al$_3$FeSi$_2$, whose
space-group symmetry is Pbcn (No. 60), is one of the nodal-line semimetals
arising from this mechanism.
",0,1,0,0,0,0
11812,11813,Three dimensional free-surface flow over arbitrary bottom topography,"  We consider steady nonlinear free surface flow past an arbitrary bottom
topography in three dimensions, concentrating on the shape of the wave pattern
that forms on the surface of the fluid. Assuming ideal fluid flow, the problem
is formulated using a boundary integral method and discretised to produce a
nonlinear system of algebraic equations. The Jacobian of this system is dense
due to integrals being evaluated over the entire free surface. To overcome the
computational difficulty and large memory requirements, a Jacobian-free Newton
Krylov (JFNK) method is utilised. Using a block-banded approximation of the
Jacobian from the linearised system as a preconditioner for the JFNK scheme, we
find significant reductions in computational time and memory required for
generating numerical solutions. These improvements also allow for a larger
number of mesh points over the free surface and the bottom topography. We
present a range of numerical solutions for both subcritical and supercritical
regimes, and for a variety of bottom configurations. We discuss nonlinear
features of the wave patterns as well as their relationship to ship wakes.
",0,1,0,0,0,0
3350,3351,Adversarial Variational Optimization of Non-Differentiable Simulators,"  Complex computer simulators are increasingly used across fields of science as
generative models tying parameters of an underlying theory to experimental
observations. Inference in this setup is often difficult, as simulators rarely
admit a tractable density or likelihood function. We introduce Adversarial
Variational Optimization (AVO), a likelihood-free inference algorithm for
fitting a non-differentiable generative model incorporating ideas from
generative adversarial networks, variational optimization and empirical Bayes.
We adapt the training procedure of generative adversarial networks by replacing
the differentiable generative network with a domain-specific simulator. We
solve the resulting non-differentiable minimax problem by minimizing
variational upper bounds of the two adversarial objectives. Effectively, the
procedure results in learning a proposal distribution over simulator
parameters, such that the JS divergence between the marginal distribution of
the synthetic data and the empirical distribution of observed data is
minimized. We evaluate and compare the method with simulators producing both
discrete and continuous data.
",1,0,0,1,0,0
8476,8477,Test results of a prototype device to calibrate the Large Size Telescope camera proposed for the Cherenkov Telescope Array,"  A Large Size air Cherenkov Telescope (LST) prototype, proposed for the
Cherenkov Telescope Array (CTA), is under construction at the Canary Island of
La Palma (Spain) this year. The LST camera, which comprises an array of about
500 photomultipliers (PMTs), requires a precise and regular calibration over a
large dynamic range, up to $10^3$ photo-electrons (pe's), for each PMT. We
present a system built to provide the optical calibration of the camera
consisting of a pulsed laser (355 nm wavelength, 400 ps pulse width), a set of
filters to guarantee a large dynamic range of photons on the sensors, and a
diffusing sphere to uniformly spread the laser light, with flat fielding within
3%, over the camera focal plane 28 m away. The prototype of the system
developed at INFN is hermetically closed and filled with dry air to make the
system completely isolated from the external environment. In the paper we
present the results of the tests for the evaluation of the photon density at
the camera plane, the system isolation from the environment, and the shape of
the signal as detected by the PMTs. The description of the communication of the
system with the rest of detector is also given.
",0,1,0,0,0,0
7386,7387,Dispersive Magnetic and Electronic Excitations in Iridate Perovskites Probed with Oxygen $K$-Edge Resonant Inelastic X-ray Scattering,"  Resonant inelastic X-ray scattering (RIXS) experiments performed at the
oxygen-$K$ edge on the iridate perovskites {\SIOS} and {\SION} reveal a
sequence of well-defined dispersive modes over the energy range up to $\sim
0.8$ eV. The momentum dependence of these modes and their variation with the
experimental geometry allows us to assign each of them to specific collective
magnetic and/or electronic excitation processes, including single and
bi-magnons, and spin-orbit and electron-hole excitons. We thus demonstrated
that dispersive magnetic and electronic excitations are observable at the O-$K$
edge in the presence of the strong spin-orbit coupling in the $5d$ shell of
iridium and strong hybridization between Ir $5d$ and O $2p$ orbitals, which
confirm and expand theoretical expectations. More generally, our results
establish the utility of O-$K$ edge RIXS for studying the collective
excitations in a range of $5d$ materials that are attracting increasing
attention due to their novel magnetic and electronic properties. Especially,
the strong RIXS response at O-$K$ edge opens up the opportunity for
investigating collective excitations in thin films and heterostructures
fabricated from these materials.
",0,1,0,0,0,0
11276,11277,Blind Source Separation Using Mixtures of Alpha-Stable Distributions,"  We propose a new blind source separation algorithm based on mixtures of
alpha-stable distributions. Complex symmetric alpha-stable distributions have
been recently showed to better model audio signals in the time-frequency domain
than classical Gaussian distributions thanks to their larger dynamic range.
However, inference of these models is notoriously hard to perform because their
probability density functions do not have a closed-form expression in general.
Here, we introduce a novel method for estimating mixture of alpha-stable
distributions based on characteristic function matching. We apply this to the
blind estimation of binary masks in individual frequency bands from
multichannel convolutive audio mixes. We show that the proposed method yields
better separation performance than Gaussian-based binary-masking methods.
",1,0,0,1,0,0
20923,20924,Lattice Operations on Terms over Similar Signatures,"  Unification and generalization are operations on two terms computing
respectively their greatest lower bound and least upper bound when the terms
are quasi-ordered by subsumption up to variable renaming (i.e., $t_1\preceq
t_2$ iff $t_1 = t_2\sigma$ for some variable substitution $\sigma$). When term
signatures are such that distinct functor symbols may be related with a fuzzy
equivalence (called a similarity), these operations can be formally extended to
tolerate mismatches on functor names and/or arity or argument order. We
reformulate and extend previous work with a declarative approach defining
unification and generalization as sets of axioms and rules forming a complete
constraint-normalization proof system. These include the Reynolds-Plotkin
term-generalization procedures, Maria Sessa's ""weak"" unification with partially
fuzzy signatures and its corresponding generalization, as well as novel
extensions of such operations to fully fuzzy signatures (i.e., similar functors
with possibly different arities). One advantage of this approach is that it
requires no modification of the conventional data structures for terms and
substitutions. This and the fact that these declarative specifications are
efficiently executable conditional Horn-clauses offers great practical
potential for fuzzy information-handling applications.
",1,0,0,0,0,0
3075,3076,Heterogeneous inputs to central pattern generators can shape insect gaits,"  In our previous work, we studied an interconnected bursting neuron model for
insect locomotion, and its corresponding phase oscillator model, which at high
speed can generate stable tripod gaits with three legs off the ground
simultaneously in swing, and at low speed can generate stable tetrapod gaits
with two legs off the ground simultaneously in swing. However, at low speed
several other stable locomotion patterns, that are not typically observed as
insect gaits, may coexist. In the present paper, by adding heterogeneous
external input to each oscillator, we modify the bursting neuron model so that
its corresponding phase oscillator model produces only one stable gait at each
speed, specifically: a unique stable tetrapod gait at low speed, a unique
stable tripod gait at high speed, and a unique branch of stable transition
gaits connecting them. This suggests that control signals originating in the
brain and central nervous system can modify gait patterns.
",0,0,0,0,1,0
15243,15244,The square lattice Ising model on the rectangle II: Finite-size scaling limit,"  Based on the results published recently [J. Phys. A: Math. Theor. 50, 065201
(2017)], the universal finite-size contributions to the free energy of the
square lattice Ising model on the $L\times M$ rectangle, with open boundary
conditions in both directions, are calculated exactly in the finite-size
scaling limit $L,M\to\infty$, $T\to T_\mathrm{c}$, with fixed temperature
scaling variable $x\propto(T/T_\mathrm{c}-1)M$ and fixed aspect ratio
$\rho\propto L/M$. We derive exponentially fast converging series for the
related Casimir potential and Casimir force scaling functions. At the critical
point $T=T_\mathrm{c}$ we confirm predictions from conformal field theory by
Cardy & Peschel [Nucl. Phys. B 300, 377 (1988)] and by Kleban & Vassileva [J.
Phys. A: Math. Gen. 24, 3407 (1991)]. The presence of corners and the related
corner free energy has dramatic impact on the Casimir scaling functions and
leads to a logarithmic divergence of the Casimir potential scaling function at
criticality.
",0,1,1,0,0,0
13913,13914,A Mixture of Matrix Variate Bilinear Factor Analyzers,"  Over the years data has become increasingly higher dimensional, which has
prompted an increased need for dimension reduction techniques. This is perhaps
especially true for clustering (unsupervised classification) as well as
semi-supervised and supervised classification. Although dimension reduction in
the area of clustering for multivariate data has been quite thoroughly
discussed within the literature, there is relatively little work in the area of
three-way, or matrix variate, data. Herein, we develop a mixture of matrix
variate bilinear factor analyzers (MMVBFA) model for use in clustering
high-dimensional matrix variate data. This work can be considered both the
first matrix variate bilinear factor analysis model as well as the first MMVBFA
model. Parameter estimation is discussed, and the MMVBFA model is illustrated
using simulated and real data.
",0,0,0,1,0,0
7011,7012,Geohyperbolic Routing and Addressing Schemes,"  The key requirement to routing in any telecommunication network, and
especially in Internet-of-Things (IoT) networks, is scalability. Routing must
route packets between any source and destination in the network without
incurring unmanageable routing overhead that grows quickly with increasing
network size and dynamics. Here we present an addressing scheme and a coupled
network topology design scheme that guarantee essentially optimal routing
scalability. The FIB sizes are as small as they can be, equal to the number of
adjacencies a node has, while the routing control overhead is minimized as
nearly zero routing control messages are exchanged even upon catastrophic
failures in the network. The key new ingredient is the addressing scheme, which
is purely local, based only on geographic coordinates of nodes and a centrality
measure, and does not require any sophisticated non-local computations or
global network topology knowledge for network embedding. The price paid for
these benefits is that network topology cannot be arbitrary but should follow a
specific design, resulting in Internet-like topologies. The proposed schemes
can be most easily deployed in overlay networks, and also in other network
deployments, where geolocation information is available, and where network
topology can grow following the design specifications.
",1,1,0,0,0,0
18112,18113,Linear convergence of SDCA in statistical estimation,"  In this paper, we consider stochastic dual coordinate (SDCA) {\em without}
strongly convex assumption or convex assumption. We show that SDCA converges
linearly under mild conditions termed restricted strong convexity. This covers
a wide array of popular statistical models including Lasso, group Lasso, and
logistic regression with $\ell_1$ regularization, corrected Lasso and linear
regression with SCAD regularizer. This significantly improves previous
convergence results on SDCA for problems that are not strongly convex. As a by
product, we derive a dual free form of SDCA that can handle general
regularization term, which is of interest by itself.
",1,0,0,1,0,0
20351,20352,The Mass Transference Principle: Ten Years On,"  In this article we discuss the Mass Transference Principle due to Beresnevich
and Velani and survey several generalisations and variants, both deterministic
and random. Using a Hausdorff measure analogue of the inhomogeneous
Khintchine-Groshev Theorem, proved recently via an extension of the Mass
Transference Principle to systems of linear forms, we give an alternative proof
of a general inhomogeneous Jarn\'{\i}k-Besicovitch Theorem which was originally
proved by Levesley. We additionally show that without monotonicity Levesley's
theorem no longer holds in general. Thereafter, we discuss recent advances by
Wang, Wu and Xu towards mass transference principles where one transitions from
$\limsup$ sets defined by balls to $\limsup$ sets defined by rectangles (rather
than from ""balls to balls"" as is the case in the original Mass Transference
Principle). Furthermore, we consider mass transference principles for
transitioning from rectangles to rectangles and extend known results using a
slicing technique. We end this article with a brief survey of random analogues
of the Mass Transference Principle.
",0,0,1,0,0,0
6190,6191,Photonic Band Structure of Two-dimensional Atomic Lattices,"  Two-dimensional atomic arrays exhibit a number of intriguing quantum optical
phenomena, including subradiance, nearly perfect reflection of radiation and
long-lived topological edge states. Studies of emission and scattering of
photons in such lattices require complete treatment of the radiation pattern
from individual atoms, including long-range interactions. We describe a
systematic approach to perform the calculations of collective energy shifts and
decay rates in the presence of such long-range interactions for arbitrary
two-dimensional atomic lattices. As applications of our method, we investigate
the topological properties of atomic lattices both in free-space and near
plasmonic surfaces.
",0,1,0,0,0,0
3773,3774,Robust and Flexible Estimation of Stochastic Mediation Effects: A Proposed Method and Example in a Randomized Trial Setting,"  Causal mediation analysis can improve understanding of the mechanisms
underlying epidemiologic associations. However, the utility of natural direct
and indirect effect estimation has been limited by the assumption of no
confounder of the mediator-outcome relationship that is affected by prior
exposure---an assumption frequently violated in practice. We build on recent
work that identified alternative estimands that do not require this assumption
and propose a flexible and double robust semiparametric targeted minimum
loss-based estimator for data-dependent stochastic direct and indirect effects.
The proposed method treats the intermediate confounder affected by prior
exposure as a time-varying confounder and intervenes stochastically on the
mediator using a distribution which conditions on baseline covariates and
marginalizes over the intermediate confounder. In addition, we assume the
stochastic intervention is given, conditional on observed data, which results
in a simpler estimator and weaker identification assumptions. We demonstrate
the estimator's finite sample and robustness properties in a simple simulation
study. We apply the method to an example from the Moving to Opportunity
experiment. In this application, randomization to receive a housing voucher is
the treatment/instrument that influenced moving to a low-poverty neighborhood,
which is the intermediate confounder. We estimate the data-dependent stochastic
direct effect of randomization to the voucher group on adolescent marijuana use
not mediated by change in school district and the stochastic indirect effect
mediated by change in school district. We find no evidence of mediation. Our
estimator is easy to implement in standard statistical software, and we provide
annotated R code to further lower implementation barriers.
",0,0,0,1,0,0
10468,10469,Fundamental bounds on MIMO antennas,"  Antenna current optimization is often used to analyze the optimal performance
of antennas. Antenna performance can be quantified in e.g., minimum Q-factor
and efficiency. The performance of MIMO antennas is more involved and, in
general, a single parameter is not sufficient to quantify it. Here, the
capacity of an idealized channel is used as the main performance quantity. An
optimization problem in the current distribution for optimal capacity, measured
in spectral efficiency, given a fixed Q-factor and efficiency is formulated as
a semi-definite optimization problem. A model order reduction based on
characteristic and energy modes is employed to improve the computational
efficiency. The performance bound is illustrated by solving the optimization
problem numerically for rectangular plates and spherical shells.
",0,1,1,0,0,0
12297,12298,Position-sensitive propagation of information on social media using social physics approach,"  The excitement and convergence of tweets on specific topics are well studied.
However, by utilizing the position information of Tweet, it is also possible to
analyze the position-sensitive tweet. In this research, we focus on bomb
terrorist attacks and propose a method for separately analyzing the number of
tweets at the place where the incident occurred, nearby, and far. We made
measurements of position-sensitive tweets and suggested a theory to explain it.
This theory is an extension of the mathematical model of the hit phenomenon.
",1,1,0,0,0,0
9119,9120,The Dynamic Geometry of Interaction Machine: A Call-by-need Graph Rewriter,"  Girard's Geometry of Interaction (GoI), a semantics designed for linear logic
proofs, has been also successfully applied to programming language semantics.
One way is to use abstract machines that pass a token on a fixed graph along a
path indicated by the GoI. These token-passing abstract machines are space
efficient, because they handle duplicated computation by repeating the same
moves of a token on the fixed graph. Although they can be adapted to obtain
sound models with regard to the equational theories of various evaluation
strategies for the lambda calculus, it can be at the expense of significant
time costs. In this paper we show a token-passing abstract machine that can
implement evaluation strategies for the lambda calculus, with certified time
efficiency. Our abstract machine, called the Dynamic GoI Machine (DGoIM),
rewrites the graph to avoid replicating computation, using the token to find
the redexes. The flexibility of interleaving token transitions and graph
rewriting allows the DGoIM to balance the trade-off of space and time costs.
This paper shows that the DGoIM can implement call-by-need evaluation for the
lambda calculus by using a strategy of interleaving token passing with as much
graph rewriting as possible. Our quantitative analysis confirms that the DGoIM
with this strategy of interleaving the two kinds of possible operations on
graphs can be classified as ""efficient"" following Accattoli's taxonomy of
abstract machines.
",1,0,0,0,0,0
19879,19880,The fundamental Lepage form in variational theory for submanifolds,"  A setting for global variational geometry on Grassmann fibrations is
presented. The integral variational functionals for finite dimensional immersed
submanifolds are studied by means of the fundamental Lepage equivalent of a
homogeneous Lagrangian, which can be regarded as a generalization of the
well-known Hilbert form in the classical mechanics. Prolongations of
immersions, diffeomorphisms and vector fields to the Grassmann fibrations are
introduced as geometric tools for the variations of immersions. The first
infinitesimal variation formula together with its consequences, the
Euler-Lagrange equations for extremal submanifolds and the Noether theorem for
invariant variational functionals are proved. The theory is illustrated on the
variational functional for minimal submanifolds.
",0,0,1,0,0,0
1230,1231,Injective stabilization of additive functors. I. Preliminaries,"  This paper is the first one in a series of three dealing with the concept of
injective stabilization of the tensor product and its applications. Its primary
goal is to collect known facts and establish a basic operational calculus that
will be used in the subsequent parts. This is done in greater generality than
is necessary for the stated goal. Several results of independent interest are
also established. They include, among other things, connections with
satellites, an explicit construction of the stabilization of a finitely
presented functor, various exactness properties of the injectively stable
functors, a construction, from a functor and a short exact sequence, of a
doubly-infinite exact sequence by splicing the injective stabilization of the
functor and its derived functors. When specialized to the tensor product with a
finitely presented module, the injective stabilization with coefficients in the
ring is isomorphic to the 1-torsion functor. The Auslander-Reiten formula is
extended to a more general formula, which holds for arbitrary (i.e., not
necessarily finite) modules over arbitrary associative rings with identity.
Weakening of the assumptions in the theorems of Eilenberg and Watts leads to
characterizations of the requisite zeroth derived functors.
The subsequent papers, provide applications of the developed techniques.
Part~II deals with new notions of torsion module and cotorsion module of a
module. This is done for arbitrary modules over arbitrary rings. Part~III
introduces a new concept, called the asymptotic stabilization of the tensor
product. The result is closely related to different variants of stable homology
(these are generalizations of Tate homology to arbitrary rings). A comparison
transformation from Vogel homology to the asymptotic stabilization of the
tensor product is constructed and shown to be epic.
",0,0,1,0,0,0
10804,10805,Generalised Seiberg-Witten equations and almost-Hermitian geometry,"  In this article, we study a generalisation of the Seiberg-Witten equations,
replacing the spinor representation with a hyperKahler manifold equipped with
certain symmetries. Central to this is the construction of a (non-linear) Dirac
operator acting on the sections of the non-linear fibre-bundle. For hyperKahler
manifolds admitting a hyperKahler potential, we derive a transformation formula
for the Dirac operator under the conformal change of metric on the base
manifold.
As an application, we show that when the hyperKahler manifold is of dimension
four, then away from a singular set, the equations can be expressed as a second
order PDE in terms of almost-complex structure on the base manifold and a
conformal factor. This extends a result of Donaldson to generalised
Seiberg-Witten equations.
",0,0,1,0,0,0
13375,13376,Simulation Methods for Stochastic Storage Problems: A Statistical Learning Perspective,"  We consider solution of stochastic storage problems through regression Monte
Carlo (RMC) methods. Taking a statistical learning perspective, we develop the
dynamic emulation algorithm (DEA) that unifies the different existing
approaches in a single modular template. We then investigate the two central
aspects of regression architecture and experimental design that constitute DEA.
For the regression piece, we discuss various non-parametric approaches, in
particular introducing the use of Gaussian process regression in the context of
stochastic storage. For simulation design, we compare the performance of
traditional design (grid discretization), against space-filling, and several
adaptive alternatives. The overall DEA template is illustrated with multiple
examples drawing from natural gas storage valuation and optimal control of
back-up generator in a microgrid.
",0,0,0,0,0,1
19009,19010,From Principal Subspaces to Principal Components with Linear Autoencoders,"  The autoencoder is an effective unsupervised learning model which is widely
used in deep learning. It is well known that an autoencoder with a single
fully-connected hidden layer, a linear activation function and a squared error
cost function trains weights that span the same subspace as the one spanned by
the principal component loading vectors, but that they are not identical to the
loading vectors. In this paper, we show how to recover the loading vectors from
the autoencoder weights.
",0,0,0,1,0,0
3627,3628,"Hidden Truncation Hyperbolic Distributions, Finite Mixtures Thereof, and Their Application for Clustering","  A hidden truncation hyperbolic (HTH) distribution is introduced and finite
mixtures thereof are applied for clustering. A stochastic representation of the
HTH distribution is given and a density is derived. A hierarchical
representation is described, which aids in parameter estimation. Finite
mixtures of HTH distributions are presented and their identifiability is
proved. The convexity of the HTH distribution is discussed, which is important
in clustering applications, and some theoretical results in this direction are
presented. The relationship between the HTH distribution and other skewed
distributions in the literature is discussed. Illustrations are provided ---
both of the HTH distribution and application of finite mixtures thereof for
clustering.
",0,0,0,1,0,0
3516,3517,"Don't Panic! Better, Fewer, Syntax Errors for LR Parsers","  Syntax errors are generally easy to fix for humans, but not for parsers, in
general, and LR parsers, in particular. Traditional 'panic mode' error
recovery, though easy to implement and applicable to any grammar, often leads
to a cascading chain of errors that drown out the original. More advanced error
recovery techniques suffer less from this problem but have seen little
practical use because their typical performance was seen as poor, their worst
case unbounded, and the repairs they reported arbitrary. In this paper we show
two generic error recovery algorithms that fix all three problems. First, our
algorithms are the first to report the complete set of possible repair
sequences for a given location, allowing programmers to select the one that
best fits their intention. Second, on a corpus of 200,000 real-world
syntactically invalid Java programs, we show that our best performing algorithm
is able to repair 98.71% of files within a cut-off of 0.5s. Furthermore, we are
also able to use the complete set of repair sequences to reduce the cascading
error problem even further than previous approaches. Our best performing
algorithm reports 442,252.0 error locations in the corpus to the user, while
the panic mode algorithm reports 980,848.0 error locations: in other words, our
algorithms reduce the cascading error problem by well over half.
",1,0,0,0,0,0
16293,16294,Riemannian curvature measures,"  A famous theorem of Weyl states that if $M$ is a compact submanifold of
euclidean space, then the volumes of small tubes about $M$ are given by a
polynomial in the radius $r$, with coefficients that are expressible as
integrals of certain scalar invariants of the curvature tensor of $M$ with
respect to the induced metric. It is natural to interpret this phenomenon in
terms of curvature measures and smooth valuations, in the sense of Alesker,
canonically associated to the Riemannian structure of $M$. This perspective
yields a fundamental new structure in Riemannian geometry, in the form of a
certain abstract module over the polynomial algebra $\mathbb R[t]$ that
reflects the behavior of Alesker multiplication. This module encodes a key
piece of the array of kinematic formulas of any Riemannian manifold on which a
group of isometries acts transitively on the sphere bundle. We illustrate this
principle in precise terms in the case where $M$ is a complex space form.
",0,0,1,0,0,0
2781,2782,Klein-Gordonization: mapping superintegrable quantum mechanics to resonant spacetimes,"  We describe a procedure naturally associating relativistic Klein-Gordon
equations in static curved spacetimes to non-relativistic quantum motion on
curved spaces in the presence of a potential. Our procedure is particularly
attractive in application to (typically, superintegrable) problems whose energy
spectrum is given by a quadratic function of the energy level number, since for
such systems the spacetimes one obtains possess evenly spaced, resonant spectra
of frequencies for scalar fields of a certain mass. This construction emerges
as a generalization of the previously studied correspondence between the Higgs
oscillator and Anti-de Sitter spacetime, which has been useful for both
understanding weakly nonlinear dynamics in Anti-de Sitter spacetime and
algebras of conserved quantities of the Higgs oscillator. Our conversion
procedure (""Klein-Gordonization"") reduces to a nonlinear elliptic equation
closely reminiscent of the one emerging in relation to the celebrated Yamabe
problem of differential geometry. As an illustration, we explicitly demonstrate
how to apply this procedure to superintegrable Rosochatius systems, resulting
in a large family of spacetimes with resonant spectra for massless wave
equations.
",0,1,1,0,0,0
2089,2090,Multipermutation Ulam Sphere Analysis Toward Characterizing Maximal Code Size,"  Permutation codes, in the form of rank modulation, have shown promise for
applications such as flash memory. One of the metrics recently suggested as
appropriate for rank modulation is the Ulam metric, which measures the minimum
translocation distance between permutations. Multipermutation codes have also
been proposed as a generalization of permutation codes that would improve code
size (and consequently the code rate). In this paper we analyze the Ulam metric
in the context of multipermutations, noting some similarities and differences
between the Ulam metric in the context of permutations. We also consider sphere
sizes for multipermutations under the Ulam metric and resulting bounds on code
size.
",1,0,1,0,0,0
3024,3025,Normality of the Thue--Morse sequence along Piatetski-Shapiro sequences,"  We prove that for $1<c<4/3$ the subsequence of the Thue--Morse sequence
$\mathbf t$ indexed by $\lfloor n^c\rfloor$ defines a normal sequence, that is,
each finite sequence $(\varepsilon_0,\ldots,\varepsilon_{T-1})\in \{0,1\}^T$
occurs as a contiguous subsequence of the sequence $n\mapsto \mathbf
t\left(\lfloor n^c\rfloor\right)$ with asymptotic frequency $2^{-T}$.
",0,0,1,0,0,0
14877,14878,Smooth and Efficient Policy Exploration for Robot Trajectory Learning,"  Many policy search algorithms have been proposed for robot learning and
proved to be practical in real robot applications. However, there are still
hyperparameters in the algorithms, such as the exploration rate, which requires
manual tuning. The existing methods to design the exploration rate manually or
automatically may not be general enough or hard to apply in the real robot. In
this paper, we propose a learning model to update the exploration rate
adaptively. The overall algorithm is a combination of methods proposed by other
researchers. Smooth trajectories for the robot can be produced by the algorithm
and the updated exploration rate maximizes the lower bound of the expected
return. Our method is tested in the ball-in-cup problem. The results show that
our method can receive the same learning outcome as the previous methods but
with fewer iterations.
",1,0,0,0,0,0
13940,13941,Threshold-activated transport stabilizes chaotic populations to steady states,"  We explore Random Scale-Free networks of populations, modelled by chaotic
Ricker maps, connected by transport that is triggered when population density
in a patch is in excess of a critical threshold level. Our central result is
that threshold-activated dispersal leads to stable fixed populations, for a
wide range of threshold levels. Further, suppression of chaos is facilitated
when the threshold-activated migration is more rapid than the intrinsic
population dynamics of a patch. Additionally, networks with large number of
nodes open to the environment, readily yield stable steady states. Lastly we
demonstrate that in networks with very few open nodes, the degree and
betweeness centrality of the node open to the environment has a pronounced
influence on control. All qualitative trends are corroborated by quantitative
measures, reflecting the efficiency of control, and the width of the steady
state window.
",0,1,0,0,0,0
4621,4622,Interplay of Fluorescence and Phosphorescence in Organic Biluminescent Emitters,"  Biluminescent organic emitters show simultaneous fluorescence and
phosphorescence at room temperature. So far, the optimization of the room
temperature phosphorescence (RTP) in these materials has drawn the attention of
research. However, the continuous wave operation of these emitters will
consequently turn them into systems with vastly imbalanced singlet and triplet
populations, which is due to the respective excited state lifetimes. This study
reports on the exciton dynamics of the biluminophore NPB
(N,N-di(1-naphthyl)-N,N-diphenyl-(1,1-biphenyl)-4,4-diamine). In the extreme
case, the singlet and triplet exciton lifetimes stretch from 3 ns to 300 ms,
respectively. Through sample engineering and oxygen quenching experiments, the
triplet exciton density can be controlled over several orders of magnitude
allowing to studying exciton interactions between singlet and triplet
manifolds. The results show, that singlet-triplet annihilation reduces the
overall biluminescence efficiency already at moderate excitation levels.
Additionally, the presented system represents an illustrative role model to
study excitonic effects in organic materials.
",0,1,0,0,0,0
14045,14046,Thermal Pressure in Diffuse H2 Gas Measured by Herschel [C II] Emission and FUSE UV H2 Absorption,"  UV absorption studies with FUSE have observed H2 molecular gas in translucent
and diffuse clouds. Observations of the 158 micron [C II] fine structure line
with Herschel also trace the same H2 molecular gas in emission. We present [C
II] observations along 27 lines of sight (LOSs) towards target stars of which
25 have FUSE H2 UV absorption. We detect [C II] emission features in all but
one target LOS. For three Target LOSs, which are close to the Galactic plane,
we also present position-velocity maps of [C II] emission observed by HIFI in
on-the-fly spectral line mapping. We use the velocity resolved [C II] spectra
towards the target LOSs observed by FUSE to identify C II] velocity components
associated with the H2 clouds. We analyze the observed velocity integrated [C
II] spectral line intensities in terms of the densities and thermal pressures
in the H2 gas using the H2 column densities and temperatures measured by the UV
absorption data. We present the H2 gas densities and thermal pressures for 26
target LOSs and from the [C II] intensities derive a mean thermal pressure in
the range 6100 to 7700 K cm^-3 in diffuse H2 clouds. We discuss the thermal
pressures and densities towards 14 targets, comparing them to results obtained
using the UV absorption data for two other tracers CI and CO.
",0,1,0,0,0,0
17278,17279,Non-zero constant curvature factorable surfaces in pseudo-Galilean space,"  Factorable surfaces, i.e. graphs associated with the product of two functions
of one variable, constitute a wide class of surfaces. Such surfaces in the
pseudo-Galilean space with zero Gaussian and mean curvature were obtained in
[1]. In this study, we provide new classification results relating to the
factorable surfaces with non-zero Gaussian and mean curvature.
",0,0,1,0,0,0
12732,12733,On the maximum principle for a time-fractional diffusion equation,"  In this paper, we discuss the maximum principle for a time-fractional
diffusion equation $$ \partial_t^\alpha u(x,t) = \sum_{i,j=1}^n
\partial_i(a_{ij}(x)\partial_j u(x,t)) + c(x)u(x,t) + F(x,t),\ t>0,\ x \in
\Omega \subset {\mathbb R}^n$$ with the Caputo time-derivative of the order
$\alpha \in (0,1)$ in the case of the homogeneous Dirichlet boundary condition.
Compared to the already published results, our findings have two important
special features. First, we derive a maximum principle for a suitably defined
weak solution in the fractional Sobolev spaces, not for the strong solution.
Second, for the non-negative source functions $F = F(x,t)$ we prove the
non-negativity of the weak solution to the problem under consideration without
any restrictions on the sign of the coefficient $c=c(x)$ by the derivative of
order zero in the spatial differential operator. Moreover, we prove the
monotonicity of the solution with respect to the coefficient $c=c(x)$.
",0,0,1,0,0,0
15890,15891,Error analysis for global minima of semilinear optimal control problems,"  In [1] we consider an optimal control problem subject to a semilinear
elliptic PDE together with its variational discretization, where we provide a
condition which allows to decide whether a solution of the necessary first
order conditions is a global minimum. This condition can be explicitly
evaluated at the discrete level. Furthermore, we prove that if the above
condition holds uniformly with respect to the discretization parameter the
sequence of discrete solutions converges to a global solution of the
corresponding limit problem. With the present work we complement our
investigations of [1] in that we prove an error estimate for those discrete
global solutions. Numerical experiments confirm our analytical findings.
",0,0,1,0,0,0
7737,7738,Notes on complexity of packing coloring,"  A packing $k$-coloring for some integer $k$ of a graph $G=(V,E)$ is a mapping
$\varphi:V\to\{1,\ldots,k\}$ such that any two vertices $u, v$ of color
$\varphi(u)=\varphi(v)$ are in distance at least $\varphi(u)+1$. This concept
is motivated by frequency assignment problems. The \emph{packing chromatic
number} of $G$ is the smallest $k$ such that there exists a packing
$k$-coloring of $G$.
Fiala and Golovach showed that determining the packing chromatic number for
chordal graphs is \NP-complete for diameter exactly 5. While the problem is
easy to solve for diameter 2, we show \NP-completeness for any diameter at
least 3. Our reduction also shows that the packing chromatic number is hard to
approximate within $n^{{1/2}-\varepsilon}$ for any $\varepsilon > 0$.
In addition, we design an \FPT algorithm for interval graphs of bounded
diameter. This leads us to exploring the problem of finding a partial coloring
that maximizes the number of colored vertices.
",1,0,0,0,0,0
214,215,Joint Power and Admission Control based on Channel Distribution Information: A Novel Two-Timescale Approach,"  In this letter, we consider the joint power and admission control (JPAC)
problem by assuming that only the channel distribution information (CDI) is
available. Under this assumption, we formulate a new chance (probabilistic)
constrained JPAC problem, where the signal to interference plus noise ratio
(SINR) outage probability of the supported links is enforced to be not greater
than a prespecified tolerance. To efficiently deal with the chance SINR
constraint, we employ the sample approximation method to convert them into
finitely many linear constraints. Then, we propose a convex approximation based
deflation algorithm for solving the sample approximation JPAC problem. Compared
to the existing works, this letter proposes a novel two-timescale JPAC
approach, where admission control is performed by the proposed deflation
algorithm based on the CDI in a large timescale and transmission power is
adapted instantly with fast fadings in a small timescale. The effectiveness of
the proposed algorithm is illustrated by simulations.
",1,0,1,0,0,0
9195,9196,Road Detection Technique Using Filters with Application to Autonomous Driving System,"  Autonomous driving systems are broadly used equipment in the industries and
in our daily lives, they assist in production, but are majorly used for
exploration in dangerous or unfamiliar locations. Thus, for a successful
exploration, navigation plays a significant role. Road detection is an
essential factor that assists autonomous robots achieved perfect navigation.
Various techniques using camera sensors have been proposed by numerous scholars
with inspiring results, but their techniques are still vulnerable to these
environmental noises: rain, snow, light intensity and shadow. In addressing
these problems, this paper proposed to enhance the road detection system with
filtering algorithm to overcome these limitations. Normalized Differences Index
(NDI) and morphological operation are the filtering algorithms used to address
the effect of shadow and guidance and re-guidance image filtering algorithms
are used to address the effect of rain and/or snow, while dark channel image
and specular-to-diffuse are the filters used to address light intensity
effects. The experimental performance of the road detection system with
filtering algorithms was tested qualitatively and quantitatively using the
following evaluation schemes: False Negative Rate (FNR) and False Positive Rate
(FPR). Comparison results of the road detection system with and without
filtering algorithm shows the filtering algorithm's capability to suppress the
effect of environmental noises because better road/non-road classification is
achieved by the road detection system. with filtering algorithm. This
achievement has further improved path planning/region classification for
autonomous driving system
",1,0,0,0,0,0
12298,12299,The application of Monte Carlo methods for learning generalized linear model,"  Monte Carlo method is a broad class of computational algorithms that rely on
repeated random sampling to obtain numerical results. They are often used in
physical and mathematical problems and are most useful when it is difficult or
impossible to use other mathematical methods. Basically, many statisticians
have been increasingly drawn to Monte Carlo method in three distinct problem
classes: optimization, numerical integration, and generating draws from a
probability distribution. In this paper, we will introduce the Monte Carlo
method for calculating coefficients in Generalized Linear Model(GLM),
especially for Logistic Regression. Our main methods are Metropolis
Hastings(MH) Algorithms and Stochastic Approximation in Monte Carlo
Computation(SAMC). For comparison, we also get results automatically using MLE
method in R software.
",0,0,0,1,0,0
12348,12349,Instrument Orientation-Based Metrics for Surgical Skill Evaluation in Robot-Assisted and Open Needle Driving,"  The technical skill of surgeons directly impacts patient outcomes. Advanced
tracking systems enable the development of objective motion-based metrics for
skill evaluation, but these metrics are not sufficient to evaluate the
performance in complex surgical tasks. In this study, we developed metrics for
surgical skill evaluation that are based on the orientation of the surgical
instruments. Experienced robotic surgeons and novice users performed
teleoperated (using the da Vinci Research Kit) and open needle-driving. Task
time and the rate of orientation change successfully distinguished between
experienced surgeons and novice users. Path length and the normalized angular
displacement allowed for a good separation only in part of the experiment. Our
new promising metrics for surgical skill evaluation captured technical aspects
that are taught during surgeons' training. They provide complementing
evaluation to those of classical metrics. Orientation-based metrics add value
to skill assessment and may be an adjunct to classic objective metrics
providing more granular discrimination of skills.
",1,0,0,0,0,0
14138,14139,Data Modelling for the Evaluation of Virtualized Network Functions Resource Allocation Algorithms,"  To conduct a more realistic evaluation on Virtualized Network Functions
resource allocation algorithms, researches needed data on: (1) potential NFs
chains (policies), (2) traffic flows passing through these NFs chains, (3) how
the dynamic traffic changes affect the NFs (scale out/in) and (4) different
data center architectures for the NFC. However, there are no publicly available
real data sets on NF chains and traffic that pass through NF chains. Therefore
we have used data from previous empirical analyses and made some assumptions to
derive the required data to evaluate resource allocation algorithms for VNFs.
We developed four programs to model the gathered data and generate the required
data. All gathered data and data modelling programs are publicly available at
github repository.
",1,0,0,0,0,0
190,191,Topologically Invariant Double Dirac States in Bismuth based Perovskites: Consequence of Ambivalent Charge States and Covalent Bonding,"  Bulk and surface electronic structures, calculated using density functional
theory and a tight-binding model Hamiltonian, reveal the existence of two
topologically invariant (TI) surface states in the family of cubic Bi
perovskites (ABiO$_3$; A = Na, K, Rb, Cs, Mg, Ca, Sr and Ba). The two TI
states, one lying in the valence band (TI-V) and other lying in the conduction
band (TI-C) are formed out of bonding and antibonding states of the
Bi-$\{$s,p$\}$ - O-$\{$p$\}$ coordinated covalent interaction. Below a certain
critical thickness of the film, which varies with A, TI states of top and
bottom surfaces couple to destroy the Dirac type linear dispersion and
consequently to open surface energy gaps. The origin of s-p band inversion,
necessary to form a TI state, classifies the family of ABiO$_3$ into two. For
class-I (A = Na, K, Rb, Cs and Mg) the band inversion, leading to TI-C state,
is induced by spin-orbit coupling of the Bi-p states and for class-II (A = Ca,
Sr and Ba) the band inversion is induced through weak but sensitive second
neighbor Bi-Bi interactions.
",0,1,0,0,0,0
15883,15884,Sets of lengths in atomic unit-cancellative finitely presented monoids,"  For an element $a$ of a monoid $H$, its set of lengths $\mathsf L (a) \subset
\mathbb N$ is the set of all positive integers $k$ for which there is a
factorization $a=u_1 \cdot \ldots \cdot u_k$ into $k$ atoms. We study the
system $\mathcal L (H) = \{\mathsf L (a) \mid a \in H \}$ with a focus on the
unions $\mathcal U_k (H) \subset \mathbb N$ which are the unions of all sets of
lengths containing a given $k \in \mathbb N$. The Structure Theorem for Unions
-- stating that for all sufficiently large $k$, the sets $\mathcal U_k (H)$ are
almost arithmetical progressions with the same difference and global bound --
has found much attention for commutative monoids and domains. We show that it
holds true for the not necessarily commutative monoids in the title satisfying
suitable algebraic finiteness conditions. Furthermore, we give an explicit
description of the system of sets of lengths of monoids $B_{n} = \langle a,b
\mid ba=b^{n} \rangle$ for $n \in \N_{\ge 2}$. Based on this description, we
show that the monoids $B_n$ are not transfer Krull, which implies that their
systems $\mathcal L (B_n)$ are distinct from systems of sets of lengths of
commutative Krull monoids and others.
",0,0,1,0,0,0
15601,15602,Transverse-spin correlations of the random transverse-field Ising model,"  The critical behavior of the random transverse-field Ising model in finite
dimensional lattices is governed by infinite disorder fixed points, several
properties of which have already been calculated by the use of the strong
disorder renormalization group (SDRG) method. Here we extend these studies and
calculate the connected transverse-spin correlation function by a numerical
implementation of the SDRG method in $d=1,2$ and $3$ dimensions. At the
critical point an algebraic decay of the form $\sim r^{-\eta_t}$ is found, with
a decay exponent being approximately $\eta_t \approx 2+2d$. In $d=1$ the
results are related to dimer-dimer correlations in the random AF XX-chain and
have been tested by numerical calculations using free-fermionic techniques.
",0,1,0,0,0,0
12017,12018,Better Protocol for XOR Game using Communication Protocol and Nonlocal Boxes,"  Buhrman showed that an efficient communication protocol implies a reliable
XOR game protocol. This idea rederives Linial and Shraibman's lower bounds of
communication complexity, which was derived by using factorization norms, with
worse constant factor in much more intuitive way. In this work, we improve and
generalize Buhrman's idea, and obtain a class of lower bounds for classical
communication complexity including an exact Linial and Shraibman's lower bound
as a special case. In the proof, we explicitly construct a protocol for XOR
game from a classical communication protocol by using a concept of nonlocal
boxes and Paw{\l}owski et al.'s elegant protocol, which was used for showing
the violation of information causality in superquantum theories.
",1,0,1,0,0,0
20740,20741,On the maximal halfspace depth of permutation-invariant distributions on the simplex,"  We compute the maximal halfspace depth for a class of permutation-invariant
distributions on the probability simplex. The derivations are based on
stochastic ordering results that so far were only showed to be relevant for the
Behrens-Fisher problem.
",0,0,1,1,0,0
17604,17605,Shattering the glass ceiling? How the institutional context mitigates the gender gap in entrepreneurship,"  We examine how the institutional context affects the relationship between
gender and opportunity entrepreneurship. To do this, we develop a multi-level
model that connects feminist theory at the micro-level to institutional theory
at the macro-level. It is hypothesized that the gender gap in opportunity
entrepreneurship is more pronounced in low-quality institutional contexts and
less pronounced in high-quality institutional contexts. Using data from the
Global Entrepreneurship Monitor (GEM) and regulation data from the economic
freedom of the world index (EFW), we test our predictions and find evidence in
support of our model. Our findings suggest that, while there is a gender gap in
entrepreneurship, these disparities are reduced as the quality of the
institutional context improves.
",0,0,0,0,0,1
1905,1906,An Efficient Load Balancing Method for Tree Algorithms,"  Nowadays, multiprocessing is mainstream with exponentially increasing number
of processors. Load balancing is, therefore, a critical operation for the
efficient execution of parallel algorithms. In this paper we consider the
fundamental class of tree-based algorithms that are notoriously irregular, and
hard to load-balance with existing static techniques. We propose a hybrid load
balancing method using the utility of statistical random sampling in estimating
the tree depth and node count distributions to uniformly partition an input
tree. To conduct an initial performance study, we implemented the method on an
Intel Xeon Phi accelerator system. We considered the tree traversal operation
on both regular and irregular unbalanced trees manifested by Fibonacci and
unbalanced (biased) randomly generated trees, respectively. The results show
scalable performance for up to the 60 physical processors of the accelerator,
as well as an extrapolated 128 processors case.
",1,0,0,0,0,0
3182,3183,Using High-Rising Cities to Visualize Performance in Real-Time,"  For developers concerned with a performance drop or improvement in their
software, a profiler allows a developer to quickly search and identify
bottlenecks and leaks that consume much execution time. Non real-time profilers
analyze the history of already executed stack traces, while a real-time
profiler outputs the results concurrently with the execution of software, so
users can know the results instantaneously. However, a real-time profiler risks
providing overly large and complex outputs, which is difficult for developers
to quickly analyze. In this paper, we visualize the performance data from a
real-time profiler. We visualize program execution as a three-dimensional (3D)
city, representing the structure of the program as artifacts in a city (i.e.,
classes and packages expressed as buildings and districts) and their program
executions expressed as the fluctuating height of artifacts. Through two case
studies and using a prototype of our proposed visualization, we demonstrate how
our visualization can easily identify performance issues such as a memory leak
and compare performance changes between versions of a program. A demonstration
of the interactive features of our prototype is available at
this https URL.
",1,0,0,0,0,0
16814,16815,MOG: Mapper on Graphs for Relationship Preserving Clustering,"  The interconnected nature of graphs often results in difficult to interpret
clutter. Typically techniques focus on either decluttering by clustering nodes
with similar properties or grouping edges with similar relationship. We propose
using mapper, a powerful topological data analysis tool, to summarize the
structure of a graph in a way that both clusters data with similar properties
and preserves relationships. Typically, mapper operates on a given data by
utilizing a scalar function defined on every point in the data and a cover for
scalar function codomain. The output of mapper is a graph that summarize the
shape of the space. In this paper, we outline how to use this mapper
construction on an input graphs, outline three filter functions that capture
important structures of the input graph, and provide an interface for
interactively modifying the cover. To validate our approach, we conduct several
case studies on synthetic and real world data sets and demonstrate how our
method can give meaningful summaries for graphs with various complexities
",0,0,0,1,0,0
9554,9555,Black holes in vector-tensor theories,"  We study static and spherically symmetric black hole (BH) solutions in
second-order generalized Proca theories with nonminimal vector field derivative
couplings to the Ricci scalar, the Einstein tensor, and the double dual Riemann
tensor. We find concrete Lagrangians which give rise to exact BH solutions by
imposing two conditions of the two identical metric components and the constant
norm of the vector field. These exact solutions are described by either
Reissner-Nordström (RN), stealth Schwarzschild, or extremal RN solutions
with a non-trivial longitudinal mode of the vector field. We then numerically
construct BH solutions without imposing these conditions. For cubic and quartic
Lagrangians with power-law couplings which encompass vector Galileons as the
specific cases, we show the existence of BH solutions with the difference
between two non-trivial metric components. The quintic-order power-law
couplings do not give rise to non-trivial BH solutions regular throughout the
horizon exterior. The sixth-order and intrinsic vector-mode couplings can lead
to BH solutions with a secondary hair. For all the solutions, the vector field
is regular at least at the future or past horizon. The deviation from General
Relativity induced by the Proca hair can be potentially tested by future
measurements of gravitational waves in the nonlinear regime of gravity.
",0,1,0,0,0,0
1536,1537,A comprehensive study of batch construction strategies for recurrent neural networks in MXNet,"  In this work we compare different batch construction methods for mini-batch
training of recurrent neural networks. While popular implementations like
TensorFlow and MXNet suggest a bucketing approach to improve the
parallelization capabilities of the recurrent training process, we propose a
simple ordering strategy that arranges the training sequences in a stochastic
alternatingly sorted way. We compare our method to sequence bucketing as well
as various other batch construction strategies on the CHiME-4 noisy speech
recognition corpus. The experiments show that our alternated sorting approach
is able to compete both in training time and recognition performance while
being conceptually simpler to implement.
",1,0,0,1,0,0
3927,3928,A Data Science Approach to Understanding Residential Water Contamination in Flint,"  When the residents of Flint learned that lead had contaminated their water
system, the local government made water-testing kits available to them free of
charge. The city government published the results of these tests, creating a
valuable dataset that is key to understanding the causes and extent of the lead
contamination event in Flint. This is the nation's largest dataset on lead in a
municipal water system.
In this paper, we predict the lead contamination for each household's water
supply, and we study several related aspects of Flint's water troubles, many of
which generalize well beyond this one city. For example, we show that elevated
lead risks can be (weakly) predicted from observable home attributes. Then we
explore the factors associated with elevated lead. These risk assessments were
developed in part via a crowd sourced prediction challenge at the University of
Michigan. To inform Flint residents of these assessments, they have been
incorporated into a web and mobile application funded by \texttt{Google.org}.
We also explore questions of self-selection in the residential testing program,
examining which factors are linked to when and how frequently residents
voluntarily sample their water.
",1,0,0,1,0,0
16720,16721,"Self-Trapping of G-Mode Oscillations in Relativistic Thin Disks, Revisited","  We examine by a perturbation method how the self-trapping of g-mode
oscillations in geometrically thin relativistic disks is affected by uniform
vertical magnetic fields. Disks which we consider are isothermal in the
vertical direction, but are truncated at a certain height by presence of hot
coronae. We find that the characteristics of self-trapping of axisymmetric
g-mode oscillations in non-magnetized disks is kept unchanged in magnetized
disks at least till a strength of the fields, depending on vertical thickness
of disks. These magnetic fields become stronger as the disk becomes thinner.
This result suggests that trapped g-mode oscillations still remain as one of
possible candidates of quasi-periodic oscillations observed in black-hole and
neutron-star X-ray binaries in the cases where vertical magnetic fields in
disks are weak.
",0,1,0,0,0,0
6644,6645,Obstacle Avoidance Using Stereo Camera,"  In this paper we present a novel method for obstacle avoidance using the
stereo camera. The conventional obstacle avoidance methods and their
limitations are discussed. A new algorithm is developed for the real-time
obstacle avoidance which responds faster to unexpected obstacles. In this
approach the depth map is divided into optimized number of regions and the
minimum depth at each section is assigned as the depth of that region. A fuzzy
controller is designed to create the drive commands for the robot/quadcopter.
The system was tested on multiple paths with different obstacles and the
results demonstrated the high accuracy of the developed system.
",1,0,0,0,0,0
19549,19550,Learning Universal Adversarial Perturbations with Generative Models,"  Neural networks are known to be vulnerable to adversarial examples, inputs
that have been intentionally perturbed to remain visually similar to the source
input, but cause a misclassification. It was recently shown that given a
dataset and classifier, there exists so called universal adversarial
perturbations, a single perturbation that causes a misclassification when
applied to any input. In this work, we introduce universal adversarial
networks, a generative network that is capable of fooling a target classifier
when it's generated output is added to a clean sample from a dataset. We show
that this technique improves on known universal adversarial attacks.
",1,0,0,1,0,0
17977,17978,A Statistical Perspective on Inverse and Inverse Regression Problems,"  Inverse problems, where in broad sense the task is to learn from the noisy
response about some unknown function, usually represented as the argument of
some known functional form, has received wide attention in the general
scientific disciplines. How- ever, in mainstream statistics such inverse
problem paradigm does not seem to be as popular. In this article we provide a
brief overview of such problems from a statistical, particularly Bayesian,
perspective.
We also compare and contrast the above class of problems with the perhaps
more statistically familiar inverse regression problems, arguing that this
class of problems contains the traditional class of inverse problems. In course
of our review we point out that the statistical literature is very scarce with
respect to both the inverse paradigms, and substantial research work is still
necessary to develop the fields.
",0,0,1,1,0,0
20030,20031,A Large Term Rewrite System Modelling a Pioneering Cryptographic Algorithm,"  We present a term rewrite system that formally models the Message
Authenticator Algorithm (MAA), which was one of the first cryptographic
functions for computing a Message Authentication Code and was adopted, between
1987 and 2001, in international standards (ISO 8730 and ISO 8731-2) to ensure
the authenticity and integrity of banking transactions. Our term rewrite system
is large (13 sorts, 18 constructors, 644 non-constructors, and 684 rewrite
rules), confluent, and terminating. Implementations in thirteen different
languages have been automatically derived from this model and used to validate
200 official test vectors for the MAA.
",1,0,0,0,0,0
4959,4960,Lifelong Generative Modeling,"  Lifelong learning is the problem of learning multiple consecutive tasks in a
sequential manner where knowledge gained from previous tasks is retained and
used for future learning. It is essential towards the development of
intelligent machines that can adapt to their surroundings. In this work we
focus on a lifelong learning approach to generative modeling where we
continuously incorporate newly observed distributions into our learnt model. We
do so through a student-teacher Variational Autoencoder architecture which
allows us to learn and preserve all the distributions seen so far without the
need to retain the past data nor the past models. Through the introduction of a
novel cross-model regularizer, inspired by a Bayesian update rule, the student
model leverages the information learnt by the teacher, which acts as a summary
of everything seen till now. The regularizer has the additional benefit of
reducing the effect of catastrophic interference that appears when we learn
over sequences of distributions. We demonstrate its efficacy in learning
sequentially observed distributions as well as its ability to learn a common
latent representation across a complex transfer learning scenario.
",1,0,0,1,0,0
7564,7565,Hierarchical Behavioral Repertoires with Unsupervised Descriptors,"  Enabling artificial agents to automatically learn complex, versatile and
high-performing behaviors is a long-lasting challenge. This paper presents a
step in this direction with hierarchical behavioral repertoires that stack
several behavioral repertoires to generate sophisticated behaviors. Each
repertoire of this architecture uses the lower repertoires to create complex
behaviors as sequences of simpler ones, while only the lowest repertoire
directly controls the agent's movements. This paper also introduces a novel
approach to automatically define behavioral descriptors thanks to an
unsupervised neural network that organizes the produced high-level behaviors.
The experiments show that the proposed architecture enables a robot to learn
how to draw digits in an unsupervised manner after having learned to draw lines
and arcs. Compared to traditional behavioral repertoires, the proposed
architecture reduces the dimensionality of the optimization problems by orders
of magnitude and provides behaviors with a twice better fitness. More
importantly, it enables the transfer of knowledge between robots: a
hierarchical repertoire evolved for a robotic arm to draw digits can be
transferred to a humanoid robot by simply changing the lowest layer of the
hierarchy. This enables the humanoid to draw digits although it has never been
trained for this task.
",1,0,0,0,0,0
3458,3459,Controlling light in complex media beyond the acoustic diffraction-limit using the acousto-optic transmission matrix,"  Studying the internal structure of complex samples with light is an important
task, but a difficult challenge due to light scattering. While the complex
optical distortions induced by multiple scattering can be effectively undone
with the knowledge of the medium's scattering-matrix, this matrix is generally
unknown, and cannot be measured with high resolution without the presence of
fluorescent or absorbing probes at all points of interest. To overcome these
limitations, we introduce here the concept of the acousto-optic transmission
matrix (AOTM). Taking advantage of the near scattering-free propagation of
ultrasound in complex samples, we noninvasively measure an
ultrasonically-encoded, spatially-resolved, optical scattering-matrix. We
demonstrate that a singular value decomposition analysis of the AOTM, acquired
using a single or multiple ultrasonic beams, allows controlled optical focusing
beyond the acoustic diffraction limit in scattering media. Our approach
provides a generalized framework for analyzing acousto-optical experiments, and
for noninvasive, high-resolution study of complex media.
",0,1,0,0,0,0
12800,12801,Implementation and Analysis of QUIC for MQTT,"  Transport and security protocols are essential to ensure reliable and secure
communication between two parties. For IoT applications, these protocols must
be lightweight, since IoT devices are usually resource constrained.
Unfortunately, the existing transport and security protocols -- namely TCP/TLS
and UDP/DTLS -- fall short in terms of connection overhead, latency, and
connection migration when used in IoT applications. In this paper, after
studying the root causes of these shortcomings, we show how utilizing QUIC in
IoT scenarios results in a higher performance. Based on these observations, and
given the popularity of MQTT as an IoT application layer protocol, we integrate
MQTT with QUIC. By presenting the main APIs and functions developed, we explain
how connection establishment and message exchange functionalities work. We
evaluate the performance of MQTTw/QUIC versus MQTTw/TCP using wired, wireless,
and long-distance testbeds. Our results show that MQTTw/QUIC reduces connection
overhead in terms of the number of packets exchanged with the broker by up to
56%. In addition, by eliminating half-open connections, MQTTw/QUIC reduces
processor and memory usage by up to 83% and 50%, respectively. Furthermore, by
removing the head-of-line blocking problem, delivery latency is reduced by up
to 55%. We also show that the throughput drops experienced by MQTTw/QUIC when a
connection migration happens is considerably lower than that of MQTTw/TCP.
",1,0,0,0,0,0
15558,15559,Tuple-oriented Compression for Large-scale Mini-batch Stochastic Gradient Descent,"  Data compression is a popular technique for improving the efficiency of data
processing workloads such as SQL queries and more recently, machine learning
(ML) with classical batch gradient methods. But the efficacy of such ideas for
mini-batch stochastic gradient descent (MGD), arguably the workhorse algorithm
of modern ML, is an open question. MGD's unique data access pattern renders
prior art, including those designed for batch gradient methods, less effective.
We fill this crucial research gap by proposing a new lossless compression
scheme we call tuple-oriented compression (TOC) that is inspired by an unlikely
source, the string/text compression scheme Lempel-Ziv-Welch, but tailored to
MGD in a way that preserves tuple boundaries within mini-batches. We then
present a suite of novel compressed matrix operation execution techniques
tailored to the TOC compression scheme that operate directly over the
compressed data representation and avoid decompression overheads. An extensive
empirical evaluation with real-world datasets shows that TOC consistently
achieves substantial compression ratios by up to 51x and reduces runtimes for
MGD workloads by up to 10.2x in popular ML systems.
",1,0,0,1,0,0
7650,7651,A distributed primal-dual algorithm for computation of generalized Nash equilibria with shared affine coupling constraints via operator splitting methods,"  In this paper, we propose a distributed primal-dual algorithm for computation
of a generalized Nash equilibrium (GNE) in noncooperative games over network
systems. In the considered game, not only each player's local objective
function depends on other players' decisions, but also the feasible decision
sets of all the players are coupled together with a globally shared affine
inequality constraint. Adopting the variational GNE, that is the solution of a
variational inequality, as a refinement of GNE, we introduce a primal-dual
algorithm that players can use to seek it in a distributed manner. Each player
only needs to know its local objective function, local feasible set, and a
local block of the affine constraint. Meanwhile, each player only needs to
observe the decisions on which its local objective function explicitly depends
through the interference graph and share information related to multipliers
with its neighbors through a multiplier graph. Through a primal-dual analysis
and an augmentation of variables, we reformulate the problem as finding the
zeros of a sum of monotone operators. Our distributed primal-dual algorithm is
based on forward-backward operator splitting methods. We prove its convergence
to the variational GNE for fixed step-sizes under some mild assumptions. Then a
distributed algorithm with inertia is also introduced and analyzed for
variational GNE seeking. Finally, numerical simulations for network Cournot
competition are given to illustrate the algorithm efficiency and performance.
",1,0,1,0,0,0
19473,19474,Measuring heterogeneity in urban expansion via spatial entropy,"  The lack of efficiency in urban diffusion is a debated issue, important for
biologists, urban specialists, planners and statisticians, both in developed
and new developing countries. Many approaches have been considered to measure
urban sprawl, i.e. chaotic urban expansion; such idea of chaos is here linked
to the concept of entropy. Entropy, firstly introduced in information theory,
rapidly became a standard tool in ecology, biology and geography to measure the
degree of heterogeneity among observations; in these contexts, entropy measures
should include spatial information. The aim of this paper is to employ a
rigorous spatial entropy based approach to measure urban sprawl associated to
the diffusion of metropolitan cities. In order to assess the performance of the
considered measures, a comparative study is run over alternative urban
scenarios; afterwards, measures are used to quantify the degree of disorder in
the urban expansion of three cities in Europe. Results are easily interpretable
and can be used both as an absolute measure of urban sprawl and for comparison
over space and time.
",0,0,0,1,0,0
20868,20869,"A unified continuum and variational multiscale formulation for fluids, solids, and fluid-structure interaction","  We develop a unified continuum modeling framework for viscous fluids and
hyperelastic solids using the Gibbs free energy as the thermodynamic potential.
This framework naturally leads to a pressure primitive variable formulation for
the continuum body, which is well-behaved in both compressible and
incompressible regimes. Our derivation also provides a rational justification
of the isochoric-volumetric additive split of free energies in nonlinear
continuum mechanics. The variational multiscale analysis is performed for the
continuum model to construct a foundation for numerical discretization. We
first consider the continuum body instantiated as a hyperelastic material and
develop a variational multiscale formulation for the hyper-elastodynamic
problem. The generalized-alpha method is applied for temporal discretization. A
segregated algorithm for the nonlinear solver is designed and carefully
analyzed. Second, we apply the new formulation to construct a novel unified
formulation for fluid-solid coupled problems. The variational multiscale
formulation is utilized for spatial discretization in both fluid and solid
subdomains. The generalized-alpha method is applied for the whole continuum
body, and optimal high-frequency dissipation is achieved in both fluid and
solid subproblems. A new predictor multi-corrector algorithm is developed based
on the segregated algorithm to attain a good balance between robustness and
efficiency. The efficacy of the new formulations is examined in several
benchmark problems. The results indicate that the proposed modeling and
numerical methodologies constitute a promising technology for biomedical and
engineering applications, particularly those necessitating incompressible
models.
",0,1,0,0,0,0
19830,19831,3D printable multimaterial cellular auxetics with tunable stiffness,"  Auxetic materials are a novel class of mechanical metamaterials which exhibit
an interesting property of negative Poisson ratio by virtue of their
architecture rather than composition. It has been well established that a wide
range of negative Poisson ratio can be obtained by varying the geometry and
architecture of the cellular materials. However, the limited range of stiffness
values obtained from a given geometry restricts their applications. Research
trials have revealed that multi-material cellular designs have the capability
to generate range of stiffness values as per the requirement of application.
With the advancements in 3D printing, multi-material cellular designs can be
realized in practice. In this work, multi-material cellular designs are
investigated using finite element method. It was observed that introduction of
material gradient/distribution in the cell provides a means to tune cellular
stiffness as per the specific requirement. These results will aid in the design
of wearable auxetic impact protection devices which rely on stiffness gradients
and variable auxeticity.
",0,1,0,0,0,0
1548,1549,Nesterov's Smoothing Technique and Minimizing Differences of Convex Functions for Hierarchical Clustering,"  A bilevel hierarchical clustering model is commonly used in designing optimal
multicast networks. In this paper, we consider two different formulations of
the bilevel hierarchical clustering problem, a discrete optimization problem
which can be shown to be NP-hard. Our approach is to reformulate the problem as
a continuous optimization problem by making some relaxations on the
discreteness conditions. Then Nesterov's smoothing technique and a numerical
algorithm for minimizing differences of convex functions called the DCA are
applied to cope with the nonsmoothness and nonconvexity of the problem.
Numerical examples are provided to illustrate our method.
",0,0,1,0,0,0
15000,15001,Stability analysis of a system coupled to a heat equation,"  As a first approach to the study of systems coupling finite and infinite
dimensional natures, this article addresses the stability of a system of
ordinary differential equations coupled with a classic heat equation using a
Lyapunov functional technique. Inspired from recent developments in the area of
time delay systems, a new methodology to study the stability of such a class of
distributed parameter systems is presented here. The idea is to use a
polynomial approximation of the infinite dimensional state of the heat equation
in order to build an enriched energy functional. A well known efficient
integral inequality (Bessel inequality) will allow to obtain stability
conditions expressed in terms of linear matrix inequalities. We will eventually
test our approach on academic examples in order to illustrate the efficiency of
our theoretical results.
",0,0,1,0,0,0
1694,1695,Fixed points of polarity type operators,"  A well-known result says that the Euclidean unit ball is the unique fixed
point of the polarity operator. This result implies that if, in $\mathbb{R}^n$,
the unit ball of some norm is equal to the unit ball of the dual norm, then the
norm must be Euclidean. Motivated by these results and by relatively recent
results in convex analysis and convex geometry regarding various properties of
order reversing operators, we consider, in a real Hilbert space setting, a more
general fixed point equation in which the polarity operator is composed with a
continuous invertible linear operator. We show that if the linear operator is
positive definite, then the considered equation is uniquely solvable by an
ellipsoid. Otherwise, the equation can have several (possibly infinitely many)
solutions or no solution at all. Our analysis yields a few by-products of
possible independent interest, among them results related to coercive bilinear
forms (essentially a quantitative convex analytic converse to the celebrated
Lax-Milgram theorem from partial differential equations) and a characterization
of real Hilbertian spaces.
",0,0,1,0,0,0
5400,5401,QuanFuzz: Fuzz Testing of Quantum Program,"  Nowadays, quantum program is widely used and quickly developed. However, the
absence of testing methodology restricts their quality. Different input format
and operator from traditional program make this issue hard to resolve.
In this paper, we present QuanFuzz, a search-based test input generator for
quantum program. We define the quantum sensitive information to evaluate test
input for quantum program and use matrix generator to generate test cases with
higher coverage. First, we extract quantum sensitive information -- measurement
operations on those quantum registers and the sensitive branches associated
with those measurement results, from the quantum source code. Then, we use the
sensitive information guided algorithm to mutate the initial input matrix and
select those matrices which improve the probability weight for a value of the
quantum register to trigger the sensitive branch. The process keeps iterating
until the sensitive branch triggered. We tested QuanFuzz on benchmarks and
acquired 20% - 60% more coverage compared to traditional testing input
generation.
",1,0,0,0,0,0
6267,6268,Average treatment effects in the presence of unknown interference,"  We investigate large-sample properties of treatment effect estimators under
unknown interference in randomized experiments. The inferential target is a
generalization of the average treatment effect estimand that marginalizes over
potential spillover effects. We show that estimators commonly used to estimate
treatment effects under no-interference are consistent for the generalized
estimand for several common experimental designs under limited but otherwise
arbitrary and unknown interference. The rates of convergence depend on the rate
at which the amount of interference grows and the degree to which it aligns
with dependencies in treatment assignment. Importantly for practitioners, the
results imply that if one erroneously assumes that units do not interfere in a
setting with limited, or even moderate, interference, standard estimators are
nevertheless likely to be close to an average treatment effect if the sample is
sufficiently large.
",0,0,1,1,0,0
11519,11520,When Is the First Spurious Variable Selected by Sequential Regression Procedures?,"  Applied statisticians use sequential regression procedures to produce a
ranking of explanatory variables and, in settings of low correlations between
variables and strong true effect sizes, expect that variables at the very top
of this ranking are truly relevant to the response. In a regime of certain
sparsity levels, however, three examples of sequential procedures--forward
stepwise, the lasso, and least angle regression--are shown to include the first
spurious variable unexpectedly early. We derive a rigorous, sharp prediction of
the rank of the first spurious variable for these three procedures,
demonstrating that the first spurious variable occurs earlier and earlier as
the regression coefficients become denser. This counterintuitive phenomenon
persists for statistically independent Gaussian random designs and an
arbitrarily large magnitude of the true effects. We gain a better understanding
of the phenomenon by identifying the underlying cause and then leverage the
insights to introduce a simple visualization tool termed the double-ranking
diagram to improve on sequential methods. As a byproduct of these findings, we
obtain the first provable result certifying the exact equivalence between the
lasso and least angle regression in the early stages of solution paths beyond
orthogonal designs. This equivalence can seamlessly carry over many important
model selection results concerning the lasso to least angle regression.
",0,0,1,1,0,0
9145,9146,Learning Sublinear-Time Indexing for Nearest Neighbor Search,"  Most of the efficient sublinear-time indexing algorithms for the
high-dimensional nearest neighbor search problem (NNS) are based on space
partitions of the ambient space $\mathbb{R}^d$. Inspired by recent theoretical
work on NNS for general metric spaces [Andoni, Naor, Nikolov, Razenshteyn,
Waingarten STOC 2018, FOCS 2018], we develop a new framework for constructing
such partitions that reduces the problem to balanced graph partitioning
followed by supervised classification. We instantiate this general approach
with the KaHIP graph partitioner [Sanders, Schulz SEA 2013] and neural
networks, respectively, to obtain a new partitioning procedure called Neural
Locality-Sensitive Hashing (Neural LSH). On several standard benchmarks for
NNS, our experiments show that the partitions found by Neural LSH consistently
outperform partitions found by quantization- and tree-based methods.
",1,0,0,1,0,0
7348,7349,"How far are we from solving the 2D & 3D Face Alignment problem? (and a dataset of 230,000 3D facial landmarks)","  This paper investigates how far a very deep neural network is from attaining
close to saturating performance on existing 2D and 3D face alignment datasets.
To this end, we make the following 5 contributions: (a) we construct, for the
first time, a very strong baseline by combining a state-of-the-art architecture
for landmark localization with a state-of-the-art residual block, train it on a
very large yet synthetically expanded 2D facial landmark dataset and finally
evaluate it on all other 2D facial landmark datasets. (b) We create a guided by
2D landmarks network which converts 2D landmark annotations to 3D and unifies
all existing datasets, leading to the creation of LS3D-W, the largest and most
challenging 3D facial landmark dataset to date ~230,000 images. (c) Following
that, we train a neural network for 3D face alignment and evaluate it on the
newly introduced LS3D-W. (d) We further look into the effect of all
""traditional"" factors affecting face alignment performance like large pose,
initialization and resolution, and introduce a ""new"" one, namely the size of
the network. (e) We show that both 2D and 3D face alignment networks achieve
performance of remarkable accuracy which is probably close to saturating the
datasets used. Training and testing code as well as the dataset can be
downloaded from this https URL
",1,0,0,0,0,0
3718,3719,Applied Evaluative Informetrics: Part 1,"  This manuscript is a preprint version of Part 1 (General Introduction and
Synopsis) of the book Applied Evaluative Informetrics, to be published by
Springer in the summer of 2017. This book presents an introduction to the field
of applied evaluative informetrics, and is written for interested scholars and
students from all domains of science and scholarship. It sketches the field's
history, recent achievements, and its potential and limits. It explains the
notion of multi-dimensional research performance, and discusses the pros and
cons of 28 citation-, patent-, reputation- and altmetrics-based indicators. In
addition, it presents quantitative research assessment as an evaluation
science, and focuses on the role of extra-informetric factors in the
development of indicators, and on the policy context of their application. It
also discusses the way forward, both for users and for developers of
informetric tools.
",1,0,0,0,0,0
13776,13777,Time consistency for scalar multivariate risk measures,"  In this paper we present results on dynamic multivariate scalar risk
measures, which arise in markets with transaction costs and systemic risk. Dual
representations of such risk measures are presented. These are then used to
obtain the main results of this paper on time consistency; namely, an
equivalent recursive formulation of multivariate scalar risk measures to
multiportfolio time consistency. We are motivated to study time consistency of
multivariate scalar risk measures as the superhedging risk measure in markets
with transaction costs (with a single eligible asset) (Jouini and Kallal
(1995), Roux and Zastawniak (2016), Loehne and Rudloff (2014)) does not satisfy
the usual scalar concept of time consistency. In fact, as demonstrated in
(Feinstein and Rudloff (2018)), scalar risk measures with the same
scalarization weight at all times would not be time consistent in general. The
deduced recursive relation for the scalarizations of multiportfolio time
consistent set-valued risk measures provided in this paper requires
consideration of the entire family of scalarizations. In this way we develop a
direct notion of a ""moving scalarization"" for scalar time consistency that
corroborates recent research on scalarizations of dynamic multi-objective
problems (Karnam, Ma, and Zhang (2017), Kovacova and Rudloff (2018)).
",0,0,0,0,0,1
9649,9650,Gradual Tuning: a better way of Fine Tuning the parameters of a Deep Neural Network,"  In this paper we present an alternative strategy for fine-tuning the
parameters of a network. We named the technique Gradual Tuning. Once trained on
a first task, the network is fine-tuned on a second task by modifying a
progressively larger set of the network's parameters. We test Gradual Tuning on
different transfer learning tasks, using networks of different sizes trained
with different regularization techniques. The result shows that compared to the
usual fine tuning, our approach significantly reduces catastrophic forgetting
of the initial task, while still retaining comparable if not better performance
on the new task.
",1,0,0,0,0,0
20510,20511,Experimental observations and modelling of intrinsic rotation reversals in tokamaks,"  The progress made in understanding spontaneous toroidal rotation reversals in
tokamaks is reviewed and current ideas to solve this ten-year-old puzzle are
explored. The paper includes a summarial synthesis of the experimental
observations in AUG, C-Mod, KSTAR, MAST and TCV tokamaks, reasons why turbulent
momentum transport is thought to be responsible for the reversals, a review of
the theory of turbulent momentum transport and suggestions for future
investigations.
",0,1,0,0,0,0
19571,19572,The role of complex analysis in modeling economic growth,"  Development and growth are complex and tumultuous processes. Modern economic
growth theories identify some key determinants of economic growth. However, the
relative importance of the determinants remains unknown, and additional
variables may help clarify the directions and dimensions of the interactions.
The novel stream of literature on economic complexity goes beyond aggregate
measures of productive inputs, and considers instead a more granular and
structural view of the productive possibilities of countries, i.e. their
capabilities. Different endowments of capabilities are crucial ingredients in
explaining differences in economic performances. In this paper we employ
economic fitness, a measure of productive capabilities obtained through complex
network techniques. Focusing on the combined roles of fitness and some more
traditional drivers of growth, we build a bridge between economic growth
theories and the economic complexity literature. Our findings, in agreement
with other recent empirical studies, show that fitness plays a crucial role in
fostering economic growth and, when it is included in the analysis, can be
either complementary to traditional drivers of growth or can completely
overshadow them.
",0,0,0,0,0,1
7685,7686,Accumulation Bit-Width Scaling For Ultra-Low Precision Training Of Deep Networks,"  Efforts to reduce the numerical precision of computations in deep learning
training have yielded systems that aggressively quantize weights and
activations, yet employ wide high-precision accumulators for partial sums in
inner-product operations to preserve the quality of convergence. The absence of
any framework to analyze the precision requirements of partial sum
accumulations results in conservative design choices. This imposes an
upper-bound on the reduction of complexity of multiply-accumulate units. We
present a statistical approach to analyze the impact of reduced accumulation
precision on deep learning training. Observing that a bad choice for
accumulation precision results in loss of information that manifests itself as
a reduction in variance in an ensemble of partial sums, we derive a set of
equations that relate this variance to the length of accumulation and the
minimum number of bits needed for accumulation. We apply our analysis to three
benchmark networks: CIFAR-10 ResNet 32, ImageNet ResNet 18 and ImageNet
AlexNet. In each case, with accumulation precision set in accordance with our
proposed equations, the networks successfully converge to the single precision
floating-point baseline. We also show that reducing accumulation precision
further degrades the quality of the trained network, proving that our equations
produce tight bounds. Overall this analysis enables precise tailoring of
computation hardware to the application, yielding area- and power-optimal
systems.
",1,0,0,1,0,0
1116,1117,Stacco: Differentially Analyzing Side-Channel Traces for Detecting SSL/TLS Vulnerabilities in Secure Enclaves,"  Intel Software Guard Extension (SGX) offers software applications enclave to
protect their confidentiality and integrity from malicious operating systems.
The SSL/TLS protocol, which is the de facto standard for protecting
transport-layer network communications, has been broadly deployed for a secure
communication channel. However, in this paper, we show that the marriage
between SGX and SSL may not be smooth sailing.
Particularly, we consider a category of side-channel attacks against SSL/TLS
implementations in secure enclaves, which we call the control-flow inference
attacks. In these attacks, the malicious operating system kernel may perform a
powerful man-in-the-kernel attack to collect execution traces of the enclave
programs at page, cacheline, or branch level, while positioning itself in the
middle of the two communicating parties. At the center of our work is a
differential analysis framework, dubbed Stacco, to dynamically analyze the
SSL/TLS implementations and detect vulnerabilities that can be exploited as
decryption oracles. Surprisingly, we found exploitable vulnerabilities in the
latest versions of all the SSL/TLS libraries we have examined.
To validate the detected vulnerabilities, we developed a man-in-the-kernel
adversary to demonstrate Bleichenbacher attacks against the latest OpenSSL
library running in the SGX enclave (with the help of Graphene) and completely
broke the PreMasterSecret encrypted by a 4096-bit RSA public key with only
57286 queries. We also conducted CBC padding oracle attacks against the latest
GnuTLS running in Graphene-SGX and an open-source SGX-implementation of mbedTLS
(i.e., mbedTLS-SGX) that runs directly inside the enclave, and showed that it
only needs 48388 and 25717 queries, respectively, to break one block of AES
ciphertext. Empirical evaluation suggests these man-in-the-kernel attacks can
be completed within 1 or 2 hours.
",1,0,0,0,0,0
6273,6274,Dynamic k-Struve Sumudu Solutions for Fractional Kinetic Equations,"  In this present study, we investigate solutions for fractional kinetic
equations, involving k-Struve functions using Sumudu transform. The methodology
and results can be considered and applied to various related fractional
problems in mathematical physics.
",0,0,1,0,0,0
17466,17467,Further remarks on liftings of crossed modules,"  In this paper we define the notion of pullback lifting of a lifting crossed
module over a crossed module morphism and interpret this notion in the category
of group-groupoid actions as pullback action. Moreover, we give a criterion for
the lifting of homotopic crossed module morphisms to be homotopic, which will
be called homotopy lifting property for crossed module morphisms. Finally, we
investigate some properties of derivations of lifting crossed modules according
to base crossed module derivations.
",0,0,1,0,0,0
1923,1924,Static and Fluctuating Magnetic Moments in the Ferroelectric Metal LiOsO$_3$,"  LiOsO$_3$ is the first example of a new class of material called a
ferroelectric metal. We performed zero-field and longitudinal-field $\mu$SR,
along with a combination of electronic structure and dipole field calculations,
to determine the magnetic ground state of LiOsO$_3$. We find that the sample
contains both static Li nuclear moments and dynamic Os electronic moments.
Below $\approx 0.7\,$K, the fluctuations of the Os moments slow down, though
remain dynamic down to 0.08$\,$K. We expect this could result in a frozen-out,
disordered ground state at even lower temperatures.
",0,1,0,0,0,0
1294,1295,A unified view of entropy-regularized Markov decision processes,"  We propose a general framework for entropy-regularized average-reward
reinforcement learning in Markov decision processes (MDPs). Our approach is
based on extending the linear-programming formulation of policy optimization in
MDPs to accommodate convex regularization functions. Our key result is showing
that using the conditional entropy of the joint state-action distributions as
regularization yields a dual optimization problem closely resembling the
Bellman optimality equations. This result enables us to formalize a number of
state-of-the-art entropy-regularized reinforcement learning algorithms as
approximate variants of Mirror Descent or Dual Averaging, and thus to argue
about the convergence properties of these methods. In particular, we show that
the exact version of the TRPO algorithm of Schulman et al. (2015) actually
converges to the optimal policy, while the entropy-regularized policy gradient
methods of Mnih et al. (2016) may fail to converge to a fixed point. Finally,
we illustrate empirically the effects of using various regularization
techniques on learning performance in a simple reinforcement learning setup.
",1,0,0,1,0,0
18886,18887,Evaluation of Classical Features and Classifiers in Brain-Computer Interface Tasks,"  Brain-Computer Interface (BCI) uses brain signals in order to provide a new
method for communication between human and outside world. Feature extraction,
selection and classification are among the main matters of concerns in signal
processing stage of BCI. In this article, we present our findings about the
most effective features and classifiers in some brain tasks. Six different
groups of classical features and twelve classifiers have been examined in nine
datasets of brain signal. The results indicate that energy of brain signals in
{\alpha} and \b{eta} frequency bands, together with some statistical parameters
are more effective, comparing to the other types of extracted features. In
addition, Bayesian classifier with Gaussian distribution assumption and also
Support Vector Machine (SVM) show to classify different BCI datasets more
accurately than the other classifiers. We believe that the results can give an
insight about a strategy for blind classification of brain signals in
brain-computer interface.
",1,0,0,1,0,0
930,931,General mixed multi-soliton solution to the multi-component Maccari system,"  Based on the KP hierarchy reduction method, the general bright-dark mixed
multi-soliton solution of the multi-component Maccari system is constructed.
The multi-component Maccari system considered comprised of multiple (say $M$)
short-wave components and one long-wave component with all possible
combinations of nonlinearities including all-focusing, all-defocusing and mixed
types. We firstly derive the two-bright-one-dark (2-b-1-d) and
one-bright-two-dark (1-b-2-d) mixed multi-soliton solutions to the
three-component Maccari system in detail. For the interaction between two
solitons, the asymptotic analysis shows that inelastic collision can take place
in a $M$-component Maccari system with $M \geq 3$ only if the bright parts of
the mixed solitons appear at least in two short-wave components. The
energy-exchanging inelastic collision characterized by an intensity
redistribution among the bright parts of the mixed solitons. While the dark
parts of the mixed solitons and the solitons in the long-wave component always
undergo elastic collision which just accompanied by a position shift. In the
end, we extend the corresponding analysis to the $M$-component Maccari system
to obtain its mixed multi-soliton solution. The formula obtained unifies the
all-bright, all-dark and mixed multi-soliton solutions.
",0,1,0,0,0,0
3586,3587,Absence of long range order in the frustrated magnet SrDy$_2$O$_4$ due to trapped defects from a dimensionality crossover,"  Magnetic frustration and low dimensionality can prevent long range magnetic
order and lead to exotic correlated ground states. SrDy$_2$O$_4$ consists of
magnetic Dy$^{3+}$ ions forming magnetically frustrated zig-zag chains along
the c-axis and shows no long range order to temperatures as low as $T=60$ mK.
We carried out neutron scattering and AC magnetic susceptibility measurements
using powder and single crystals of SrDy$_2$O$_4$. Diffuse neutron scattering
indicates strong one-dimensional (1D) magnetic correlations along the chain
direction that can be qualitatively accounted for by the axial next-nearest
neighbour Ising (ANNNI) model with nearest-neighbor and next-nearest-neighbor
exchange $J_1=0.3$ meV and $J_2=0.2$ meV, respectively. Three-dimensional (3D)
correlations become important below $T^*\approx0.7$ K. At $T=60$ mK, the short
range correlations are characterized by a putative propagation vector
$\textbf{k}_{1/2}=(0,\frac{1}{2},\frac{1}{2})$. We argue that the absence of
long range order arises from the presence of slowly decaying 1D domain walls
that are trapped due to 3D correlations. This stabilizes a low-temperature
phase without long range magnetic order, but with well-ordered chain segments
separated by slowly-moving domain walls.
",0,1,0,0,0,0
5954,5955,Matching neural paths: transfer from recognition to correspondence search,"  Many machine learning tasks require finding per-part correspondences between
objects. In this work we focus on low-level correspondences - a highly
ambiguous matching problem. We propose to use a hierarchical semantic
representation of the objects, coming from a convolutional neural network, to
solve this ambiguity. Training it for low-level correspondence prediction
directly might not be an option in some domains where the ground-truth
correspondences are hard to obtain. We show how transfer from recognition can
be used to avoid such training. Our idea is to mark parts as ""matching"" if
their features are close to each other at all the levels of convolutional
feature hierarchy (neural paths). Although the overall number of such paths is
exponential in the number of layers, we propose a polynomial algorithm for
aggregating all of them in a single backward pass. The empirical validation is
done on the task of stereo correspondence and demonstrates that we achieve
competitive results among the methods which do not use labeled target domain
data.
",1,0,0,0,0,0
14426,14427,The Lifetimes of Phases in High-Mass Star-Forming Regions,"  High-mass stars form within star clusters from dense, molecular regions, but
is the process of cluster formation slow and hydrostatic or quick and dynamic?
We link the physical properties of high-mass star-forming regions with their
evolutionary stage in a systematic way, using Herschel and Spitzer data. In
order to produce a robust estimate of the relative lifetimes of these regions,
we compare the fraction of dense, molecular regions above a column density
associated with high-mass star formation, N(H2) > 0.4-2.5 x 10^22 cm^-2, in the
'starless (no signature of stars > 10 Msun forming) and star-forming phases in
a 2x2 degree region of the Galactic Plane centered at l=30deg. Of regions
capable of forming high-mass stars on ~1 pc scales, the starless (or embedded
beyond detection) phase occupies about 60-70% of the dense, molecular region
lifetime and the star-forming phase occupies about 30-40%. These relative
lifetimes are robust over a wide range of thresholds. We outline a method by
which relative lifetimes can be anchored to absolute lifetimes from large-scale
surveys of methanol masers and UCHII regions. A simplistic application of this
method estimates the absolute lifetimes of the starless phase to be 0.2-1.7 Myr
(about 0.6-4.1 fiducial cloud free-fall times) and the star-forming phase to be
0.1-0.7 Myr (about 0.4-2.4 free-fall times), but these are highly uncertain.
This work uniquely investigates the star-forming nature of high-column density
gas pixel-by-pixel and our results demonstrate that the majority of high-column
density gas is in a starless or embedded phase.
",0,1,0,0,0,0
4915,4916,Distributions and Statistical Power of Optimal Signal-Detection Methods In Finite Cases,"  In big data analysis for detecting rare and weak signals among $n$ features,
some grouping-test methods such as Higher Criticism test (HC), Berk-Jones test
(B-J), and $\phi$-divergence test share the similar asymptotical optimality
when $n \rightarrow \infty$. However, in practical data analysis $n$ is
frequently small and moderately large at most. In order to properly apply these
optimal tests and wisely choose them for practical studies, it is important to
know how to get the p-values and statistical power of them. To address this
problem in an even broader context, this paper provides analytical solutions
for a general family of goodness-of-fit (GOF) tests, which covers these optimal
tests. For any given i.i.d. and continuous distributions of the input test
statistics of the $n$ features, both p-value and statistical power of such a
GOF test can be calculated. By calculation we compared the finite-sample
performances of asymptotically optimal tests under the normal mixture
alternative. Results show that HC is the best choice when signals are rare,
while B-J is more robust over various signal patterns. In the application to a
real genome-wide association study, results illustrate that the p-value
calculation works well, and the optimal tests have potentials for detecting
novel disease genes with weak genetic effects. The calculations have been
implemented in an R package SetTest and published on the CRAN.
",0,0,1,1,0,0
9073,9074,Explicit estimates for the distribution of numbers free of large prime factors,"  There is a large literature on the asymptotic distribution of numbers free of
large prime factors, so-called $\textit{smooth}$ or $\textit{friable}$ numbers.
But there is very little known about this distribution that is numerically
explicit. In this paper we follow the general plan for the saddle point
argument of Hildebrand and Tenenbaum, giving explicit and fairly tight
intervals in which the true count lies. We give two numerical examples of our
method, and with the larger one, our interval is so tight we can exclude the
famous Dickman-de Bruijn asymptotic estimate as too small and the
Hildebrand-Tenenbaum main term as too large.
",0,0,1,0,0,0
5219,5220,Electron affinities of water clusters from density-functional and many-body-perturbation theory,"  In this work, we assess the accuracy of dielectric-dependent hybrid density
functionals and many-body perturbation theory methods for the calculation of
electron affinities of small water clusters, including hydrogen-bonded water
dimer and water hexamer isomers. We show that many-body perturbation theory in
the G$_0$W$_0$ approximation starting with the dielectric-dependent hybrid
functionals predicts electron affinities of clusters within 0.1 eV of the
coupled-cluster results with single, double, and perturbative triple
excitations.
",0,1,0,0,0,0
15011,15012,Proportional Closeness Estimation of Probability of Contamination Under Group Testing,"  The paper is focused on the problem of estimating the probability $p$ of
individual contaminated sample, under group testing. The precision of the
estimator is given by the probability of proportional closeness, a concept
defined in the Introduction. Two-stage and sequential sampling procedures are
characterized. An adaptive procedure is examined.
",0,0,1,1,0,0
4157,4158,Multi-Task Learning Using Neighborhood Kernels,"  This paper introduces a new and effective algorithm for learning kernels in a
Multi-Task Learning (MTL) setting. Although, we consider a MTL scenario here,
our approach can be easily applied to standard single task learning, as well.
As shown by our empirical results, our algorithm consistently outperforms the
traditional kernel learning algorithms such as uniform combination solution,
convex combinations of base kernels as well as some kernel alignment-based
models, which have been proven to give promising results in the past. We
present a Rademacher complexity bound based on which a new Multi-Task Multiple
Kernel Learning (MT-MKL) model is derived. In particular, we propose a Support
Vector Machine-regularized model in which, for each task, an optimal kernel is
learned based on a neighborhood-defining kernel that is not restricted to be
positive semi-definite. Comparative experimental results are showcased that
underline the merits of our neighborhood-defining framework in both
classification and regression problems.
",1,0,0,1,0,0
17372,17373,Dynamical tides in exoplanetary systems containing Hot Jupiters: confronting theory and observations,"  We study the effect of dynamical tides associated with the excitation of
gravity waves in an interior radiative region of the central star on orbital
evolution in observed systems containing Hot Jupiters. We consider WASP-43,
Ogle-tr-113, WASP-12, and WASP-18 which contain stars on the main sequence
(MS). For these systems there are observational estimates regarding the rate of
change of the orbital period. We also investigate Kepler-91 which contains an
evolved giant star. We adopt the formalism of Ivanov et al. for calculating the
orbital evolution.
For the MS stars we determine expected rates of orbital evolution under
different assumptions about the amount of dissipation acting on the tides,
estimate the effect of stellar rotation for the two most rapidly rotating stars
and compare results with observations. All cases apart from possibly WASP-43
are consistent with a regime in which gravity waves are damped during their
propagation over the star. However, at present this is not definitive as
observational errors are large. We find that although it is expected to apply
to Kepler-91, linear radiative damping cannot explain this dis- sipation regime
applying to MS stars. Thus, a nonlinear mechanism may be needed.
Kepler-91 is found to be such that the time scale for evolution of the star
is comparable to that for the orbit. This implies that significant orbital
circularisation may have occurred through tides acting on the star.
Quasi-static tides, stellar winds, hydrodynamic drag and tides acting on the
planet have likely played a minor role.
",0,1,0,0,0,0
10634,10635,A locally quasi-convex abelian group without Mackey topology,"  We give the first example of a locally quasi-convex (even countable reflexive
and $k_\omega$) abelian group $G$ which does not admit the strongest compatible
locally quasi-convex group topology. Our group $G$ is the Graev free abelian
group $A_G(\mathbf{s})$ over a convergent sequence $\mathbf{s}$.
",0,0,1,0,0,0
16792,16793,Re-Evaluating the Netflix Prize - Human Uncertainty and its Impact on Reliability,"  In this paper, we examine the statistical soundness of comparative
assessments within the field of recommender systems in terms of reliability and
human uncertainty. From a controlled experiment, we get the insight that users
provide different ratings on same items when repeatedly asked. This volatility
of user ratings justifies the assumption of using probability densities instead
of single rating scores. As a consequence, the well-known accuracy metrics
(e.g. MAE, MSE, RMSE) yield a density themselves that emerges from convolution
of all rating densities. When two different systems produce different RMSE
distributions with significant intersection, then there exists a probability of
error for each possible ranking. As an application, we examine possible ranking
errors of the Netflix Prize. We are able to show that all top rankings are more
or less subject to high probabilities of error and that some rankings may be
deemed to be caused by mere chance rather than system quality.
",1,0,0,0,0,0
7669,7670,Google Scholar and the gray literature: A reply to Bonato's review,"  Recently, a review concluded that Google Scholar (GS) is not a suitable
source of information ""for identifying recent conference papers or other gray
literature publications"". The goal of this letter is to demonstrate that GS can
be an effective tool to search and find gray literature, as long as appropriate
search strategies are used. To do this, we took as examples the same two case
studies used by the original review, describing first how GS processes
original's search strategies, then proposing alternative search strategies, and
finally generalizing each case study to compose a general search procedure
aimed at finding gray literature in Google Scholar for two wide selected case
studies: a) all contributions belonging to a congress (the ASCO Annual
Meeting); and b) indexed guidelines as well as gray literature within medical
institutions (National Institutes of Health) and governmental agencies (U.S.
Department of Health & Human Services). The results confirm that original
search strategies were undertrained offering misleading results and erroneous
conclusions. Google Scholar lacks many of the advanced search features
available in other bibliographic databases (such as Pubmed), however, it is one
thing to have a friendly search experience, and quite another to find gray
literature. We finally conclude that Google Scholar is a powerful tool for
searching gray literature, as long as the users are familiar with all the
possibilities it offers as a search engine. Poorly formulated searches will
undoubtedly return misleading results.
",1,0,0,0,0,0
2109,2110,"Analytic and arithmetic properties of the $(Γ,χ)$-automorphic reproducing kernel function","  We consider the reproducing kernel function of the theta Bargmann-Fock
Hilbert space associated to given full-rank lattice and pseudo-character, and
we deal with some of its analytical and arithmetical properties. Specially, the
distribution and discreteness of its zeros are examined and analytic sets
inside a product of fundamental cells is characterized and shown to be finite
and of cardinal less or equal to the dimension of the theta Bargmann-Fock
Hilbert space. Moreover, we obtain some remarkable lattice sums by evaluating
the so-called complex Hermite-Taylor coefficients. Some of them generalize some
of the arithmetic identities established by Perelomov in the framework of
coherent states for the specific case of von Neumann lattice. Such complex
Hermite-Taylor coefficients are nontrivial examples of the so-called lattice's
functions according the Serre terminology. The perfect use of the basic
properties of the complex Hermite polynomials is crucial in this framework.
",0,0,1,0,0,0
5246,5247,Interacting Chaplygin gas revisited,"  The implications of considering interaction between Chaplygin gas and a
barotropic fluid with constant equation of state have been explored. The unique
feature of this work is that assuming an interaction $Q \propto H\rho_d$,
analytic expressions for the energy density and pressure have been derived in
terms of the Hypergeometric $_2\text{F}_1$ function. It is worthwhile to
mention that an interacting Chaplygin gas model was considered in 2006 by Zhang
and Zhu, nevertheless, analytic solutions for the continuity equations could
not be determined assuming an interaction proportional to $H$ times the sum of
the energy densities of Chaplygin gas and dust. Our model can successfully
explain the transition from the early decelerating phase to the present phase
of cosmic acceleration. Arbitrary choice of the free parameters of our model
through trial and error show at recent observational data strongly favors
$w_m=0$ and $w_m=-\frac{1}{3}$ over the $w_m=\frac{1}{3}$ case. Interestingly,
the present model also incorporates the transition of dark energy into the
phantom domain, however, future deceleration is forbidden.
",0,1,0,0,0,0
11389,11390,Primordial perturbations generated by Higgs field and $R^2$ operator,"  If the very early Universe is dominated by the non-minimally coupled Higgs
field and Starobinsky's curvature-squared term together, the potential diagram
would mimic the landscape of a valley, serving as a cosmological attractor. The
inflationary dynamics along this valley is studied, model parameters are
constrained against observational data, and the isocurvature perturbation is
evaluated.
",0,1,0,0,0,0
12302,12303,Towards Decoding as Continuous Optimization in Neural Machine Translation,"  We propose a novel decoding approach for neural machine translation (NMT)
based on continuous optimisation. We convert decoding - basically a discrete
optimization problem - into a continuous optimization problem. The resulting
constrained continuous optimisation problem is then tackled using
gradient-based methods. Our powerful decoding framework enables decoding
intractable models such as the intersection of left-to-right and right-to-left
(bidirectional) as well as source-to-target and target-to-source (bilingual)
NMT models. Our empirical results show that our decoding framework is
effective, and leads to substantial improvements in translations generated from
the intersected models where the typical greedy or beam search is not feasible.
We also compare our framework against reranking, and analyse its advantages and
disadvantages.
",1,0,0,0,0,0
20241,20242,Fourier Transform of Schwartz Algebras on Groups in the Harish-Chandra class,"  It is well-known that the Harish-Chandra transform, $f\mapsto\mathcal{H}f,$
is a topological isomorphism of the spherical (Schwartz) convolution algebra
$\mathcal{C}^{p}(G//K)$ (where $K$ is a maximal compact subgroup of any
arbitrarily chosen group $G$ in the Harish-Chandra class and $0<p\leq2$) onto
the (Schwartz) multiplication algebra
$\bar{\mathcal{Z}}({\mathfrak{F}}^{\epsilon})$ (of $\mathfrak{w}-$invariant
members of $\mathcal{Z}({\mathfrak{F}}^{\epsilon}),$ with $\epsilon=(2/p)-1$).
The same cannot however be said of the full Schwartz convolution algebra
$\mathcal{C}^{p}(G),$ except for few specific examples of groups (notably
$G=SL(2,\mathbb{R})$) and for some notable values of $p$ (with restrictions on
$G$ and/or on $\mathcal{C}^{p}(G)$). Nevertheless the full Harish-Chandra
Plancherel formula on $G$ is known for all of
$\mathcal{C}^{2}(G)=:\mathcal{C}(G).$ In order to then understand the structure
of Harish-Chandra transform more clearly and to compute the image of
$\mathcal{C}^{p}(G)$ under it (without any restriction) we derive an absolutely
convergent series expansion (in terms of known functions) for the
Harish-Chandra transform by an application of the full Plancherel formula on
$G.$ This leads to a computation of the image of $\mathcal{C}(G)$ under the
Harish-Chandra transform which may be seen as a concrete realization of
Arthur's result and be easily extended to all of $\mathcal{C}^{p}(G)$ in much
the same way as it is known in the work of Trombi and Varadarajan.
",0,0,1,0,0,0
10127,10128,Robust Regression via Mutivariate Regression Depth,"  This paper studies robust regression in the settings of Huber's
$\epsilon$-contamination models. We consider estimators that are maximizers of
multivariate regression depth functions. These estimators are shown to achieve
minimax rates in the settings of $\epsilon$-contamination models for various
regression problems including nonparametric regression, sparse linear
regression, reduced rank regression, etc. We also discuss a general notion of
depth function for linear operators that has potential applications in robust
functional linear regression.
",0,0,1,1,0,0
15027,15028,Comparison of ontology alignment systems across single matching task via the McNemar's test,"  Ontology alignment is widely-used to find the correspondences between
different ontologies in diverse fields.After discovering the alignments,several
performance scores are available to evaluate them.The scores typically require
the identified alignment and a reference containing the underlying actual
correspondences of the given ontologies.The current trend in the alignment
evaluation is to put forward a new score(e.g., precision, weighted precision,
etc.)and to compare various alignments by juxtaposing the obtained scores.
However,it is substantially provocative to select one measure among others for
comparison.On top of that, claiming if one system has a better performance than
one another cannot be substantiated solely by comparing two scalars.In this
paper,we propose the statistical procedures which enable us to theoretically
favor one system over one another.The McNemar's test is the statistical means
by which the comparison of two ontology alignment systems over one matching
task is drawn.The test applies to a 2x2 contingency table which can be
constructed in two different ways based on the alignments,each of which has
their own merits/pitfalls.The ways of the contingency table construction and
various apposite statistics from the McNemar's test are elaborated in minute
detail.In the case of having more than two alignment systems for comparison,
the family-wise error rate is expected to happen. Thus, the ways of preventing
such an error are also discussed.A directed graph visualizes the outcome of the
McNemar's test in the presence of multiple alignment systems.From this graph,
it is readily understood if one system is better than one another or if their
differences are imperceptible.The proposed statistical methodologies are
applied to the systems participated in the OAEI 2016 anatomy track, and also
compares several well-known similarity metrics for the same matching problem.
",1,0,0,0,0,0
11604,11605,Zampa's systems theory: a comprehensive theory of measurement in dynamic systems,"  The article outlines in memoriam Prof. Pavel Zampa's concepts of system
theory which enable to devise a measurement in dynamic systems independently of
the particular system behaviour. From the point of view of Zampa's theory,
terms like system time, system attributes, system link, system element, input,
output, subsystems, and state variables are defined. In Conclusions, Zampa's
theory is discussed together with another mathematical approaches of
qualitative dynamics known since the 19th century. In Appendices, we present
applications of Zampa's technical approach to measurement of complex dynamical
(chemical and biological) systems at the Institute of Complex Systems,
University of South Bohemia in Ceske Budejovice.
",1,0,0,0,0,0
4616,4617,Robust Computation in 2D Absolute EIT (a-EIT) Using D-bar Methods with the `exp' Approximation,"  Objective: Absolute images have important applications in medical Electrical
Impedance Tomography (EIT) imaging, but the traditional minimization and
statistical based computations are very sensitive to modeling errors and noise.
In this paper, it is demonstrated that D-bar reconstruction methods for
absolute EIT are robust to such errors. Approach: The effects of errors in
domain shape and electrode placement on absolute images computed with 2D D-bar
reconstruction algorithms are studied on experimental data. Main Results: It is
demonstrated with tank data from several EIT systems that these methods are
quite robust to such modeling errors, and furthermore the artefacts arising
from such modeling errors are similar to those occurring in classic
time-difference EIT imaging. Significance: This study is promising for clinical
applications where absolute EIT images are desirable, but previously thought
impossible.
",1,0,0,0,0,0
6075,6076,On a result of Fel'dman on linear forms in the values of some E-functions,"  We shall consider a result of Fel'dman, where a sharp Baker-type lower bound
is obtained for linear forms in the values of some E-functions. Fel'dman's
proof is based on an explicit construction of Padé approximations of the
first kind for these functions. In the present paper we introduce Padé
approximations of the second kind for the same functions and use these to
obtain a slightly improved version of Fel'dman's result.
",0,0,1,0,0,0
2212,2213,Deep Learning on Attributed Graphs: A Journey from Graphs to Their Embeddings and Back,"  A graph is a powerful concept for representation of relations between pairs
of entities. Data with underlying graph structure can be found across many
disciplines and there is a natural desire for understanding such data better.
Deep learning (DL) has achieved significant breakthroughs in a variety of
machine learning tasks in recent years, especially where data is structured on
a grid, such as in text, speech, or image understanding. However, surprisingly
little has been done to explore the applicability of DL on arbitrary
graph-structured data directly.
The goal of this thesis is to investigate architectures for DL on graphs and
study how to transfer, adapt or generalize concepts that work well on
sequential and image data to this domain. We concentrate on two important
primitives: embedding graphs or their nodes into a continuous vector space
representation (encoding) and, conversely, generating graphs from such vectors
back (decoding). To that end, we make the following contributions.
First, we introduce Edge-Conditioned Convolutions (ECC), a convolution-like
operation on graphs performed in the spatial domain where filters are
dynamically generated based on edge attributes. The method is used to encode
graphs with arbitrary and varying structure.
Second, we propose SuperPoint Graph, an intermediate point cloud
representation with rich edge attributes encoding the contextual relationship
between object parts. Based on this representation, ECC is employed to segment
large-scale point clouds without major sacrifice in fine details.
Third, we present GraphVAE, a graph generator allowing us to decode graphs
with variable but upper-bounded number of nodes making use of approximate graph
matching for aligning the predictions of an autoencoder with its inputs. The
method is applied to the task of molecule generation.
",1,0,0,1,0,0
15734,15735,Categorical entropy for Fourier-Mukai transforms on generic abelian surfaces,"  In this note, we shall compute the categorical entropy of an autoequivalence
on a generic abelian surface.
",0,0,1,0,0,0
11293,11294,On the Necessity of Superparametric Geometry Representation for Discontinuous Galerkin Methods on Domains with Curved Boundaries,"  We provide numerical evidence demonstrating the necessity of employing a
superparametric geometry representation in order to obtain optimal convergence
orders on two-dimensional domains with curved boundaries when solving the Euler
equations using Discontinuous Galerkin methods. However, concerning the
obtention of optimal convergence orders for the Navier-Stokes equations, we
demonstrate numerically that the use of isoparametric geometry representation
is sufficient for the case considered here.
",1,0,0,0,0,0
14807,14808,Frequency Principle: Fourier Analysis Sheds Light on Deep Neural Networks,"  We study the training process of Deep Neural Networks (DNNs) from the Fourier
analysis perspective. Our starting point is a Frequency Principle (F-Principle)
--- DNNs initialized with small parameters often fit target functions from low
to high frequencies --- which was first proposed by Xu et al. (2018) and
Rahaman et al. (2018) on synthetic datasets. In this work, we first show the
universality of the F-Principle by demonstrating this phenomenon on
high-dimensional benchmark datasets, such as MNIST and CIFAR10. Then, based on
experiments, we show that the F-Principle provides insight into both the
success and failure of DNNs in different types of problems. Based on the
F-Principle, we further propose that DNN can be adopted to accelerate the
convergence of low frequencies for scientific computing problems, in which most
of the conventional methods (e.g., Jacobi method) exhibit the opposite
convergence behavior --- faster convergence for higher frequencies. Finally, we
prove a theorem for DNNs of one hidden layer as a first step towards a
mathematical explanation of the F-Principle. Our work indicates that the
F-Principle with Fourier analysis is a promising approach to the study of DNNs
because it seems ubiquitous, applicable, and explainable.
",1,0,0,1,0,0
12172,12173,Unsupervised learning of object frames by dense equivariant image labelling,"  One of the key challenges of visual perception is to extract abstract models
of 3D objects and object categories from visual measurements, which are
affected by complex nuisance factors such as viewpoint, occlusion, motion, and
deformations. Starting from the recent idea of viewpoint factorization, we
propose a new approach that, given a large number of images of an object and no
other supervision, can extract a dense object-centric coordinate frame. This
coordinate frame is invariant to deformations of the images and comes with a
dense equivariant labelling neural network that can map image pixels to their
corresponding object coordinates. We demonstrate the applicability of this
method to simple articulated objects and deformable objects such as human
faces, learning embeddings from random synthetic transformations or optical
flow correspondences, all without any manual supervision.
",1,0,0,1,0,0
16594,16595,Invariant tori for the Nosé Thermostat near the High-Temperature Limit,"  Let H(q,p) = p^2/2 + V(q) be a 1-degree of freedom mechanical Hamiltonian
with a C^n periodic potential V where n>4. The Nosé-thermostated system
associated to H is shown to have invariant tori near the infinite temperature
limit. This is shown to be true for all thermostats similar to Nosé's. These
results complement the result of Legoll, Luskin and Moeckel who proved the
existence of such tori near the decoupling limit.
",0,0,1,0,0,0
18679,18680,Discriminant analysis in small and large dimensions,"  We study the distributional properties of the linear discriminant function
under the assumption of normality by comparing two groups with the same
covariance matrix but different mean vectors. A stochastic representation for
the discriminant function coefficients is derived which is then used to obtain
their asymptotic distribution under the high-dimensional asymptotic regime. We
investigate the performance of the classification analysis based on the
discriminant function in both small and large dimensions. A stochastic
representation is established which allows to compute the error rate in an
efficient way. We further compare the calculated error rate with the optimal
one obtained under the assumption that the covariance matrix and the two mean
vectors are known. Finally, we present an analytical expression of the error
rate calculated in the high-dimensional asymptotic regime. The finite-sample
properties of the derived theoretical results are assessed via an extensive
Monte Carlo study.
",0,0,1,1,0,0
17280,17281,Nearly second-order asymptotic optimality of sequential change-point detection with one-sample updates,"  Sequential change-point detection when the distribution parameters are
unknown is a fundamental problem in statistics and machine learning. When the
post-change parameters are unknown, we consider a set of detection procedures
based on sequential likelihood ratios with non-anticipating estimators
constructed using online convex optimization algorithms such as online mirror
descent, which provides a more versatile approach to tackle complex situations
where recursive maximum likelihood estimators cannot be found. When the
underlying distributions belong to a exponential family and the estimators
satisfy the logarithm regret property, we show that this approach is nearly
second-order asymptotically optimal. This means that the upper bound for the
false alarm rate of the algorithm (measured by the average-run-length) meets
the lower bound asymptotically up to a log-log factor when the threshold tends
to infinity. Our proof is achieved by making a connection between sequential
change-point and online convex optimization and leveraging the logarithmic
regret bound property of online mirror descent algorithm. Numerical and real
data examples validate our theory.
",1,0,1,1,0,0
8844,8845,A new method of joint nonparametric estimation of probability density and its support,"  In this paper we propose a new method of joint nonparametric estimation of
probability density and its support. As is well known, nonparametric kernel
density estimator has ""boundary bias problem"" when the support of the
population density is not the whole real line. To avoid the unknown boundary
effects, our estimator detects the boundary, and eliminates the boundary-bias
of the estimator simultaneously. Moreover, we refer an extension to a simple
multivariate case, and propose an improved estimator free from the unknown
boundary bias.
",0,0,1,1,0,0
6902,6903,Evidence synthesis for stochastic epidemic models,"  In recent years the role of epidemic models in informing public health
policies has progressively grown. Models have become increasingly realistic and
more complex, requiring the use of multiple data sources to estimate all
quantities of interest. This review summarises the different types of
stochastic epidemic models that use evidence synthesis and highlights current
challenges.
",0,0,0,1,0,0
12598,12599,Concentration of quadratic forms under a Bernstein moment assumption,"  A concentration result for quadratic form of independent subgaussian random
variables is derived. If the moments of the random variables satisfy a
""Bernstein condition"", then the variance term of the Hanson-Wright inequality
can be improved. The Bernstein condition is satisfied, for instance, by all
log-concave subgaussian distributions.
",0,0,1,1,0,0
16618,16619,Development of a passive Rehabilitation Robot for the wrist joint through the implementation of an Arduino UNO microcontroller,"  In this research was implemented the use of an Arduino UNO R3 microcontroller
to control the movements of a prototype robotic functional developed to perform
rehabilitation exercises in the wrist joint; This device can be used to assist
the physiatrist to rehabilitate the tendinitis, synovitis, rheumatoid arthritis
and for pre-operative and post-operative therapy in this joint. During the
design stage of the functional prototype, the methodology of the industrial
design process was used from a concurrent engineering approach, through which
anthropometric studies could be performed related to the dimensions and angles
of movement of the wrist joint in the population Venezuelan from the
information collected, the design proposal was elaborated, and the use of CAD
programs defined the different forms, geometries and materials of the
components of the rehabilitation device, which were later analyzed using the
finite element method for the determination The tensional state of efforts and
safety factors through the use of CAE programs. In addition, a software was
developed for the acquisition, registration, reproduction and execution of the
different movements produced during the rehabilitation therapy. Through the
research developed, a device was designed that will help the rehabilitation of
the wrist joint allowing the combination of dorsal-palmar flexion and
ulnar-radial movements to recover the joint function of various pathologies
presented in the Venezuelan population.
",1,1,0,0,0,0
10366,10367,A Relaxed Kačanov Iteration for the $p$-Poisson Problem,"  In this paper, we introduce an iterative linearization scheme that allows to
approximate the weak solution of the $p$-Poisson problem
\begin{align*}
-\operatorname{div}(|\nabla u|^{p-2}\nabla u) &= f\quad\text{in }\Omega,
u&= 0\quad\text{on}\partial\Omega
\end{align*} for $1 < p \leq 2$. The algorithm can be interpreted as a
relaxed Kačanov iteration. We prove that the algorithm converges at least
with an algebraic rate.
",0,0,1,0,0,0
19771,19772,WKB solutions of difference equations and reconstruction by the topological recursion,"  The purpose of this article is to analyze the connection between
Eynard-Orantin topological recursion and formal WKB solutions of a
$\hbar$-difference equation: $\Psi(x+\hbar)=\left(e^{\hbar\frac{d}{dx}}\right)
\Psi(x)=L(x;\hbar)\Psi(x)$ with $L(x;\hbar)\in GL_2( (\mathbb{C}(x))[\hbar])$.
In particular, we extend the notion of determinantal formulas and topological
type property proposed for formal WKB solutions of $\hbar$-differential systems
to this setting. We apply our results to a specific $\hbar$-difference system
associated to the quantum curve of the Gromov-Witten invariants of
$\mathbb{P}^1$ for which we are able to prove that the correlation functions
are reconstructed from the Eynard-Orantin differentials computed from the
topological recursion applied to the spectral curve $y=\cosh^{-1}\frac{x}{2}$.
Finally, identifying the large $x$ expansion of the correlation functions,
proves a recent conjecture made by B. Dubrovin and D. Yang regarding a new
generating series for Gromov-Witten invariants of $\mathbb{P}^1$.
",0,1,1,0,0,0
4887,4888,Anharmonicity and the isotope effect in superconducting lithium at high pressures: a first-principles approach,"  Recent experiments [Schaeffer 2015] have shown that lithium presents an
extremely anomalous isotope effect in the 15-25 GPa pressure range. In this
article we have calculated the anharmonic phonon dispersion of $\mathrm{^7Li}$
and $\mathrm{^6Li}$ under pressure, their superconducting transition
temperatures, and the associated isotope effect. We have found a huge
anharmonic renormalization of a transverse acoustic soft mode along $\Gamma$K
in the fcc phase, the expected structure at the pressure range of interest. In
fact, the anharmonic correction dynamically stabilizes the fcc phase above 25
GPa. However, we have not found any anomalous scaling of the superconducting
temperature with the isotopic mass. Additionally, we have also analyzed whether
the two lithium isotopes adopting different structures could explain the
observed anomalous behavior. According to our enthalpy calculations including
zero-point motion and anharmonicity it would not be possible in a stable
regime.
",0,1,0,0,0,0
20782,20783,A Study of MAC Address Randomization in Mobile Devices and When it Fails,"  MAC address randomization is a privacy technique whereby mobile devices
rotate through random hardware addresses in order to prevent observers from
singling out their traffic or physical location from other nearby devices.
Adoption of this technology, however, has been sporadic and varied across
device manufacturers. In this paper, we present the first wide-scale study of
MAC address randomization in the wild, including a detailed breakdown of
different randomization techniques by operating system, manufacturer, and model
of device.
We then identify multiple flaws in these implementations which can be
exploited to defeat randomization as performed by existing devices. First, we
show that devices commonly make improper use of randomization by sending
wireless frames with the true, global address when they should be using a
randomized address. We move on to extend the passive identification techniques
of Vanhoef et al. to effectively defeat randomization in ~96% of Android
phones. Finally, we show a method that can be used to track 100% of devices
using randomization, regardless of manufacturer, by exploiting a previously
unknown flaw in the way existing wireless chipsets handle low-level control
frames.
",1,0,0,0,0,0
6209,6210,Enhanced Quantum Synchronization via Quantum Machine Learning,"  We study the quantum synchronization between a pair of two-level systems
inside two coupled cavities. By using a digital-analog decomposition of the
master equation that rules the system dynamics, we show that this approach
leads to quantum synchronization between both two-level systems. Moreover, we
can identify in this digital-analog block decomposition the fundamental
elements of a quantum machine learning protocol, in which the agent and the
environment (learning units) interact through a mediating system, namely, the
register. If we can additionally equip this algorithm with a classical feedback
mechanism, which consists of projective measurements in the register,
reinitialization of the register state and local conditional operations on the
agent and environment subspace, a powerful and flexible quantum machine
learning protocol emerges. Indeed, numerical simulations show that this
protocol enhances the synchronization process, even when every subsystem
experience different loss/decoherence mechanisms, and give us the flexibility
to choose the synchronization state. Finally, we propose an implementation
based on current technologies in superconducting circuits.
",1,0,0,1,0,0
4629,4630,Exploring a search for long-duration transient gravitational waves associated with magnetar bursts,"  Soft gamma repeaters and anomalous X-ray pulsars are thought to be magnetars,
neutron stars with strong magnetic fields of order $\mathord{\sim}
10^{13}$--$10^{15} \, \mathrm{gauss}$. These objects emit intermittent bursts
of hard X-rays and soft gamma rays. Quasiperiodic oscillations in the X-ray
tails of giant flares imply the existence of neutron star oscillation modes
which could emit gravitational waves powered by the magnetar's magnetic energy
reservoir. We describe a method to search for transient gravitational-wave
signals associated with magnetar bursts with durations of 10s to 1000s of
seconds. The sensitivity of this method is estimated by adding simulated
waveforms to data from the sixth science run of Laser Interferometer
Gravitational-wave Observatory (LIGO). We find a search sensitivity in terms of
the root sum square strain amplitude of $h_{\mathrm{rss}} = 1.3 \times 10^{-21}
\, \mathrm{Hz}^{-1/2}$ for a half sine-Gaussian waveform with a central
frequency $f_0 = 150 \, \mathrm{Hz}$ and a characteristic time $\tau = 400 \,
\mathrm{s}$. This corresponds to a gravitational wave energy of
$E_{\mathrm{GW}} = 4.3 \times 10^{46} \, \mathrm{erg}$, the same order of
magnitude as the 2004 giant flare which had an estimated electromagnetic energy
of $E_{\mathrm{EM}} = \mathord{\sim} 1.7 \times 10^{46} (d/ 8.7 \,
\mathrm{kpc})^2 \, \mathrm{erg}$, where $d$ is the distance to SGR 1806-20. We
present an extrapolation of these results to Advanced LIGO, estimating a
sensitivity to a gravitational wave energy of $E_{\mathrm{GW}} = 3.2 \times
10^{43} \, \mathrm{erg}$ for a magnetar at a distance of $1.6 \, \mathrm{kpc}$.
These results suggest this search method can probe significantly below the
energy budgets for magnetar burst emission mechanisms such as crust cracking
and hydrodynamic deformation.
",0,1,0,0,0,0
14888,14889,To Wait or Not to Wait: Two-way Functional Hazards Model for Understanding Waiting in Call Centers,"  Telephone call centers offer a convenient communication channel between
businesses and their customers. Efficient management of call centers needs
accurate modeling of customer waiting behavior, which contains important
information about customer patience (how long a customer is willing to wait)
and service quality (how long a customer needs to wait to get served). Hazard
functions offer dynamic characterization of customer waiting behavior, and
provide critical inputs for agent scheduling. Motivated by this application, we
develop a two-way functional hazards (tF-Hazards) model to study customer
waiting behavior as a function of two timescales, waiting duration and the time
of day that a customer calls in. The model stems from a two-way piecewise
constant hazard function, and imposes low-rank structure and smoothness on the
hazard rates to enhance interpretability. We exploit an alternating direction
method of multipliers (ADMM) algorithm to optimize a penalized likelihood
function of the model. We carefully analyze the data from a US bank call
center, and provide informative insights about customer patience and service
quality patterns along waiting time and across different times of a day. The
findings provide primitive inputs for call center agent staffing and
scheduling, as well as for call center practitioners to understand the effect
of system protocols on customer waiting behavior.
",0,0,0,1,0,0
14493,14494,Continual Lifelong Learning with Neural Networks: A Review,"  Humans and animals have the ability to continually acquire, fine-tune, and
transfer knowledge and skills throughout their lifespan. This ability, referred
to as lifelong learning, is mediated by a rich set of neurocognitive mechanisms
that together contribute to the development and specialization of our
sensorimotor skills as well as to the long-term memory consolidation and
retrieval without catastrophic forgetting. Consequently, lifelong learning
capabilities are crucial for autonomous agents interacting in the real world
and processing continuous streams of information. However, lifelong learning
remains a long-standing challenge for machine learning and neural network
models since the continual acquisition of incrementally available information
from non-stationary data distributions generally leads to catastrophic
forgetting or interference. This limitation represents a major drawback for
state-of-the-art deep neural network models that typically learn
representations from stationary batches of training data, thus without
accounting for situations in which information becomes incrementally available
over time. In this review, we critically summarize the main challenges linked
to lifelong learning for artificial learning systems and compare existing
neural network approaches that alleviate, to different extents, catastrophic
forgetting. We discuss well-established and emerging research motivated by
lifelong learning factors in biological systems such as structural plasticity,
memory replay, curriculum and transfer learning, intrinsic motivation, and
multisensory integration.
",0,0,0,1,1,0
5590,5591,Scalable and Efficient Statistical Inference with Estimating Functions in the MapReduce Paradigm for Big Data,"  The theory of statistical inference along with the strategy of
divide-and-conquer for large- scale data analysis has recently attracted
considerable interest due to great popularity of the MapReduce programming
paradigm in the Apache Hadoop software framework. The central analytic task in
the development of statistical inference in the MapReduce paradigm pertains to
the method of combining results yielded from separately mapped data batches.
One seminal solution based on the confidence distribution has recently been
established in the setting of maximum likelihood estimation in the literature.
This paper concerns a more general inferential methodology based on estimating
functions, termed as the Rao-type confidence distribution, of which the maximum
likelihood is a special case. This generalization provides a unified framework
of statistical inference that allows regression analyses of massive data sets
of important types in a parallel and scalable fashion via a distributed file
system, including longitudinal data analysis, survival data analysis, and
quantile regression, which cannot be handled using the maximum likelihood
method. This paper investigates four important properties of the proposed
method: computational scalability, statistical optimality, methodological
generality, and operational robustness. In particular, the proposed method is
shown to be closely connected to Hansen's generalized method of moments (GMM)
and Crowder's optimality. An interesting theoretical finding is that the
asymptotic efficiency of the proposed Rao-type confidence distribution
estimator is always greater or equal to the estimator obtained by processing
the full data once. All these properties of the proposed method are illustrated
via numerical examples in both simulation studies and real-world data analyses.
",0,0,0,1,0,0
19504,19505,Sales Forecast in E-commerce using Convolutional Neural Network,"  Sales forecast is an essential task in E-commerce and has a crucial impact on
making informed business decisions. It can help us to manage the workforce,
cash flow and resources such as optimizing the supply chain of manufacturers
etc. Sales forecast is a challenging problem in that sales is affected by many
factors including promotion activities, price changes, and user preferences
etc. Traditional sales forecast techniques mainly rely on historical sales data
to predict future sales and their accuracies are limited. Some more recent
learning-based methods capture more information in the model to improve the
forecast accuracy. However, these methods require case-by-case manual feature
engineering for specific commercial scenarios, which is usually a difficult,
time-consuming task and requires expert knowledge. To overcome the limitations
of existing methods, we propose a novel approach in this paper to learn
effective features automatically from the structured data using the
Convolutional Neural Network (CNN). When fed with raw log data, our approach
can automatically extract effective features from that and then forecast sales
using those extracted features. We test our method on a large real-world
dataset from CaiNiao.com and the experimental results validate the
effectiveness of our method.
",1,0,0,0,0,0
4862,4863,Tension and chemical efficiency of Myosin-II motors,"  Recent experiments demonstrate that molecular motors from the Myosin II
family serve as cross-links inducing active tension in the cytoskeletal
network. Here we revise the Brownian ratchet model, previously studied in the
context of active transport along polymer tracks, in setups resembling a motor
in a polymer network, also taking into account the effect of electrostatic
changes in the motor heads. We explore important mechanical quantities and show
that such a model is also capable of mechanosensing. Finally, we introduce a
novel efficiency based on excess heat production by the chemical cycle which is
directly related to the active tension the motor exerts. The chemical
efficiencies differ considerably for motors with a different number of heads,
while their mechanical properties remain qualitatively similar. For motors with
a small number of heads, the chemical efficiency is maximal when they are
frustrated, a trait that is not found in larger motors.
",0,1,0,0,0,0
11033,11034,The Passive Eavesdropper Affects my Channel: Secret-Key Rates under Real-World Conditions (Extended Version),"  Channel-reciprocity based key generation (CRKG) has gained significant
importance as it has recently been proposed as a potential lightweight security
solution for IoT devices. However, the impact of the attacker's position in
close range has only rarely been evaluated in practice, posing an open research
problem about the security of real-world realizations. Furthermore, this would
further bridge the gap between theoretical channel models and their
practice-oriented realizations. For security metrics, we utilize
cross-correlation, mutual information, and a lower bound on secret-key
capacity. We design a practical setup of three parties such that the channel
statistics, although based on joint randomness, are always reproducible. We run
experiments to obtain channel states and evaluate the aforementioned metrics
for the impact of an attacker depending on his position. It turns out the
attacker himself affects the outcome, which has not been adequately regarded
yet in standard channel models.
",1,0,1,0,0,0
13720,13721,Dynamical and Topological Aspects of Consensus Formation in Complex Networks,"  The present work analyses a particular scenario of consensus formation, where
the individuals navigate across an underlying network defining the topology of
the walks. The consensus, associated to a given opinion coded as a simple
messages, is generated by interactions during the agent's walk and manifest
itself in the collapse of the various opinions into a single one. We analyze
how the topology of the underlying networks and the rules of interaction
between the agents promote or inhibit the emergence of this consensus. We find
that non-linear interaction rules are required to form consensus and that
consensus is more easily achieved in networks whose degree distribution is
narrower.
",1,1,0,0,0,0
16208,16209,Carbon stars in the X-Shooter Spectral Library: II. Comparison with models,"  In a previous paper, we assembled a collection of medium-resolution spectra
of 35 carbon stars, covering optical and near-infrared wavelengths from 400 to
2400 nm. The sample includes stars from the Milky Way and the Magellanic
Clouds, with a variety of $(J-K_s)$ colors and pulsation properties. In the
present paper, we compare these observations to a new set of high-resolution
synthetic spectra, based on hydrostatic model atmospheres. We find that the
broad-band colors and the molecular-band strengths measured by
spectrophotometric indices match those of the models when $(J-K_s)$ is bluer
than about 1.6, while the redder stars require either additional reddening or
dust emission or both. Using a grid of models to fit the full observed spectra,
we estimate the most likely atmospheric parameters $T_\mathrm{eff}$, $\log(g)$,
$[\mathrm{Fe/H}]$ and C/O. These parameters derived independently in the
optical and near-infrared are generally consistent when $(J-K_s)<1.6$. The
temperatures found based on either wavelength range are typically within
$\pm$100K of each other, and $\log(g)$ and $[\mathrm{Fe/H}]$ are consistent
with the values expected for this sample. The reddest stars ($(J-K_s)$ $>$ 1.6)
are divided into two families, characterized by the presence or absence of an
absorption feature at 1.53\,$\mu$m, generally associated with HCN and
C$_2$H$_2$. Stars from the first family begin to be more affected by
circumstellar extinction. The parameters found using optical or near-infrared
wavelengths are still compatible with each other, but the error bars become
larger. In stars showing the 1.53\,$\mu$m feature, which are all
large-amplitude variables, the effects of pulsation are strong and the spectra
are poorly matched with hydrostatic models. For these, atmospheric parameters
could not be derived reliably, and dynamical models are needed for proper
interpretation.
",0,1,0,0,0,0
11395,11396,LitStoryTeller: An Interactive System for Visual Exploration of Scientific Papers Leveraging Named entities and Comparative Sentences,"  The present study proposes LitStoryTeller, an interactive system for visually
exploring the semantic structure of a scientific article. We demonstrate how
LitStoryTeller could be used to answer some of the most fundamental research
questions, such as how a new method was built on top of existing methods, based
on what theoretical proof and experimental evidences. More importantly,
LitStoryTeller can assist users to understand the full and interesting story a
scientific paper, with a concise outline and important details. The proposed
system borrows a metaphor from screen play, and visualizes the storyline of a
scientific paper by arranging its characters (scientific concepts or
terminologies) and scenes (paragraphs/sentences) into a progressive and
interactive storyline. Such storylines help to preserve the semantic structure
and logical thinking process of a scientific paper. Semantic structures, such
as scientific concepts and comparative sentences, are extracted using existing
named entity recognition APIs and supervised classifiers, from a scientific
paper automatically. Two supplementary views, ranked entity frequency view and
entity co-occurrence network view, are provided to help users identify the
""main plot"" of such scientific storylines. When collective documents are ready,
LitStoryTeller also provides a temporal entity evolution view and entity
community view for collection digestion.
",1,0,0,0,0,0
11555,11556,A statistical approach to identify superluminous supernovae and probe their diversity,"  We investigate the identification of hydrogen-poor superluminous supernovae
(SLSNe I) using a photometric analysis, without including an arbitrary
magnitude threshold. We assemble a homogeneous sample of previously classified
SLSNe I from the literature, and fit their light curves using Gaussian
processes. From the fits, we identify four photometric parameters that have a
high statistical significance when correlated, and combine them in a parameter
space that conveys information on their luminosity and color evolution. This
parameter space presents a new definition for SLSNe I, which can be used to
analyse existing and future transient datasets. We find that 90% of previously
classified SLSNe I meet our new definition. We also examine the evidence for
two subclasses of SLSNe I, combining their photometric evolution with
spectroscopic information, namely the photospheric velocity and its gradient. A
cluster analysis reveals the presence of two distinct groups. `Fast' SLSNe show
fast light curves and color evolution, large velocities, and a large velocity
gradient. `Slow' SLSNe show slow light curve and color evolution, small
expansion velocities, and an almost non-existent velocity gradient. Finally, we
discuss the impact of our analyses in the understanding of the powering engine
of SLSNe, and their implementation as cosmological probes in current and future
surveys.
",0,1,0,0,0,0
12337,12338,Some new bounds of placement delivery arrays,"  Coded caching scheme is a technique which reduce the load during peak traffic
times in a wireless network system. Placement delivery array (PDA in short) was
first introduced by Yan et al.. It can be used to design coded caching scheme.
In this paper, we prove some lower bounds of PDA on the element and some lower
bounds of PDA on the column. We also give some constructions for optimal PDA.
",1,0,0,0,0,0
11060,11061,Spatial cytoskeleton organization supports targeted intracellular transport,"  The efficiency of intracellular cargo transport from specific source to
target locations is strongly dependent upon molecular motor-assisted motion
along the cytoskeleton. Radial transport along microtubules and lateral
transport along the filaments of the actin cortex underneath the cell membrane
are characteristic for cells with a centrosome. The interplay between the
specific cytoskeleton organization and the motor performance realizes a
spatially inhomogeneous intermittent search strategy. In order to analyze the
efficiency of such intracellular search strategies we formulate a random
velocity model with intermittent arrest states. We evaluate efficiency in terms
of mean first passage times for three different, frequently encountered
intracellular transport tasks: i) the narrow escape problem, which emerges
during cargo transport to a synapse or other specific region of the cell
membrane, ii) the reaction problem, which considers the binding time of two
particles within the cell, and iii) the reaction-escape problem, which arises
when cargo must be released at a synapse only after pairing with another
particle. Our results indicate that cells are able to realize efficient search
strategies for various intracellular transport tasks economically through a
spatial cytoskeleton organization that involves only a narrow actin cortex
rather than a cell body filled with randomly oriented actin filaments.
",0,1,0,0,0,0
2551,2552,Linear density-based clustering with a discrete density model,"  Density-based clustering techniques are used in a wide range of data mining
applications. One of their most attractive features con- sists in not making
use of prior knowledge of the number of clusters that a dataset contains along
with their shape. In this paper we propose a new algorithm named Linear DBSCAN
(Lin-DBSCAN), a simple approach to clustering inspired by the density model
introduced with the well known algorithm DBSCAN. Designed to minimize the
computational cost of density based clustering on geospatial data, Lin-DBSCAN
features a linear time complexity that makes it suitable for real-time
applications on low-resource devices. Lin-DBSCAN uses a discrete version of the
density model of DBSCAN that takes ad- vantage of a grid-based scan and merge
approach. The name of the algorithm stems exactly from its main features
outlined above. The algorithm was tested with well known data sets.
Experimental results prove the efficiency and the validity of this approach
over DBSCAN in the context of spatial data clustering, enabling the use of a
density-based clustering technique on large datasets with low computational
cost.
",0,0,0,1,0,0
12062,12063,A perturbation theory for water with an associating reference fluid,"  The theoretical description of the thermodynamics of water is challenged by
the structural transition towards tetrahedral symmetry at ambient conditions.
As perturbation theories typically assume a spherically symmetric reference
fluid, they are incapable of accurately describing the liquid properties of
water at ambient conditions. In this paper we solve this problem, by
introducing the concept of an associated reference perturbation theory (APT).
In APT we treat the reference fluid as an associating hard sphere fluid which
transitions to tetrahedral symmetry in the fully hydrogen bonded limit. We
calculate this transition in a theoretically self-consistent manner without
appealing to molecular simulations. This associated reference provides the
reference fluid for a second order Barker-Hendersen perturbative treatment of
the long-range attractions. We demonstrate that this new approach gives a
significantly improved description of water as compared to standard
perturbation theories.
",0,1,0,0,0,0
7940,7941,Bootstrapping kernel intensity estimation for nonhomogeneous point processes depending on spatial covariates,"  In the spatial point process context, kernel intensity estimation has been
mainly restricted to exploratory analysis due to its lack of consistency.
Different methods have been analysed to overcome this problem, and the
inclusion of covariates resulted to be one possible solution. In this paper we
focus on de\-fi\-ning a theoretical framework to derive a consistent kernel
intensity estimator using covariates, as well as a consistent smooth bootstrap
procedure. We define two new data-driven bandwidth selectors specifically
designed for our estimator: a rule-of-thumb and a plug-in bandwidth based on
our consistent bootstrap method. A simulation study is accomplished to
understand the performance of our proposals in finite samples. Finally, we
describe an application to a real data set consisting of the wildfires in
Canada during June 2015, using meteorological information as covariates.
",0,0,0,1,0,0
17001,17002,Energy and time measurements with high-granular silicon devices,"  This note is a short summary of the workshop on ""Energy and time measurements
with high-granular silicon devices"" that took place on the 13/6/16 and the
14/6/16 at DESY/Hamburg in the frame of the first AIDA-2020 Annual Meeting.
This note tries to put forward trends that could be spotted and to emphasise in
particular open issues that were addressed by the speakers.
",0,1,0,0,0,0
9018,9019,Semisimple characters for inner froms I: GL_n(D),"  The article is about the representation theory of an inner form~$G$ of a
general linear group over a non-archimedean local field. We introduce
semisimple characters for~$G$ whose intertwining classes describe conjecturally
via Local Langlands correspondence the behavior on wild inertia. These
characters also play a potential role to understand the classification of
irreducible smooth representations of inner forms of classical groups. We prove
the intertwining formula for semisimple characters and an intertwining implies
conjugacy like theorem. Further we show that endo-parameters for~$G$, i.e.
invariants consisting of simple endo-classes and a numerical part, classify the
intertwining classes of semisimple characters for~$G$. They should be the
counter part for restrictions of Langlands-parameters to wild inertia under
Local Langlands correspondence.
",0,0,1,0,0,0
2190,2191,Wright-Fisher diffusions for evolutionary games with death-birth updating,"  We investigate spatial evolutionary games with death-birth updating in large
finite populations. Within growing spatial structures subject to appropriate
conditions, the density processes of a fixed type are proven to converge to the
Wright-Fisher diffusions with drift. In addition, convergence in the
Wasserstein distance of the laws of their occupation measures holds. The proofs
of these results develop along an equivalence between the laws of the
evolutionary games and certain voter models and rely on the analogous results
of voter models on large finite sets by convergences of the Radon-Nikodym
derivative processes. As another application of this equivalence of laws, we
show that in a general, large population of size $N$, for which the stationary
probabilities of the corresponding voting kernel are comparable to uniform
probabilities, a first-derivative test among the major methods for these
evolutionary games is applicable at least up to weak selection strengths in the
usual biological sense (that is, selection strengths of the order $\mathcal
O(1/N)$).
",0,0,1,0,0,0
7058,7059,Betting on Quantum Objects,"  Dutch book arguments have been applied to beliefs about the outcomes of
measurements of quantum systems, but not to beliefs about quantum objects prior
to measurement. In this paper, we prove a quantum version of the probabilists'
Dutch book theorem that applies to both sorts of beliefs: roughly, if ideal
beliefs are given by vector states, all and only Born-rule probabilities avoid
Dutch books. This theorem and associated results have implications for
operational and realist interpretations of the logic of a Hilbert lattice. In
the latter case, we show that the defenders of the eigenstate-value orthodoxy
face a trilemma. Those who favor vague properties avoid the trilemma, admitting
all and only those beliefs about quantum objects that avoid Dutch books.
",0,1,0,0,0,0
9524,9525,High-Speed Demodulation of weak FBGs Based on Microwave Photonics and Chromatic Dispersion,"  A high speed quasi-distributed demodulation method based on the microwave
photonics and the chromatic dispersion effect is designed and implemented for
weak fiber Bragg gratings (FBGs). Due to the effect of dispersion compensation
fiber (DCF), FBG wavelength shift leads to the change of the difference
frequency signal at the mixer. With the way of crossing microwave sweep cycle,
all wavelengths of cascade FBGs can be high speed obtained by measuring the
frequencies change. Moreover, through the introduction of Chirp-Z and Hanning
window algorithm, the analysis of difference frequency signal is achieved very
well. By adopting the single-peak filter as a reference, the length disturbance
of DCF caused by temperature can be also eliminated. Therefore, the accuracy of
this novel method is greatly improved, and high speed demodulation of FBGs can
easily realize. The feasibility and performance are experimentally demonstrated
using 105 FBGs with 0.1% reflectivity, 1 m spatial interval. Results show that
each grating can be distinguished well, and the demodulation rate is as high as
40 kHz, the accuracy is about 8 pm.
",0,1,0,0,0,0
14103,14104,The Effect of Mixing on the Observed Metallicity of the Smith Cloud,"  Measurements of high-velocity clouds' metallicities provide important clues
about their origins, and hence on whether they play a role in fueling ongoing
star formation in the Galaxy. However, accurate interpretation of these
measurements requires compensating for the galactic material that has been
mixed into the clouds. In order to determine how much the metallicity changes
as a result of this mixing, we have carried out three-dimensional
wind-tunnel-like hydrodynamical simulations of an example cloud. Our model
cloud is patterned after the Smith Cloud, a particularly well-studied cloud of
mass $\sim 5 \times 10^6~M_\odot$. We calculated the fraction of the
high-velocity material that had originated in the galactic halo,
$F_\mathrm{h}$, for various sight lines passing through our model cloud. We
find that $F_\mathrm{h}$ generally increases with distance from the head of the
cloud, reaching $\sim$0.5 in the tail of the cloud. Models in which the
metallicities (relative to solar) of the original cloud, $Z_\mathrm{cl}$, and
of the halo, $Z_\mathrm{h}$, are in the approximate ranges $0.1 \lesssim
Z_\mathrm{cl} \lesssim 0.3$ and $0.7 \lesssim Z_\mathrm{h} \lesssim 1.0$,
respectively, are in rough agreement with the observations. Models with
$Z_\mathrm{h} \sim 0.1$ and $Z_\mathrm{cl} \gtrsim 0.5$ are also in rough
agreement with the observations, but such a low halo metallicity is
inconsistent with recent independent measurements. We conclude that the Smith
Cloud's observed metallicity may not be a true reflection of its original
metallicity and that the cloud's ultimate origin remains uncertain.
",0,1,0,0,0,0
19036,19037,Bounds for fidelity of semiclassical Lagrangian states in K{ä}hler quantization,"  We define mixed states associated with submanifolds with probability
densities in quantizable closed K{ä}hler manifolds. Then, we address the
problem of comparing two such states via their fidelity. Firstly, we estimate
the sub-fidelity and super-fidelity of two such states, giving lower and upper
bounds for their fidelity, when the underlying submanifolds are two Lagrangian
submanifolds intersecting transversally at a finite number of points, in the
semiclassical limit. Secondly, we investigate a family of examples on the
sphere, for which we manage to obtain a better upper bound for the fidelity. We
conclude by stating a conjecture regarding the fidelity in the general case.
",0,0,1,0,0,0
18284,18285,Spreading of correlations in the Falicov-Kimball model,"  We study dynamical properties of the one- and two-dimensional Falicov-Kimball
model using lattice Monte Carlo simulations. In particular, we calculate the
spreading of charge correlations in the equilibrium model and after an
interaction quench. The results show a reduction of the light-cone velocity
with interaction strength at low temperature, while the phase velocity
increases. At higher temperature, the initial spreading is determined by the
Fermi velocity of the noninteracting system and the maximum range of the
correlations decreases with increasing interaction strength. Charge order
correlations in the disorder potential enhance the range of the correlations.
We also use the numerically exact lattice Monte Carlo results to benchmark the
accuracy of equilibrium and nonequilibrium dynamical cluster approximation
calculations. It is shown that the bias introduced by the mapping to a
periodized cluster is substantial, and that from a numerical point of view, it
is more efficient to simulate the lattice model directly.
",0,1,0,0,0,0
4950,4951,Designing nearly tight window for improving time-frequency masking,"  Many audio signal processing methods are formulated in the time-frequency
(T-F) domain which is obtained by the short-time Fourier transform (STFT). The
property of STFT is fully characterized by window function, and thus designing
a better window is important for improving the performance of the processing
especially when a less redundant T-F representation is desirable. While many
window functions have been proposed in the literature, they are designed to
have a good frequency response for analysis, which may not perform well in
terms of signal processing. The window design must take the effect of the
reconstruction (from the T-F domain into the time domain) into account for
improving the performance. In this paper, an optimization-based design method
of a nearly tight window is proposed to obtain a window performing well for the
T-F domain signal processing.
",1,0,0,0,0,0
14113,14114,Do altmetrics correlate with the quality of papers? A large-scale empirical study based on F1000Prime data,"  In this study, we address the question whether (and to what extent,
respectively) altmetrics are related to the scientific quality of papers (as
measured by peer assessments). Only a few studies have previously investigated
the relationship between altmetrics and assessments by peers. In the first
step, we analyse the underlying dimensions of measurement for traditional
metrics (citation counts) and altmetrics - by using principal component
analysis (PCA) and factor analysis (FA). In the second step, we test the
relationship between the dimensions and quality of papers (as measured by the
post-publication peer-review system of F1000Prime assessments) - using
regression analysis. The results of the PCA and FA show that altmetrics operate
along different dimensions, whereas Mendeley counts are related to citation
counts, and tweets form a separate dimension. The results of the regression
analysis indicate that citation-based metrics and readership counts are
significantly more related to quality, than tweets. This result on the one hand
questions the use of Twitter counts for research evaluation purposes and on the
other hand indicates potential use of Mendeley reader counts.
",1,0,0,0,0,0
15050,15051,JDFTx: software for joint density-functional theory,"  Density-functional theory (DFT) has revolutionized computational prediction
of atomic-scale properties from first principles in physics, chemistry and
materials science. Continuing development of new methods is necessary for
accurate predictions of new classes of materials and properties, and for
connecting to nano- and mesoscale properties using coarse-grained theories.
JDFTx is a fully-featured open-source electronic DFT software designed
specifically to facilitate rapid development of new theories, models and
algorithms. Using an algebraic formulation as an abstraction layer, compact
C++11 code automatically performs well on diverse hardware including GPUs. This
code hosts the development of joint density-functional theory (JDFT) that
combines electronic DFT with classical DFT and continuum models of liquids for
first-principles calculations of solvated and electrochemical systems. In
addition, the modular nature of the code makes it easy to extend and interface
with, facilitating the development of multi-scale toolkits that connect to ab
initio calculations, e.g. photo-excited carrier dynamics combining electron and
phonon calculations with electromagnetic simulations.
",0,1,0,0,0,0
19232,19233,Boltzmann Encoded Adversarial Machines,"  Restricted Boltzmann Machines (RBMs) are a class of generative neural network
that are typically trained to maximize a log-likelihood objective function. We
argue that likelihood-based training strategies may fail because the objective
does not sufficiently penalize models that place a high probability in regions
where the training data distribution has low probability. To overcome this
problem, we introduce Boltzmann Encoded Adversarial Machines (BEAMs). A BEAM is
an RBM trained against an adversary that uses the hidden layer activations of
the RBM to discriminate between the training data and the probability
distribution generated by the model. We present experiments demonstrating that
BEAMs outperform RBMs and GANs on multiple benchmarks.
",0,0,0,1,0,0
1991,1992,Coherent modulation up to 100 GBd 16QAM using silicon-organic hybrid (SOH) devices,"  We demonstrate the generation of higher-order modulation formats using
silicon-based inphase/quadrature (IQ) modulators at symbol rates of up to 100
GBd. Our devices exploit the advantages of silicon-organic hybrid (SOH)
integration, which combines silicon-on-insulator waveguides with highly
efficient organic electro-optic (EO) cladding materials to enable small drive
voltages and sub-millimeter device lengths. In our experiments, we use an SOH
IQ modulator with a {\pi}-voltage of 1.6 V to generate 100 GBd 16QAM signals.
This is the first time that the 100 GBd mark is reached with an IQ modulator
realized on a semiconductor substrate, leading to a single-polarization line
rate of 400 Gbit/s. The peak-to-peak drive voltages amount to 1.5 Vpp,
corresponding to an electrical energy dissipation in the modulator of only 25
fJ/bit.
",0,1,0,0,0,0
6091,6092,On the radius of spatial analyticity for the quartic generalized KdV equation,"  Lower bound on the rate of decrease in time of the uniform radius of spatial
analyticity of solutions to the quartic generalized KdV equation is derived,
which improves an earlier result by Bona, Grujić and Kalisch.
",0,0,1,0,0,0
7472,7473,Fractional quiver W-algebras,"  We introduce quiver gauge theory associated with the non-simply-laced type
fractional quiver, and define fractional quiver W-algebras by using
construction of arXiv:1512.08533 and arXiv:1608.04651 with representation of
fractional quivers.
",0,0,1,0,0,0
4184,4185,Conditional Variance Penalties and Domain Shift Robustness,"  When training a deep network for image classification, one can broadly
distinguish between two types of latent features of images that will drive the
classification. Following the notation of Gong et al. (2016), we can divide
latent features into (i) ""core"" features $X^\text{core}$ whose distribution
$X^\text{core}\vert Y$ does not change substantially across domains and (ii)
""style"" features $X^{\text{style}}$ whose distribution $X^{\text{style}}\vert
Y$ can change substantially across domains. These latter orthogonal features
would generally include features such as rotation, image quality or brightness
but also more complex ones like hair color or posture for images of persons.
Guarding against future adversarial domain shifts implies that the influence of
the second type of style features in the prediction has to be limited. We
assume that the domain itself is not observed and hence a latent variable. We
do assume, however, that we can sometimes observe a typically discrete
identifier or $\mathrm{ID}$ variable. We know in some applications, for
example, that two images show the same person, and $\mathrm{ID}$ then refers to
the identity of the person. The method requires only a small fraction of images
to have an $\mathrm{ID}$ variable. We group data samples if they share the same
class and identifier $(Y,\mathrm{ID})=(y,\mathrm{id})$ and penalize the
conditional variance of the prediction if we condition on $(Y,\mathrm{ID})$.
Using this approach is shown to protect against shifts in the distribution of
the style variables for both regression and classification models.
Specifically, the conditional variance penalty CoRe is shown to be equivalent
to minimizing the risk under noise interventions in a regression setting and is
shown to lead to adversarial risk consistency in a partially linear
classification setting.
",1,0,0,1,0,0
11773,11774,Data clustering with edge domination in complex networks,"  This paper presents a model for a dynamical system where particles dominate
edges in a complex network. The proposed dynamical system is then extended to
an application on the problem of community detection and data clustering. In
the case of the data clustering problem, 6 different techniques were simulated
on 10 different datasets in order to compare with the proposed technique. The
results show that the proposed algorithm performs well when prior knowledge of
the number of clusters is known to the algorithm.
",1,1,0,0,0,0
254,255,Utilizing artificial neural networks to predict demand for weather-sensitive products at retail stores,"  One key requirement for effective supply chain management is the quality of
its inventory management. Various inventory management methods are typically
employed for different types of products based on their demand patterns,
product attributes, and supply network. In this paper, our goal is to develop
robust demand prediction methods for weather sensitive products at retail
stores. We employ historical datasets from Walmart, whose customers and markets
are often exposed to extreme weather events which can have a huge impact on
sales regarding the affected stores and products. We want to accurately predict
the sales of 111 potentially weather-sensitive products around the time of
major weather events at 45 of Walmart retails locations in the U.S.
Intuitively, we may expect an uptick in the sales of umbrellas before a big
thunderstorm, but it is difficult for replenishment managers to predict the
level of inventory needed to avoid being out-of-stock or overstock during and
after that storm. While they rely on a variety of vendor tools to predict sales
around extreme weather events, they mostly employ a time-consuming process that
lacks a systematic measure of effectiveness. We employ all the methods critical
to any analytics project and start with data exploration. Critical features are
extracted from the raw historical dataset for demand forecasting accuracy and
robustness. In particular, we employ Artificial Neural Network for forecasting
demand for each product sold around the time of major weather events. Finally,
we evaluate our model to evaluate their accuracy and robustness.
",1,0,0,1,0,0
2797,2798,Image Reconstruction using Matched Wavelet Estimated from Data Sensed Compressively using Partial Canonical Identity Matrix,"  This paper proposes a joint framework wherein lifting-based, separable,
image-matched wavelets are estimated from compressively sensed (CS) images and
used for the reconstruction of the same. Matched wavelet can be easily designed
if full image is available. Also matched wavelet may provide better
reconstruction results in CS application compared to standard wavelet
sparsifying basis. Since in CS application, we have compressively sensed image
instead of full image, existing methods of designing matched wavelet cannot be
used. Thus, we propose a joint framework that estimates matched wavelet from
the compressively sensed images and also reconstructs full images. This paper
has three significant contributions. First, lifting-based, image-matched
separable wavelet is designed from compressively sensed images and is also used
to reconstruct the same. Second, a simple sensing matrix is employed to sample
data at sub-Nyquist rate such that sensing and reconstruction time is reduced
considerably without any noticeable degradation in the reconstruction
performance. Third, a new multi-level L-Pyramid wavelet decomposition strategy
is provided for separable wavelet implementation on images that leads to
improved reconstruction performance. Compared to CS-based reconstruction using
standard wavelets with Gaussian sensing matrix and with existing wavelet
decomposition strategy, the proposed methodology provides faster and better
image reconstruction in compressive sensing application.
",1,0,0,0,0,0
18342,18343,Accurate and Efficient Estimation of Small P-values with the Cross-Entropy Method: Applications in Genomic Data Analysis,"  Small $p$-values are often required to be accurately estimated in large scale
genomic studies for the adjustment of multiple hypothesis tests and the ranking
of genomic features based on their statistical significance. For those
complicated test statistics whose cumulative distribution functions are
analytically intractable, existing methods usually do not work well with small
$p$-values due to lack of accuracy or computational restrictions. We propose a
general approach for accurately and efficiently calculating small $p$-values
for a broad range of complicated test statistics based on the principle of the
cross-entropy method and Markov chain Monte Carlo sampling techniques. We
evaluate the performance of the proposed algorithm through simulations and
demonstrate its application to three real examples in genomic studies. The
results show that our approach can accurately evaluate small to extremely small
$p$-values (e.g. $10^{-6}$ to $10^{-100}$). The proposed algorithm is helpful
to the improvement of existing test procedures and the development of new test
procedures in genomic studies.
",0,0,0,1,0,0
2233,2234,Context Prediction for Unsupervised Deep Learning on Point Clouds,"  Point clouds provide a flexible and natural representation usable in
countless applications such as robotics or self-driving cars. Recently, deep
neural networks operating on raw point cloud data have shown promising results
on supervised learning tasks such as object classification and semantic
segmentation. While massive point cloud datasets can be captured using modern
scanning technology, manually labelling such large 3D point clouds for
supervised learning tasks is a cumbersome process. This necessitates effective
unsupervised learning methods that can produce representations such that
downstream tasks require significantly fewer annotated samples. We propose a
novel method for unsupervised learning on raw point cloud data in which a
neural network is trained to predict the spatial relationship between two point
cloud segments. While solving this task, representations that capture semantic
properties of the point cloud are learned. Our method outperforms previous
unsupervised learning approaches in downstream object classification and
segmentation tasks and performs on par with fully supervised methods.
",1,0,0,1,0,0
9514,9515,Benchmarks for single-phase flow in fractured porous media,"  This paper presents several test cases intended to be benchmarks for
numerical schemes for single-phase fluid flow in fractured porous media. A
number of solution strategies are compared, including a vertex and a
cell-centered finite volume method, a non-conforming embedded discrete fracture
model, a primal and a dual extended finite element formulation, and a mortar
discrete fracture model. The proposed benchmarks test the schemes by increasing
the difficulties in terms of network geometry, e.g. intersecting fractures, and
physical parameters, e.g. low and high fracture-matrix permeability ratio as
well as heterogeneous fracture permeabilities. For each problem, the results
presented by the participants are the number of unknowns, the approximation
errors in the porous matrix and in the fractures with respect to a reference
solution, and the sparsity and condition number of the discretized linear
system. All data and meshes used in this study are publicly available for
further comparisons.
",1,0,1,0,0,0
11342,11343,Twitter and the Press: an Ego-Centred Analysis,"  Ego networks have proved to be a valuable tool for understanding the
relationships that individuals establish with their peers, both in offline and
online social networks. Particularly interesting are the cognitive constraints
associated with the interactions between the ego and the members of their ego
network, whereby individuals cannot maintain meaningful interactions with more
than 150 people, on average. In this work, we focus on the ego networks of
journalists on Twitter, and we investigate whether they feature the same
characteristics observed for other relevant classes of Twitter users, like
politicians and generic users. Our findings are that journalists are generally
more active and interact with more people than generic users. Their ego network
structure is very aligned with reference models derived from the social brain
hypothesis and observed in general human ego networks. Remarkably, the
similarity is even higher than the one of politicians and generic users ego
networks. This may imply a greater cognitive involvement with Twitter than with
other social interaction means. Moreover, the ego networks of journalists are
much stabler than those of politicians and generic users, and the ego-alter
ties are often information-driven.
",1,0,0,0,0,0
20925,20926,Complex Valued Risk Diversification,"  Risk diversification is one of the dominant concerns for portfolio managers.
Various portfolio constructions have been proposed to minimize the risk of the
portfolio under some constrains including expected returns. We propose a
portfolio construction method that incorporates the complex valued principal
component analysis into the risk diversification portfolio construction. The
proposed method is verified to outperform the conventional risk parity and risk
diversification portfolio constructions.
",0,0,0,0,0,1
15738,15739,Investigating Collaboration Within Online Communities: Software Development Vs. Artistic Creation,"  Online creative communities have been able to develop large, open source
software (OSS) projects like Linux and Firefox throughout the successful
collaborations carried out over the Internet. These communities have also
expanded to creative arts domains such as animation, video games, and music.
Despite their growing popularity, the factors that lead to successful
collaborations in these communities are not entirely understood. In the
following, I describe my PhD research project aimed at improving communication,
collaboration, and retention in creative arts communities, starting from the
experience gained from the literature about OSS communities.
",1,0,0,0,0,0
12437,12438,Inferring short-term volatility indicators from Bitcoin blockchain,"  In this paper, we study the possibility of inferring early warning indicators
(EWIs) for periods of extreme bitcoin price volatility using features obtained
from Bitcoin daily transaction graphs. We infer the low-dimensional
representations of transaction graphs in the time period from 2012 to 2017
using Bitcoin blockchain, and demonstrate how these representations can be used
to predict extreme price volatility events. Our EWI, which is obtained with a
non-negative decomposition, contains more predictive information than those
obtained with singular value decomposition or scalar value of the total Bitcoin
transaction volume.
",1,0,0,0,0,1
17082,17083,Traveling-wave parametric amplifier based on three-wave mixing in a Josephson metamaterial,"  We have developed a recently proposed Josephson traveling-wave parametric
amplifier with three-wave mixing [A. B. Zorin, Phys. Rev. Applied 6, 034006,
2016]. The amplifier consists of a microwave transmission line formed by a
serial array of nonhysteretic one-junction SQUIDs. These SQUIDs are flux-biased
in a way that the phase drops across the Josephson junctions are equal to 90
degrees and the persistent currents in the SQUID loops are equal to the
Josephson critical current values. Such a one-dimensional metamaterial
possesses a maximal quadratic nonlinearity and zero cubic (Kerr) nonlinearity.
This property allows phase matching and exponential power gain of traveling
microwaves to take place over a wide frequency range. We report the
proof-of-principle experiment performed at a temperature of T = 4.2 K on Nb
trilayer samples, which has demonstrated that our concept of a practical
broadband Josephson parametric amplifier is valid and very promising for
achieving quantum-limited operation.
",0,1,0,0,0,0
13894,13895,Aggregation and Resource Scheduling in Machine-type Communication Networks: A Stochastic Geometry Approach,"  Data aggregation is a promising approach to enable massive machine-type
communication (mMTC). This paper focuses on the aggregation phase where a
massive number of machine-type devices (MTDs) transmit to aggregators. By using
non-orthogonal multiple access (NOMA) principles, we allow several MTDs to
share the same orthogonal channel in our proposed hybrid access scheme. We
develop an analytical framework based on stochastic geometry to investigate the
system performance in terms of average success probability and average number
of simultaneously served MTDs, under imperfect successive interference
cancellation (SIC) at the aggregators, for two scheduling schemes: random
resource scheduling (RRS) and channel-aware resource scheduling (CRS). We
identify the power constraints on the MTDs sharing the same channel to attain a
fair coexistence with purely orthogonal multiple access (OMA) setups, then
power control coefficients are found so that these MTDs perform with similar
reliability. We show that under high access demand, the hybrid scheme with CRS
outperforms the OMA setup by simultaneously serving more MTDs with reduced
power consumption.
",1,0,0,1,0,0
9021,9022,Resonant inelastic x-ray scattering probes the electron-phonon coupling in the spin-liquid kappa-(BEDT-TTF)2Cu2(CN)3,"  Resonant inelastic x-ray scattering at the N K edge reveals clearly resolved
harmonics of the anion plane vibrations in the kappa-(BEDT-TTF)2Cu2(CN)3
spin-liquid insulator. Tuning the incoming light energy at the K edge of two
distinct N sites permits to excite different sets of phonon modes. Cyanide CN
stretching mode is selected at the edge of the ordered N sites which are more
strongly connected to the BEDT-TTF molecules, while positionally disordered N
sites show multi-mode excitation. Combining measurements with calculations on
an anion plane cluster permits to estimate the sitedependent electron-phonon
coupling of the modes related to nitrogen excitation.
",0,1,0,0,0,0
19165,19166,Self-supervised Knowledge Distillation Using Singular Value Decomposition,"  To solve deep neural network (DNN)'s huge training dataset and its high
computation issue, so-called teacher-student (T-S) DNN which transfers the
knowledge of T-DNN to S-DNN has been proposed. However, the existing T-S-DNN
has limited range of use, and the knowledge of T-DNN is insufficiently
transferred to S-DNN. To improve the quality of the transferred knowledge from
T-DNN, we propose a new knowledge distillation using singular value
decomposition (SVD). In addition, we define a knowledge transfer as a
self-supervised task and suggest a way to continuously receive information from
T-DNN. Simulation results show that a S-DNN with a computational cost of 1/5 of
the T-DNN can be up to 1.1\% better than the T-DNN in terms of classification
accuracy. Also assuming the same computational cost, our S-DNN outperforms the
S-DNN driven by the state-of-the-art distillation with a performance advantage
of 1.79\%. code is available on this https URL\_SVD.
",0,0,0,1,0,0
12001,12002,New Horizons Ring Collision Hazard: Constraints from Earth-based Observations,"  The New Horizons spacecraft's nominal trajectory crosses the planet's
satellite plane at $\sim 10,000\ \rm{km}$ from the barycenter, between the
orbits of Pluto and Charon. I have investigated the risk to the spacecraft
based on observational limits of rings and dust within this region, assuming
various particle size distributions. The best limits are placed by 2011 and
2012 HST observations, which significantly improve on the limits from stellar
occultations, although they do not go as close to the planet. From the HST data
and assuming a `reasonable worst case' for the size distribution, we place a
limit of $N < 20$ damaging impacts by grains of radius $> 0.2\ \textrm{mm}$
onto the spacecraft during the encounter. The number of hits is $\approx$
200$\times$ above the NH mission requirement, and $\approx$ $2000\times$ above
the mission's desired level. Stellar occultations remain valuable because they
are able to measure $N$ closer to the Pluto surface than direct imaging,
although with a sensitivity limit several orders of magnitude higher than that
from HST imaging. Neither HST nor occultations are sensitive enough to place
limits on $N$ at or below the mission requirements.
",0,1,0,0,0,0
9479,9480,On Asymptotic Properties of Hyperparameter Estimators for Kernel-based Regularization Methods,"  The kernel-based regularization method has two core issues: kernel design and
hyperparameter estimation. In this paper, we focus on the second issue and
study the properties of several hyperparameter estimators including the
empirical Bayes (EB) estimator, two Stein's unbiased risk estimators (SURE) and
their corresponding Oracle counterparts, with an emphasis on the asymptotic
properties of these hyperparameter estimators. To this goal, we first derive
and then rewrite the first order optimality conditions of these hyperparameter
estimators, leading to several insights on these hyperparameter estimators.
Then we show that as the number of data goes to infinity, the two SUREs
converge to the best hyperparameter minimizing the corresponding mean square
error, respectively, while the more widely used EB estimator converges to
another best hyperparameter minimizing the expectation of the EB estimation
criterion. This indicates that the two SUREs are asymptotically optimal but the
EB estimator is not. Surprisingly, the convergence rate of two SUREs is slower
than that of the EB estimator, and moreover, unlike the two SUREs, the EB
estimator is independent of the convergence rate of $\Phi^T\Phi/N$ to its
limit, where $\Phi$ is the regression matrix and $N$ is the number of data. A
Monte Carlo simulation is provided to demonstrate the theoretical results.
",1,0,0,0,0,0
304,305,Distinct evolutions of Weyl fermion quasiparticles and Fermi arcs with bulk band topology in Weyl semimetals,"  The Weyl semimetal phase is a recently discovered topological quantum state
of matter characterized by the presence of topologically protected degeneracies
near the Fermi level. These degeneracies are the source of exotic phenomena,
including the realization of chiral Weyl fermions as quasiparticles in the bulk
and the formation of Fermi arc states on the surfaces. Here, we demonstrate
that these two key signatures show distinct evolutions with the bulk band
topology by performing angle-resolved photoemission spectroscopy, supported by
first-principle calculations, on transition-metal monophosphides. While Weyl
fermion quasiparticles exist only when the chemical potential is located
between two saddle points of the Weyl cone features, the Fermi arc states
extend in a larger energy scale and are robust across the bulk Lifshitz
transitions associated with the recombination of two non-trivial Fermi surfaces
enclosing one Weyl point into a single trivial Fermi surface enclosing two Weyl
points of opposite chirality. Therefore, in some systems (e.g. NbP),
topological Fermi arc states are preserved even if Weyl fermion quasiparticles
are absent in the bulk. Our findings not only provide insight into the
relationship between the exotic physical phenomena and the intrinsic bulk band
topology in Weyl semimetals, but also resolve the apparent puzzle of the
different magneto-transport properties observed in TaAs, TaP and NbP, where the
Fermi arc states are similar.
",0,1,0,0,0,0
18304,18305,Asymmetric Matrix-Valued Covariances for Multivariate Random Fields on Spheres,"  Matrix-valued covariance functions are crucial to geostatistical modeling of
multivariate spatial data. The classical assumption of symmetry of a
multivariate covariance function is overlay restrictive and has been considered
as unrealistic for most of real data applications. Despite of that, the
literature on asymmetric covariance functions has been very sparse. In
particular, there is some work related to asymmetric covariances on Euclidean
spaces, depending on the Euclidean distance. However, for data collected over
large portions of planet Earth, the most natural spatial domain is a sphere,
with the corresponding geodesic distance being the natural metric. In this
work, we propose a strategy based on spatial rotations to generate asymmetric
covariances for multivariate random fields on the $d$-dimensional unit sphere.
We illustrate through simulations as well as real data analysis that our
proposal allows to achieve improvements in the predictive performance in
comparison to the symmetric counterpart.
",0,0,1,1,0,0
5925,5926,A numerical study of the F-model with domain-wall boundaries,"  We perform a numerical study of the F-model with domain-wall boundary
conditions. Various exact results are known for this particular case of the
six-vertex model, including closed expressions for the partition function for
any system size as well as its asymptotics and leading finite-size corrections.
To complement this picture we use a full lattice multi-cluster algorithm to
study equilibrium properties of this model for systems of moderate size, up to
L=512. We compare the energy to its exactly known large-L asymptotics. We
investigate the model's infinite-order phase transition by means of finite-size
scaling for an observable derived from the staggered polarization in order to
test the method put forward in our recent joint work with Duine and Barkema. In
addition we analyse local properties of the model. Our data are perfectly
consistent with analytical expressions for the arctic curves. We investigate
the structure inside the temperate region of the lattice, confirming the
oscillations in vertex densities that were first observed by Sylju{\aa}sen and
Zvonarev, and recently studied by Lyberg et al. We point out
'(anti)ferroelectric' oscillations close to the corresponding frozen regions as
well as 'higher-order' oscillations forming an intricate pattern with
saddle-point-like features.
",0,1,0,0,0,0
1097,1098,Backward Monte-Carlo applied to muon transport,"  We discuss a backward Monte-Carlo technique for muon transport problem, with
emphasis on its application in muography. Backward Monte-Carlo allows exclusive
sampling of a final state by reversing the simulation flow. In practice it can
be made analogous to an adjoint Monte-Carlo, though it is more versatile for
muon transport. A backward Monte-Carlo was implemented as a dedicated muon
transport library: PUMAS. It is shown for case studies relevant for muography
imaging that the implementations of forward and backward Monte-Carlo schemes
agree to better than 1%.
",0,1,0,0,0,0
9972,9973,A Decision Support Method for Recommending Degrees of Exploration in Exploratory Testing,"  Exploratory testing is neither black nor white, but rather a continuum of
exploration exists. In this research we propose an approach for decision
support helping practitioners to distribute time between different degrees of
exploratory testing on that continuum. To make the continuum manageable, five
levels have been defined: freestyle testing, high, medium and low degrees of
exploration, and scripted testing. The decision support approach is based on
the repertory grid technique. The approach has been used in one company. The
method for data collection was focus groups. The results showed that the
proposed approach aids practitioners in the reflection of what exploratory
testing levels to use, and aligns their understanding for priorities of
decision criteria and the performance of exploratory testing levels in their
contexts. The findings also showed that the participating company, which is
currently conducting mostly scripted testing, should spend more time on testing
using higher degrees of exploration in comparison to scripted testing.
",1,0,0,0,0,0
16361,16362,Deep Architectures for Neural Machine Translation,"  It has been shown that increasing model depth improves the quality of neural
machine translation. However, different architectural variants to increase
model depth have been proposed, and so far, there has been no thorough
comparative study.
In this work, we describe and evaluate several existing approaches to
introduce depth in neural machine translation. Additionally, we explore novel
architectural variants, including deep transition RNNs, and we vary how
attention is used in the deep decoder. We introduce a novel ""BiDeep"" RNN
architecture that combines deep transition RNNs and stacked RNNs.
Our evaluation is carried out on the English to German WMT news translation
dataset, using a single-GPU machine for both training and inference. We find
that several of our proposed architectures improve upon existing approaches in
terms of speed and translation quality. We obtain best improvements with a
BiDeep RNN of combined depth 8, obtaining an average improvement of 1.5 BLEU
over a strong shallow baseline.
We release our code for ease of adoption.
",1,0,0,0,0,0
898,899,Biocompatible Writing of Data into DNA,"  A simple DNA-based data storage scheme is demonstrated in which information
is written using ""addressing"" oligonucleotides. In contrast to other methods
that allow arbitrary code to be stored, the resulting DNA is suitable for
downstream enzymatic and biological processing. This capability is crucial for
DNA computers, and may allow for a diverse array of computational operations to
be carried out using this DNA. Although here we use gel-based methods for
information readout, we also propose more advanced methods involving
protein/DNA complexes and atomic force microscopy/nano-pore schemes for data
readout.
",1,1,0,0,0,0
11865,11866,Assessing the level of merging errors for coauthorship data: a Bayesian model,"  Robust analysis of coauthorship networks is based on high quality data.
However, ground-truth data are usually unavailable. Empirical data suffer
several types of errors, a typical one of which is called merging error,
identifying different persons as one entity. Specific features of authors have
been used to reduce these errors. We proposed a Bayesian model to calculate the
information of any given features of authors. Based on the features, the model
can be utilized to calculate the rate of merging errors for entities.
Therefore, the model helps to find informative features for detecting heavily
compromised entities. It has potential contributions to improving the quality
of empirical data.
",1,0,0,0,0,0
19517,19518,Spectral and scattering theory for perturbed block Toeplitz operators,"  We analyse spectral properties of a class of compact perturbations of block
Toeplitz operators associated with analytic symbols. In particular, a limiting
absorption principle and the absence of singular continuous spectrum are shown.
The existence and the completeness of wave operators are also obtained. Our
study is based on the construction of a conjugate operator in Mourre sense for
the corresponding Laurent operators.
",0,0,1,0,0,0
17948,17949,Existence of infinite Viterbi path for pairwise Markov models,"  For hidden Markov models one of the most popular estimates of the hidden
chain is the Viterbi path -- the path maximising the posterior probability. We
consider a more general setting, called the pairwise Markov model, where the
joint process consisting of finite-state hidden regime and observation process
is assumed to be a Markov chain. We prove that under some conditions it is
possible to extend the Viterbi path to infinity for almost every observation
sequence which in turn enables to define an infinite Viterbi decoding of the
observation process, called the Viterbi process. This is done by constructing a
block of observations, called a barrier, which ensures that the Viterbi path
goes trough a given state whenever this block occurs in the observation
sequence.
",0,0,1,1,0,0
567,568,An Extended Low Fat Allocator API and Applications,"  The primary function of memory allocators is to allocate and deallocate
chunks of memory primarily through the malloc API. Many memory allocators also
implement other API extensions, such as deriving the size of an allocated
object from the object's pointer, or calculating the base address of an
allocation from an interior pointer. In this paper, we propose a general
purpose extended allocator API built around these common extensions. We argue
that such extended APIs have many applications and demonstrate several use
cases, such as (manual) memory error detection, meta data storage, typed
pointers and compact data-structures. Because most existing allocators were not
designed for the extended API, traditional implementations are expensive or not
possible.
Recently, the LowFat allocator for heap and stack objects has been developed.
The LowFat allocator is an implementation of the idea of low-fat pointers,
where object bounds information (size and base) are encoded into the native
machine pointer representation itself. The ""killer app"" for low-fat pointers is
automated bounds check instrumentation for program hardening and bug detection.
However, the LowFat allocator can also be used to implement highly optimized
version of the extended allocator API, which makes the new applications (listed
above) possible. In this paper, we implement and evaluate several applications
based efficient memory allocator API extensions using low-fat pointers. We also
extend the LowFat allocator to cover global objects for the first time.
",1,0,0,0,0,0
6550,6551,Automatic Disambiguation of French Discourse Connectives,"  Discourse connectives (e.g. however, because) are terms that can explicitly
convey a discourse relation within a text. While discourse connectives have
been shown to be an effective clue to automatically identify discourse
relations, they are not always used to convey such relations, thus they should
first be disambiguated between discourse-usage non-discourse-usage. In this
paper, we investigate the applicability of features proposed for the
disambiguation of English discourse connectives for French. Our results with
the French Discourse Treebank (FDTB) show that syntactic and lexical features
developed for English texts are as effective for French and allow the
disambiguation of French discourse connectives with an accuracy of 94.2%.
",1,0,0,0,0,0
8510,8511,T-duality in rational homotopy theory via $L_\infty$-algebras,"  We combine Sullivan models from rational homotopy theory with Stasheff's
$L_\infty$-algebras to describe a duality in string theory. Namely, what in
string theory is known as topological T-duality between $K^0$-cocycles in type
IIA string theory and $K^1$-cocycles in type IIB string theory, or as Hori's
formula, can be recognized as a Fourier-Mukai transform between twisted
cohomologies when looked through the lenses of rational homotopy theory. We
show this as an example of topological T-duality in rational homotopy theory,
which in turn can be completely formulated in terms of morphisms of
$L_\infty$-algebras.
",0,0,1,0,0,0
12181,12182,CTCModel: a Keras Model for Connectionist Temporal Classification,"  We report an extension of a Keras Model, called CTCModel, to perform the
Connectionist Temporal Classification (CTC) in a transparent way. Combined with
Recurrent Neural Networks, the Connectionist Temporal Classification is the
reference method for dealing with unsegmented input sequences, i.e. with data
that are a couple of observation and label sequences where each label is
related to a subset of observation frames. CTCModel makes use of the CTC
implementation in the Tensorflow backend for training and predicting sequences
of labels using Keras. It consists of three branches made of Keras models: one
for training, computing the CTC loss function; one for predicting, providing
sequences of labels; and one for evaluating that returns standard metrics for
analyzing sequences of predictions.
",1,0,0,1,0,0
2395,2396,Resampling Strategy in Sequential Monte Carlo for Constrained Sampling Problems,"  Sequential Monte Carlo (SMC) methods are a class of Monte Carlo methods that
are used to obtain random samples of a high dimensional random variable in a
sequential fashion. Many problems encountered in applications often involve
different types of constraints. These constraints can make the problem much
more challenging. In this paper, we formulate a general framework of using SMC
for constrained sampling problems based on forward and backward pilot
resampling strategies. We review some existing methods under the framework and
develop several new algorithms. It is noted that all information observed or
imposed on the underlying system can be viewed as constraints. Hence the
approach outlined in this paper can be useful in many applications.
",0,0,0,1,0,0
19761,19762,Space of initial conditions for a cubic Hamiltonian system,"  In this paper we perform the analysis that leads to the space of initial
conditions for the Hamiltonian system $q' = p^2 + zq + \alpha$, $p' = -q^2 - zp
- \beta$, studied by the author in an earlier article. By compactifying the
phase space of the system from $\mathbb{C}^2$ to $\mathbb{CP}^2$ three base
points arise in the standard coordinate charts covering the complex projective
space. Each of these is removed by a sequence of three blow-ups, a construction
to regularise the system at these points. The resulting space, where the
exceptional curves introduced after the first and second blow-up are removed,
is the so-called Okamoto's space of initial conditions for this system which,
at every point, defines a regular initial value problem in some coordinate
chart of the space.
",0,1,1,0,0,0
13635,13636,A Toolbox For Property Checking From Simulation Using Incremental SAT (Extended Abstract),"  We present a tool that primarily supports the ability to check bounded
properties starting from a sequence of states in a run. The target design is
compiled into an AIGNET which is then selectively and iteratively translated
into an incremental SAT instance in which clauses are added for new terms and
simplified by the assignment of existing literals. Additional applications of
the tool can be derived by the user providing alternative attachments of
constrained functions which guide the iterations and SAT checks performed. Some
Verilog RTL examples are included for reference.
",1,0,0,0,0,0
9646,9647,Liquid crystal induced elasto-capillary suppression of crack formation in thin colloidal films,"  Drying of colloidal droplets on solid, rigid substrates is associated with a
capillary pressure developing within the droplet. In due course of time, the
capillary pressure builds up due to droplet evaporation resulting in the
formation of a colloidal thin film that is prone to crack formation. In this
study, we show that introducing a minimal amount of nematic liquid crystal
(NLC) can completely suppress the crack formation. The mechanism behind the
curbing of the crack formation may be attributed to the capillary
stress-absorbing cushion provided by the elastic arrangements of the liquid
crystal at the substrate-droplet interface. Cracks and allied surface
instabilities are detrimental to the quality of the final product like surface
coatings, and therefore, its suppression by an external inert additive is a
promising technique that will be of immense importance for several industrial
applications. We believe this fundamental investigation of crack suppression
will open up an entire avenue of applications for the NLCs in the field of
coatings, broadening its already existing wide range of benefits.
",0,1,0,0,0,0
466,467,Non-Asymptotic Analysis of Fractional Langevin Monte Carlo for Non-Convex Optimization,"  Recent studies on diffusion-based sampling methods have shown that Langevin
Monte Carlo (LMC) algorithms can be beneficial for non-convex optimization, and
rigorous theoretical guarantees have been proven for both asymptotic and
finite-time regimes. Algorithmically, LMC-based algorithms resemble the
well-known gradient descent (GD) algorithm, where the GD recursion is perturbed
by an additive Gaussian noise whose variance has a particular form. Fractional
Langevin Monte Carlo (FLMC) is a recently proposed extension of LMC, where the
Gaussian noise is replaced by a heavy-tailed {\alpha}-stable noise. As opposed
to its Gaussian counterpart, these heavy-tailed perturbations can incur large
jumps and it has been empirically demonstrated that the choice of
{\alpha}-stable noise can provide several advantages in modern machine learning
problems, both in optimization and sampling contexts. However, as opposed to
LMC, only asymptotic convergence properties of FLMC have been yet established.
In this study, we analyze the non-asymptotic behavior of FLMC for non-convex
optimization and prove finite-time bounds for its expected suboptimality. Our
results show that the weak-error of FLMC increases faster than LMC, which
suggests using smaller step-sizes in FLMC. We finally extend our results to the
case where the exact gradients are replaced by stochastic gradients and show
that similar results hold in this setting as well.
",1,0,0,1,0,0
13407,13408,Modified mean curvature flow of entire locally Lipschitz radial graphs in hyperbolic space,"  In a previous joint work of Xiao and the second author, the modified mean
curvature flow (MMCF) in hyperbolic space $\mathbb{H}^{n+1}$: $$\frac{\partial
\mathbf{F}}{\partial t} = (H-\sigma)\,\vnu\,,\quad \quad \sigma\in (-n,n)$$ was
first introduced and the flow starting from an entire Lipschitz continuous
radial graph with uniform local ball condition on the asymptotic boundary was
shown to exist for all time and converge to a complete hypersurface of constant
mean curvature with prescribed asymptotic boundary at infinity. In this paper,
we remove the uniform local ball condition on the asymptotic boundary of the
initial hypersurface, and prove that the MMCF starting from an entire locally
Lipschitz continuous radial graph exists and stays radially graphic for all
time.
",0,0,1,0,0,0
5008,5009,"Where Classification Fails, Interpretation Rises","  An intriguing property of deep neural networks is their inherent
vulnerability to adversarial inputs, which significantly hinders their
application in security-critical domains. Most existing detection methods
attempt to use carefully engineered patterns to distinguish adversarial inputs
from their genuine counterparts, which however can often be circumvented by
adaptive adversaries. In this work, we take a completely different route by
leveraging the definition of adversarial inputs: while deceiving for deep
neural networks, they are barely discernible for human visions. Building upon
recent advances in interpretable models, we construct a new detection framework
that contrasts an input's interpretation against its classification. We
validate the efficacy of this framework through extensive experiments using
benchmark datasets and attacks. We believe that this work opens a new direction
for designing adversarial input detection methods.
",1,0,0,1,0,0
8348,8349,A weak law of large numbers for estimating the correlation in bivariate Brownian semistationary processes,"  This article presents various weak laws of large numbers for the so-called
realised covariation of a bivariate stationary stochastic process which is not
a semimartingale. More precisely, we consider two cases: Bivariate moving
average processes with stochastic correlation and bivariate Brownian
semistationary processes with stochastic correlation. In both cases, we can
show that the (possibly scaled) realised covariation converges to the
integrated (possibly volatility modulated) stochastic correlation process.
",0,0,1,1,0,0
8744,8745,Smith Ideals of Operadic Algebras in Monoidal Model Categories,"  Building upon Hovey's work on Smith ideals for monoids, we develop a homotopy
theory of Smith ideals for general operads in a symmetric monoidal category.
For a sufficiently nice stable monoidal model category and an operad satisfying
a cofibrancy condition, we show that there is a Quillen equivalence between a
model structure on Smith ideals and a model structure on algebra maps induced
by the cokernel and the kernel. For symmetric spectra this applies to the
commutative operad and all Sigma-cofibrant operads. For chain complexes over a
field of characteristic zero and the stable module category, this Quillen
equivalence holds for all operads.
",0,0,1,0,0,0
8509,8510,On Diophantine equations involving sums of Fibonacci numbers and powers of $2$,"  In this paper, we completely solve the Diophantine equations $F_{n_1} +
F_{n_2} = 2^{a_1} + 2^{a_2} + 2^{a_3}$ and $ F_{m_1} + F_{m_2} + F_{m_3}
=2^{t_1} + 2^{t_2} $, where $F_k$ denotes the $k$-th Fibonacci number. In
particular, we prove that $\max \{n_1, n_2, a_1, a_2, a_3 \}\leq 18$ and $\max
\{ m_1, m_2, m_3, t_1, t_2 \}\leq 16$.
",0,0,1,0,0,0
13280,13281,Convergence Rates of Variational Posterior Distributions,"  We study convergence rates of variational posterior distributions for
nonparametric and high-dimensional inference. We formulate general conditions
on prior, likelihood, and variational class that characterize the convergence
rates. Under similar ""prior mass and testing"" conditions considered in the
literature, the rate is found to be the sum of two terms. The first term stands
for the convergence rate of the true posterior distribution, and the second
term is contributed by the variational approximation error. For a class of
priors that admit the structure of a mixture of product measures, we propose a
novel prior mass condition, under which the variational approximation error of
the generalized mean-field class is dominated by convergence rate of the true
posterior. We demonstrate the applicability of our general results for various
models, prior distributions and variational classes by deriving convergence
rates of the corresponding variational posteriors.
",0,0,1,1,0,0
11987,11988,Negative electronic compressibility and nanoscale inhomogeneity in ionic-liquid gated two-dimensional superconductors,"  When the electron density of highly crystalline thin films is tuned by
chemical doping or ionic liq- uid gating, interesting effects appear including
unconventional superconductivity, sizeable spin-orbit coupling, competition
with charge-density waves, and a debated low-temperature metallic state that
seems to avoid the superconducting or insulating fate of standard
two-dimensional electron systems. Some experiments also find a marked tendency
to a negative electronic compressibility. We suggest that this indicates an
inclination for electronic phase separation resulting in a nanoscopic inhomo-
geneity. Although the mild modulation of the inhomogeneous landscape is
compatible with a high electron mobility in the metallic state, this
intrinsically inhomogeneous character is highlighted by the peculiar behaviour
of the metal-to-superconductor transition. Modelling the system with super-
conducting puddles embedded in a metallic matrix, we fit the peculiar
resistance vs. temperature curves of systems like TiSe2, MoS2, and ZrNCl. In
this framework also the low-temperature debated metallic state finds a natural
explanation in terms of the pristine metallic background embedding
non-percolating superconducting clusters. An intrinsically inhomogeneous
character naturally raises the question of the formation mechanism(s). We
propose a mechanism based on the interplay be- tween electrons and the charges
of the gating ionic liquid.
",0,1,0,0,0,0
20971,20972,Why optional stopping is a problem for Bayesians,"  Recently, optional stopping has been a subject of debate in the Bayesian
psychology community. Rouder (2014) argues that optional stopping is no problem
for Bayesians, and even recommends the use of optional stopping in practice, as
do Wagenmakers et al. (2012). This article addresses the question whether
optional stopping is problematic for Bayesian methods, and specifies under
which circumstances and in which sense it is and is not. By slightly varying
and extending Rouder's (2014) experiment, we illustrate that, as soon as the
parameters of interest are equipped with default or pragmatic priors - which
means, in most practical applications of Bayes Factor hypothesis testing -
resilience to optional stopping can break down. We distinguish between four
types of default priors, each having their own specific issues with optional
stopping, ranging from no-problem-at-all (Type 0 priors) to quite severe (Type
II and III priors).
",0,0,1,1,0,0
14385,14386,Automorphisms of Partially Commutative Groups III: Inversions and Transvections,"  The structure of a certain subgroup $S$ of the automorphism group of a
partially commutative group (RAAG) $G$ is described in detail: namely the
subgroup generated by inversions and elementary transvections. We define
admissible subsets of the generators of $G$, and show that $S$ is the subgroup
of automorphisms which fix all subgroups $\langle Y\rangle$ of $G$, for all
admissible subsets $Y$. A decomposition of $S$ as an iterated tower of
semi-direct products in given and the structure of the factors of this
decomposition described. The construction allows a presentation of $S$ to be
computed, from the commutation graph of $G$.
",0,0,1,0,0,0
16508,16509,Detecting Hierarchical Ties Using Link-Analysis Ranking at Different Levels of Time Granularity,"  Social networks contain implicit knowledge that can be used to infer
hierarchical relations that are not explicitly present in the available data.
Interaction patterns are typically affected by users' social relations. We
present an approach to inferring such information that applies a link-analysis
ranking algorithm at different levels of time granularity. In addition, a
voting scheme is employed for obtaining the hierarchical relations. The
approach is evaluated on two datasets: the Enron email data set, where the goal
is to infer manager-subordinate relationships, and the Co-author data set,
where the goal is to infer PhD advisor-advisee relations. The experimental
results indicate that the proposed approach outperforms more traditional
approaches to inferring hierarchical relations from social networks.
",1,1,0,0,0,0
5212,5213,On the Binary Lossless Many-Help-One Problem with Independently Degraded Helpers,"  Although the rate region for the lossless many-help-one problem with
independently degraded helpers is already ""solved"", its solution is given in
terms of a convex closure over a set of auxiliary random variables. Thus, for
any such a problem in particular, an optimization over the set of auxiliary
random variables is required to truly solve the rate region. Providing the
solution is surprisingly difficult even for an example as basic as binary
sources. In this work, we derive a simple and tight inner bound on the rate
region's lower boundary for the lossless many-help-one problem with
independently degraded helpers when specialized to sources that are binary,
uniformly distributed, and interrelated through symmetric channels. This
scenario finds important applications in emerging cooperative communication
schemes in which the direct-link transmission is assisted via multiple lossy
relaying links. Numerical results indicate that the derived inner bound proves
increasingly tight as the helpers become more degraded.
",1,0,0,0,0,0
3337,3338,Optimal Task Scheduling in Communication-Constrained Mobile Edge Computing Systems for Wireless Virtual Reality,"  Mobile edge computing (MEC) is expected to be an effective solution to
deliver 360-degree virtual reality (VR) videos over wireless networks. In
contrast to previous computation-constrained MEC framework, which reduces the
computation-resource consumption at the mobile VR device by increasing the
communication-resource consumption, we develop a communications-constrained MEC
framework to reduce communication-resource consumption by increasing the
computation-resource consumption and exploiting the caching resources at the
mobile VR device in this paper. Specifically, according to the task
modularization, the MEC server can only deliver the components which have not
been stored in the VR device, and then the VR device uses the received
components and the corresponding cached components to construct the task,
resulting in low communication-resource consumption but high delay. The MEC
server can also compute the task by itself to reduce the delay, however, it
consumes more communication-resource due to the delivery of entire task.
Therefore, we then propose a task scheduling strategy to decide which
computation model should the MEC server operates, in order to minimize the
communication-resource consumption under the delay constraint. Finally, we
discuss the tradeoffs between communications, computing, and caching in the
proposed system.
",1,0,0,0,0,0
19484,19485,Replica Analysis for Maximization of Net Present Value,"  In this paper, we use replica analysis to determine the investment strategy
that can maximize the net present value for portfolios containing multiple
development projects. Replica analysis was developed in statistical mechanical
informatics and econophysics to evaluate disordered systems, and here we use it
to formulate the maximization of the net present value as an optimization
problem under budget and investment concentration constraints. Furthermore, we
confirm that a common approach from operations research underestimates the true
maximal net present value as the maximal expected net present value by
comparing our results with the maximal expected net present value as derived in
operations research. Moreover, it is shown that the conventional method for
estimating the net present value does not consider variance in the cash flow.
",0,0,0,0,0,1
17277,17278,Electronic structure of ThRu2Si2 studied by angle-resolved photoelectron spectroscopy: Elucidating the contribution of U 5f states in URu2Si2,"  The electronic structure of ThRu2Si2 was studied by angle-resolved
photoelectron spectroscopy (ARPES) with incident photon energies of hn=655-745
eV. Detailed band structure and the three-dimensional shapes of Fermi surfaces
were derived experimentally, and their characteristic features were mostly
explained by means of band structure calculations based on the density
functional theory. Comparison of the experimental ARPES spectra of ThRu2Si2
with those of URu2Si2 shows that they have considerably different spectral
profiles particularly in the energy range of 1 eV from the Fermi level,
suggesting that U 5f states are substantially hybridized in these bands. The
relationship between the ARPES spectra of URu2Si2 and ThRu2Si2 is very
different from the one between the ARPES spectra of CeRu2Si2 and LaRu2Si2,
where the intrinsic difference in their spectra is limited only in the very
vicinity of the Fermi energy. The present result suggests that the U 5f
electrons in URu2Si2 have strong hybridization with ligand states and have an
essentially itinerant character.
",0,1,0,0,0,0
13202,13203,New Algorithms for Unordered Tree Inclusion,"  The tree inclusion problem is, given two node-labeled trees $P$ and $T$ (the
""pattern tree"" and the ""text tree""), to locate every minimal subtree in $T$ (if
any) that can be obtained by applying a sequence of node insertion operations
to $P$. The ordered tree inclusion problem is known to be solvable in
polynomial time while the unordered tree inclusion problem is NP-hard. The
currently fastest algorithm for the latter is from 1995 and runs in
$O(poly(m,n) \cdot 2^{2d}) = O^{\ast}(4^{d})$ time, where $m$ and $n$ are the
sizes of the pattern and text trees, respectively, and $d$ is the degree of the
pattern tree. Here, we develop a new algorithm that improves the exponent $2d$
to $d$ by considering a particular type of ancestor-descendant relationships
and applying dynamic programming, thus reducing the time complexity to
$O^{\ast}(2^{d})$. We then study restricted variants of the unordered tree
inclusion problem where the number of occurrences of different node labels
and/or the input trees' heights are bounded and show that although the problem
remains NP-hard in many such cases, if the leaves of $P$ are distinctly labeled
and each label occurs at most $c$ times in $T$ then it can be solved in
polynomial time for $c = 2$ and in $O^{\ast}(1.8^d)$ time for $c = 3$.
",1,0,0,0,0,0
10116,10117,Topological dimension tunes activity patterns in hierarchical modular network models,"  Connectivity patterns of relevance in neuroscience and systems biology can be
encoded in hierarchical modular networks (HMNs). Moreover, recent studies
highlight the role of hierarchical modular organization in shaping brain
activity patterns, providing an excellent substrate to promote both the
segregation and integration of neural information. Here we propose an extensive
numerical analysis of the critical spreading rate (or ""epidemic"" threshold)
--separating a phase with endemic persistent activity from one in which
activity ceases-- on diverse HMNs. By employing analytical and computational
techniques we determine the nature of such a threshold and scrutinize how it
depends on general structural features of the underlying HMN. We critically
discuss the extent to which current graph-spectral methods can be applied to
predict the onset of spreading in HMNs, and we propose the network topological
dimension as a relevant and unifying structural parameter, controlling the
epidemic threshold.
",0,1,0,0,0,0
19106,19107,Hybrid Dirac Semimetal in CaAgBi Materials Family,"  Based on their formation mechanisms, Dirac points in three-dimensional
systems can be classified as accidental or essential. The former can be further
distinguished into type-I and type-II, depending on whether the Dirac cone
spectrum is completely tipped over along certain direction. Here, we predict
the coexistence of all three kinds of Dirac points in the low-energy band
structure of CaAgBi-family materials with a stuffed Wurtzite structure. Two
pairs of accidental Dirac points reside on the rotational axis, with one pair
being type-I and the other pair type-II; while another essential Dirac point is
pinned at the high symmetry point on the Brillouin zone boundary. Due to broken
inversion symmetry, the band degeneracy around accidental Dirac points is
completely lifted except along the rotational axis, which may enable the
splitting of chiral carriers at a ballistic p-n junction with a double negative
refraction effect. We clarify their symmetry protections, and find both the
Dirac-cone and Fermi arc topological surface states.
",0,1,0,0,0,0
13241,13242,Tuning Pairing Amplitude and Spin-Triplet Texture by Curving Superconducting Nanostructures,"  We investigate the nature of the superconducting state in curved
nanostructures with Rashba spin-orbit coupling (RSOC). In bent nanostructures
with inhomogeneous curvature we find a local enhancement or suppression of the
superconducting order parameter, with the effect that can be tailored by tuning
either the RSOC strength or the carrier density. Apart from the local
superconducting spin-singlet amplitude control, the geometric curvature
generates non-trivial textures of the spin-triplet pairs through a spatial
variation of the d-vector. By employing the representative case of an
elliptically deformed quantum ring, we demonstrate that the amplitude of the
d-vector strongly depends on the strength of the local curvature and it
generally exhibits a three-dimensional profile whose winding is tied to that of
the single electron spin in the normal state. Our findings unveil novel paths
to manipulate the quantum structure of the superconducting state in RSOC
nanostructures through their geometry.
",0,1,0,0,0,0
473,474,Comparative Investigation of the High Pressure Autoignition of the Butanol Isomers,"  Investigation of the autoignition delay of the butanol isomers has been
performed at elevated pressures of 15 bar and 30 bar and low to intermediate
temperatures of 680-860 K. The reactivity of the stoichiometric isomers of
butanol, in terms of inverse ignition delay, was ranked as n-butanol >
sec-butanol ~ iso-butanol > tert-butanol at a compressed pressure of 15 bar but
changed to n-butanol > tert-butanol > sec-butanol > iso-butanol at 30 bar. For
the temperature and pressure conditions in this study, no NTC or two-stage
ignition behavior were observed. However, for both of the compressed pressures
studied in this work, tert-butanol exhibited unique pre-ignition heat release
characteristics. As such, tert-butanol was further studied at two additional
equivalence ratios ($\phi$ = 0.5 and 2.0) to help determine the cause of the
heat release.
",0,1,0,0,0,0
5242,5243,Introduction to the Special Issue on Approaches to Control Biological and Biologically Inspired Networks,"  The emerging field at the intersection of quantitative biology, network
modeling, and control theory has enjoyed significant progress in recent years.
This Special Issue brings together a selection of papers on complementary
approaches to observe, identify, and control biological and biologically
inspired networks. These approaches advance the state of the art in the field
by addressing challenges common to many such networks, including high
dimensionality, strong nonlinearity, uncertainty, and limited opportunities for
observation and intervention. Because these challenges are not unique to
biological systems, it is expected that many of the results presented in these
contributions will also find applications in other domains, including physical,
social, and technological networks.
",1,0,0,0,1,0
19865,19866,Identifiability of Nonparametric Mixture Models and Bayes Optimal Clustering,"  Motivated by problems in data clustering, we establish general conditions
under which families of nonparametric mixture models are identifiable, by
introducing a novel framework involving clustering overfitted \emph{parametric}
(i.e. misspecified) mixture models. These identifiability conditions generalize
existing conditions in the literature, and are flexible enough to include for
example mixtures of Gaussian mixtures. In contrast to the recent literature on
estimating nonparametric mixtures, we allow for general nonparametric mixture
components, and instead impose regularity assumptions on the underlying mixing
measure. As our primary application, we apply these results to partition-based
clustering, generalizing the notion of a Bayes optimal partition from classical
parametric model-based clustering to nonparametric settings. Furthermore, this
framework is constructive so that it yields a practical algorithm for learning
identified mixtures, which is illustrated through several examples on real
data. The key conceptual device in the analysis is the convex, metric geometry
of probability measures on metric spaces and its connection to the Wasserstein
convergence of mixing measures. The result is a flexible framework for
nonparametric clustering with formal consistency guarantees.
",0,0,0,1,0,0
3990,3991,Evolutionary dynamics of cooperation in neutral populations,"  Cooperation is a difficult proposition in the face of Darwinian selection.
Those that defect have an evolutionary advantage over cooperators who should
therefore die out. However, spatial structure enables cooperators to survive
through the formation of homogeneous clusters, which is the hallmark of network
reciprocity. Here we go beyond this traditional setup and study the
spatiotemporal dynamics of cooperation in a population of populations. We use
the prisoner's dilemma game as the mathematical model and show that considering
several populations simultaneously give rise to fascinating spatiotemporal
dynamics and pattern formation. Even the simplest assumption that strategies
between different populations are payoff-neutral with one another results in
the spontaneous emergence of cyclic dominance, where defectors of one
population become prey of cooperators in the other population, and vice versa.
Moreover, if social interactions within different populations are characterized
by significantly different temptations to defect, we observe that defectors in
the population with the largest temptation counterintuitively vanish the
fastest, while cooperators that hang on eventually take over the whole
available space. Our results reveal that considering the simultaneous presence
of different populations significantly expands the complexity of evolutionary
dynamics in structured populations, and it allow us to understand the stability
of cooperation under adverse conditions that could never be bridged by network
reciprocity alone.
",1,0,0,0,0,0
19429,19430,Monochromatic knots and other unusual electromagnetic disturbances: light localised in 3D,"  We introduce and examine a collection of unusual electromagnetic
disturbances. Each of these is an exact, monochromatic solution of Maxwell's
equations in free space with looped electric and magnetic field lines of finite
extent and a localised appearance in all three spatial dimensions. Included are
the first explicit examples of monochromatic electromagnetic knots. We also
consider the generation of our unusual electromagnetic disturbances in the
laboratory, at both low and high frequencies, and highlight possible directions
for future research, including the use of unusual electromagnetic disturbances
as the basis of a new form of three-dimensional display.
",0,1,0,0,0,0
19831,19832,MACS J0416.1-2403: Impact of line-of-sight structures on strong gravitational lensing modelling of galaxy clusters,"  Exploiting the powerful tool of strong gravitational lensing by galaxy
clusters to study the highest-redshift Universe and cluster mass distributions
relies on precise lens mass modelling. In this work, we present the first
attempt at modelling line-of-sight mass distribution in addition to that of the
cluster, extending previous modelling techniques that assume mass distributions
to be on a single lens plane. We focus on the Hubble Frontier Field cluster
MACS J0416.1-2403, and our multi-plane model reproduces the observed image
positions with a rms offset of ~0.53"". Starting from this best-fitting model,
we simulate a mock cluster that resembles MACS J0416.1-2403 in order to explore
the effects of line-of-sight structures on cluster mass modelling. By
systematically analysing the mock cluster under different model assumptions, we
find that neglecting the lensing environment has a significant impact on the
reconstruction of image positions (rms ~0.3""); accounting for line-of-sight
galaxies as if they were at the cluster redshift can partially reduce this
offset. Moreover, foreground galaxies are more important to include into the
model than the background ones. While the magnification factors of the lensed
multiple images are recovered within ~10% for ~95% of them, those ~5% that lie
near critical curves can be significantly affected by the exclusion of the
lensing environment in the models (up to a factor of ~200). In addition,
line-of-sight galaxies cannot explain the apparent discrepancy in the
properties of massive subhalos between MACS J0416.1-2403 and N-body simulated
clusters. Since our model of MACS J0416.1-2403 with line-of-sight galaxies only
reduced modestly the rms offset in the image positions, we conclude that
additional complexities, such as more flexible halo shapes, would be needed in
future models of MACS J0416.1-2403.
",0,1,0,0,0,0
13591,13592,Machine Assisted Analysis of Vowel Length Contrasts in Wolof,"  Growing digital archives and improving algorithms for automatic analysis of
text and speech create new research opportunities for fundamental research in
phonetics. Such empirical approaches allow statistical evaluation of a much
larger set of hypothesis about phonetic variation and its conditioning factors
(among them geographical / dialectal variants). This paper illustrates this
vision and proposes to challenge automatic methods for the analysis of a not
easily observable phenomenon: vowel length contrast. We focus on Wolof, an
under-resourced language from Sub-Saharan Africa. In particular, we propose
multiple features to make a fine evaluation of the degree of length contrast
under different factors such as: read vs semi spontaneous speech ; standard vs
dialectal Wolof. Our measures made fully automatically on more than 20k vowel
tokens show that our proposed features can highlight different degrees of
contrast for each vowel considered. We notably show that contrast is weaker in
semi-spontaneous speech and in a non standard semi-spontaneous dialect.
",1,0,0,0,0,0
7334,7335,Sharper and Simpler Nonlinear Interpolants for Program Verification,"  Interpolation of jointly infeasible predicates plays important roles in
various program verification techniques such as invariant synthesis and CEGAR.
Intrigued by the recent result by Dai et al.\ that combines real algebraic
geometry and SDP optimization in synthesis of polynomial interpolants, the
current paper contributes its enhancement that yields sharper and simpler
interpolants. The enhancement is made possible by: theoretical observations in
real algebraic geometry; and our continued fraction-based algorithm that rounds
off (potentially erroneous) numerical solutions of SDP solvers. Experiment
results support our tool's effectiveness; we also demonstrate the benefit of
sharp and simple interpolants in program verification examples.
",1,0,0,0,0,0
3704,3705,Prior-aware Dual Decomposition: Document-specific Topic Inference for Spectral Topic Models,"  Spectral topic modeling algorithms operate on matrices/tensors of word
co-occurrence statistics to learn topic-specific word distributions. This
approach removes the dependence on the original documents and produces
substantial gains in efficiency and provable topic inference, but at a cost:
the model can no longer provide information about the topic composition of
individual documents. Recently Thresholded Linear Inverse (TLI) is proposed to
map the observed words of each document back to its topic composition. However,
its linear characteristics limit the inference quality without considering the
important prior information over topics. In this paper, we evaluate Simple
Probabilistic Inverse (SPI) method and novel Prior-aware Dual Decomposition
(PADD) that is capable of learning document-specific topic compositions in
parallel. Experiments show that PADD successfully leverages topic correlations
as a prior, notably outperforming TLI and learning quality topic compositions
comparable to Gibbs sampling on various data.
",1,0,0,0,0,0
7253,7254,Bar formation in the Milky Way type galaxies,"  Many barred galaxies, possibly including the Milky Way, have cusps in the
centres. There is a widespread belief, however, that usual bar instability
taking place in bulgeless galaxy models is impossible for the cuspy models,
because of the presence of the inner Lindblad resonance for any pattern speed.
At the same time there are numerical evidences that the bar instability can
form a bar. We analyse this discrepancy, by accurate and diverse N-body
simulations and using the calculation of normal modes. We show that bar
formation in cuspy galaxies can be explained by taking into account the disc
thickness. The exponential growth time is moderate for typical current disc
masses (about 250 Myr), but considerably increases (factor 2 or more) upon
substitution of the live halo and bulge with a rigid halo/bulge potential;
meanwhile pattern speeds remain almost the same. Normal mode analysis with
different disc mass favours a young bar hypothesis, according to which the bar
instability saturated only recently.
",0,1,0,0,0,0
4814,4815,Definable Valuations induced by multiplicative subgroups and NIP Fields,"  We study the algebraic implications of the non-independence property (NIP)
and variants thereof (dp-minimality) on infinite fields, motivated by the
conjecture that all such fields which are neither real closed nor separably
closed admit a definable henselian valuation. Our results mainly focus on Hahn
fields and build up on Will Johnson's preprint ""dp-minimal fields"", arXiv:
1507.02745v1, July 2015.
",0,0,1,0,0,0
9402,9403,Brain Damage and Motor Cortex Impairment in Chronic Obstructive Pulmonary Disease: Implication of Nonrapid Eye Movement Sleep Desaturation,"  Nonrapid eye movement (NREM) sleep desaturation may cause neuronal damage due
to the withdrawal of cerebrovascular reactivity. The current study (1) assessed
the prevalence of NREM sleep desaturation in nonhypoxemic patients with chronic
obstructive pulmonary disease (COPD) and (2) compared a biological marker of
cerebral lesion and neuromuscular function in patients with and without NREM
sleep desaturation.
",0,1,0,0,0,0
14194,14195,What do we need to build explainable AI systems for the medical domain?,"  Artificial intelligence (AI) generally and machine learning (ML) specifically
demonstrate impressive practical success in many different application domains,
e.g. in autonomous driving, speech recognition, or recommender systems. Deep
learning approaches, trained on extremely large data sets or using
reinforcement learning methods have even exceeded human performance in visual
tasks, particularly on playing games such as Atari, or mastering the game of
Go. Even in the medical domain there are remarkable results. The central
problem of such models is that they are regarded as black-box models and even
if we understand the underlying mathematical principles, they lack an explicit
declarative knowledge representation, hence have difficulty in generating the
underlying explanatory structures. This calls for systems enabling to make
decisions transparent, understandable and explainable. A huge motivation for
our approach are rising legal and privacy aspects. The new European General
Data Protection Regulation entering into force on May 25th 2018, will make
black-box approaches difficult to use in business. This does not imply a ban on
automatic learning approaches or an obligation to explain everything all the
time, however, there must be a possibility to make the results re-traceable on
demand. In this paper we outline some of our research topics in the context of
the relatively new area of explainable-AI with a focus on the application in
medicine, which is a very special domain. This is due to the fact that medical
professionals are working mostly with distributed heterogeneous and complex
sources of data. In this paper we concentrate on three sources: images, *omics
data and text. We argue that research in explainable-AI would generally help to
facilitate the implementation of AI/ML in the medical domain, and specifically
help to facilitate transparency and trust.
",1,0,0,1,0,0
1287,1288,Scalable Realistic Recommendation Datasets through Fractal Expansions,"  Recommender System research suffers currently from a disconnect between the
size of academic data sets and the scale of industrial production systems. In
order to bridge that gap we propose to generate more massive user/item
interaction data sets by expanding pre-existing public data sets. User/item
incidence matrices record interactions between users and items on a given
platform as a large sparse matrix whose rows correspond to users and whose
columns correspond to items. Our technique expands such matrices to larger
numbers of rows (users), columns (items) and non zero values (interactions)
while preserving key higher order statistical properties. We adapt the
Kronecker Graph Theory to user/item incidence matrices and show that the
corresponding fractal expansions preserve the fat-tailed distributions of user
engagements, item popularity and singular value spectra of user/item
interaction matrices. Preserving such properties is key to building large
realistic synthetic data sets which in turn can be employed reliably to
benchmark Recommender Systems and the systems employed to train them. We
provide algorithms to produce such expansions and apply them to the MovieLens
20 million data set comprising 20 million ratings of 27K movies by 138K users.
The resulting expanded data set has 10 billion ratings, 2 million items and
864K users in its smaller version and can be scaled up or down. A larger
version features 655 billion ratings, 7 million items and 17 million users.
",1,0,0,1,0,0
6874,6875,A Projection Method for Metric-Constrained Optimization,"  We outline a new approach for solving optimization problems which enforce
triangle inequalities on output variables. We refer to this as
metric-constrained optimization, and give several examples where problems of
this form arise in machine learning applications and theoretical approximation
algorithms for graph clustering. Although these problem are interesting from a
theoretical perspective, they are challenging to solve in practice due to the
high memory requirement of black-box solvers. In order to address this
challenge we first prove that the metric-constrained linear program relaxation
of correlation clustering is equivalent to a special case of the metric
nearness problem. We then developed a general solver for metric-constrained
linear and quadratic programs by generalizing and improving a simple projection
algorithm originally developed for metric nearness. We give several novel
approximation guarantees for using our framework to find lower bounds for
optimal solutions to several challenging graph clustering problems. We also
demonstrate the power of our framework by solving optimizing problems involving
up to 10^{8} variables and 10^{11} constraints.
",1,0,0,1,0,0
1424,1425,On the Relation between Color Image Denoising and Classification,"  Large amount of image denoising literature focuses on single channel images
and often experimentally validates the proposed methods on tens of images at
most. In this paper, we investigate the interaction between denoising and
classification on large scale dataset. Inspired by classification models, we
propose a novel deep learning architecture for color (multichannel) image
denoising and report on thousands of images from ImageNet dataset as well as
commonly used imagery. We study the importance of (sufficient) training data,
how semantic class information can be traded for improved denoising results. As
a result, our method greatly improves PSNR performance by 0.34 - 0.51 dB on
average over state-of-the art methods on large scale dataset. We conclude that
it is beneficial to incorporate in classification models. On the other hand, we
also study how noise affect classification performance. In the end, we come to
a number of interesting conclusions, some being counter-intuitive.
",1,0,0,0,0,0
19791,19792,Deep Hyperspherical Learning,"  Convolution as inner product has been the founding basis of convolutional
neural networks (CNNs) and the key to end-to-end visual representation
learning. Benefiting from deeper architectures, recent CNNs have demonstrated
increasingly strong representation abilities. Despite such improvement, the
increased depth and larger parameter space have also led to challenges in
properly training a network. In light of such challenges, we propose
hyperspherical convolution (SphereConv), a novel learning framework that gives
angular representations on hyperspheres. We introduce SphereNet, deep
hyperspherical convolution networks that are distinct from conventional inner
product based convolutional networks. In particular, SphereNet adopts
SphereConv as its basic convolution operator and is supervised by generalized
angular softmax loss - a natural loss formulation under SphereConv. We show
that SphereNet can effectively encode discriminative representation and
alleviate training difficulty, leading to easier optimization, faster
convergence and comparable (even better) classification accuracy over
convolutional counterparts. We also provide some theoretical insights for the
advantages of learning on hyperspheres. In addition, we introduce the learnable
SphereConv, i.e., a natural improvement over prefixed SphereConv, and
SphereNorm, i.e., hyperspherical learning as a normalization method.
Experiments have verified our conclusions.
",1,0,0,1,0,0
3779,3780,Topological quantization of energy transport in micro- and nano-mechanical lattices,"  Topological effects typically discussed in the context of quantum physics are
emerging as one of the central paradigms of physics. Here, we demonstrate the
role of topology in energy transport through dimerized micro- and
nano-mechanical lattices in the classical regime, i.e., essentially ""masses and
springs"". We show that the thermal conductance factorizes into topological and
non-topological components. The former takes on three discrete values and
arises due to the appearance of edge modes that prevent good contact between
the heat reservoirs and the bulk, giving a length-independent reduction of the
conductance. In essence, energy input at the boundary mostly stays there, an
effect robust against disorder and nonlinearity. These results bridge two
seemingly disconnected disciplines of physics, namely topology and thermal
transport, and suggest ways to engineer thermal contacts, opening a direction
to explore the ramifications of topological properties on nanoscale technology.
",0,1,0,0,0,0
17370,17371,code2vec: Learning Distributed Representations of Code,"  We present a neural model for representing snippets of code as continuous
distributed vectors (""code embeddings""). The main idea is to represent a code
snippet as a single fixed-length $\textit{code vector}$, which can be used to
predict semantic properties of the snippet. This is performed by decomposing
code to a collection of paths in its abstract syntax tree, and learning the
atomic representation of each path $\textit{simultaneously}$ with learning how
to aggregate a set of them. We demonstrate the effectiveness of our approach by
using it to predict a method's name from the vector representation of its body.
We evaluate our approach by training a model on a dataset of 14M methods. We
show that code vectors trained on this dataset can predict method names from
files that were completely unobserved during training. Furthermore, we show
that our model learns useful method name vectors that capture semantic
similarities, combinations, and analogies. Comparing previous techniques over
the same data set, our approach obtains a relative improvement of over 75%,
being the first to successfully predict method names based on a large,
cross-project, corpus. Our trained model, visualizations and vector
similarities are available as an interactive online demo at
this http URL. The code, data, and trained models are available at
this https URL.
",1,0,0,1,0,0
16668,16669,Computation on Encrypted Data using Data Flow Authentication,"  Encrypting data before sending it to the cloud protects it against hackers
and malicious insiders, but requires the cloud to compute on encrypted data.
Trusted (hardware) modules, e.g., secure enclaves like Intel's SGX, can very
efficiently run entire programs in encrypted memory. However, it already has
been demonstrated that software vulnerabilities give an attacker ample
opportunity to insert arbitrary code into the program. This code can then
modify the data flow of the program and leak any secret in the program to an
observer in the cloud via SGX side-channels. Since any larger program is rife
with software vulnerabilities, it is not a good idea to outsource entire
programs to an SGX enclave. A secure alternative with a small trusted code base
would be fully homomorphic encryption (FHE) -- the holy grail of encrypted
computation. However, due to its high computational complexity it is unlikely
to be adopted in the near future. As a result researchers have made several
proposals for transforming programs to perform encrypted computations on less
powerful encryption schemes. Yet, current approaches fail on programs that make
control-flow decisions based on encrypted data. In this paper, we introduce the
concept of data flow authentication (DFAuth). DFAuth prevents an adversary from
arbitrarily deviating from the data flow of a program. Hence, an attacker
cannot perform an attack as outlined before on SGX. This enables that all
programs, even those including operations on control-flow decision variables,
can be computed on encrypted data. We implemented DFAuth using a novel
authenticated homomorphic encryption scheme, a Java bytecode-to-bytecode
compiler producing fully executable programs, and SGX enclaves. A transformed
neural network that performs machine learning on sensitive medical data can be
evaluated on encrypted inputs and encrypted weights in 0.86 seconds.
",1,0,0,0,0,0
13530,13531,Generalised Lyapunov Functions and Functionally Generated Trading Strategies,"  This paper investigates the dependence of functional portfolio generation,
introduced by Fernholz (1999), on an extra finite variation process. The
framework of Karatzas and Ruf (2017) is used to formulate conditions on trading
strategies to be strong arbitrage relative to the market over sufficiently
large time horizons. A mollification argument and Komlos theorem yield a
general class of potential arbitrage strategies. These theoretical results are
complemented by several empirical examples using data from the S&P 500 stocks.
",0,0,0,0,0,1
12092,12093,Continuous DR-submodular Maximization: Structure and Algorithms,"  DR-submodular continuous functions are important objectives with wide
real-world applications spanning MAP inference in determinantal point processes
(DPPs), and mean-field inference for probabilistic submodular models, amongst
others. DR-submodularity captures a subclass of non-convex functions that
enables both exact minimization and approximate maximization in polynomial
time.
In this work we study the problem of maximizing non-monotone DR-submodular
continuous functions under general down-closed convex constraints. We start by
investigating geometric properties that underlie such objectives, e.g., a
strong relation between (approximately) stationary points and global optimum is
proved. These properties are then used to devise two optimization algorithms
with provable guarantees. Concretely, we first devise a ""two-phase"" algorithm
with $1/4$ approximation guarantee. This algorithm allows the use of existing
methods for finding (approximately) stationary points as a subroutine, thus,
harnessing recent progress in non-convex optimization. Then we present a
non-monotone Frank-Wolfe variant with $1/e$ approximation guarantee and
sublinear convergence rate. Finally, we extend our approach to a broader class
of generalized DR-submodular continuous functions, which captures a wider
spectrum of applications. Our theoretical findings are validated on synthetic
and real-world problem instances.
",1,0,0,1,0,0
20115,20116,Prospects for detection of intermediate-mass black holes in globular clusters using integrated-light spectroscopy,"  The detection of intermediate mass black holes (IMBHs) in Galactic globular
clusters (GCs) has so far been controversial. In order to characterize the
effectiveness of integrated-light spectroscopy through integral field units, we
analyze realistic mock data generated from state-of-the-art Monte Carlo
simulations of GCs with a central IMBH, considering different setups and
conditions varying IMBH mass, cluster distance, and accuracy in determination
of the center. The mock observations are modeled with isotropic Jeans models to
assess the success rate in identifying the IMBH presence, which we find to be
primarily dependent on IMBH mass. However, even for a IMBH of considerable mass
(3% of the total GC mass), the analysis does not yield conclusive results in 1
out of 5 cases, because of shot noise due to bright stars close to the IMBH
line-of-sight. This stochastic variability in the modeling outcome grows with
decreasing BH mass, with approximately 3 failures out of 4 for IMBHs with 0.1%
of total GC mass. Finally, we find that our analysis is generally unable to
exclude at 68% confidence an IMBH with mass of $10^3~M_\odot$ in snapshots
without a central BH. Interestingly, our results are not sensitive to GC
distance within 5-20 kpc, nor to mis-identification of the GC center by less
than 2'' (<20% of the core radius). These findings highlight the value of
ground-based integral field spectroscopy for large GC surveys, where systematic
failures can be accounted for, but stress the importance of discrete kinematic
measurements that are less affected by stochasticity induced by bright stars.
",0,1,0,0,0,0
9994,9995,X-Ray bright optically faint active galactic nuclei in the Subaru Hyper Suprime-Cam wide survey,"  We construct a sample of X-ray bright optically faint active galactic nuclei
by combining Subaru Hyper Suprime-Cam, XMM-Newton, and infrared source
catalogs. 53 X-ray sources satisfying i band magnitude fainter than 23.5 mag
and X-ray counts with EPIC-PN detector larger than 70 are selected from 9.1
deg^2, and their spectral energy distributions (SEDs) and X-ray spectra are
analyzed. 44 objects with an X-ray to i-band flux ratio F_X/F_i>10 are
classified as extreme X-ray-to-optical flux sources. SEDs of 48 among 53 are
represented by templates of type 2 AGNs or starforming galaxies and show
signature of stellar emission from host galaxies in the optical in the source
rest frame. Infrared/optical SEDs indicate significant contribution of emission
from dust to infrared fluxes and that the central AGN is dust obscured.
Photometric redshifts determined from the SEDs are in the range of 0.6-2.5.
X-ray spectra are fitted by an absorbed power law model, and the intrinsic
absorption column densities are modest (best-fit log N_H = 20.5-23.5 cm^-2 in
most cases). The absorption corrected X-ray luminosities are in the range of
6x10^42 - 2x10^45 erg s^-1. 20 objects are classified as type 2 quasars based
on X-ray luminsosity and N_H. The optical faintness is explained by a
combination of redshifts (mostly z>1.0), strong dust extinction, and in part a
large ratio of dust/gas.
",0,1,0,0,0,0
1132,1133,Exact partial information decompositions for Gaussian systems based on dependency constraints,"  The Partial Information Decomposition (PID) [arXiv:1004.2515] provides a
theoretical framework to characterize and quantify the structure of
multivariate information sharing. A new method (Idep) has recently been
proposed for computing a two-predictor PID over discrete spaces.
[arXiv:1709.06653] A lattice of maximum entropy probability models is
constructed based on marginal dependency constraints, and the unique
information that a particular predictor has about the target is defined as the
minimum increase in joint predictor-target mutual information when that
particular predictor-target marginal dependency is constrained. Here, we apply
the Idep approach to Gaussian systems, for which the marginally constrained
maximum entropy models are Gaussian graphical models. Closed form solutions for
the Idep PID are derived for both univariate and multivariate Gaussian systems.
Numerical and graphical illustrations are provided, together with practical and
theoretical comparisons of the Idep PID with the minimum mutual information PID
(Immi). [arXiv:1411.2832] In particular, it is proved that the Immi method
generally produces larger estimates of redundancy and synergy than does the
Idep method. In discussion of the practical examples, the PIDs are complemented
by the use of deviance tests for the comparison of Gaussian graphical models.
",0,0,0,1,1,0
1253,1254,Separatrix crossing in rotation of a body with changing geometry of masses,"  We consider free rotation of a body whose parts move slowly with respect to
each other under the action of internal forces. This problem can be considered
as a perturbation of the Euler-Poinsot problem. The dynamics has an approximate
conservation law - an adiabatic invariant. This allows to describe the
evolution of rotation in the adiabatic approximation. The evolution leads to an
overturn in the rotation of the body: the vector of angular velocity crosses
the separatrix of the Euler-Poinsot problem. This crossing leads to a
quasi-random scattering in body's dynamics. We obtain formulas for
probabilities of capture into different domains in the phase space at
separatrix crossings.
",0,1,0,0,0,0
9244,9245,Group-like projections for locally compact quantum groups,"  Let $\mathbb{G}$ be a locally compact quantum group. We give a 1-1
correspondence between group-like projections in $L^\infty(\mathbb{G})$
preserved by the scaling group and idempotent states on the dual quantum group
$\widehat{\mathbb{G}}$. As a byproduct we give a simple proof that normal
integrable coideals in $L^\infty(\mathbb{G})$ which are preserved by the
scaling group are in 1-1 correspondence with compact quantum subgroups of
$\mathbb{G}$.
",0,0,1,0,0,0
11154,11155,Complex tensor factorisation with PARAFAC2 for the estimation of brain connectivity from the EEG,"  Objective: The coupling between neuronal populations and its magnitude have
been shown to be informative for various clinical applications. One method to
estimate brain connectivity is with electroencephalography (EEG) from which the
cross-spectrum between different sensor locations is derived. We wish to test
the efficacy of tensor factorisation in the estimation of brain connectivity.
Methods: Complex tensor factorisation based on PARAFAC2 is used to decompose
the EEG into scalp components described by the spatial, spectral, and complex
trial profiles. An EEG model in the complex domain was derived that shows the
suitability of PARAFAC2. A connectivity metric was also derived on the complex
trial profiles of the extracted components. Results: Results on a benchmark EEG
dataset confirmed that PARAFAC2 can estimate connectivity better than
traditional tensor analysis such as PARAFAC within a range of signal-to-noise
ratios. The analysis of EEG from patients with mild cognitive impairment or
Alzheimer's disease showed that PARAFAC2 identifies loss of brain connectivity
better than traditional approaches and agreeing with prior pathological
knowledge. Conclusion: The complex PARAFAC2 algorithm is suitable for EEG
connectivity estimation since it allows to extract meaningful coupled sources
and provides better estimates than complex PARAFAC. Significance: A new
paradigm that employs complex tensor factorisation has demonstrated to be
successful in identifying brain connectivity and the location of couples
sources for both a benchmark and a real-world EEG dataset. This can enable
future applications and has the potential to solve some the issues that
deteriorate the performance of traditional connectivity metrics.
",1,0,0,0,0,0
5519,5520,Characterization of multivariate Bernoulli distributions with given margins,"  We express each Fréchet class of multivariate Bernoulli distributions with
given margins as the convex hull of a set of densities, which belong to the
same Fréchet class. This characterisation allows us to establish whether a
given correlation matrix is compatible with the assigned margins and, if it is,
to easily construct one of the corresponding joint densities. % Such
%representation is based on a polynomial expression of the distributions of a
Fréchet class. We reduce the problem of finding a density belonging to a
Fréchet class and with given correlation matrix to the solution of a linear
system of equations. Our methodology also provides the bounds that each
correlation must satisfy to be compatible with the assigned margins. An
algorithm and its use in some examples is shown.
",0,0,1,1,0,0
2033,2034,Non-locality of the meet levels of the Trotter-Weil Hierarchy,"  We prove that the meet level $m$ of the Trotter-Weil, $\mathsf{V}_m$ is not
local for all $m \geq 1$, as conjectured in a paper by Kufleitner and Lauser.
In order to show this, we explicitly provide a language whose syntactic
semigroup is in $L \mathsf{V}_m$ and not in $\mathsf{V}_m*\mathsf{D}$.
",1,0,1,0,0,0
15635,15636,Joint Rate and Resource Allocation in Hybrid Digital-Analog Transmission over Fading Channels,"  In hybrid digital-analog (HDA) systems, resource allocation has been utilized
to achieve desired distortion performance. However, existing studies on this
issue assume error-free digital transmission, which is not valid for fading
channels. With time-varying channel fading, the exact channel state information
is not available at the transmitter. Thus, random outage and resulting digital
distortion cannot be ignored. Moreover, rate allocation should be considered in
resource allocation, since it not only determines the amount of information for
digital transmission and that for analog transmission, but also affects the
outage probability. Based on above observations, in this paper, we attempt to
perform joint rate and resource allocation strategies to optimize system
distortion in HDA systems over fading channels. Consider a bandwidth expansion
scenario where a memoryless Gaussian source is transmitted in an HDA system
with the entropy-constrained scalar quantizer (ECSQ). Firstly, we formulate the
joint allocation problem as an expected system distortion minimization problem
where both analog and digital distortion are considered. Then, in the limit of
low outage probability, we decompose the problem into two coupled sub-problems
based on the block coordinate descent method, and propose an iterative gradient
algorithm to approach the optimal solution. Furthermore, we extend our work to
the multivariate Gaussian source scenario where a two-stage fast algorithm
integrating rounding and greedy strategies is proposed to optimize the joint
rate and resource allocation problem. Finally, simulation results demonstrate
that the proposed algorithms can achieve up to 2.3dB gains in terms of
signal-to-distortion ratio over existing schemes under the single Gaussian
source scenario, and up to 3.5dB gains under the multivariate Gaussian source
scenario.
",1,0,0,0,0,0
17941,17942,Stability and instability in saddle point dynamics - Part I,"  We consider the problem of convergence to a saddle point of a concave-convex
function via gradient dynamics. Since first introduced by Arrow, Hurwicz and
Uzawa in [1] such dynamics have been extensively used in diverse areas, there
are, however, features that render their analysis non trivial. These include
the lack of convergence guarantees when the function considered is not strictly
concave-convex and also the non-smoothness of subgradient dynamics. Our aim in
this two part paper is to provide an explicit characterization to the
asymptotic behaviour of general gradient and subgradient dynamics applied to a
general concave-convex function. We show that despite the nonlinearity and
non-smoothness of these dynamics their $\omega$-limit set is comprised of
trajectories that solve only explicit linear ODEs that are characterized within
the paper.
More precisely, in Part I an exact characterization is provided to the
asymptotic behaviour of unconstrained gradient dynamics. We also show that when
convergence to a saddle point is not guaranteed then the system behaviour can
be problematic, with arbitrarily small noise leading to an unbounded variance.
In Part II we consider a general class of subgradient dynamics that restrict
trajectories in an arbitrary convex domain, and show that their limiting
trajectories are solutions of subgradient dynamics on only affine subspaces.
The latter is a smooth class of dynamics with an asymptotic behaviour exactly
characterized in Part I, as solutions to explicit linear ODEs. These results
are used to formulate corresponding convergence criteria and are demonstrated
with several examples and applications presented in Part II.
",1,0,1,0,0,0
4530,4531,Control Synthesis for Permutation-Symmetric High-Dimensional Systems With Counting Constraints,"  General purpose correct-by-construction synthesis methods are limited to
systems with low dimensionality or simple specifications. In this work we
consider highly symmetrical counting problems and exploit the symmetry to
synthesize provably correct controllers for systems with tens of thousands of
states. The key ingredients of the solution are an aggregate abstraction
procedure for mildly heterogeneous systems and a formulation of counting
constraints as linear inequalities.
",1,0,1,0,0,0
19362,19363,Vacancy-driven extended stability of cubic metastable Ta-Al-N and Nb-Al-N phases,"  Quantum mechanical calculations had been previously applied to predict phase
stability in many ternary and multinary nitride systems. While the predictions
were very accurate for the Ti-Al-N system, some discrepancies between theory
and experiment were obtained in the case of other systems. Namely, in the case
of Ta-Al-N, the calculations tend to overestimate the minimum Al content
necessary to obtain a metastable solid solution with a cubic structure. In this
work, we present a comprehensive study of the impact of vacancies on the phase
fields in quasi-binary TaN-AlN and NbN-AlN systems. Our calculations clearly
show that presence of point defects strongly enlarges the cubic phase field in
the TaN-AlN system, while the effect is less pronounced in the NbN-AlN case.
The present phase stability predictions agree better with experimental
observations of physical vapour deposited thin films reported in the literature
than that based on perfect, non-defected structures. This study shows that a
representative structural model is crucial for a meaningful comparison with
experimental data.
",0,1,0,0,0,0
19523,19524,Advanced Quantizer Designs for FDD-Based FD-MIMO Systems Using Uniform Planar Arrays,"  Massive multiple-input multiple-output (MIMO) systems, which utilize a large
number of antennas at the base station, are expected to enhance network
throughput by enabling improved multiuser MIMO techniques. To deploy many
antennas in reasonable form factors, base stations are expected to employ
antenna arrays in both horizontal and vertical dimensions, which is known as
full-dimension (FD) MIMO. The most popular two-dimensional array is the uniform
planar array (UPA), where antennas are placed in a grid pattern. To exploit the
full benefit of massive MIMO in frequency division duplexing (FDD), the
downlink channel state information (CSI) should be estimated, quantized, and
fed back from the receiver to the transmitter. However, it is difficult to
accurately quantize the channel in a computationally efficient manner due to
the high dimensionality of the massive MIMO channel. In this paper, we develop
both narrowband and wideband CSI quantizers for FD-MIMO taking the properties
of realistic channels and the UPA into consideration. To improve quantization
quality, we focus on not only quantizing dominant radio paths in the channel,
but also combining the quantized beams. We also develop a hierarchical beam
search approach, which scans both vertical and horizontal domains jointly with
moderate computational complexity. Numerical simulations verify that the
performance of the proposed quantizers is better than that of previous CSI
quantization techniques.
",1,0,0,0,0,0
1474,1475,On the quantum differentiation of smooth real-valued functions,"  Calculating the value of $C^{k\in\{1,\infty\}}$ class of smoothness
real-valued function's derivative in point of $\mathbb{R}^+$ in radius of
convergence of its Taylor polynomial (or series), applying an analog of
Newton's binomial theorem and $q$-difference operator. $(P,q)$-power difference
introduced in section 5. Additionally, by means of Newton's interpolation
formula, the discrete analog of Taylor series, interpolation using
$q$-difference and $p,q$-power difference is shown.
",0,0,1,0,0,0
16844,16845,Some Ageing Properties of Dynamic Additive Mean Residual Life Model,"  Although proportional hazard rate model is a very popular model to analyze
failure time data, sometimes it becomes important to study the additive hazard
rate model. Again, sometimes the concept of the hazard rate function is
abstract, in comparison to the concept of mean residual life function. A new
model called `dynamic additive mean residual life model' where the covariates
are time-dependent has been defined in the literature. Here we study the
closure properties of the model for different positive and negative ageing
classes under certain condition(s). Quite a few examples are presented to
illustrate different properties of the model.
",0,0,1,1,0,0
4103,4104,Deictic Image Maps: An Abstraction For Learning Pose Invariant Manipulation Policies,"  In applications of deep reinforcement learning to robotics, it is often the
case that we want to learn pose invariant policies: policies that are invariant
to changes in the position and orientation of objects in the world. For
example, consider a peg-in-hole insertion task. If the agent learns to insert a
peg into one hole, we would like that policy to generalize to holes presented
in different poses. Unfortunately, this is a challenge using conventional
methods. This paper proposes a novel state and action abstraction that is
invariant to pose shifts called \textit{deictic image maps} that can be used
with deep reinforcement learning. We provide broad conditions under which
optimal abstract policies are optimal for the underlying system. Finally, we
show that the method can help solve challenging robotic manipulation problems.
",1,0,0,0,0,0
14351,14352,Zero-Modified Poisson-Lindley distribution with applications in zero-inflated and zero-deflated count data,"  The main object of this article is to present an extension of the
zero-inflated Poisson-Lindley distribution, called of zero-modified
Poisson-Lindley. The additional parameter $\pi$ of the zero-modified
Poisson-Lindley has a natural interpretation in terms of either
zero-deflated/inflated proportion. Inference is dealt with by using the
likelihood approach. In particular the maximum likelihood estimators of the
distribution's parameter are compared in small and large samples. We also
consider an alternative bias-correction mechanism based on Efron's bootstrap
resampling. The model is applied to real data sets and found to perform better
than other competing models.
",0,0,0,1,0,0
19057,19058,Testing Degree Corrections in Stochastic Block Models,"  We study sharp detection thresholds for degree corrections in Stochastic
Block Models in the context of a goodness of fit problem. When degree
corrections are relatively dense, a simple test based on the total number of
edges is asymptotically optimal. For sparse degree corrections in non-dense
graphs, simple degree based Higher Criticism Test (Mukherjee, Mukherjee, Sen
2016) is optimal with sharp constants. In contrast, for dense graphs, the
optimal procedure runs in two stages. It involves running a suitable community
recovery algorithm in stage 1, followed by a Higher Criticism Test based on a
linear combination of within and across (estimated) community degrees in stage
2. The necessity of the two step procedure is demonstrated by the failure of
the ordinary Maximum Degree Test in achieving sharp constants. As necessary
tools we also derive asymptotic distribution of the Maximum Degree in
Stochastic Block Models along with moderate deviation and local central limit
type asymptotics of positive linear combinations of independent Binomial random
variables.
",0,0,1,1,0,0
13979,13980,Modeling of a self-sustaining ignition in a solid energetic material,"  In the present work we analyze some necessary conditions for ignition of
solid energetic materials by low velocity impact ignition mechanism. Basing on
reported results of {\it ab initio} computations we assume that the energetic
activation barriers for the primary endothermic dissociation in some energetic
materials may be locally lowered due to the effect of shear strain caused by
the impact. We show that the ignition may be initiated in regions with the
reduced activation barriers, even at moderately low exothermicity of the
subsequent exothermic reactions thus suggesting that the above regions may
serve as ""hot spots"" for the ignition. We apply our results to analyze initial
steps of ignition in DADNE and TATB molecular crystals.
",0,1,0,0,0,0
580,581,A Globally Linearly Convergent Method for Pointwise Quadratically Supportable Convex-Concave Saddle Point Problems,"  We study the \emph{Proximal Alternating Predictor-Corrector} (PAPC) algorithm
introduced recently by Drori, Sabach and Teboulle to solve nonsmooth structured
convex-concave saddle point problems consisting of the sum of a smooth convex
function, a finite collection of nonsmooth convex functions and bilinear terms.
We introduce the notion of pointwise quadratic supportability, which is a
relaxation of a standard strong convexity assumption and allows us to show that
the primal sequence is R-linearly convergent to an optimal solution and the
primal-dual sequence is globally Q-linearly convergent. We illustrate the
proposed method on total variation denoising problems and on locally adaptive
estimation in signal/image deconvolution and denoising with multiresolution
statistical constraints.
",0,0,1,0,0,0
20157,20158,The Hurwitz-type theorem for the regular Coulomb wave function via Hankel determinants,"  We derive a closed formula for the determinant of the Hankel matrix whose
entries are given by sums of negative powers of the zeros of the regular
Coulomb wave function. This new identity applied together with results of
Grommer and Chebotarev allows us to prove a Hurwitz-type theorem about the
zeros of the regular Coulomb wave function. As a particular case, we obtain a
new proof of the classical Hurwitz's theorem from the theory of Bessel
functions that is based on algebraic arguments. In addition, several Hankel
determinants with entries given by the Rayleigh function and Bernoulli numbers
are also evaluated.
",0,0,1,0,0,0
2299,2300,Ultrahigh capacitive energy storage in highly oriented BaZr(x)Ti(1-x)O3 thin films prepared by pulsed laser deposition,"  We report structural, optical, temperature and frequency dependent
dielectric, and energy storage properties of pulsed laser deposited (100)
highly textured BaZr(x)Ti(1-x)O3 (x = 0.3, 0.4 and 0.5) relaxor ferroelectric
thin films on La0.7Sr0.3MnO3/MgO substrates which make this compound as a
potential lead-free capacitive energy storage material for scalable electronic
devices. A high dielectric constant of ~1400 - 3500 and a low dielectric loss
of <0.025 were achieved at 10 kHz for all three compositions at ambient
conditions. Ultrahigh stored and recoverable electrostatic energy densities as
high as 214 +/- 1 and 156 +/- 1 J/cm3, respectively, were demonstrated at a
sustained high electric field of ~3 MV/cm with an efficiency of 72.8 +/- 0.6 %
in optimum 30% Zr substituted BaTiO3 composition.
",0,1,0,0,0,0
15417,15418,Dual quadratic differentials and entire minimal graphs in Heisenberg space,"  We define holomorphic quadratic differentials for spacelike surfaces with
constant mean curvature in the Lorentzian homogeneous spaces
$\mathbb{L}(\kappa,\tau)$ with isometry group of dimension 4, which are dual to
the Abresch-Rosenberg differentials in the Riemannian counterparts
$\mathbb{E}(\kappa,\tau)$, and obtain some consequences. On the one hand, we
give a very short proof of the Bernstein problem in Heisenberg space, and
provide a geometric description of the family of entire graphs sharing the same
differential in terms of a 2-parameter conformal deformation. On the other
hand, we prove that entire minimal graphs in Heisenberg space have negative
Gauss curvature.
",0,0,1,0,0,0
4012,4013,The Rees algebra of a two-Borel ideal is Koszul,"  Let $M$ and $N$ be two monomials of the same degree, and let $I$ be the
smallest Borel ideal containing $M$ and $N$. We show that the toric ring of $I$
is Koszul by constructing a quadratic Gröbner basis for the associated toric
ideal. Our proofs use the construction of graphs corresponding to fibers of the
toric map. As a consequence, we conclude that the Rees algebra is also Koszul.
",0,0,1,0,0,0
16155,16156,Manipulation of type-I and type-II Dirac points in PdTe2 superconductor by external pressure,"  A pair of type-II Dirac cones in PdTe$_2$ was recently predicted by theories
and confirmed in experiments, making PdTe$_2$ the first material that processes
both superconductivity and type-II Dirac fermions. In this work, we study the
evolution of Dirac cones in PdTe$_2$ under hydrostatic pressure by the
first-principles calculations. Our results show that the pair of type-II Dirac
points disappears at 6.1 GPa. Interestingly, a new pair of type-I Dirac points
from the same two bands emerges at 4.7 GPa. Due to the distinctive band
structures compared with those of PtSe$_2$ and PtTe$_2$, the two types of Dirac
points can coexist in PdTe$_2$ under proper pressure (4.7-6.1 GPa). The
emergence of type-I Dirac cones and the disappearance of type-II Dirac ones are
attributed to the increase/decrease of the energy of the states at $\Gamma$ and
$A$ point, which have the anti-bonding/bonding characters of interlayer Te-Te
atoms. On the other hand, we find that the superconductivity of PdTe$_2$
slightly decreases with pressure. The pressure-induced different types of Dirac
cones combined with superconductivity may open a promising way to investigate
the complex interactions between Dirac fermions and superconducting
quasi-particles.
",0,1,0,0,0,0
10917,10918,Encoding Multi-Resolution Brain Networks Using Unsupervised Deep Learning,"  The main goal of this study is to extract a set of brain networks in multiple
time-resolutions to analyze the connectivity patterns among the anatomic
regions for a given cognitive task. We suggest a deep architecture which learns
the natural groupings of the connectivity patterns of human brain in multiple
time-resolutions. The suggested architecture is tested on task data set of
Human Connectome Project (HCP) where we extract multi-resolution networks, each
of which corresponds to a cognitive task. At the first level of this
architecture, we decompose the fMRI signal into multiple sub-bands using
wavelet decompositions. At the second level, for each sub-band, we estimate a
brain network extracted from short time windows of the fMRI signal. At the
third level, we feed the adjacency matrices of each mesh network at each
time-resolution into an unsupervised deep learning algorithm, namely, a Stacked
De- noising Auto-Encoder (SDAE). The outputs of the SDAE provide a compact
connectivity representation for each time window at each sub-band of the fMRI
signal. We concatenate the learned representations of all sub-bands at each
window and cluster them by a hierarchical algorithm to find the natural
groupings among the windows. We observe that each cluster represents a
cognitive task with a performance of 93% Rand Index and 71% Adjusted Rand
Index. We visualize the mean values and the precisions of the networks at each
component of the cluster mixture. The mean brain networks at cluster centers
show the variations among cognitive tasks and the precision of each cluster
shows the within cluster variability of networks, across the subjects.
",0,0,0,1,0,0
9937,9938,Experimental study of extrinsic spin Hall effect in CuPt alloy,"  We have experimentally studied the effects on the spin Hall angle due to
systematic addition of Pt into the light metal Cu. We perform spin torque
ferromagnetic resonance measurements on Py/CuPt bilayer and find that as the Pt
concentration increases, the spin Hall angle of CuPt alloy increases. Moreover,
only 28% Pt in CuPt alloy can give rise to a spin Hall angle close to that of
Pt. We further extract the spin Hall resistivity of CuPt alloy for different Pt
concentrations and find that the contribution of skew scattering is larger for
lower Pt concentrations, while the side-jump contribution is larger for higher
Pt concentrations. From technological perspective, since the CuPt alloy can
sustain high processing temperatures and Cu is the most common metallization
element in the Si platform, it would be easier to integrate the CuPt alloy
based spintronic devices into existing Si fabrication technology.
",0,1,0,0,0,0
15846,15847,A geometric realization of the $m$-cluster categories of type $\tilde{D_n}$,"  We show that a subcategory of the $m$-cluster category of type $\tilde{D_n}$
is isomorphic to a category consisting of arcs in an $(n-2)m$-gon with two
central $(m-1)$-gons inside of it. We show that the mutation of colored quivers
and $m$-cluster-tilting objects is compatible with the flip of an
$(m+2)$-angulation. In the final part of this paper, we detail an example of a
quiver of type $\tilde{D_7}$.
",0,0,1,0,0,0
18472,18473,Molecular Gas during the Post-Starburst Phase: Low Gas Fractions in Green Valley Seyfert Post-Starburst Galaxies,"  Post-starbursts (PSBs) are candidate for rapidly transitioning from
star-bursting to quiescent galaxies. We study the molecular gas evolution of
PSBs at z ~ 0.03 - 0.2. We undertook new CO (2-1) observations of 22 Seyfert
PSBs candidates using the ARO Submillimeter Telescope. This sample complements
previous samples of PSBs by including green valley PSBs with Seyfert-like
emission, allowing us to analyze for the first time the molecular gas
properties of 116 PSBs with a variety of AGN properties. The distribution of
molecular gas to stellar mass fractions in PSBs is significantly different than
normal star-forming galaxies in the COLD GASS survey. The combined samples of
PSBs with Seyfert-like emission line ratios have a gas fraction distribution
which is even more significantly different and is broader (~ 0.03-0.3). Most of
them have lower gas fractions than normal star-forming galaxies. We find a
highly significant correlation between the WISE 12 micron to 4.6 micron flux
ratios and molecular gas fractions in both PSBs and normal galaxies. We detect
molecular gas in 27% of our Seyfert PSBs. Taking into account the upper limits,
the mean and the dispersion of the distribution of the gas fraction in our
Seyfert PSB sample are much smaller (mean = 0.025, std dev. = 0.018) than
previous samples of Seyfert PSBs or PSBs in general (mean ~ 0.1 - 0.2, std dev.
~ 0.1 - 0.2).
",0,1,0,0,0,0
11161,11162,Cross-stream migration of active particles,"  For natural microswimmers, the interplay of swimming activity and external
flow can promote robust motion, e.g. propulsion against (""upstream rheotaxis"")
or perpendicular to the direction of flow. These effects are generally
attributed to their complex body shapes and flagellar beat patterns. Here,
using catalytic Janus particles as a model experimental system, we report on a
strong directional response that occurs for spherical active particles in a
channel flow. The particles align their propulsion axes to be nearly
perpendicular to both the direction of flow and the normal vector of a nearby
bounding surface. We develop a deterministic theoretical model of spherical
microswimmers near a planar wall that captures the experimental observations.
We show how the directional response emerges from the interplay of shear flow
and near-surface swimming activity. Finally, adding the effect of thermal
noise, we obtain probability distributions for the swimmer orientation that
semi-quantitatively agree with the experimental distributions.
",0,1,0,0,0,0
9735,9736,A Data-Driven Approach for Predicting Vegetation-Related Outages in Power Distribution Systems,"  This paper presents a novel data-driven approach for predicting the number of
vegetation-related outages that occur in power distribution systems on a
monthly basis. In order to develop an approach that is able to successfully
fulfill this objective, there are two main challenges that ought to be
addressed. The first challenge is to define the extent of the target area. An
unsupervised machine learning approach is proposed to overcome this difficulty.
The second challenge is to correctly identify the main causes of
vegetation-related outages and to thoroughly investigate their nature. In this
paper, these outages are categorized into two main groups: growth-related and
weather-related outages, and two types of models, namely time series and
non-linear machine learning regression models are proposed to conduct the
prediction tasks, respectively. Moreover, various features that can explain the
variability in vegetation-related outages are engineered and employed. Actual
outage data, obtained from a major utility in the U.S., in addition to
different types of weather and geographical data are utilized to build the
proposed approach. Finally, a comprehensive case study is carried out to
demonstrate how the proposed approach can be used to successfully predict the
number of vegetation-related outages and to help decision-makers to detect
vulnerable zones in their systems.
",0,0,0,1,0,0
8672,8673,Privacy in Information-Rich Intelligent Infrastructure,"  Intelligent infrastructure will critically rely on the dense instrumentation
of sensors and actuators that constantly transmit streaming data to cloud-based
analytics for real-time monitoring. For example, driverless cars communicate
real-time location and other data to companies like Google, which aggregate
regional data in order to provide real-time traffic maps. Such traffic maps can
be extremely useful to the driver (for optimal travel routing), as well as to
city transportation administrators for real-time accident response that can
have an impact on traffic capacity. Intelligent infrastructure monitoring
compromises the privacy of drivers who continuously share their location to
cloud aggregators, with unpredictable consequences.
Without a framework for protecting the privacy of the driver's data, drivers
may be very conservative about sharing their data with cloud-based analytics
that will be responsible for adding the intelligence to intelligent
infrastructure. In the energy sector, the Smart Grid revolution relies
critically on real-time metering of energy supply and demand with very high
granularity. This is turn enables real-time demand response and creates a new
energy market that can incorporate unpredictable renewable energy sources while
ensuring grid stability and reliability. However, real-time streaming data
captured by smart meters contain a lot of private information, such as our home
activities or lack of, which can be easily inferred by anyone that has access
to the smart meter data, resulting not only in loss of privacy but potentially
also putting us at risk.
",1,0,0,0,0,0
10238,10239,Deformation mechanism map of Cu/Nb nanoscale metallic multilayers as a function of temperature and layer thickness,"  The mechanical properties and deformation mechanisms of Cu/Nb nanoscale
metallic multilayers (NMMs) manufactured by accumulative roll bonding (ARB) are
studied at 25C and 400C. Cu/Nb NMMs with individual layer thicknesses between 7
and 63 nm were tested by in-situ micropillar compression inside a scanning
electron microscope Yield strength, strain-rate sensitivities and activation
volumes were obtained from the pillar compression tests. The deformed
micropillars were examined under scanning and transmission electron microscopy
in order to examine the deformation mechanisms active for different layer
thicknesses and temperatures. The analysis suggests that room temperature
deformation was determined by dislocation glide at larger layer thicknesses and
interface-related mechanisms at the thinner layer thicknesses. The high
temperature compression tests, in contrast, revealed superior thermo-mechanical
stability and strength retention for the NMMs with larger layer thicknesses
with deformation controlled by dislocation glide. A remarkable transition in
deformation mechanism occurred as the layer thickness decreased, to a
deformation response controlled by diffusion processes along the interfaces,
which resulted in temperature-induced softening. A deformation mechanism map,
in terms of layer thickness and temperature, is proposed from the results
obtained in this investigation.
",0,1,0,0,0,0
12880,12881,An age-structured continuum model for myxobacteria,"  Myxobacteria are social bacteria, that can glide in 2D and form
counter-propagating, interacting waves. Here we present a novel age-structured,
continuous macroscopic model for the movement of myxobacteria. The derivation
is based on microscopic interaction rules that can be formulated as a
particle-based model and set within the SOH (Self-Organized Hydrodynamics)
framework. The strength of this combined approach is that microscopic knowledge
or data can be incorporated easily into the particle model, whilst the
continuous model allows for easy numerical analysis of the different effects.
However we found that the derived macroscopic model lacks a diffusion term in
the density equations, which is necessary to control the number of waves,
indicating that a higher order approximation during the derivation is crucial.
Upon ad-hoc addition of the diffusion term, we found very good agreement
between the age-structured model and the biology. In particular we analyzed the
influence of a refractory (insensitivity) period following a reversal of
movement. Our analysis reveals that the refractory period is not necessary for
wave formation, but essential to wave synchronization, indicating separate
molecular mechanisms.
",0,1,1,0,0,0
18773,18774,Conditionally conjugate mean-field variational Bayes for logistic models,"  Variational Bayes (VB) is a common strategy for approximate Bayesian
inference, but simple methods are only available for specific classes of models
including, in particular, representations having conditionally conjugate
constructions within an exponential family. Models with logit components are an
apparently notable exception to this class, due to the absence of conjugacy
between the logistic likelihood and the Gaussian priors for the coefficients in
the linear predictor. To facilitate approximate inference within this widely
used class of models, Jaakkola and Jordan (2000) proposed a simple variational
approach which relies on a family of tangent quadratic lower bounds of logistic
log-likelihoods, thus restoring conjugacy between these approximate bounds and
the Gaussian priors. This strategy is still implemented successfully, but less
attempts have been made to formally understand the reasons underlying its
excellent performance. To cover this key gap, we provide a formal connection
between the above bound and a recent Pólya-gamma data augmentation for
logistic regression. Such a result places the computational methods associated
with the aforementioned bounds within the framework of variational inference
for conditionally conjugate exponential family models, thereby allowing recent
advances for this class to be inherited also by the methods relying on Jaakkola
and Jordan (2000).
",0,0,1,1,0,0
1067,1068,Isomorphism and Morita equivalence classes for crossed products of irrational rotation algebras by cyclic subgroups of $SL_2(\mathbb{Z})$,"  Let $\theta, \theta'$ be irrational numbers and $A, B$ be matrices in
$SL_2(\mathbb{Z})$ of infinite order. We compute the $K$-theory of the crossed
product $\mathcal{A}_{\theta}\rtimes_A \mathbb{Z}$ and show that
$\mathcal{A}_{\theta} \rtimes_A\mathbb{Z}$ and $\mathcal{A}_{\theta'} \rtimes_B
\mathbb{Z}$ are $*$-isomorphic if and only if $\theta = \pm\theta'
\pmod{\mathbb{Z}}$ and $I-A^{-1}$ is matrix equivalent to $I-B^{-1}$. Combining
this result and an explicit construction of equivariant bimodules, we show that
$\mathcal{A}_{\theta} \rtimes_A\mathbb{Z}$ and $\mathcal{A}_{\theta'} \rtimes_B
\mathbb{Z}$ are Morita equivalent if and only if $\theta$ and $\theta'$ are in
the same $GL_2(\mathbb{Z})$ orbit and $I-A^{-1}$ is matrix equivalent to
$I-B^{-1}$. Finally, we determine the Morita equivalence class of
$\mathcal{A}_{\theta} \rtimes F$ for any finite subgroup $F$ of
$SL_2(\mathbb{Z})$.
",0,0,1,0,0,0
12450,12451,Minimal hard surface-unlink and classical unlink diagrams,"  We describe a method for generating minimal hard prime surface-link diagrams.
We extend the known examples of minimal hard prime classical unknot and unlink
diagrams up to three components and generate figures of all minimal hard prime
surface-unknot and surface-unlink diagrams with prime base surface components
up to ten crossings.
",0,0,1,0,0,0
134,135,Spatial Regression and the Bayesian Filter,"  Regression for spatially dependent outcomes poses many challenges, for
inference and for computation. Non-spatial models and traditional spatial
mixed-effects models each have their advantages and disadvantages, making it
difficult for practitioners to determine how to carry out a spatial regression
analysis. We discuss the data-generating mechanisms implicitly assumed by
various popular spatial regression models, and discuss the implications of
these assumptions. We propose Bayesian spatial filtering as an approximate
middle way between non-spatial models and traditional spatial mixed models. We
show by simulation that our Bayesian spatial filtering model has several
desirable properties and hence may be a useful addition to a spatial
statistician's toolkit.
",0,0,0,1,0,0
10714,10715,Coupling parallel adaptive mesh refinement with a nonoverlapping domain decomposition solver,"  We study the effect of adaptive mesh refinement on a parallel domain
decomposition solver of a linear system of algebraic equations. These concepts
need to be combined within a parallel adaptive finite element software. A
prototype implementation is presented for this purpose. It uses adaptive mesh
refinement with one level of hanging nodes. Two and three-level versions of the
Balancing Domain Decomposition based on Constraints (BDDC) method are used to
solve the arising system of algebraic equations. The basic concepts are
recalled and components necessary for the combination are studied in detail. Of
particular interest is the effect of disconnected subdomains, a typical output
of the employed mesh partitioning based on space-filling curves, on the
convergence and solution time of the BDDC method. It is demonstrated using a
large set of experiments that while both refined meshes and disconnected
subdomains have a negative effect on the convergence of BDDC, the number of
iterations remains acceptable. In addition, scalability of the three-level BDDC
solver remains good on up to a few thousands of processor cores. The largest
presented problem using adaptive mesh refinement has over 10^9 unknowns and is
solved on 2048 cores.
",1,0,1,0,0,0
9488,9489,On the construction of small subsets containing special elements in a finite field,"  In this note we construct a series of small subsets containing a non-d-th
power element in a finite field by applying certain bounds on incomplete
character sums.
Precisely, let $h=\lfloor q^{\delta}\rfloor>1$ and $d\mid q^h-1$. Let $r$ be
a prime divisor of $q-1$ such that the largest prime power part of $q-1$ has
the form $r^s$. Then there is a constant $0<\epsilon<1$ such that for a ratio
at least $ {q^{-\epsilon h}}$ of $\alpha\in \mathbb{F}_{q^{h}}
\backslash\mathbb{F}_{q}$, the set $S=\{ \alpha-x^t, x\in\mathbb{F}_{q}\}$ of
cardinality $1+\frac {q-1} {M(h)}$ contains a non-d-th power in
$\mathbb{F}_{q^{\lfloor q^\delta\rfloor}}$, where $t$ is the largest power of
$r$ such that $t<\sqrt{q}/h$ and $M(h)$ is defined as $$M(h)=\max_{r \mid
(q-1)} r^{\min\{v_r(q-1), \lfloor\log_r{q}/2-\log_r h\rfloor\}}.$$ Here $r$
runs thourgh prime divisors and $v_r(x)$ is the $r$-adic oder of $x$. For odd
$q$, the choice of $\delta=\frac 12-d, d=o(1)>0$ shows that there exists an
explicit subset of cardinality $q^{1-d}=O(\log^{2+\epsilon'}(q^h))$ containing
a non-quadratic element in the field $\mathbb{F}_{q^h}$. On the other hand, the
choice of $h=2$ shows that for any odd prime power $q$, there is an explicit
subset of cardinality $1+\frac {q-1}{M(2)}$ containing a non-quadratic element
in $\mathbb{F}_{q^2}$. This improves a $q-1$ construction by Coulter and Kosick
\cite{CK} since $\lfloor \log_2{(q-1)}\rfloor\leq M(2) < \sqrt{q}$.
In addition, we obtain a similar construction for small sets containing a
primitive element. The construction works well provided $\phi(q^h-1)$ is very
small, where $\phi$ is the Euler's totient function.
",1,0,1,0,0,0
14196,14197,Coresets for Dependency Networks,"  Many applications infer the structure of a probabilistic graphical model from
data to elucidate the relationships between variables. But how can we train
graphical models on a massive data set? In this paper, we show how to construct
coresets -compressed data sets which can be used as proxy for the original data
and have provably bounded worst case error- for Gaussian dependency networks
(DNs), i.e., cyclic directed graphical models over Gaussians, where the parents
of each variable are its Markov blanket. Specifically, we prove that Gaussian
DNs admit coresets of size independent of the size of the data set.
Unfortunately, this does not extend to DNs over members of the exponential
family in general. As we will prove, Poisson DNs do not admit small coresets.
Despite this worst-case result, we will provide an argument why our coreset
construction for DNs can still work well in practice on count data. To
corroborate our theoretical results, we empirically evaluated the resulting
Core DNs on real data sets. The results
",1,0,0,1,0,0
19029,19030,Asymptotic Independence of Bivariate Order Statistics,"  It is well known that an extreme order statistic and a central order
statistic (os) as well as an intermediate os and a central os from a sample of
iid univariate random variables get asymptotically independent as the sample
size increases. We extend this result to bivariate random variables, where the
os are taken componentwise. An explicit representation of the conditional
distribution of bivariate os turns out to be a powerful tool.
",0,0,1,1,0,0
15733,15734,Geometry-Based Optimization of One-Way Quantum Computation Measurement Patterns,"  In one-way quantum computation (1WQC) model, an initial highly entangled
state called a graph state is used to perform universal quantum computations by
a sequence of adaptive single-qubit measurements and post-measurement Pauli-X
and Pauli-Z corrections. The needed computations are organized as measurement
patterns, or simply patterns, in the 1WQC model. The entanglement operations in
a pattern can be shown by a graph which together with the set of its input and
output qubits is called the geometry of the pattern. Since a one-way quantum
computation pattern is based on quantum measurements, which are fundamentally
nondeterministic evolutions, there must be conditions over geometries to
guarantee determinism. Causal flow is a sufficient and generalized flow (gflow)
is a necessary and sufficient condition over geometries to identify a
dependency structure for the measurement sequences in order to achieve
determinism. Previously, three optimization methods have been proposed to
simplify 1WQC patterns which are called standardization, signal shifting and
Pauli simplification. These optimizations can be performed using measurement
calculus formalism by rewriting rules. However, maintaining and searching these
rules in the library can be complicated with respect to implementation.
Moreover, serial execution of these rules is time consuming due to executing
many ineffective commutation rules. To overcome this problem, in this paper, a
new scheme is proposed to perform optimization techniques on patterns with flow
or gflow only based on their geometries instead of using rewriting rules.
Furthermore, the proposed scheme obtains the maximally delayed gflow order for
geometries with flow. It is shown that the time complexity of the proposed
approach is improved over the previous ones.
",1,0,0,0,0,0
4122,4123,Structured Uncertainty Prediction Networks,"  This paper is the first work to propose a network to predict a structured
uncertainty distribution for a synthesized image. Previous approaches have been
mostly limited to predicting diagonal covariance matrices. Our novel model
learns to predict a full Gaussian covariance matrix for each reconstruction,
which permits efficient sampling and likelihood evaluation.
We demonstrate that our model can accurately reconstruct ground truth
correlated residual distributions for synthetic datasets and generate plausible
high frequency samples for real face images. We also illustrate the use of
these predicted covariances for structure preserving image denoising.
",0,0,0,1,0,0
1125,1126,Nondestructive testing of grating imperfections using grating-based X-ray phase-contrast imaging,"  We reported the usage of grating-based X-ray phase-contrast imaging in
nondestructive testing of grating imperfections. It was found that
electroplating flaws could be easily detected by conventional absorption
signal, and in particular, we observed that the grating defects resulting from
uneven ultraviolet exposure could be clearly discriminated with phase-contrast
signal. The experimental results demonstrate that grating-based X-ray
phase-contrast imaging, with a conventional low-brilliance X-ray source, a
large field of view and a reasonable compact setup, which simultaneously yields
phase- and attenuation-contrast signal of the sample, can be ready-to-use in
fast nondestructive testing of various imperfections in gratings and other
similar photoetching products.
",0,1,0,0,0,0
18917,18918,Community Detection with Colored Edges,"  In this paper, we prove a sharp limit on the community detection problem with
colored edges. We assume two equal-sized communities and there are $m$
different types of edges. If two vertices are in the same community, the
distribution of edges follows $p_i=\alpha_i\log{n}/n$ for $1\leq i \leq m$,
otherwise the distribution of edges is $q_i=\beta_i\log{n}/n$ for $1\leq i \leq
m$, where $\alpha_i$ and $\beta_i$ are positive constants and $n$ is the total
number of vertices. Under these assumptions, a fundamental limit on community
detection is characterized using the Hellinger distance between the two
distributions. If $\sum_{i=1}^{m} {(\sqrt{\alpha_i} - \sqrt{\beta_i})}^2 >2$,
then the community detection via maximum likelihood (ML) estimator is possible
with high probability. If $\sum_{i=1}^m {(\sqrt{\alpha_i} - \sqrt{\beta_i})}^2
< 2$, the probability that the ML estimator fails to detect the communities
does not go to zero.
",1,1,0,0,0,0
13540,13541,Learning Non-local Image Diffusion for Image Denoising,"  Image diffusion plays a fundamental role for the task of image denoising.
Recently proposed trainable nonlinear reaction diffusion (TNRD) model defines a
simple but very effective framework for image denoising. However, as the TNRD
model is a local model, the diffusion behavior of which is purely controlled by
information of local patches, it is prone to create artifacts in the homogenous
regions and over-smooth highly textured regions, especially in the case of
strong noise levels. Meanwhile, it is widely known that the non-local
self-similarity (NSS) prior stands as an effective image prior for image
denoising, which has been widely exploited in many non-local methods. In this
work, we are highly motivated to embed the NSS prior into the TNRD model to
tackle its weaknesses. In order to preserve the expected property that
end-to-end training is available, we exploit the NSS prior by a set of
non-local filters, and derive our proposed trainable non-local reaction
diffusion (TNLRD) model for image denoising. Together with the local filters
and influence functions, the non-local filters are learned by employing
loss-specific training. The experimental results show that the trained TNLRD
model produces visually plausible recovered images with more textures and less
artifacts, compared to its local versions. Moreover, the trained TNLRD model
can achieve strongly competitive performance to recent state-of-the-art image
denoising methods in terms of peak signal-to-noise ratio (PSNR) and structural
similarity index (SSIM).
",1,0,0,0,0,0
717,718,Information Retrieval and Recommendation System for Astronomical Observatories,"  We present a machine learning based information retrieval system for
astronomical observatories that tries to address user defined queries related
to an instrument. In the modern instrumentation scenario where heterogeneous
systems and talents are simultaneously at work, the ability to supply with the
right information helps speeding up the detector maintenance operations.
Enhancing the detector uptime leads to increased coincidence observation and
improves the likelihood for the detection of astrophysical signals. Besides,
such efforts will efficiently disseminate technical knowledge to a wider
audience and will help the ongoing efforts to build upcoming detectors like the
LIGO-India etc even at the design phase to foresee possible challenges. The
proposed method analyses existing documented efforts at the site to
intelligently group together related information to a query and to present it
on-line to the user. The user in response can further go into interesting links
and find already developed solutions or probable ways to address the present
situation optimally. A web application that incorporates the above idea has
been implemented and tested for LIGO Livingston, LIGO Hanford and Virgo
observatories.
",0,1,0,0,0,0
9828,9829,On a spiked model for large volatility matrix estimation from noisy high-frequency data,"  Recently, inference about high-dimensional integrated covariance matrices
(ICVs) based on noisy high-frequency data has emerged as a challenging problem.
In the literature, a pre-averaging estimator (PA-RCov) is proposed to deal with
the microstructure noise. Using the large-dimensional random matrix theory, it
has been established that the eigenvalue distribution of the PA-RCov matrix is
intimately linked to that of the ICV through the Marcenko-Pastur equation.
Consequently, the spectrum of the ICV can be inferred from that of the PA-RCov.
However, extensive data analyses demonstrate that the spectrum of the PA-RCov
is spiked, that is, a few large eigenvalues (spikes) stay away from the others
which form a rather continuous distribution with a density function (bulk).
Therefore, any inference on the ICVs must take into account this spiked
structure. As a methodological contribution, we propose a spiked model for the
ICVs where spikes can be inferred from those of the available PA-RCov matrices.
The consistency of the inference procedure is established and is checked by
extensive simulation studies. In addition, we apply our method to the real data
from the US and Hong Kong markets. It is found that our model clearly
outperforms the existing one in predicting the existence of spikes and in
mimicking the empirical PA-RCov matrices.
",0,0,0,1,0,0
10684,10685,The Capacity of Some Classes of Polyhedra,"  K. Borsuk in 1979, in the Topological Conference in Moscow, introduced the
concept of the capacity of a compactum. In this paper, we compute the capacity
of the product of two spheres of the same or different dimensions and the
capacity of lense spaces. Also, we present an upper bound for the capacity of a
$\mathbb{Z}_n$-complex, i.e., a connected finite 2-dimensional CW-complex with
finite cyclic fundamental group $\mathbb{Z}_n$.
",0,0,1,0,0,0
8457,8458,Gaussian Process Subset Scanning for Anomalous Pattern Detection in Non-iid Data,"  Identifying anomalous patterns in real-world data is essential for
understanding where, when, and how systems deviate from their expected
dynamics. Yet methods that separately consider the anomalousness of each
individual data point have low detection power for subtle, emerging
irregularities. Additionally, recent detection techniques based on subset
scanning make strong independence assumptions and suffer degraded performance
in correlated data. We introduce methods for identifying anomalous patterns in
non-iid data by combining Gaussian processes with novel log-likelihood ratio
statistic and subset scanning techniques. Our approaches are powerful,
interpretable, and can integrate information across multiple data streams. We
illustrate their performance on numeric simulations and three open source
spatiotemporal datasets of opioid overdose deaths, 311 calls, and storm
reports.
",0,0,0,1,0,0
13328,13329,Modeling Label Ambiguity for Neural List-Wise Learning to Rank,"  List-wise learning to rank methods are considered to be the state-of-the-art.
One of the major problems with these methods is that the ambiguous nature of
relevance labels in learning to rank data is ignored. Ambiguity of relevance
labels refers to the phenomenon that multiple documents may be assigned the
same relevance label for a given query, so that no preference order should be
learned for those documents. In this paper we propose a novel sampling
technique for computing a list-wise loss that can take into account this
ambiguity. We show the effectiveness of the proposed method by training a
3-layer deep neural network. We compare our new loss function to two strong
baselines: ListNet and ListMLE. We show that our method generalizes better and
significantly outperforms other methods on the validation and test sets.
",1,0,0,1,0,0
4757,4758,A Network of Networks Approach to Interconnected Power Grids,"  We present two different approaches to model power grids as interconnected
networks of networks. Both models are derived from a model for spatially
embedded mono-layer networks and are generalised to handle an arbitrary number
of network layers. The two approaches are distinguished by their use case. The
static glue stick construction model yields a multi-layer network from a
predefined layer interconnection scheme, i.e. different layers are attached
with transformer edges. It is especially suited to construct multi-layer power
grids with a specified number of nodes in and transformers between layers. We
contrast it with a genuine growth model which we label interconnected layer
growth model.
",0,1,0,0,0,0
5673,5674,Computation of optimal transport and related hedging problems via penalization and neural networks,"  This paper presents a widely applicable approach to solving (multi-marginal,
martingale) optimal transport and related problems via neural networks. The
core idea is to penalize the optimization problem in its dual formulation and
reduce it to a finite dimensional one which corresponds to optimizing a neural
network with smooth objective function. We present numerical examples from
optimal transport, martingale optimal transport, portfolio optimization under
uncertainty and generative adversarial networks that showcase the generality
and effectiveness of the approach.
",0,0,0,1,0,1
2583,2584,Universal Function Approximation by Deep Neural Nets with Bounded Width and ReLU Activations,"  This article concerns the expressive power of depth in neural nets with ReLU
activations and bounded width. We are particularly interested in the following
questions: what is the minimal width $w_{\text{min}}(d)$ so that ReLU nets of
width $w_{\text{min}}(d)$ (and arbitrary depth) can approximate any continuous
function on the unit cube $[0,1]^d$ aribitrarily well? For ReLU nets near this
minimal width, what can one say about the depth necessary to approximate a
given function? Our approach to this paper is based on the observation that,
due to the convexity of the ReLU activation, ReLU nets are particularly
well-suited for representing convex functions. In particular, we prove that
ReLU nets with width $d+1$ can approximate any continuous convex function of
$d$ variables arbitrarily well. These results then give quantitative depth
estimates for the rate of approximation of any continuous scalar function on
the $d$-dimensional cube $[0,1]^d$ by ReLU nets with width $d+3.$
",1,0,1,1,0,0
574,575,Language Modeling by Clustering with Word Embeddings for Text Readability Assessment,"  We present a clustering-based language model using word embeddings for text
readability prediction. Presumably, an Euclidean semantic space hypothesis
holds true for word embeddings whose training is done by observing word
co-occurrences. We argue that clustering with word embeddings in the metric
space should yield feature representations in a higher semantic space
appropriate for text regression. Also, by representing features in terms of
histograms, our approach can naturally address documents of varying lengths. An
empirical evaluation using the Common Core Standards corpus reveals that the
features formed on our clustering-based language model significantly improve
the previously known results for the same corpus in readability prediction. We
also evaluate the task of sentence matching based on semantic relatedness using
the Wiki-SimpleWiki corpus and find that our features lead to superior matching
performance.
",1,0,0,0,0,0
11061,11062,Backdoor Embedding in Convolutional Neural Network Models via Invisible Perturbation,"  Deep learning models have consistently outperformed traditional machine
learning models in various classification tasks, including image
classification. As such, they have become increasingly prevalent in many real
world applications including those where security is of great concern. Such
popularity, however, may attract attackers to exploit the vulnerabilities of
the deployed deep learning models and launch attacks against security-sensitive
applications. In this paper, we focus on a specific type of data poisoning
attack, which we refer to as a {\em backdoor injection attack}. The main goal
of the adversary performing such attack is to generate and inject a backdoor
into a deep learning model that can be triggered to recognize certain embedded
patterns with a target label of the attacker's choice. Additionally, a backdoor
injection attack should occur in a stealthy manner, without undermining the
efficacy of the victim model. Specifically, we propose two approaches for
generating a backdoor that is hardly perceptible yet effective in poisoning the
model. We consider two attack settings, with backdoor injection carried out
either before model training or during model updating. We carry out extensive
experimental evaluations under various assumptions on the adversary model, and
demonstrate that such attacks can be effective and achieve a high attack
success rate (above $90\%$) at a small cost of model accuracy loss (below
$1\%$) with a small injection rate (around $1\%$), even under the weakest
assumption wherein the adversary has no knowledge either of the original
training data or the classifier model.
",0,0,0,1,0,0
13022,13023,A General and Adaptive Robust Loss Function,"  We present a generalization of the Cauchy/Lorentzian, Geman-McClure,
Welsch/Leclerc, generalized Charbonnier, Charbonnier/pseudo-Huber/L1-L2, and L2
loss functions. By introducing robustness as a continous parameter, our loss
function allows algorithms built around robust loss minimization to be
generalized, which improves performance on basic vision tasks such as
registration and clustering. Interpreting our loss as the negative log of a
univariate density yields a general probability distribution that includes
normal and Cauchy distributions as special cases. This probabilistic
interpretation enables the training of neural networks in which the robustness
of the loss automatically adapts itself during training, which improves
performance on learning-based tasks such as generative image synthesis and
unsupervised monocular depth estimation, without requiring any manual parameter
tuning.
",1,0,0,1,0,0
14346,14347,Eulerian and Lagrangian solutions to the continuity and Euler equations with $L^1$ vorticity,"  In the first part of this paper we establish a uniqueness result for
continuity equations with velocity field whose derivative can be represented by
a singular integral operator of an $L^1$ function, extending the Lagrangian
theory in \cite{BouchutCrippa13}. The proof is based on a combination of a
stability estimate via optimal transport techniques developed in \cite{Seis16a}
and some tools from harmonic analysis introduced in \cite{BouchutCrippa13}. In
the second part of the paper, we address a question that arose in
\cite{FilhoMazzucatoNussenzveig06}, namely whether 2D Euler solutions obtained
via vanishing viscosity are renormalized (in the sense of DiPerna and Lions)
when the initial data has low integrability. We show that this is the case even
when the initial vorticity is only in~$L^1$, extending the proof for the $L^p$
case in \cite{CrippaSpirito15}.
",0,1,1,0,0,0
2540,2541,Schoenberg Representations and Gramian Matrices of Matérn Functions,"  We represent Matérn functions in terms of Schoenberg's integrals which
ensure the positive definiteness and prove the systems of translates of
Matérn functions form Riesz sequences in $L^2(\R^n)$ or Sobolev spaces. Our
approach is based on a new class of integral transforms that generalize Fourier
transforms for radial functions. We also consider inverse multi-quadrics and
obtain similar results.
",0,0,1,0,0,0
1546,1547,The neighborhood lattice for encoding partial correlations in a Hilbert space,"  Neighborhood regression has been a successful approach in graphical and
structural equation modeling, with applications to learning undirected and
directed graphical models. We extend these ideas by defining and studying an
algebraic structure called the neighborhood lattice based on a generalized
notion of neighborhood regression. We show that this algebraic structure has
the potential to provide an economic encoding of all conditional independence
statements in a Gaussian distribution (or conditional uncorrelatedness in
general), even in the cases where no graphical model exists that could
""perfectly"" encode all such statements. We study the computational complexity
of computing these structures and show that under a sparsity assumption, they
can be computed in polynomial time, even in the absence of the assumption of
perfectness to a graph. On the other hand, assuming perfectness, we show how
these neighborhood lattices may be ""graphically"" computed using the separation
properties of the so-called partial correlation graph. We also draw connections
with directed acyclic graphical models and Bayesian networks. We derive these
results using an abstract generalization of partial uncorrelatedness, called
partial orthogonality, which allows us to use algebraic properties of
projection operators on Hilbert spaces to significantly simplify and extend
existing ideas and arguments. Consequently, our results apply to a wide range
of random objects and data structures, such as random vectors, data matrices,
and functions.
",1,0,1,1,0,0
8591,8592,$HD(M\setminus L)>0.353$,"  The complement $M\setminus L$ of the Lagrange spectrum $L$ in the Markov
spectrum $M$ was studied by many authors (including Freiman, Berstein, Cusick
and Flahive). After their works, we disposed of a countable collection of
points in $M\setminus L$.
In this article, we describe the structure of $M\setminus L$ near a
non-isolated point $\alpha_{\infty}$ found by Freiman in 1973, and we use this
description to exhibit a concrete Cantor set $X$ whose Hausdorff dimension
coincides with the Hausdorff dimension of $M\setminus L$ near
$\alpha_{\infty}$.
A consequence of our results is the lower bound $HD(M\setminus L)>0.353$ on
the Hausdorff dimension $HD(M\setminus L)$ of $M\setminus L$. Another
by-product of our analysis is the explicit construction of new elements of
$M\setminus L$, including its largest known member $c\in M\setminus L$
(surpassing the former largest known number $\alpha_4\in M\setminus L$ obtained
by Cusick and Flahive in 1989).
",0,0,1,0,0,0
15068,15069,Odd-integer quantum Hall states and giant spin susceptibility in p-type few-layer WSe2,"  We fabricate high-mobility p-type few-layer WSe2 field-effect transistors and
surprisingly observe a series of quantum Hall (QH) states following an
unconventional sequence predominated by odd-integer states under a moderate
strength magnetic field. By tilting the magnetic field, we discover Landau
level (LL) crossing effects at ultra-low coincident angles, revealing that the
Zeeman energy is about three times as large as the cyclotron energy near the
valence band top at {\Gamma} valley. This result implies the significant roles
played by the exchange interactions in p-type few-layer WSe2, in which
itinerant or QH ferromagnetism likely occurs. Evidently, the {\Gamma} valley of
few-layer WSe2 offers a unique platform with unusually heavy hole-carriers and
a substantially enhanced g-factor for exploring strongly correlated phenomena.
",0,1,0,0,0,0
712,713,Optimal Input Placement in Lattice Graphs,"  The control of dynamical, networked systems continues to receive much
attention across the engineering and scientific research fields. Of particular
interest is the proper way to determine which nodes of the network should
receive external control inputs in order to effectively and efficiently control
portions of the network. Published methods to accomplish this task either find
a minimal set of driver nodes to guarantee controllability or a larger set of
driver nodes which optimizes some control metric. Here, we investigate the
control of lattice systems which provides analytical insight into the
relationship between network structure and controllability. First we derive a
closed form expression for the individual elements of the controllability
Gramian of infinite lattice systems. Second, we focus on nearest neighbor
lattices for which the distance between nodes appears in the expression for the
controllability Gramian. We show that common control energy metrics scale
exponentially with respect to the maximum distance between a driver node and a
target node.
",1,0,0,0,0,0
14537,14538,Efficient Online Timed Pattern Matching by Automata-Based Skipping,"  The timed pattern matching problem is an actively studied topic because of
its relevance in monitoring of real-time systems. There one is given a log $w$
and a specification $\mathcal{A}$ (given by a timed word and a timed automaton
in this paper), and one wishes to return the set of intervals for which the log
$w$, when restricted to the interval, satisfies the specification
$\mathcal{A}$. In our previous work we presented an efficient timed pattern
matching algorithm: it adopts a skipping mechanism inspired by the classic
Boyer--Moore (BM) string matching algorithm. In this work we tackle the problem
of online timed pattern matching, towards embedded applications where it is
vital to process a vast amount of incoming data in a timely manner.
Specifically, we start with the Franek-Jennings-Smyth (FJS) string matching
algorithm---a recent variant of the BM algorithm---and extend it to timed
pattern matching. Our experiments indicate the efficiency of our FJS-type
algorithm in online and offline timed pattern matching.
",1,0,0,0,0,0
1008,1009,The Generalized Cross Validation Filter,"  Generalized cross validation (GCV) is one of the most important approaches
used to estimate parameters in the context of inverse problems and
regularization techniques. A notable example is the determination of the
smoothness parameter in splines. When the data are generated by a state space
model, like in the spline case, efficient algorithms are available to evaluate
the GCV score with complexity that scales linearly in the data set size.
However, these methods are not amenable to on-line applications since they rely
on forward and backward recursions. Hence, if the objective has been evaluated
at time $t-1$ and new data arrive at time t, then O(t) operations are needed to
update the GCV score. In this paper we instead show that the update cost is
$O(1)$, thus paving the way to the on-line use of GCV. This result is obtained
by deriving the novel GCV filter which extends the classical Kalman filter
equations to efficiently propagate the GCV score over time. We also illustrate
applications of the new filter in the context of state estimation and on-line
regularized linear system identification.
",1,0,0,1,0,0
20858,20859,Asymptotic properties of a componentwise ARH(1) plug-in predictor,"  This paper presents new results on prediction of linear processes in function
spaces. The autoregressive Hilbertian process framework of order one (ARH(1)
process framework) is adopted. A componentwise estimator of the autocorrelation
operator is formulated, from the moment-based estimation of its diagonal
coefficients, with respect to the orthogonal eigenvectors of the
auto-covariance operator, which are assumed to be known. Mean-square
convergence to the theoretical autocorrelation operator, in the space of
Hilbert-Schmidt operators, is proved. Consistency then follows in that space.
For the associated ARH(1) plug-in predictor, mean absolute convergence to the
corresponding conditional expectation, in the considered Hilbert space, is
obtained. Hence, consistency in that space also holds. A simulation study is
undertaken to illustrate the finite-large sample behavior of the formulated
componentwise estimator and predictor. The performance of the presented
approach is compared with alternative approaches in the previous and current
ARH(1) framework literature, including the case of unknown eigenvectors.
",0,0,1,1,0,0
8124,8125,All the people around me: face discovery in egocentric photo-streams,"  Given an unconstrained stream of images captured by a wearable photo-camera
(2fpm), we propose an unsupervised bottom-up approach for automatic clustering
appearing faces into the individual identities present in these data. The
problem is challenging since images are acquired under real world conditions;
hence the visible appearance of the people in the images undergoes intensive
variations. Our proposed pipeline consists of first arranging the photo-stream
into events, later, localizing the appearance of multiple people in them, and
finally, grouping various appearances of the same person across different
events. Experimental results performed on a dataset acquired by wearing a
photo-camera during one month, demonstrate the effectiveness of the proposed
approach for the considered purpose.
",1,0,0,0,0,0
4570,4571,Models for the Displacement Calculus,"  The displacement calculus $\mathbf{D}$ is a conservative extension of the
Lambek calculus $\mathbf{L1}$ (with empty antecedents allowed in sequents).
$\mathbf{L1}$ can be said to be the logic of concatenation, while $\mathbf{D}$
can be said to be the logic of concatenation and intercalation. In many senses,
it can be claimed that $\mathbf{D}$ mimics $\mathbf{L1}$ in that the proof
theory, generative capacity and complexity of the former calculus are natural
extensions of the latter calculus. In this paper, we strengthen this claim. We
present the appropriate classes of models for $\mathbf{D}$ and prove some
completeness results; strikingly, we see that these results and proofs are
natural extensions of the corresponding ones for $\mathbf{L1}$.
",1,0,0,0,0,0
19977,19978,TensorQuant - A Simulation Toolbox for Deep Neural Network Quantization,"  Recent research implies that training and inference of deep neural networks
(DNN) can be computed with low precision numerical representations of the
training/test data, weights and gradients without a general loss in accuracy.
The benefit of such compact representations is twofold: they allow a
significant reduction of the communication bottleneck in distributed DNN
training and faster neural network implementations on hardware accelerators
like FPGAs. Several quantization methods have been proposed to map the original
32-bit floating point problem to low-bit representations. While most related
publications validate the proposed approach on a single DNN topology, it
appears to be evident, that the optimal choice of the quantization method and
number of coding bits is topology dependent. To this end, there is no general
theory available, which would allow users to derive the optimal quantization
during the design of a DNN topology. In this paper, we present a quantization
tool box for the TensorFlow framework. TensorQuant allows a transparent
quantization simulation of existing DNN topologies during training and
inference. TensorQuant supports generic quantization methods and allows
experimental evaluation of the impact of the quantization on single layers as
well as on the full topology. In a first series of experiments with
TensorQuant, we show an analysis of fix-point quantizations of popular CNN
topologies.
",1,0,0,1,0,0
3451,3452,The Motion of Small Bodies in Space-time,"  We consider the motion of small bodies in general relativity. The key result
captures a sense in which such bodies follow timelike geodesics (or, in the
case of charged bodies, Lorentz-force curves). This result clarifies the
relationship between approaches that model such bodies as distributions
supported on a curve, and those that employ smooth fields supported in small
neighborhoods of a curve. This result also applies to ""bodies"" constructed from
wave packets of Maxwell or Klein-Gordon fields. There follows a simple and
precise formulation of the optical limit for Maxwell fields.
",0,1,1,0,0,0
14508,14509,Reduced Modeling of Unknown Trajectories,"  This paper deals with model order reduction of parametrical dynamical
systems. We consider the specific setup where the distribution of the system's
trajectories is unknown but the following two sources of information are
available: \textit{(i)} some ""rough"" prior knowledge on the system's
realisations; \textit{(ii)} a set of ""incomplete"" observations of the system's
trajectories. We propose a Bayesian methodological framework to build
reduced-order models (ROMs) by exploiting these two sources of information. We
emphasise that complementing the prior knowledge with the collected data
provably enhances the knowledge of the distribution of the system's
trajectories. We then propose an implementation of the proposed methodology
based on Monte-Carlo methods. In this context, we show that standard ROM
learning techniques, such e.g. Proper Orthogonal Decomposition or Dynamic Mode
Decomposition, can be revisited and recast within the probabilistic framework
considered in this paper.~We illustrate the performance of the proposed
approach by numerical results obtained for a standard geophysical model.
",0,0,0,1,0,0
10659,10660,PROOF OF VALUE ALIENATION (PoVA) - a concept of a cryptocurrency issuance protocol,"  In this paper, we will describe a concept of a cryptocurrency issuance
protocol which supports digital currencies in a Proof-of-Work (< PoW >) like
manner. However, the methods assume alternative utilization of assets used for
cryptocurrency creation (rather than purchasing electricity necessary for <
mining >).
",0,0,0,0,0,1
16497,16498,On the choice of the low-dimensional domain for global optimization via random embeddings,"  The challenge of taking many variables into account in optimization problems
may be overcome under the hypothesis of low effective dimensionality. Then, the
search of solutions can be reduced to the random embedding of a low dimensional
space into the original one, resulting in a more manageable optimization
problem. Specifically, in the case of time consuming black-box functions and
when the budget of evaluations is severely limited, global optimization with
random embeddings appears as a sound alternative to random search. Yet, in the
case of box constraints on the native variables, defining suitable bounds on a
low dimensional domain appears to be complex. Indeed, a small search domain
does not guarantee to find a solution even under restrictive hypotheses about
the function, while a larger one may slow down convergence dramatically. Here
we tackle the issue of low-dimensional domain selection based on a detailed
study of the properties of the random embedding, giving insight on the
aforementioned difficulties. In particular, we describe a minimal
low-dimensional set in correspondence with the embedded search space. We
additionally show that an alternative equivalent embedding procedure yields
simultaneously a simpler definition of the low-dimensional minimal set and
better properties in practice. Finally, the performance and robustness gains of
the proposed enhancements for Bayesian optimization are illustrated on
numerical examples.
",0,0,1,1,0,0
9392,9393,Groupoid of morphisms of groupoids,"  In this paper we construct two groupoids from morphisms of groupoids, with
one from a categorical viewpoint and the other from a geometric viewpoint. We
show that for each pair of groupoids, the two kinds of groupoids of morphisms
are equivalent. Then we study the automorphism groupoid of a groupoid.
",0,0,1,0,0,0
2195,2196,Do triangle-free planar graphs have exponentially many 3-colorings?,"  Thomassen conjectured that triangle-free planar graphs have an exponential
number of $3$-colorings. We show this conjecture to be equivalent to the
following statement: there exists a positive real $\alpha$ such that whenever
$G$ is a planar graph and $A$ is a subset of its edges whose deletion makes $G$
triangle-free, there exists a subset $A'$ of $A$ of size at least $\alpha|A|$
such that $G-(A\setminus A')$ is $3$-colorable. This equivalence allows us to
study restricted situations, where we can prove the statement to be true.
",0,0,1,0,0,0
12913,12914,Tensor Regression Meets Gaussian Processes,"  Low-rank tensor regression, a new model class that learns high-order
correlation from data, has recently received considerable attention. At the
same time, Gaussian processes (GP) are well-studied machine learning models for
structure learning. In this paper, we demonstrate interesting connections
between the two, especially for multi-way data analysis. We show that low-rank
tensor regression is essentially learning a multi-linear kernel in Gaussian
processes, and the low-rank assumption translates to the constrained Bayesian
inference problem. We prove the oracle inequality and derive the average case
learning curve for the equivalent GP model. Our finding implies that low-rank
tensor regression, though empirically successful, is highly dependent on the
eigenvalues of covariance functions as well as variable correlations.
",1,0,0,1,0,0
7468,7469,Entire holomorphic curves into projective spaces intersecting a generic hypersurface of high degree,"  In this note, we establish the following Second Main Theorem type estimate
for every entire non-algebraically degenerate holomorphic curve
$f\colon\mathbb{C}\rightarrow\mathbb{P}^n(\mathbb{C})$, in present of a {\sl
generic} hypersuface $D\subset\mathbb{P}^n(\mathbb{C})$ of sufficiently high
degree $d\geq 15(5n+1)n^n$: \[ T_f(r) \leq \,N_f^{[1]}(r,D) + O\big(\log T_f(r)
+ \log r \big)\parallel, \] where $T_f(r)$ and $N_f^{[1]}(r,D)$ stand for the
order function and the $1$-truncated counting function in Nevanlinna theory.
This inequality quantifies recent results on the logarithmic Green--Griffiths
conjecture.
",0,0,1,0,0,0
9005,9006,Ultracold heteronuclear three-body systems: How diabaticity limits the universality of recombination into shallow dimers,"  The mass-imbalanced three-body recombination process that forms a shallow
dimer is shown to possess a rich Efimov-Stückelberg landscape, with
corresponding spectra that differ fundamentally from the homonuclear case. A
semi-analytical treatment of the three-body recombination predicts an unusual
spectra with intertwined resonance peaks and minima, and yields in-depth
insight into the behavior of the corresponding Efimov spectra. In particular,
the patterns of the Efimov-Stückelberg landscape are shown to depend
inherently on the degree of diabaticity of the three-body collisions, which
strongly affects the universality of the heteronuclear Efimov states.
",0,1,0,0,0,0
2430,2431,Focused Hierarchical RNNs for Conditional Sequence Processing,"  Recurrent Neural Networks (RNNs) with attention mechanisms have obtained
state-of-the-art results for many sequence processing tasks. Most of these
models use a simple form of encoder with attention that looks over the entire
sequence and assigns a weight to each token independently. We present a
mechanism for focusing RNN encoders for sequence modelling tasks which allows
them to attend to key parts of the input as needed. We formulate this using a
multi-layer conditional sequence encoder that reads in one token at a time and
makes a discrete decision on whether the token is relevant to the context or
question being asked. The discrete gating mechanism takes in the context
embedding and the current hidden state as inputs and controls information flow
into the layer above. We train it using policy gradient methods. We evaluate
this method on several types of tasks with different attributes. First, we
evaluate the method on synthetic tasks which allow us to evaluate the model for
its generalization ability and probe the behavior of the gates in more
controlled settings. We then evaluate this approach on large scale Question
Answering tasks including the challenging MS MARCO and SearchQA tasks. Our
models shows consistent improvements for both tasks over prior work and our
baselines. It has also shown to generalize significantly better on synthetic
tasks as compared to the baselines.
",0,0,0,1,0,0
11093,11094,Approximate and Stochastic Greedy Optimization,"  We consider two greedy algorithms for minimizing a convex function in a
bounded convex set: an algorithm by Jones [1992] and the Frank-Wolfe (FW)
algorithm. We first consider approximate versions of these algorithms. For
smooth convex functions, we give sufficient conditions for convergence, a
unified analysis for the well-known convergence rate of O(1/k) together with a
result showing that this rate is the best obtainable from the proof technique,
and an equivalence result for the two algorithms. We also consider approximate
stochastic greedy algorithms for minimizing expectations. We show that
replacing the full gradient by a single stochastic gradient can fail even on
smooth convex functions. We give a convergent approximate stochastic Jones
algorithm and a convergent approximate stochastic FW algorithm for smooth
convex functions. In addition, we give a convergent approximate stochastic FW
algorithm for nonsmooth convex functions. Convergence rates for these
algorithms are given and proved.
",1,0,1,0,0,0
11257,11258,The topology on Berkovich affine lines over complete valuation rings,"  In this article, we give a full description of the topology of the one
dimensional affine analytic space $\mathbb{A}_R^1$ over a complete valuation
ring $R$ (i.e. a valuation ring with ""real valued valuation"" which is complete
under the induced metric), when its field of fractions $K$ is algebraically
closed. In particular, we show that $\mathbb{A}_R^1$ is both connected and
locally path connected. Furthermore, $\mathbb{A}_R^1$ is the completion of
$K\times (1,\infty)$ under a canonical uniform structure. As an application, we
describe the Berkovich spectrum $\mathfrak{M}(\mathbb{Z}_p[G])$ of the Banach
group ring $\mathbb{Z}_p[G]$ of a cyclic $p$-group $G$ over the ring
$\mathbb{Z}_p$ of $p$-adic integers.
",0,0,1,0,0,0
7326,7327,Gaussian Graphical Models: An Algebraic and Geometric Perspective,"  Gaussian graphical models are used throughout the natural sciences, social
sciences, and economics to model the statistical relationships between
variables of interest in the form of a graph. We here provide a pedagogic
introduction to Gaussian graphical models and review recent results on maximum
likelihood estimation for such models. Throughout, we highlight the rich
algebraic and geometric properties of Gaussian graphical models and explain how
these properties relate to convex optimization and ultimately result in
insights on the existence of the maximum likelihood estimator (MLE) and
algorithms for computing the MLE.
",0,0,1,1,0,0
8108,8109,Remarks to the article: New Light on the Invention of the Achromatic Telescope Objective,"  The article analysis was carried out within the confines of the replication
project of the telescope, which was used by Mikhail Lomonosov at observation
the transit of Venus in 1761. At that time he discovered the Venusian
atmosphere. It is known that Lomonosov used Dollond 4.5 feet long achromatic
telescope. The investigation revealed significant faults in the description of
the approximation method, which most likely was used by J. Dollond & Son during
manufacturing of the early achromatic lenses.
",0,1,0,0,0,0
14686,14687,Oncilla robot: a versatile open-source quadruped research robot with compliant pantograph legs,"  We present Oncilla robot, a novel mobile, quadruped legged locomotion
machine. This large-cat sized, 5.1 robot is one of a kind of a recent,
bioinspired legged robot class designed with the capability of model-free
locomotion control. Animal legged locomotion in rough terrain is clearly shaped
by sensor feedback systems. Results with Oncilla robot show that agile and
versatile locomotion is possible without sensory signals to some extend, and
tracking becomes robust when feedback control is added (Ajaoolleian 2015). By
incorporating mechanical and control blueprints inspired from animals, and by
observing the resulting robot locomotion characteristics, we aim to understand
the contribution of individual components. Legged robots have a wide mechanical
and control design parameter space, and a unique potential as research tools to
investigate principles of biomechanics and legged locomotion control. But the
hardware and controller design can be a steep initial hurdle for academic
research. To facilitate the easy start and development of legged robots,
Oncilla-robot's blueprints are available through open-source. [...]
",1,0,0,0,0,0
8138,8139,Unsupervised Body Part Regression via Spatially Self-ordering Convolutional Neural Networks,"  Automatic body part recognition for CT slices can benefit various medical
image applications. Recent deep learning methods demonstrate promising
performance, with the requirement of large amounts of labeled images for
training. The intrinsic structural or superior-inferior slice ordering
information in CT volumes is not fully exploited. In this paper, we propose a
convolutional neural network (CNN) based Unsupervised Body part Regression
(UBR) algorithm to address this problem. A novel unsupervised learning method
and two inter-sample CNN loss functions are presented. Distinct from previous
work, UBR builds a coordinate system for the human body and outputs a
continuous score for each axial slice, representing the normalized position of
the body part in the slice. The training process of UBR resembles a
self-organization process: slice scores are learned from inter-slice
relationships. The training samples are unlabeled CT volumes that are abundant,
thus no extra annotation effort is needed. UBR is simple, fast, and accurate.
Quantitative and qualitative experiments validate its effectiveness. In
addition, we show two applications of UBR in network initialization and anomaly
detection.
",1,0,0,0,0,0
4703,4704,Optimally Gathering Two Robots,"  We present an algorithm that ensures in finite time the gathering of two
robots in the non-rigid ASYNC model. To circumvent established impossibility
results, we assume robots are equipped with 2-colors lights and are able to
measure distances between one another. Aside from its light, a robot has no
memory of its past actions, and its protocol is deterministic. Since, in the
same model, gathering is impossible when lights have a single color, our
solution is optimal with respect to the number of used colors.
",1,0,0,0,0,0
2879,2880,Is One Hyperparameter Optimizer Enough?,"  Hyperparameter tuning is the black art of automatically finding a good
combination of control parameters for a data miner. While widely applied in
empirical Software Engineering, there has not been much discussion on which
hyperparameter tuner is best for software analytics. To address this gap in the
literature, this paper applied a range of hyperparameter optimizers (grid
search, random search, differential evolution, and Bayesian optimization) to
defect prediction problem. Surprisingly, no hyperparameter optimizer was
observed to be `best' and, for one of the two evaluation measures studied here
(F-measure), hyperparameter optimization, in 50\% cases, was no better than
using default configurations.
We conclude that hyperparameter optimization is more nuanced than previously
believed. While such optimization can certainly lead to large improvements in
the performance of classifiers used in software analytics, it remains to be
seen which specific optimizers should be applied to a new dataset.
",1,0,0,0,0,0
9677,9678,The Complexity of Factors of Multivariate Polynomials,"  The existence of string functions, which are not polynomial time computable,
but whose graph is checkable in polynomial time, is a basic assumption in
cryptography. We prove that in the framework of algebraic complexity, there are
no such families of polynomial functions of polynomially bounded degree over
fields of characteristic zero. The proof relies on a polynomial upper bound on
the approximative complexity of a factor g of a polynomial f in terms of the
(approximative) complexity of f and the degree of the factor g. This extends a
result by Kaltofen (STOC 1986). The concept of approximative complexity allows
to cope with the case that a factor has an exponential multiplicity, by using a
perturbation argument. Our result extends to randomized (two-sided error)
decision complexity.
",1,0,0,0,0,0
15826,15827,"The Theory is Predictive, but is it Complete? An Application to Human Perception of Randomness","  When we test a theory using data, it is common to focus on correctness: do
the predictions of the theory match what we see in the data? But we also care
about completeness: how much of the predictable variation in the data is
captured by the theory? This question is difficult to answer, because in
general we do not know how much ""predictable variation"" there is in the
problem. In this paper, we consider approaches motivated by machine learning
algorithms as a means of constructing a benchmark for the best attainable level
of prediction.
We illustrate our methods on the task of predicting human-generated random
sequences. Relative to an atheoretical machine learning algorithm benchmark, we
find that existing behavioral models explain roughly 15 percent of the
predictable variation in this problem. This fraction is robust across several
variations on the problem. We also consider a version of this approach for
analyzing field data from domains in which human perception and generation of
randomness has been used as a conceptual framework; these include sequential
decision-making and repeated zero-sum games. In these domains, our framework
for testing the completeness of theories provides a way of assessing their
effectiveness over different contexts; we find that despite some differences,
the existing theories are fairly stable across our field domains in their
performance relative to the benchmark. Overall, our results indicate that (i)
there is a significant amount of structure in this problem that existing models
have yet to capture and (ii) there are rich domains in which machine learning
may provide a viable approach to testing completeness.
",1,0,0,1,0,0
4196,4197,"Inflationary magneto-(non)genesis, increasing kinetic couplings, and the strong coupling problem","  We study the generation of magnetic fields during inflation making use of a
coupling of the inflaton and moduli fields to electromagnetism via the photon
kinetic term, and assuming that the coupling is an increasing function of time.
We demonstrate that the strong coupling problem of inflationary magnetogenesis
can be avoided by incorporating the destabilization of moduli fields after
inflation. The magnetic field always dominates over the electric one, and thus
the severe constraints on the latter from backreaction, which are the demanding
obstacles in the case of a decreasing coupling function, do not apply to the
current scenario. However, we show that this loophole to the strong coupling
problem comes at a price: the normalization of the amplitude of magnetic fields
is determined by this coupling term and is therefore suppressed by a large
factor after the moduli destabilization completes. From this we conclude that
there is no self-consistent and generic realization of primordial
magnetogenesis producing scale-invariant fields in the case of an increasing
kinetic coupling.
",0,1,0,0,0,0
12427,12428,Automatic Error Analysis of Human Motor Performance for Interactive Coaching in Virtual Reality,"  In the context of fitness coaching or for rehabilitation purposes, the motor
actions of a human participant must be observed and analyzed for errors in
order to provide effective feedback. This task is normally carried out by human
coaches, and it needs to be solved automatically in technical applications that
are to provide automatic coaching (e.g. training environments in VR). However,
most coaching systems only provide coarse information on movement quality, such
as a scalar value per body part that describes the overall deviation from the
correct movement. Further, they are often limited to static body postures or
rather simple movements of single body parts. While there are many approaches
to distinguish between different types of movements (e.g., between walking and
jumping), the detection of more subtle errors in a motor performance is less
investigated. We propose a novel approach to classify errors in sports or
rehabilitation exercises such that feedback can be delivered in a rapid and
detailed manner: Homogeneous sub-sequences of exercises are first temporally
aligned via Dynamic Time Warping. Next, we extract a feature vector from the
aligned sequences, which serves as a basis for feature selection using Random
Forests. The selected features are used as input for Support Vector Machines,
which finally classify the movement errors. We compare our algorithm to a well
established state-of-the-art approach in time series classification, 1-Nearest
Neighbor combined with Dynamic Time Warping, and show our algorithm's
superiority regarding classification quality as well as computational cost.
",1,0,0,0,0,0
4296,4297,Ground-state properties of unitary bosons: from clusters to matter,"  The properties of cold Bose gases at unitarity have been extensively
investigated in the last few years both theoretically and experimentally. In
this paper we use a family of interactions tuned to two-body unitarity and very
weak three-body binding to demonstrate the universal properties of both
clusters and matter. We determine the universal properties of finite clusters
up to 60 particles and, for the first time, explicitly demonstrate the
saturation of energy and density with particle number and compare with bulk
properties. At saturation in the bulk we determine the energy, density, two-
and three-body contacts and the condensate fraction. We find that uniform
matter is more bound than three-body clusters by nearly two orders of
magnitude, the two-body contact is very large in absolute terms, and yet the
condensate fraction is also very large, greater than 90%. Equilibrium
properties of these systems may be experimentally accessible through rapid
quenching of weakly-interacting boson superfluids.
",0,1,0,0,0,0
2485,2486,Flexible Attributed Network Embedding,"  Network embedding aims to find a way to encode network by learning an
embedding vector for each node in the network. The network often has property
information which is highly informative with respect to the node's position and
role in the network. Most network embedding methods fail to utilize this
information during network representation learning. In this paper, we propose a
novel framework, FANE, to integrate structure and property information in the
network embedding process. In FANE, we design a network to unify heterogeneity
of the two information sources, and define a new random walking strategy to
leverage property information and make the two information compensate. FANE is
conceptually simple and empirically powerful. It improves over the
state-of-the-art methods on Cora dataset classification task by over 5%, more
than 10% on WebKB dataset classification task. Experiments also show that the
results improve more than the state-of-the-art methods as increasing training
size. Moreover, qualitative visualization show that our framework is helpful in
network property information exploration. In all, we present a new way for
efficiently learning state-of-the-art task-independent representations in
complex attributed networks. The source code and datasets of this paper can be
obtained from this https URL.
",1,0,0,0,0,0
20573,20574,On 2-Verma modules for quantum $\mathfrak{sl}_2$,"  In this paper we study the superalgebra $A_n$, introduced by the authors in
previous work on categorification of Verma modules for quantum
$\mathfrak{sl}_2$. The superalgebra $A_n$ is akin to the nilHecke algebra, and
shares similar properties. In particular, we prove a uniqueness result about
2-Verma modules on $\Bbbk$-linear 2-categories.
",0,0,1,0,0,0
16322,16323,Variational Community Partition with Novel Network Structure Centrality Prior,"  In this paper, we proposed a novel two-stage optimization method for network
community partition, which is based on inherent network structure information.
The introduced optimization approach utilizes the new network centrality
measure of both links and vertices to construct the key affinity description of
the given network, where the direct similarities between graph nodes or nodal
features are not available to obtain the classical affinity matrix. Indeed,
such calculated network centrality information presents the essential structure
of network, hence, the proper measure for detecting network communities, which
also introduces a `confidence' criterion for referencing new labeled benchmark
nodes. For the resulted challenging combinatorial optimization problem of graph
clustering, the proposed optimization method iteratively employs an efficient
convex optimization algorithm which is developed based under a new variational
perspective of primal and dual. Experiments over both artificial and real-world
network datasets demonstrate that the proposed optimization strategy of
community detection significantly improves result accuracy and outperforms the
state-of-the-art algorithms in terms of accuracy and reliability.
",1,0,0,0,0,0
10771,10772,Zero-Inflated Autoregressive Conditional Duration Model for Discrete Trade Durations with Excessive Zeros,"  In finance, durations between successive transactions are usually modeled by
the autoregressive conditional duration model based on a continuous
distribution omitting frequent zero values. Zero durations can be caused by
either split transactions or independent transactions. We propose a discrete
model allowing for excessive zero values based on the zero-inflated negative
binomial distribution with score dynamics. We establish the invertibility of
the score filter. Additionally, we derive sufficient conditions for the
consistency and asymptotic normality of the maximum likelihood of the model
parameters. In an empirical study of DJIA stocks, we find that split
transactions cause on average 63% of zero values. Furthermore, the loss of
decimal places in the proposed model is less severe than incorrect treatment of
zero values in continuous models.
",0,0,0,0,0,1
14031,14032,Discovering objects and their relations from entangled scene representations,"  Our world can be succinctly and compactly described as structured scenes of
objects and relations. A typical room, for example, contains salient objects
such as tables, chairs and books, and these objects typically relate to each
other by their underlying causes and semantics. This gives rise to correlated
features, such as position, function and shape. Humans exploit knowledge of
objects and their relations for learning a wide spectrum of tasks, and more
generally when learning the structure underlying observed data. In this work,
we introduce relation networks (RNs) - a general purpose neural network
architecture for object-relation reasoning. We show that RNs are capable of
learning object relations from scene description data. Furthermore, we show
that RNs can act as a bottleneck that induces the factorization of objects from
entangled scene description inputs, and from distributed deep representations
of scene images provided by a variational autoencoder. The model can also be
used in conjunction with differentiable memory mechanisms for implicit relation
discovery in one-shot learning tasks. Our results suggest that relation
networks are a potentially powerful architecture for solving a variety of
problems that require object relation reasoning.
",1,0,0,0,0,0
1665,1666,Topological and non inertial effects on the interbank light absorption,"  In this work, we investigate the combined influence of the nontrivial
topology introduced by a disclination and non inertial effects due to rotation,
in the energy levels and the wave functions of a noninteracting electron gas
confined to a two-dimensional pseudoharmonic quantum dot, under the influence
of an external uniform magnetic field. The exact solutions for energy
eigenvalues and wave functions are computed as functions of the applied
magnetic field strength, the disclination topological charge, magnetic quantum
number and the rotation speed of the sample. We investigate the modifications
on the light interband absorption coefficient and absorption threshold
frequency. We observe novel features in the system, including a range of
magnetic field without corresponding absorption phenomena, which is due to a
tripartite term of the Hamiltonian, involving magnetic field, the topological
charge of the defect and the rotation frequency.
",0,1,0,0,0,0
9344,9345,Third Harmonic THz Generation from Graphene in a Parallel-Plate Waveguide,"  Graphene as a zero-bandgap two-dimensional semiconductor with a linear
electron band dispersion near the Dirac points has the potential to exhibit
very interesting nonlinear optical properties. In particular, third harmonic
generation of terahertz radiation should occur due to the nonlinear
relationship between the crystal momentum and the current density. In this
work, we investigate the terahertz nonlinear response of graphene inside a
parallel-plate waveguide. We optimize the plate separation and Fermi energy of
the graphene to maximize third harmonic generation, by maximizing the nonlinear
interaction while minimizing the loss and phase mismatch. The results obtained
show an increase by more than a factor of 100 in the power efficiency relative
to a normal-incidence configuration for a 2 terahertz incident field.
",0,1,0,0,0,0
16102,16103,Computable Operations on Compact Subsets of Metric Spaces with Applications to Fréchet Distance and Shape Optimization,"  We extend the Theory of Computation on real numbers, continuous real
functions, and bounded closed Euclidean subsets, to compact metric spaces
$(X,d)$: thereby generically including computational and optimization problems
over higher types, such as the compact 'hyper' spaces of (i) nonempty closed
subsets of $X$ w.r.t. Hausdorff metric, and of (ii) equicontinuous functions on
$X$. The thus obtained Cartesian closure is shown to exhibit the same
structural properties as in the Euclidean case, particularly regarding function
pre/image. This allows us to assert the computability of (iii) Fréchet
Distances between curves and between loops, as well as of (iv)
constrained/Shape Optimization.
",1,0,1,0,0,0
2105,2106,Theory of ground states for classical Heisenberg spin systems I,"  We formulate part I of a rigorous theory of ground states for classical,
finite, Heisenberg spin systems. The main result is that all ground states can
be constructed from the eigenvectors of a real, symmetric matrix with entries
comprising the coupling constants of the spin system as well as certain
Lagrange parameters. The eigenvectors correspond to the unique maximum of the
minimal eigenvalue considered as a function of the Lagrange parameters.
However, there are rare cases where all ground states obtained in this way have
unphysical dimensions $M>3$ and the theory would have to be extended. Further
results concern the degree of additional degeneracy, additional to the trivial
degeneracy of ground states due to rotations or reflections. The theory is
illustrated by a couple of elementary examples.
",0,1,1,0,0,0
19976,19977,NoScope: Optimizing Neural Network Queries over Video at Scale,"  Recent advances in computer vision-in the form of deep neural networks-have
made it possible to query increasing volumes of video data with high accuracy.
However, neural network inference is computationally expensive at scale:
applying a state-of-the-art object detector in real time (i.e., 30+ frames per
second) to a single video requires a $4000 GPU. In response, we present
NoScope, a system for querying videos that can reduce the cost of neural
network video analysis by up to three orders of magnitude via
inference-optimized model search. Given a target video, object to detect, and
reference neural network, NoScope automatically searches for and trains a
sequence, or cascade, of models that preserves the accuracy of the reference
network but is specialized to the target video and are therefore far less
computationally expensive. NoScope cascades two types of models: specialized
models that forego the full generality of the reference model but faithfully
mimic its behavior for the target video and object; and difference detectors
that highlight temporal differences across frames. We show that the optimal
cascade architecture differs across videos and objects, so NoScope uses an
efficient cost-based optimizer to search across models and cascades. With this
approach, NoScope achieves two to three order of magnitude speed-ups
(265-15,500x real-time) on binary classification tasks over fixed-angle webcam
and surveillance video while maintaining accuracy within 1-5% of
state-of-the-art neural networks.
",1,0,0,0,0,0
17953,17954,Integrability conditions for Compound Random Measures,"  Compound random measures (CoRM's) are a flexible and tractable framework for
vectors of completely random measure. In this paper, we provide conditions to
guarantee the existence of a CoRM. Furthermore, we prove some interesting
properties of CoRM's when exponential scores and regularly varying Lévy
intensities are considered.
",0,0,1,1,0,0
10084,10085,Maximum entropy and population heterogeneity in continuous cell cultures,"  Continuous cultures of mammalian cells are complex systems displaying
hallmark phenomena of nonlinear dynamics, such as multi-stability, hysteresis,
as well as sharp transitions between different metabolic states. In this
context mathematical models may suggest control strategies to steer the system
towards desired states. Although even clonal populations are known to exhibit
cell-to-cell variability, most of the currently studied models assume that the
population is homogeneous. To overcome this limitation, we use the maximum
entropy principle to model the phenotypic distribution of cells in a chemostat
as a function of the dilution rate. We consider the coupling between cell
metabolism and extracellular variables describing the state of the bioreactor
and take into account the impact of toxic byproduct accumulation on cell
viability. We present a formal solution for the stationary state of the
chemostat and show how to apply it in two examples. First, a simplified model
of cell metabolism where the exact solution is tractable, and then a
genome-scale metabolic network of the Chinese hamster ovary (CHO) cell line.
Along the way we discuss several consequences of heterogeneity, such as:
qualitative changes in the dynamical landscape of the system, increasing
concentrations of byproducts that vanish in the homogeneous case, and larger
population sizes.
",0,0,0,0,1,0
5544,5545,Deep Learning Based Large-Scale Automatic Satellite Crosswalk Classification,"  High-resolution satellite imagery have been increasingly used on remote
sensing classification problems. One of the main factors is the availability of
this kind of data. Even though, very little effort has been placed on the zebra
crossing classification problem. In this letter, crowdsourcing systems are
exploited in order to enable the automatic acquisition and annotation of a
large-scale satellite imagery database for crosswalks related tasks. Then, this
dataset is used to train deep-learning-based models in order to accurately
classify satellite images that contains or not zebra crossings. A novel dataset
with more than 240,000 images from 3 continents, 9 countries and more than 20
cities was used in the experiments. Experimental results showed that freely
available crowdsourcing data can be used to accurately (97.11%) train robust
models to perform crosswalk classification on a global scale.
",1,0,0,1,0,0
8514,8515,Adsorption and desorption of hydrogen at nonpolar GaN(1-100) surfaces: Kinetics and impact on surface vibrational and electronic properties,"  The adsorption of hydrogen at nonpolar GaN(1-100) surfaces and its impact on
the electronic and vibrational properties is investigated using surface
electron spectroscopy in combination with density functional theory (DFT)
calculations. For the surface mediated dissociation of H2 and the subsequent
adsorption of H, an energy barrier of 0.55 eV has to be overcome. The
calculated kinetic surface phase diagram indicates that the reaction is
kinetically hindered at low pressures and low temperatures. At higher
temperatures ab-initio thermodynamics show, that the H-free surface is
energetically favored. To validate these theoretical predictions experiments at
room temperature and under ultrahigh vacuum conditions were performed. They
reveal that molecular hydrogen does not dissociatively adsorb at the GaN(1-100)
surface. Only activated atomic hydrogen atoms attach to the surface. At
temperatures above 820 K, the attached hydrogen gets desorbed. The adsorbed
hydrogen atoms saturate the dangling bonds of the gallium and nitrogen surface
atoms and result in an inversion of the Ga-N surface dimer buckling. The
signatures of the Ga-H and N-H vibrational modes on the H-covered surface have
experimentally been identified and are in good agreement with the DFT
calculations of the surface phonon modes. Both theory and experiment show that
H adsorption results in a removal of occupied and unoccupied intragap electron
states of the clean GaN(1-100) surface and a reduction of the surface upward
band bending by 0.4 eV. The latter mechanism largely reduces surface electron
depletion.
",0,1,0,0,0,0
3322,3323,A free energy landscape of the capture of CO2 by frustrated Lewis pairs,"  Frustrated Lewis pairs (FLPs) are known for its ability to capture CO2.
Although many FLPs have been reported experimentally and several theoretical
studies have been carried out to address the reaction mechanism, the individual
roles of Lewis acids and bases of FLP in the capture of CO2 is still unclear.
In this study, we employed density functional theory (DFT) based metadynamics
simulations to investigate the complete path for the capture of CO2 by
tBu3P/B(C6F5)3 pair, and to understand the role of the Lewis acid and base.
Interestingly, we have found out that the Lewis acids play more important role
than Lewis bases. Specifically, the Lewis acids are crucial for catalytical
properties and are responsible for both kinetic and thermodynamics control. The
Lewis bases, however, have less impact on the catalytic performance and are
mainly responsible for the formation of FLP systems. Based on these findings,
we propose a thumb of rule for the future synthesis of FLP-based catalyst for
the utilization of CO2.
",0,1,0,0,0,0
14081,14082,Closed-form approximations in derivatives pricing: The Kristensen-Mele approach,"  Kristensen and Mele (2011) developed a new approach to obtain closed-form
approximations to continuous-time derivatives pricing models. The approach uses
a power series expansion of the pricing bias between an intractable model and
some known auxiliary model. Since the resulting approximation formula has
closed-form it is straightforward to obtain approximations of greeks. In this
thesis I will introduce Kristensen and Mele's methods and apply it to a variety
of stochastic volatility models of European style options as well as a model
for commodity futures. The focus of this thesis is the effect of different
model choices and different model parameter values on the numerical stability
of Kristensen and Mele's approximation.
",0,0,0,0,0,1
17789,17790,Transaction Support over Redis: An Overview,"  This document outlines the approach to supporting cross-node transactions
over a Redis cluster.
",1,0,0,0,0,0
2002,2003,"The role of industry, occupation, and location specific knowledge in the survival of new firms","  How do regions acquire the knowledge they need to diversify their economic
activities? How does the migration of workers among firms and industries
contribute to the diffusion of that knowledge? Here we measure the industry,
occupation, and location-specific knowledge carried by workers from one
establishment to the next using a dataset summarizing the individual work
history for an entire country. We study pioneer firms--firms operating in an
industry that was not present in a region--because the success of pioneers is
the basic unit of regional economic diversification. We find that the growth
and survival of pioneers increase significantly when their first hires are
workers with experience in a related industry, and with work experience in the
same location, but not with past experience in a related occupation. We compare
these results with new firms that are not pioneers and find that
industry-specific knowledge is significantly more important for pioneer than
non-pioneer firms. To address endogeneity we use Bartik instruments, which
leverage national fluctuations in the demand for an activity as shocks for
local labor supply. The instrumental variable estimates support the finding
that industry-related knowledge is a predictor of the survival and growth of
pioneer firms. These findings expand our understanding of the micro-mechanisms
underlying regional economic diversification events.
",0,0,0,0,0,1
16176,16177,Optimal design with EGM approach in conjugate natural convection with surface radiation in a two-dimensional enclosure,"  Analysis of conjugate natural convection with surface radiation in a
two-dimensional enclosure is carried out in order to search the optimal
location of the heat source with entropy generation minimization (EGM) approach
and conventional heat transfer parameters. The air as an incompressible fluid
and transparent media is considered the fluid filling the enclosure with the
steady and laminar regime. The enclosure internal surfaces are also gray,
opaque and diffuse. The governing equations with stream function and vorticity
formulation are solved using finite difference approach. Results include the
effect of Rayleigh number and emissivity on the dimensionless average rate of
entropy generation and its optimum location. The optimum location search with
conventional heat transfer parameters including maximum temperature and Nusselt
numbers are also examined.
",0,1,0,0,0,0
12272,12273,Information Storage and Retrieval using Macromolecules as Storage Media,"  To store information at extremely high-density and data-rate, we propose to
adapt, integrate, and extend the techniques developed by chemists and molecular
biologists for the purpose of manipulating biological and other macromolecules.
In principle, volumetric densities in excess of 10^21 bits/cm^3 can be achieved
when individual molecules having dimensions below a nanometer or so are used to
encode the 0's and 1's of a binary string of data. In practice, however, given
the limitations of electron-beam lithography, thin film deposition and
patterning technologies, molecular manipulation in submicron dimensions, etc.,
we believe that volumetric storage densities on the order of 10^16 bits/cm^3
(i.e., petabytes per cubic centimeter) should be readily attainable, leaving
plenty of room for future growth. The unique feature of the proposed new
approach is its focus on the feasibility of storing bits of information in
individual molecules, each only a few angstroms in size.
",1,1,0,0,0,0
1425,1426,A simplicial decomposition framework for large scale convex quadratic programming,"  In this paper, we analyze in depth a simplicial decomposition like
algorithmic framework for large scale convex quadratic programming. In
particular, we first propose two tailored strategies for handling the master
problem. Then, we describe a few techniques for speeding up the solution of the
pricing problem. We report extensive numerical experiments on both real
portfolio optimization and general quadratic programming problems, showing the
efficiency and robustness of the method when compared to Cplex.
",0,0,1,0,0,0
18033,18034,Dynamic Mortality Risk Predictions in Pediatric Critical Care Using Recurrent Neural Networks,"  Viewing the trajectory of a patient as a dynamical system, a recurrent neural
network was developed to learn the course of patient encounters in the
Pediatric Intensive Care Unit (PICU) of a major tertiary care center. Data
extracted from Electronic Medical Records (EMR) of about 12000 patients who
were admitted to the PICU over a period of more than 10 years were leveraged.
The RNN model ingests a sequence of measurements which include physiologic
observations, laboratory results, administered drugs and interventions, and
generates temporally dynamic predictions for in-ICU mortality at user-specified
times. The RNN's ICU mortality predictions offer significant improvements over
those from two clinically-used scores and static machine learning algorithms.
",1,0,1,1,0,0
13029,13030,Computing LPMLN Using ASP and MLN Solvers,"  LPMLN is a recent addition to probabilistic logic programming languages. Its
main idea is to overcome the rigid nature of the stable model semantics by
assigning a weight to each rule in a way similar to Markov Logic is defined. We
present two implementations of LPMLN, $\text{LPMLN2ASP}$ and
$\text{LPMLN2MLN}$. System $\text{LPMLN2ASP}$ translates LPMLN programs into
the input language of answer set solver $\text{CLINGO}$, and using weak
constraints and stable model enumeration, it can compute most probable stable
models as well as exact conditional and marginal probabilities. System
$\text{LPMLN2MLN}$ translates LPMLN programs into the input language of Markov
Logic solvers, such as $\text{ALCHEMY}$, $\text{TUFFY}$, and $\text{ROCKIT}$,
and allows for performing approximate probabilistic inference on LPMLN
programs. We also demonstrate the usefulness of the LPMLN systems for computing
other languages, such as ProbLog and Pearl's Causal Models, that are shown to
be translatable into LPMLN. (Under consideration for acceptance in TPLP)
",1,0,0,0,0,0
7403,7404,Computational Aspects of Optimal Strategic Network Diffusion,"  The diffusion of information has been widely modeled as stochastic diffusion
processes on networks. Alshamsi et al. (2018) proposed a model of strategic
diffusion in networks of related activities. In this work we investigate the
computational aspects of finding the optimal strategy of strategic diffusion.
We prove that finding an optimal solution to the problem is NP-complete in a
general case. To overcome this computational difficulty, we present an
algorithm to compute an optimal solution based on a dynamic programming
technique. We also show that the problem is fixed parameter-tractable when
parametrized by the product of the treewidth and maximum degree. We analyze the
possibility of developing an efficient approximation algorithm and show that
two heuristic algorithms proposed so far cannot have better than a logarithmic
approximation guarantee. Finally, we prove that the problem does not admit
better than a logarithmic approximation, unless P=NP.
",1,0,0,0,0,0
5179,5180,Univalent Foundations and the UniMath Library,"  We give a concise presentation of the Univalent Foundations of mathematics
outlining the main ideas, followed by a discussion of the UniMath library of
formalized mathematics implementing the ideas of the Univalent Foundations
(section 1), and the challenges one faces in attempting to design a large-scale
library of formalized mathematics (section 2). This leads us to a general
discussion about the links between architecture and mathematics where a meeting
of minds is revealed between architects and mathematicians (section 3). On the
way our odyssey from the foundations to the ""horizon"" of mathematics will lead
us to meet the mathematicians David Hilbert and Nicolas Bourbaki as well as the
architect Christopher Alexander.
",1,0,1,0,0,0
6749,6750,"Adversarial Examples, Uncertainty, and Transfer Testing Robustness in Gaussian Process Hybrid Deep Networks","  Deep neural networks (DNNs) have excellent representative power and are state
of the art classifiers on many tasks. However, they often do not capture their
own uncertainties well making them less robust in the real world as they
overconfidently extrapolate and do not notice domain shift. Gaussian processes
(GPs) with RBF kernels on the other hand have better calibrated uncertainties
and do not overconfidently extrapolate far from data in their training set.
However, GPs have poor representational power and do not perform as well as
DNNs on complex domains. In this paper we show that GP hybrid deep networks,
GPDNNs, (GPs on top of DNNs and trained end-to-end) inherit the nice properties
of both GPs and DNNs and are much more robust to adversarial examples. When
extrapolating to adversarial examples and testing in domain shift settings,
GPDNNs frequently output high entropy class probabilities corresponding to
essentially ""don't know"". GPDNNs are therefore promising as deep architectures
that know when they don't know.
",0,0,0,1,0,0
3166,3167,On convergence of the sample correlation matrices in high-dimensional data,"  In this paper, we consider an estimation problem concerning the matrix of
correlation coefficients in context of high dimensional data settings. In
particular, we revisit some results in Li and Rolsalsky [Li, D. and Rolsalsky,
A. (2006). Some strong limit theorems for the largest entries of sample
correlation matrices, The Annals of Applied Probability, 16, 1, 423-447]. Four
of the main theorems of Li and Rolsalsky (2006) are established in their full
generalities and we simplify substantially some proofs of the quoted paper.
Further, we generalize a theorem which is useful in deriving the existence of
the pth moment as well as in studying the convergence rates in law of large
numbers.
",0,0,1,1,0,0
18898,18899,"Optimizing the Wisdom of the Crowd: Inference, Learning, and Teaching","  The unprecedented demand for large amount of data has catalyzed the trend of
combining human insights with machine learning techniques, which facilitate the
use of crowdsourcing to enlist label information both effectively and
efficiently. The classic work on crowdsourcing mainly focuses on the label
inference problem under the categorization setting. However, inferring the true
label requires sophisticated aggregation models that usually can only perform
well under certain assumptions. Meanwhile, no matter how complicated the
aggregation model is, the true model that generated the crowd labels remains
unknown. Therefore, the label inference problem can never infer the ground
truth perfectly. Based on the fact that the crowdsourcing labels are abundant
and utilizing aggregation will lose such kind of rich annotation information
(e.g., which worker provided which labels), we believe that it is critical to
take the diverse labeling abilities of the crowdsourcing workers as well as
their correlations into consideration. To address the above challenge, we
propose to tackle three research problems, namely inference, learning, and
teaching.
",0,0,0,1,0,0
10290,10291,Gaia Data Release 1: The archive visualisation service,"  Context: The first Gaia data release (DR1) delivered a catalogue of
astrometry and photometry for over a billion astronomical sources. Within the
panoply of methods used for data exploration, visualisation is often the
starting point and even the guiding reference for scientific thought. However,
this is a volume of data that cannot be efficiently explored using traditional
tools, techniques, and habits.
Aims: We aim to provide a global visual exploration service for the Gaia
archive, something that is not possible out of the box for most people. The
service has two main goals. The first is to provide a software platform for
interactive visual exploration of the archive contents, using common personal
computers and mobile devices available to most users. The second aim is to
produce intelligible and appealing visual representations of the enormous
information content of the archive.
Methods: The interactive exploration service follows a client-server design.
The server runs close to the data, at the archive, and is responsible for
hiding as far as possible the complexity and volume of the Gaia data from the
client. This is achieved by serving visual detail on demand. Levels of detail
are pre-computed using data aggregation and subsampling techniques. For DR1,
the client is a web application that provides an interactive multi-panel
visualisation workspace as well as a graphical user interface.
Results: The Gaia archive Visualisation Service offers a web-based
multi-panel interactive visualisation desktop in a browser tab. It currently
provides highly configurable 1D histograms and 2D scatter plots of Gaia DR1 and
the Tycho-Gaia Astrometric Solution (TGAS) with linked views. An innovative
feature is the creation of ADQL queries from visually defined regions in plots.
[abridged]
",0,1,0,0,0,0
17762,17763,High-transmissivity Silicon Visible-wavelength Metasurface Designs based on Truncated-cone Nanoantennae,"  High-transmissivity all-dielectric metasurfaces have recently attracted
attention towards the realization of ultra-compact optical devices and systems.
Silicon based metasurfaces, in particular, are highly promising considering the
possibility of monolithic integration with VLSI circuits. Realization of
silicon based metasurfaces operational in the visible wavelengths remains a
challenge. A numerical study of silicon metasurfaces based on stepped truncated
cone shaped nanoantenna elements is presented. Metasurfaces based on the
stepped conical geometry can be designed for operation in the 700nm to 800nm
wavelength window and achieve full cycle phase response (0 to pi with an
improved transmittance in comparison with previously reported cylindrical
geometry [1]. A systematic parameter study of the influence of various
geometrical parameters on the achievable amplitude and phase coverage is
reported.
",0,1,0,0,0,0
3475,3476,A study of ancient Khmer ephemerides,"  We study ancient Khmer ephemerides described in 1910 by the French engineer
Faraut, in order to determine whether they rely on observations carried out in
Cambodia. These ephemerides were found to be of Indian origin and have been
adapted for another longitude, most likely in Burma. A method for estimating
the date and place where the ephemerides were developed or adapted is described
and applied.
",0,1,1,0,0,0
4065,4066,Viscosity solutions and the minimal surface system,"  We give a definition of viscosity solution for the minimal surface system and
prove a version of Allard regularity theorem in this setting.
",0,0,1,0,0,0
19129,19130,An Upper Bound of the Minimal Dispersion via Delta Covers,"  For a point set of $n$ elements in the $d$-dimensional unit cube and a class
of test sets we are interested in the largest volume of a test set which does
not contain any point. For all natural numbers $n$, $d$ and under the
assumption of a $delta$-cover with cardinality $\vert \Gamma_\delta \vert$ we
prove that there is a point set, such that the largest volume of such a test
set without any point is bounded by $\frac{\log \vert \Gamma_\delta \vert}{n} +
\delta$. For axis-parallel boxes on the unit cube this leads to a volume of at
most $\frac{4d}{n}\log(\frac{9n}{d})$ and on the torus to $\frac{4d}{n}\log
(2n)$.
",1,0,1,0,0,0
8985,8986,First Hochschild cohomology group and stable equivalence classification of Morita type of some tame symmetric algebras,"  We use the dimension and the Lie algebra structure of the first Hochschild
cohomology group to distinguish some algebras of dihedral, semi-dihedral and
quaternion type up to stable equivalence of Morita type. In particular, we
complete the classification of algebras of dihedral type that was mostly
determined by Zhou and Zimmermann.
",0,0,1,0,0,0
17313,17314,Clustering with Noisy Queries,"  In this paper, we initiate a rigorous theoretical study of clustering with
noisy queries (or a faulty oracle). Given a set of $n$ elements, our goal is to
recover the true clustering by asking minimum number of pairwise queries to an
oracle. Oracle can answer queries of the form : ""do elements $u$ and $v$ belong
to the same cluster?"" -- the queries can be asked interactively (adaptive
queries), or non-adaptively up-front, but its answer can be erroneous with
probability $p$. In this paper, we provide the first information theoretic
lower bound on the number of queries for clustering with noisy oracle in both
situations. We design novel algorithms that closely match this query complexity
lower bound, even when the number of clusters is unknown. Moreover, we design
computationally efficient algorithms both for the adaptive and non-adaptive
settings. The problem captures/generalizes multiple application scenarios. It
is directly motivated by the growing body of work that use crowdsourcing for
{\em entity resolution}, a fundamental and challenging data mining task aimed
to identify all records in a database referring to the same entity. Here crowd
represents the noisy oracle, and the number of queries directly relates to the
cost of crowdsourcing. Another application comes from the problem of {\em sign
edge prediction} in social network, where social interactions can be both
positive and negative, and one must identify the sign of all pair-wise
interactions by querying a few pairs. Furthermore, clustering with noisy oracle
is intimately connected to correlation clustering, leading to improvement
therein. Finally, it introduces a new direction of study in the popular {\em
stochastic block model} where one has an incomplete stochastic block model
matrix to recover the clusters.
",1,0,0,1,0,0
15621,15622,Möbius topological superconductivity in UPt$_3$,"  Intensive studies for more than three decades have elucidated multiple
superconducting phases and odd-parity Cooper pairs in a heavy fermion
superconductor UPt$_3$. We identify a time-reversal invariant superconducting
phase of UPt$_3$ as a recently proposed topological nonsymmorphic
superconductivity. Combining the band structure of UPt$_3$, order parameter of
$E_{\rm 2u}$ representation allowed by $P6_3/mmc$ space group symmetry, and
topological classification by $K$-theory, we demonstrate the nontrivial
$Z_2$-invariant of three-dimensional DIII class enriched by glide symmetry.
Correspondingly, double Majorana cone surface states appear at the surface
Brillouin zone boundary. Furthermore, we show a variety of surface states and
clarify the topological protection by crystal symmetry. Majorana arcs
corresponding to tunable Weyl points appear in the time-reversal symmetry
broken B-phase. Majorana cone protected by mirror Chern number and Majorana
flat band by glide-winding number are also revealed.
",0,1,0,0,0,0
9464,9465,Comparison of forcing functions in magnetohydrodynamic turbulence,"  Results are presented of direct numerical simulations of incompressible,
homogeneous magnetohydrodynamic turbulence without a mean magnetic field,
subject to different mechanical forcing functions commonly used in the
literature. Specifically, the forces are negative damping (which uses the
large-scale velocity field as a forcing function), a nonhelical random force,
and a nonhelical static sinusoidal force (analogous to helical ABC forcing).
The time evolution of the three ideal invariants (energy, magnetic helicity and
cross helicity), the time-averaged energy spectra, the energy ratios and the
dissipation ratios are examined. All three forcing functions produce
qualitatively similar steady states with regards to the time evolution of the
energy and magnetic helicity. However, differences in the cross helicity
evolution are observed, particularly in the case of the static sinusoidal
method of energy injection. Indeed, an ensemble of sinusoidally-forced
simulations with identical parameters shows significant variations in the cross
helicity over long time periods, casting some doubt on the validity of the
principle of ergodicity in systems in which the injection of helicity cannot be
controlled. Cross helicity can unexpectedly enter the system through the
forcing function and must be carefully monitored.
",0,1,0,0,0,0
15990,15991,Modeling open nanophotonic systems using the Fourier modal method: Generalization to 3D Cartesian coordinates,"  Recently, an open geometry Fourier modal method based on a new combination of
an open boundary condition and a non-uniform $k$-space discretization was
introduced for rotationally symmetric structures providing a more efficient
approach for modeling nanowires and micropillar cavities [J. Opt. Soc. Am. A
33, 1298 (2016)]. Here, we generalize the approach to three-dimensional (3D)
Cartesian coordinates allowing for the modeling of rectangular geometries in
open space. The open boundary condition is a consequence of having an infinite
computational domain described using basis functions that expand the whole
space. The strength of the method lies in discretizing the Fourier integrals
using a non-uniform circular ""dartboard"" sampling of the Fourier $k$ space. We
show that our sampling technique leads to a more accurate description of the
continuum of the radiation modes that leak out from the structure. We also
compare our approach to conventional discretization with direct and inverse
factorization rules commonly used in established Fourier modal methods. We
apply our method to a variety of optical waveguide structures and demonstrate
that the method leads to a significantly improved convergence enabling more
accurate and efficient modeling of open 3D nanophotonic structures.
",0,1,0,0,0,0
13458,13459,Exact completion and constructive theories of sets,"  In the present paper we use the theory of exact completions to study
categorical properties of small setoids in Martin-Löf type theory and, more
generally, of models of the Constructive Elementary Theory of the Category of
Sets, in terms of properties of their subcategories of choice objects (i.e.
objects satisfying the axiom of choice). Because of these intended
applications, we deal with categories that lack equalisers and just have weak
ones, but whose objects can be regarded as collections of global elements. In
this context, we study the internal logic of the categories involved, and
employ this analysis to give a sufficient condition for the local cartesian
closure of an exact completion. Finally, we apply these results to show when an
exact completion produces a model of CETCS.
",0,0,1,0,0,0
18621,18622,Narratives of Quantum Theory in the Age of Quantum Technologies,"  Quantum technologies can be presented to the public with or without
introducing a strange trait of quantum theory responsible for their
non-classical efficiency. Traditionally the message was centered on the
superposition principle, while entanglement and properties such as
contextuality have been gaining ground recently. A less theoretical approach is
focused on simple protocols that enable technological applications. It results
in a pragmatic narrative built with the help of the resource paradigm and
principle-based reconstructions. I discuss the advantages and weaknesses of
these methods. To illustrate the importance of new metaphors beyond the
Schrödinger cat, I briefly describe a non-mathematical narrative about
entanglement that conveys an idea of some of its unusual properties. If quantum
technologists are to succeed in building trust in their work, they ought to
provoke an aesthetic perception in the public commensurable with the
mathematical beauty of quantum theory experienced by the physicist. The power
of the narrative method lies in its capacity to do so.
",0,1,0,0,0,0
4575,4576,Towards Evolutional Compression,"  Compressing convolutional neural networks (CNNs) is essential for
transferring the success of CNNs to a wide variety of applications to mobile
devices. In contrast to directly recognizing subtle weights or filters as
redundant in a given CNN, this paper presents an evolutionary method to
automatically eliminate redundant convolution filters. We represent each
compressed network as a binary individual of specific fitness. Then, the
population is upgraded at each evolutionary iteration using genetic operations.
As a result, an extremely compact CNN is generated using the fittest
individual. In this approach, either large or small convolution filters can be
redundant, and filters in the compressed network are more distinct. In
addition, since the number of filters in each convolutional layer is reduced,
the number of filter channels and the size of feature maps are also decreased,
naturally improving both the compression and speed-up ratios. Experiments on
benchmark deep CNN models suggest the superiority of the proposed algorithm
over the state-of-the-art compression methods.
",1,0,0,1,0,0
460,461,Kinetic modelling of competition and depletion of shared miRNAs by competing endogenous RNAs,"  Non-conding RNAs play a key role in the post-transcriptional regulation of
mRNA translation and turnover in eukaryotes. miRNAs, in particular, interact
with their target RNAs through protein-mediated, sequence-specific binding,
giving rise to extended and highly heterogeneous miRNA-RNA interaction
networks. Within such networks, competition to bind miRNAs can generate an
effective positive coupling between their targets. Competing endogenous RNAs
(ceRNAs) can in turn regulate each other through miRNA-mediated crosstalk.
Albeit potentially weak, ceRNA interactions can occur both dynamically,
affecting e.g. the regulatory clock, and at stationarity, in which case ceRNA
networks as a whole can be implicated in the composition of the cell's
proteome. Many features of ceRNA interactions, including the conditions under
which they become significant, can be unraveled by mathematical and in silico
models. We review the understanding of the ceRNA effect obtained within such
frameworks, focusing on the methods employed to quantify it, its role in the
processing of gene expression noise, and how network topology can determine its
reach.
",0,0,0,0,1,0
11624,11625,Deep Learning the Physics of Transport Phenomena,"  We have developed a new data-driven paradigm for the rapid inference,
modeling and simulation of the physics of transport phenomena by deep learning.
Using conditional generative adversarial networks (cGAN), we train models for
the direct generation of solutions to steady state heat conduction and
incompressible fluid flow purely on observation without knowledge of the
underlying governing equations. Rather than using iterative numerical methods
to approximate the solution of the constitutive equations, cGANs learn to
directly generate the solutions to these phenomena, given arbitrary boundary
conditions and domain, with high test accuracy (MAE$<$1\%) and state-of-the-art
computational performance. The cGAN framework can be used to learn causal
models directly from experimental observations where the underlying physical
model is complex or unknown.
",1,1,0,0,0,0
19239,19240,optimParallel: an R Package Providing Parallel Versions of the Gradient-Based Optimization Methods of optim(),"  The R package optimParallel provides a parallel version of the gradient-based
optimization methods of optim(). The main function of the package is
optimParallel(), which has the same usage and output as optim(). Using
optimParallel() can significantly reduce optimization times. We introduce the R
package and illustrate its implementation, which takes advantage of the lexical
scoping mechanism of R.
",0,0,0,1,0,0
19649,19650,Distinct Effects of Cr Bulk Doping and Surface Deposition on the Chemical Environment and Electronic Structure of the Topological Insulator Bi2Se3,"  In this report, it is shown that Cr doped into the bulk and Cr deposited on
the surface of Bi2Se3 films produced by molecular beam epitaxy (MBE) have
strikingly different effects on both the electronic structure and chemical
environment.
",0,1,0,0,0,0
8644,8645,A Note on Bayesian Model Selection for Discrete Data Using Proper Scoring Rules,"  We consider the problem of choosing between parametric models for a discrete
observable, taking a Bayesian approach in which the within-model prior
distributions are allowed to be improper. In order to avoid the ambiguity in
the marginal likelihood function in such a case, we apply a homogeneous scoring
rule. For the particular case of distinguishing between Poisson and Negative
Binomial models, we conduct simulations that indicate that, applied
prequentially, the method will consistently select the true model.
",0,0,1,1,0,0
12163,12164,Power-of-$d$-Choices with Memory: Fluid Limit and Optimality,"  In multi-server distributed queueing systems, the access of stochastically
arriving jobs to resources is often regulated by a dispatcher, also known as
load balancer. A fundamental problem consists in designing a load balancing
algorithm that minimizes the delays experienced by jobs. During the last two
decades, the power-of-$d$-choice algorithm, based on the idea of dispatching
each job to the least loaded server out of $d$ servers randomly sampled at the
arrival of the job itself, has emerged as a breakthrough in the foundations of
this area due to its versatility and appealing asymptotic properties. In this
paper, we consider the power-of-$d$-choice algorithm with the addition of a
local memory that keeps track of the latest observations collected over time on
the sampled servers. Then, each job is sent to a server with the lowest
observation. We show that this algorithm is asymptotically optimal in the sense
that the load balancer can always assign each job to an idle server in the
large-server limit. This holds true if and only if the system load $\lambda$ is
less than $1-\frac{1}{d}$. If this condition is not satisfied, we show that
queue lengths are bounded by $j^\star+1$, where $j^\star\in\mathbb{N}$ is given
by the solution of a polynomial equation. This is in contrast with the classic
version of the power-of-$d$-choice algorithm, where queue lengths are
unbounded. Our upper bound on the size of the most loaded server, $j^*+1$, is
tight and increases slowly when $\lambda$ approaches its critical value from
below. For instance, when $\lambda= 0.995$ and $d=2$ (respectively, $d=3$), we
find that no server will contain more than just $5$ ($3$) jobs in equilibrium.
Our results quantify and highlight the importance of using memory as a means to
enhance performance in randomized load balancing.
",1,0,0,0,0,0
15518,15519,CoT: Cooperative Training for Generative Modeling of Discrete Data,"  We propose Cooperative Training (CoT) for training generative models that
measure a tractable density for discrete data. CoT coordinately trains a
generator $G$ and an auxiliary predictive mediator $M$. The training target of
$M$ is to estimate a mixture density of the learned distribution $G$ and the
target distribution $P$, and that of $G$ is to minimize the Jensen-Shannon
divergence estimated through $M$. CoT achieves independent success without the
necessity of pre-training via Maximum Likelihood Estimation or involving
high-variance algorithms like REINFORCE. This low-variance algorithm is
theoretically proved to be unbiased for both generative and predictive tasks.
We also theoretically and empirically show the superiority of CoT over most
previous algorithms in terms of generative quality and diversity, predictive
generalization ability and computational cost.
",0,0,0,1,0,0
10752,10753,Large covers and sharp resonances of hyperbolic surfaces,"  Let $\Gamma$ be a convex co-compact discrete group of isometries of the
hyperbolic plane $\mathbb{H}^2$, and $X=\Gamma\backslash \mathbb{H}^2$ the
associated surface. In this paper we investigate the behaviour of resonances of
the Laplacian for large degree covers of $X$ given by a finite index normal
subgroup of $\Gamma$. Using various techniques of thermodynamical formalism and
representation theory, we prove two new existence results of ""sharp non-trivial
resonances"" close to $\Re(s)=\delta_\Gamma$, both in the large degree limit,
for abelian covers and also infinite index congruence subgroups of
$SL2(\mathbb{Z})$.
",0,0,1,0,0,0
10704,10705,Priority effects between annual and perennial plants,"  Dominance by annual plants has traditionally been considered a brief early
stage of ecological succession preceding inevitable dominance by competitive
perennials. A more recent, alternative view suggests that interactions between
annuals and perennials can result in priority effects, causing annual dominance
to persist if they are initially more common. Such priority effects would
complicate restoration of native perennial grasslands that have been invaded by
exotic annuals. However, the conditions under which these priority effects
occur remain unknown. Using a simple simulation model, we show that long-term
(500 years) priority effects are possible as long as the plants have low
fecundity and show an establishment-longevity tradeoff, with annuals having
competitive advantage over perennial seedlings. We also show that short-term
(up to 50 years) priority effects arise solely due to low fitness difference in
cases where perennials dominate in the long term. These results provide a
theoretical basis for predicting when restoration of annual-invaded grasslands
requires active removal of annuals and timely reintroduction of perennials.
",0,0,0,0,1,0
15332,15333,Proof of an entropy conjecture of Leighton and Moitra,"  We prove the following conjecture of Leighton and Moitra. Let $T$ be a
tournament on $[n]$ and $S_n$ the set of permutations of $[n]$. For an arc $uv$
of $T$, let $A_{uv}=\{\sigma \in S_n \, : \, \sigma(u)<\sigma(v) \}$.
$\textbf{Theorem.}$ For a fixed $\varepsilon>0$, if $\mathbb{P}$ is a
probability distribution on $S_n$ such that
$\mathbb{P}(A_{uv})>1/2+\varepsilon$ for every arc $uv$ of $T$, then the binary
entropy of $\mathbb{P}$ is at most $(1-\vartheta_{\varepsilon})\log_2 n!$ for
some (fixed) positive $\vartheta_\varepsilon$.
When $T$ is transitive the theorem is due to Leighton and Moitra; for this
case we give a short proof with a better $\vartheta_\varepsilon$.
",0,0,1,0,0,0
16753,16754,Data-Driven Filtered Reduced Order Modeling Of Fluid Flows,"  We propose a data-driven filtered reduced order model (DDF-ROM) framework for
the numerical simulation of fluid flows. The novel DDF-ROM framework consists
of two steps: (i) In the first step, we use explicit ROM spatial filtering of
the nonlinear PDE to construct a filtered ROM. This filtered ROM is
low-dimensional, but is not closed (because of the nonlinearity in the given
PDE). (ii) In the second step, we use data-driven modeling to close the
filtered ROM, i.e., to model the interaction between the resolved and
unresolved modes. To this end, we use a quadratic ansatz to model this
interaction and close the filtered ROM. To find the new coefficients in the
closed filtered ROM, we solve an optimization problem that minimizes the
difference between the full order model data and our ansatz. We emphasize that
the new DDF-ROM is built on general ideas of spatial filtering and optimization
and is independent of (restrictive) phenomenological arguments.
We investigate the DDF-ROM in the numerical simulation of a 2D channel flow
past a circular cylinder at Reynolds number $Re=100$. The DDF-ROM is
significantly more accurate than the standard projection ROM. Furthermore, the
computational costs of the DDF-ROM and the standard projection ROM are similar,
both costs being orders of magnitude lower than the computational cost of the
full order model. We also compare the new DDF-ROM with modern ROM closure
models in the numerical simulation of the 1D Burgers equation. The DDF-ROM is
more accurate and significantly more efficient than these ROM closure models.
",0,1,0,0,0,0
16952,16953,Opacity limit for supermassive protostars,"  We present a model for the evolution of supermassive protostars from their
formation at $M_\star \simeq 0.1\,\text{M}_\odot$ until their growth to
$M_\star \simeq 10^5\,\text{M}_\odot$. To calculate the initial properties of
the object in the optically thick regime we follow two approaches: based on
idealized thermodynamic considerations, and on a more detailed one-zone model.
Both methods derive a similar value of $n_{\rm F} \simeq 2 \times 10^{17}
\,\text{cm}^{-3}$ for the density of the object when opacity becomes important,
i.e. the opacity limit. The subsequent evolution of the growing protostar is
determined by the accretion of gas onto the object and can be described by a
mass-radius relation of the form $R_\star \propto M_\star^{1/3}$ during the
early stages, and of the form $R_\star \propto M_\star^{1/2}$ when internal
luminosity becomes important. For the case of a supermassive protostar, this
implies that the radius of the star grows from $R_\star \simeq 0.65 \,{\rm AU}$
to $R_\star \simeq 250 \,{\rm AU}$ during its evolution. Finally, we use this
model to construct a sub-grid recipe for accreting sink particles in numerical
simulations. A prime ingredient thereof is a physically motivated prescription
for the accretion radius and the effective temperature of the growing protostar
embedded inside it. From the latter, we can conclude that photo-ionization
feedback can be neglected until very late in the assembly process of the
supermassive object.
",0,1,0,0,0,0
11318,11319,Directed unions of local quadratic transforms of regular local rings and pullbacks,"  Let $\{ R_n, {\mathfrak m}_n \}_{n \ge 0}$ be an infinite sequence of regular
local rings with $R_{n+1}$ birationally dominating $R_n$ and ${\mathfrak
m}_nR_{n+1}$ a principal ideal of $R_{n+1}$ for each $n$. We examine properties
of the integrally closed local domain $S = \bigcup_{n \ge 0}R_n$.
",0,0,1,0,0,0
2372,2373,Periodic auxetics: Structure and design,"  Materials science has adopted the term of auxetic behavior for structural
deformations where stretching in some direction entails lateral widening,
rather than lateral shrinking. Most studies, in the last three decades, have
explored repetitive or cellular structures and used the notion of negative
Poisson's ratio as the hallmark of auxetic behavior. However, no general
auxetic principle has been established from this perspective. In the present
paper, we show that a purely geometric approach to periodic auxetics is apt to
identify essential characteristics of frameworks with auxetic deformations and
can generate a systematic and endless series of periodic auxetic designs. The
critical features refer to convexity properties expressed through families of
homothetic ellipsoids.
",0,0,1,0,0,0
8654,8655,cGANs with Projection Discriminator,"  We propose a novel, projection based way to incorporate the conditional
information into the discriminator of GANs that respects the role of the
conditional information in the underlining probabilistic model. This approach
is in contrast with most frameworks of conditional GANs used in application
today, which use the conditional information by concatenating the (embedded)
conditional vector to the feature vectors. With this modification, we were able
to significantly improve the quality of the class conditional image generation
on ILSVRC2012 (ImageNet) 1000-class image dataset from the current
state-of-the-art result, and we achieved this with a single pair of a
discriminator and a generator. We were also able to extend the application to
super-resolution and succeeded in producing highly discriminative
super-resolution images. This new structure also enabled high quality category
transformation based on parametric functional transformation of conditional
batch normalization layers in the generator.
",0,0,0,1,0,0
13644,13645,Manuscripts in Time and Space: Experiments in Scriptometrics on an Old French Corpus,"  Witnesses of medieval literary texts, preserved in manuscript, are layered
objects , being almost exclusively copies of copies. This results in multiple
and hard to distinguish linguistic strata -- the author's scripta interacting
with the scriptae of the various scribes -- in a context where literary written
language is already a dialectal hybrid. Moreover, no single linguistic
phenomenon allows to distinguish between different scriptae, and only the
combination of multiple characteristics is likely to be significant [9] -- but
which ones? The most common approach is to search for these features in a set
of previously selected texts, that are supposed to be representative of a given
scripta. This can induce a circularity, in which texts are used to select
features that in turn characterise them as belonging to a linguistic area. To
counter this issue, this paper offers an unsupervised and corpus-based
approach, in which clustering methods are applied to an Old French corpus to
identify main divisions and groups. Ultimately, scriptometric profiles are
built for each of them.
",0,0,0,1,0,0
7801,7802,Relational Algebra for In-Database Process Mining,"  The execution logs that are used for process mining in practice are often
obtained by querying an operational database and storing the result in a flat
file. Consequently, the data processing power of the database system cannot be
used anymore for this information, leading to constrained flexibility in the
definition of mining patterns and limited execution performance in mining large
logs. Enabling process mining directly on a database - instead of via
intermediate storage in a flat file - therefore provides additional flexibility
and efficiency. To help facilitate this ideal of in-database process mining,
this paper formally defines a database operator that extracts the 'directly
follows' relation from an operational database. This operator can both be used
to do in-database process mining and to flexibly evaluate process mining
related queries, such as: ""which employee most frequently changes the 'amount'
attribute of a case from one task to the next"". We define the operator using
the well-known relational algebra that forms the formal underpinning of
relational databases. We formally prove equivalence properties of the operator
that are useful for query optimization and present time-complexity properties
of the operator. By doing so this paper formally defines the necessary
relational algebraic elements of a 'directly follows' operator, which are
required for implementation of such an operator in a DBMS.
",1,0,0,0,0,0
2684,2685,Hybrid Indexes to Expedite Spatial-Visual Search,"  Due to the growth of geo-tagged images, recent web and mobile applications
provide search capabilities for images that are similar to a given query image
and simultaneously within a given geographical area. In this paper, we focus on
designing index structures to expedite these spatial-visual searches. We start
by baseline indexes that are straightforward extensions of the current popular
spatial (R*-tree) and visual (LSH) index structures. Subsequently, we propose
hybrid index structures that evaluate both spatial and visual features in
tandem. The unique challenge of this type of query is that there are
inaccuracies in both spatial and visual features. Therefore, different
traversals of the index structures may produce different images as output, some
of which more relevant to the query than the others. We compare our hybrid
structures with a set of baseline indexes in both performance and result
accuracy using three real world datasets from Flickr, Google Street View, and
GeoUGV.
",1,0,0,0,0,0
14830,14831,Avoiding Communication in Proximal Methods for Convex Optimization Problems,"  The fast iterative soft thresholding algorithm (FISTA) is used to solve
convex regularized optimization problems in machine learning. Distributed
implementations of the algorithm have become popular since they enable the
analysis of large datasets. However, existing formulations of FISTA communicate
data at every iteration which reduces its performance on modern distributed
architectures. The communication costs of FISTA, including bandwidth and
latency costs, is closely tied to the mathematical formulation of the
algorithm. This work reformulates FISTA to communicate data at every k
iterations and reduce data communication when operating on large data sets. We
formulate the algorithm for two different optimization methods on the Lasso
problem and show that the latency cost is reduced by a factor of k while
bandwidth and floating-point operation costs remain the same. The convergence
rates and stability properties of the reformulated algorithms are similar to
the standard formulations. The performance of communication-avoiding FISTA and
Proximal Newton methods is evaluated on 1 to 1024 nodes for multiple benchmarks
and demonstrate average speedups of 3-10x with scaling properties that
outperform the classical algorithms.
",1,0,0,0,0,0
6630,6631,Strength Factors: An Uncertainty System for a Quantified Modal Logic,"  We present a new system S for handling uncertainty in a quantified modal
logic (first-order modal logic). The system is based on both probability theory
and proof theory. The system is derived from Chisholm's epistemology. We
concretize Chisholm's system by grounding his undefined and primitive (i.e.
foundational) concept of reasonablenes in probability and proof theory. S can
be useful in systems that have to interact with humans and provide
justifications for their uncertainty. As a demonstration of the system, we
apply the system to provide a solution to the lottery paradox. Another
advantage of the system is that it can be used to provide uncertainty values
for counterfactual statements. Counterfactuals are statements that an agent
knows for sure are false. Among other cases, counterfactuals are useful when
systems have to explain their actions to users. Uncertainties for
counterfactuals fall out naturally from our system.
Efficient reasoning in just simple first-order logic is a hard problem.
Resolution-based first-order reasoning systems have made significant progress
over the last several decades in building systems that have solved non-trivial
tasks (even unsolved conjectures in mathematics). We present a sketch of a
novel algorithm for reasoning that extends first-order resolution.
Finally, while there have been many systems of uncertainty for propositional
logics, first-order logics and propositional modal logics, there has been very
little work in building systems of uncertainty for first-order modal logics.
The work described below is in progress; and once finished will address this
lack.
",1,0,0,0,0,0
10673,10674,A Sample Complexity Measure with Applications to Learning Optimal Auctions,"  We introduce a new sample complexity measure, which we refer to as
split-sample growth rate. For any hypothesis $H$ and for any sample $S$ of size
$m$, the split-sample growth rate $\hat{\tau}_H(m)$ counts how many different
hypotheses can empirical risk minimization output on any sub-sample of $S$ of
size $m/2$. We show that the expected generalization error is upper bounded by
$O\left(\sqrt{\frac{\log(\hat{\tau}_H(2m))}{m}}\right)$. Our result is enabled
by a strengthening of the Rademacher complexity analysis of the expected
generalization error. We show that this sample complexity measure, greatly
simplifies the analysis of the sample complexity of optimal auction design, for
many auction classes studied in the literature. Their sample complexity can be
derived solely by noticing that in these auction classes, ERM on any sample or
sub-sample will pick parameters that are equal to one of the points in the
sample.
",1,0,1,1,0,0
3159,3160,Deep Learning applied to Road Traffic Speed forecasting,"  In this paper, we propose deep learning architectures (FNN, CNN and LSTM) to
forecast a regression model for time dependent data. These algorithm's are
designed to handle Floating Car Data (FCD) historic speeds to predict road
traffic data. For this we aggregate the speeds into the network inputs in an
innovative way. We compare the RMSE thus obtained with the results of a simpler
physical model, and show that the latter achieves better RMSE accuracy. We also
propose a new indicator, which evaluates the algorithms improvement when
compared to a benchmark prediction. We conclude by questioning the interest of
using deep learning methods for this specific regression task.
",1,0,0,1,0,0
15882,15883,Constrained Optimisation of Rational Functions for Accelerating Subspace Iteration,"  Earlier this decade, the so-called FEAST algorithm was released for computing
the eigenvalues of a matrix in a given interval. Previously, rational filter
functions have been examined as a parameter of FEAST. In this thesis, we expand
on existing work with the following contributions: (i) Obtaining
well-performing rational filter functions via standard minimisation algorithms,
(ii) Obtaining constrained rational filter functions efficiently, and (iii)
Improving existing rational filter functions algorithmically. Using our new
rational filter functions, FEAST requires up to one quarter fewer iterations on
average compared to state-of-art rational filter functions.
",1,0,0,0,0,0
18589,18590,Bayesian hypothesis tests with diffuse priors: Can we have our cake and eat it too?,"  We introduce a new class of priors for Bayesian hypothesis testing, which we
name ""cake priors"". These priors circumvent Bartlett's paradox (also called the
Jeffreys-Lindley paradox); the problem associated with the use of diffuse
priors leading to nonsensical statistical inferences. Cake priors allow the use
of diffuse priors (having one's cake) while achieving theoretically justified
inferences (eating it too). We demonstrate this methodology for Bayesian
hypotheses tests for scenarios under which the one and two sample t-tests, and
linear models are typically derived. The resulting Bayesian test statistic
takes the form of a penalized likelihood ratio test statistic. By considering
the sampling distribution under the null and alternative hypotheses we show for
independent identically distributed regular parametric models that Bayesian
hypothesis tests using cake priors are Chernoff-consistent, i.e., achieve zero
type I and II errors asymptotically. Lindley's paradox is also discussed. We
argue that a true Lindley's paradox will only occur with small probability for
large sample sizes.
",0,0,1,1,0,0
15985,15986,Quantum spin fluctuations in the bulk insulating state of pure and Fe-doped SmB6,"  The intermediate-valence compound SmB6 is a well-known Kondo insulator, in
which hybridization of itinerant 5d electrons with localized 4f electrons leads
to a transition from metallic to insulating behavior at low temperatures.
Recent studies suggest that SmB6 is a topological insulator, with topological
metallic surface states emerging from a fully insulating hybridized bulk band
structure. Here we locally probe the bulk magnetic properties of pure and 0.5 %
Fe-doped SmB6 by muon spin rotation/relaxation methods. Below 6 K the Fe
impurity induces simultaneous changes in the bulk local magnetism and the
electrical conductivity. In the low-temperature insulating bulk state we
observe a temperature-independent dynamic relaxation rate indicative of
low-lying magnetic excitations driven primarily by quantum fluctuations.
",0,1,0,0,0,0
3463,3464,Species tree inference from genomic sequences using the log-det distance,"  The log-det distance between two aligned DNA sequences was introduced as a
tool for statistically consistent inference of a gene tree under simple
non-mixture models of sequence evolution. Here we prove that the log-det
distance, coupled with a distance-based tree construction method, also permits
consistent inference of species trees under mixture models appropriate to
aligned genomic-scale sequences data. Data may include sites from many genetic
loci, which evolved on different gene trees due to incomplete lineage sorting
on an ultrametric species tree, with different time-reversible substitution
processes. The simplicity and speed of distance-based inference suggests
log-det based methods should serve as benchmarks for judging more elaborate and
computationally-intensive species trees inference methods.
",0,0,0,0,1,0
10836,10837,Adaptive Risk Bounds in Univariate Total Variation Denoising and Trend Filtering,"  We study trend filtering, a relatively recent method for univariate
nonparametric regression. For a given positive integer $r$, the $r$-th order
trend filtering estimator is defined as the minimizer of the sum of squared
errors when we constrain (or penalize) the sum of the absolute $r$-th order
discrete derivatives of the fitted function at the design points. For $r=1$,
the estimator reduces to total variation regularization which has received much
attention in the statistics and image processing literature. In this paper, we
study the performance of the trend filtering estimator for every positive
integer $r$, both in the constrained and penalized forms. Our main results show
that in the strong sparsity setting when the underlying function is a
(discrete) spline with few ""knots"", the risk (under the global squared error
loss) of the trend filtering estimator (with an appropriate choice of the
tuning parameter) achieves the parametric $n^{-1}$ rate, up to a logarithmic
(multiplicative) factor. Our results therefore provide support for the use of
trend filtering, for every $r$, in the strong sparsity setting.
",0,0,1,1,0,0
10660,10661,Top-Rank Enhanced Listwise Optimization for Statistical Machine Translation,"  Pairwise ranking methods are the basis of many widely used discriminative
training approaches for structure prediction problems in natural language
processing(NLP). Decomposing the problem of ranking hypotheses into pairwise
comparisons enables simple and efficient solutions. However, neglecting the
global ordering of the hypothesis list may hinder learning. We propose a
listwise learning framework for structure prediction problems such as machine
translation. Our framework directly models the entire translation list's
ordering to learn parameters which may better fit the given listwise samples.
Furthermore, we propose top-rank enhanced loss functions, which are more
sensitive to ranking errors at higher positions. Experiments on a large-scale
Chinese-English translation task show that both our listwise learning framework
and top-rank enhanced listwise losses lead to significant improvements in
translation quality.
",1,0,0,0,0,0
5370,5371,A Method Of Detecting Gravitational Wave Based On Time-frequency Analysis And Convolutional Neural Networks,"  This work investigated the detection of gravitational wave (GW) from
simulated damped sinusoid signals contaminated with Gaussian noise. We proposed
to treat it as a classification problem with one class bearing our special
attentions. Two successive steps of the proposed scheme are as following:
first, decompose the data using a wavelet packet and represent the GW signal
and noise using the derived decomposition coefficients; Second, detect the
existence of GW using a convolutional neural network (CNN). To reflect our
special attention on searching GW signals, the performance is evaluated using
not only the traditional classification accuracy (correct ratio), but also
receiver operating characteristic (ROC) curve, and experiments show excelllent
performances on both evaluation measures. The generalization of a proposed
searching scheme on GW model parameter and possible extensions to other data
analysis tasks are crucial for a machine learning based approach. On this
aspect, experiments shows that there is no significant difference between GW
model parameters on identification performances by our proposed scheme.
Therefore, the proposed scheme has excellent generalization and could be used
to search for non-trained and un-known GW signals or glitches in the future GW
astronomy era.
",0,1,0,0,0,0
14445,14446,Warped Product Pointwise Semi-slant Submanifolds of Sasakian Manifolds,"  Recently, B.-Y. Chen and O. J. Garay studied pointwise slant submanifolds of
almost Hermitian manifolds. By using this notion, we investigate pointwise
semi-slant submanifolds and their warped products in Sasakian manifolds. We
give non-trivial examples of such submanifolds and obtain several fundamental
results, including a characterization for warped product pointwise semi-slant
submanifolds of Sasakian manifolds.
",0,0,1,0,0,0
4003,4004,Spreading of an infectious disease between different locations,"  The endogenous adaptation of agents, that may adjust their local contact
network in response to the risk of being infected, can have the perverse effect
of increasing the overall systemic infectiveness of a disease. We study a
dynamical model over two geographically distinct but interacting locations, to
better understand theoretically the mechanism at play. Moreover, we provide
empirical motivation from the Italian National Bovine Database, for the period
2006-2013.
",0,0,0,0,0,1
15568,15569,Structured Local Optima in Sparse Blind Deconvolution,"  Blind deconvolution is a ubiquitous problem of recovering two unknown signals
from their convolution. Unfortunately, this is an ill-posed problem in general.
This paper focuses on the {\em short and sparse} blind deconvolution problem,
where the one unknown signal is short and the other one is sparsely and
randomly supported. This variant captures the structure of the unknown signals
in several important applications. We assume the short signal to have unit
$\ell^2$ norm and cast the blind deconvolution problem as a nonconvex
optimization problem over the sphere. We demonstrate that (i) in a certain
region of the sphere, every local optimum is close to some shift truncation of
the ground truth, and (ii) for a generic short signal of length $k$, when the
sparsity of activation signal $\theta\lesssim k^{-2/3}$ and number of
measurements $m\gtrsim poly(k)$, a simple initialization method together with a
descent algorithm which escapes strict saddle points recovers a near shift
truncation of the ground truth kernel.
",0,0,0,1,0,0
13285,13286,Dual Discriminator Generative Adversarial Nets,"  We propose in this paper a novel approach to tackle the problem of mode
collapse encountered in generative adversarial network (GAN). Our idea is
intuitive but proven to be very effective, especially in addressing some key
limitations of GAN. In essence, it combines the Kullback-Leibler (KL) and
reverse KL divergences into a unified objective function, thus it exploits the
complementary statistical properties from these divergences to effectively
diversify the estimated density in capturing multi-modes. We term our method
dual discriminator generative adversarial nets (D2GAN) which, unlike GAN, has
two discriminators; and together with a generator, it also has the analogy of a
minimax game, wherein a discriminator rewards high scores for samples from data
distribution whilst another discriminator, conversely, favoring data from the
generator, and the generator produces data to fool both two discriminators. We
develop theoretical analysis to show that, given the maximal discriminators,
optimizing the generator of D2GAN reduces to minimizing both KL and reverse KL
divergences between data distribution and the distribution induced from the
data generated by the generator, hence effectively avoiding the mode collapsing
problem. We conduct extensive experiments on synthetic and real-world
large-scale datasets (MNIST, CIFAR-10, STL-10, ImageNet), where we have made
our best effort to compare our D2GAN with the latest state-of-the-art GAN's
variants in comprehensive qualitative and quantitative evaluations. The
experimental results demonstrate the competitive and superior performance of
our approach in generating good quality and diverse samples over baselines, and
the capability of our method to scale up to ImageNet database.
",1,0,0,1,0,0
2928,2929,A Copula-based Imputation Model for Missing Data of Mixed Type in Multilevel Data Sets,"  We propose a copula based method to handle missing values in multivariate
data of mixed types in multilevel data sets. Building upon the extended rank
likelihood of \cite{hoff2007extending} and the multinomial probit model, our
model is a latent variable model which is able to capture the relationship
among variables of different types as well as accounting for the clustering
structure. We fit the model by approximating the posterior distribution of the
parameters and the missing values through a Gibbs sampling scheme. We use the
multiple imputation procedure to incorporate the uncertainty due to missing
values in the analysis of the data. Our proposed method is evaluated through
simulations to compare it with several conventional methods of handling missing
data. We also apply our method to a data set from a cluster randomized
controlled trial of a multidisciplinary intervention in acute stroke units. We
conclude that our proposed copula based imputation model for mixed type
variables achieves reasonably good imputation accuracy and recovery of
parameters in some models of interest, and that adding random effects enhances
performance when the clustering effect is strong.
",0,0,0,1,0,0
8093,8094,Theory of Compact Hausdorff Shape,"  In this paper, we aim to establish a new shape theory, compact Hausdorff
shape (CH-shape) for general Hausdorff spaces. We use the ""internal"" method and
direct system approach on the homotopy category of compact Hausdorff spaces.
Such a construction can preserve most good properties of H-shape given by Rubin
and Sanders. Most importantly, we can moreover develop the entire homology
theory for CH-shape, including the exactness, dual to the consequence of
Mardešić and Segal.
",0,0,1,0,0,0
8949,8950,Multi-variable LSTM neural network for autoregressive exogenous model,"  In this paper, we propose multi-variable LSTM capable of accurate forecasting
and variable importance interpretation for time series with exogenous
variables. Current attention mechanism in recurrent neural networks mostly
focuses on the temporal aspect of data and falls short of characterizing
variable importance. To this end, the multi-variable LSTM equipped with
tensorized hidden states is developed to learn hidden states for individual
variables, which give rise to our mixture temporal and variable attention.
Based on such attention mechanism, we infer and quantify variable importance.
Extensive experiments using real datasets with Granger-causality test and the
synthetic dataset with ground truth demonstrate the prediction performance and
interpretability of multi-variable LSTM in comparison to a variety of
baselines. It exhibits the prospect of multi-variable LSTM as an end-to-end
framework for both forecasting and knowledge discovery.
",0,0,0,1,0,0
11774,11775,sWSI: A Low-cost and Commercial-quality Whole Slide Imaging System on Android and iOS Smartphones,"  In this paper, scalable Whole Slide Imaging (sWSI), a novel high-throughput,
cost-effective and robust whole slide imaging system on both Android and iOS
platforms is introduced and analyzed. With sWSI, most mainstream smartphone
connected to a optical eyepiece of any manually controlled microscope can be
automatically controlled to capture sequences of mega-pixel fields of views
that are synthesized into giga-pixel virtual slides. Remote servers carry out
the majority of computation asynchronously to support clients running at
satisfying frame rates without sacrificing image quality nor robustness. A
typical 15x15mm sample can be digitized in 30 seconds with 4X or in 3 minutes
with 10X object magnification, costing under $1. The virtual slide quality is
considered comparable to existing high-end scanners thus satisfying for
clinical usage by surveyed pathologies. The scan procedure with features such
as supporting magnification up to 100x, recoding z-stacks,
specimen-type-neutral and giving real-time feedback, is deemed
work-flow-friendly and reliable.
",1,1,0,0,0,0
12165,12166,Testing the Young Neutron Star Scenario with Persistent Radio Emission Associated with FRB 121102,"  Recently a repeating fast radio burst (FRB) 121102 has been confirmed to be
an extragalactic event and a persistent radio counterpart has been identified.
While other possibilities are not ruled out, the emission properties are
broadly consistent with Murase et al. (2016) that theoretically proposed
quasi-steady radio emission as a counterpart of both FRBs and pulsar-driven
supernovae. Here we constrain the model parameters of such a young neutron star
scenario for FRB 121102. If the associated supernova has a conventional ejecta
mass of $M_{\rm ej}\gtrsim{\rm a \ few}\ M_\odot$, a neutron star with an age
of $t_{\rm age} \sim 10-100 \ \rm yrs$, an initial spin period of $P_{i}
\lesssim$ a few ms, and a dipole magnetic field of $B_{\rm dip} \lesssim {\rm a
\ few} \times 10^{13} \ \rm G$ can be compatible with the observations.
However, in this case, the magnetically-powered scenario may be favored as an
FRB energy source because of the efficiency problem in the rotation-powered
scenario. On the other hand, if the associated supernova is an ultra-stripped
one or the neutron star is born by the accretion-induced collapse with $M_{\rm
ej} \sim 0.1 \ M_\odot$, a younger neutron star with $t_{\rm age} \sim 1-10$
yrs can be the persistent radio source and might produce FRBs with the
spin-down power. These possibilities can be distinguished by the decline rate
of the quasi-steady radio counterpart.
",0,1,0,0,0,0
6465,6466,"A mechanistic model of connector hubs, modularity, and cognition","  The human brain network is modular--comprised of communities of tightly
interconnected nodes. This network contains local hubs, which have many
connections within their own communities, and connector hubs, which have
connections diversely distributed across communities. A mechanistic
understanding of these hubs and how they support cognition has not been
demonstrated. Here, we leveraged individual differences in hub connectivity and
cognition. We show that a model of hub connectivity accurately predicts the
cognitive performance of 476 individuals in four distinct tasks. Moreover,
there is a general optimal network structure for cognitive
performance--individuals with diversely connected hubs and consequent modular
brain networks exhibit increased cognitive performance, regardless of the task.
Critically, we find evidence consistent with a mechanistic model in which
connector hubs tune the connectivity of their neighbors to be more modular
while allowing for task appropriate information integration across communities,
which increases global modularity and cognitive performance.
",0,0,0,0,1,0
6392,6393,A Tutorial on Deep Learning for Music Information Retrieval,"  Following their success in Computer Vision and other areas, deep learning
techniques have recently become widely adopted in Music Information Retrieval
(MIR) research. However, the majority of works aim to adopt and assess methods
that have been shown to be effective in other domains, while there is still a
great need for more original research focusing on music primarily and utilising
musical knowledge and insight. The goal of this paper is to boost the interest
of beginners by providing a comprehensive tutorial and reducing the barriers to
entry into deep learning for MIR. We lay out the basic principles and review
prominent works in this hard to navigate the field. We then outline the network
structures that have been successful in MIR problems and facilitate the
selection of building blocks for the problems at hand. Finally, guidelines for
new tasks and some advanced topics in deep learning are discussed to stimulate
new research in this fascinating field.
",1,0,0,0,0,0
19522,19523,How to Ask for Technical Help? Evidence-based Guidelines for Writing Questions on Stack Overflow,"  Context: The success of Stack Overflow and other community-based
question-and-answer (Q&A) sites depends mainly on the will of their members to
answer others' questions. In fact, when formulating requests on Q&A sites, we
are not simply seeking for information. Instead, we are also asking for other
people's help and feedback. Understanding the dynamics of the participation in
Q&A communities is essential to improve the value of crowdsourced knowledge.
Objective: In this paper, we investigate how information seekers can increase
the chance of eliciting a successful answer to their questions on Stack
Overflow by focusing on the following actionable factors: affect, presentation
quality, and time.
Method: We develop a conceptual framework of factors potentially influencing
the success of questions in Stack Overflow. We quantitatively analyze a set of
over 87K questions from the official Stack Overflow dump to assess the impact
of actionable factors on the success of technical requests. The information
seeker reputation is included as a control factor. Furthermore, to understand
the role played by affective states in the success of questions, we
qualitatively analyze questions containing positive and negative emotions.
Finally, a survey is conducted to understand how Stack Overflow users perceive
the guideline suggestions for writing questions.
Results: We found that regardless of user reputation, successful questions
are short, contain code snippets, and do not abuse with uppercase characters.
As regards affect, successful questions adopt a neutral emotional style.
Conclusion: We provide evidence-based guidelines for writing effective
questions on Stack Overflow that software engineers can follow to increase the
chance of getting technical help. As for the role of affect, we empirically
confirmed community guidelines that suggest avoiding rudeness in question
writing.
",1,0,0,0,0,0
18598,18599,Group Embeddings with Algorithmic Properties,"  We show that every countable group H with solvable word problem (=computable
group) can be subnormally embedded into a 2-generated group G which also has
solvable word problem. Moreover, the membership problem for H < G is also
solvable. We also give estimates of time and space complexity of the word
problem in G and of the membership problem for H < G.
",0,0,1,0,0,0
18157,18158,"A unitary ""quantization commutes with reduction"" map for the adjoint action of a compact Lie group","  Let $K$ be a simply connected compact Lie group and $T^{\ast}(K)$ its
cotangent bundle. We consider the problem of ""quantization commutes with
reduction"" for the adjoint action of $K$ on $T^{\ast}(K).$ We quantize both
$T^{\ast}(K)$ and the reduced phase space using geometric quantization with
half-forms. We then construct a geometrically natural map from the space of
invariant elements in the quantization of $T^{\ast}(K)$ to the quantization of
the reduced phase space. We show that this map is a constant multiple of a
unitary map.
",0,0,1,0,0,0
507,508,Dynamic Shrinkage Processes,"  We propose a novel class of dynamic shrinkage processes for Bayesian time
series and regression analysis. Building upon a global-local framework of prior
construction, in which continuous scale mixtures of Gaussian distributions are
employed for both desirable shrinkage properties and computational
tractability, we model dependence among the local scale parameters. The
resulting processes inherit the desirable shrinkage behavior of popular
global-local priors, such as the horseshoe prior, but provide additional
localized adaptivity, which is important for modeling time series data or
regression functions with local features. We construct a computationally
efficient Gibbs sampling algorithm based on a Pólya-Gamma scale mixture
representation of the proposed process. Using dynamic shrinkage processes, we
develop a Bayesian trend filtering model that produces more accurate estimates
and tighter posterior credible intervals than competing methods, and apply the
model for irregular curve-fitting of minute-by-minute Twitter CPU usage data.
In addition, we develop an adaptive time-varying parameter regression model to
assess the efficacy of the Fama-French five-factor asset pricing model with
momentum added as a sixth factor. Our dynamic analysis of manufacturing and
healthcare industry data shows that with the exception of the market risk, no
other risk factors are significant except for brief periods.
",0,0,0,1,0,0
5662,5663,The extra scalar degrees of freedom from the two Higgs doublet model for dark energy,"  In principle a minimal extension of the standard model of Particle Physics,
the two Higgs doublet model, can be invoked to explain the scalar field
responsible of dark energy. The two doublets are in general mixed. After
diagonalization, the lightest CP-even Higgs and CP-odd Higgs are jointly taken
to be the dark energy candidate. The dark energy obtained from Higgs fields in
this case is indistinguishable from the cosmological constant.
",0,1,0,0,0,0
1666,1667,Evolution of antiferromagnetic domains in the all-in-all-out ordered pyrochlore Nd$_2$Zr$_2$O$_7$,"  We report the observation of magnetic domains in the exotic,
antiferromagnetically ordered all-in-all-out state of Nd$_2$Zr$_2$O$_7$,
induced by spin canting. The all-in-all-out state can be realized by Ising-like
spins on a pyrochlore lattice and is established in Nd$_2$Zr$_2$O$_7$ below
0.31 K for external magnetic fields up to 0.14 T. Two different spin
arrangements can fulfill this configuration which leads to the possibility of
magnetic domains. The all-in-all-out domain structure can be controlled by an
external magnetic field applied parallel to the [111] direction. This is a
result of different spin canting mechanism for the two all-in-all-out
configurations for such a direction of the magnetic field. The change of the
domain structure is observed through a hysteresis in the magnetic
susceptibility. No hysteresis occurs, however, in case the external magnetic
field is applied along [100].
",0,1,0,0,0,0
17287,17288,Surface group amalgams that (don't) act on 3-manifolds,"  We determine which amalgamated products of surface groups identified over
multiples of simple closed curves are not fundamental groups of 3-manifolds. We
prove each surface amalgam considered is virtually the fundamental group of a
3-manifold. We prove that each such surface group amalgam is abstractly
commensurable to a right-angled Coxeter group from a related family. In an
appendix, we determine the quasi-isometry classes among these surface amalgams
and their related right-angled Coxeter groups.
",0,0,1,0,0,0
3997,3998,Synthesis of Highly Anisotropic Semiconducting GaTe Nanomaterials and Emerging Properties Enabled by Epitaxy,"  Pseudo-one dimensional (pseudo-1D) materials are a new-class of materials
where atoms are arranged in chain like structures in two-dimensions (2D).
Examples include recently discovered black phosphorus, ReS2 and ReSe2 from
transition metal dichalcogenides, TiS3 and ZrS3 from transition metal
trichalcogenides and most recently GaTe. The presence of structural anisotropy
impacts their physical properties and leads to direction dependent light-matter
interactions, dichroic optical responses, high mobility channels, and
anisotropic thermal conduction. Despite the numerous reports on the vapor phase
growth of isotropic TMDCs and post transition metal chalcogenides such as MoS2
and GaSe, the synthesis of pseudo-1D materials is particularly difficult due to
the anisotropy in interfacial energy, which stabilizes dendritic growth rather
than single crystalline growth with well-defined orientation. The growth of
monoclinic GaTe has been demonstrated on flexible mica substrates with superior
photodetecting performance. In this work, we demonstrate that pseudo-1D
monoclinic GaTe layers can be synthesized on a variety of other substrates
including GaAs (111), Si (111) and c-cut sapphire by physical vapor transport
techniques. High resolution transmission electron microscopy (HRTEM)
measurements, together with angle resolved micro-PL and micro-Raman techniques,
provide for the very first time atomic scale resolution experiments on
pseudo-1D structures in monoclinic GaTe and anisotropic properties.
Interestingly, GaTe nanomaterials grown on sapphire exhibit highly efficient
and narrow localized emission peaks below the band gap energy, which are found
to be related to select types of line and point defects as evidenced by PL and
Raman mapping scans. It makes the samples grown on sapphire more prominent than
those grown on GaAs and Si, which demonstrate more regular properties.
",0,1,0,0,0,0
97,98,Almost euclidean Isoperimetric Inequalities in spaces satisfying local Ricci curvature lower bounds,"  Motivated by Perelman's Pseudo Locality Theorem for the Ricci flow, we prove
that if a Riemannian manifold has Ricci curvature bounded below in a metric
ball which moreover has almost maximal volume, then in a smaller ball (in a
quantified sense) it holds an almost-euclidean isoperimetric inequality. The
result is actually established in the more general framework of non-smooth
spaces satisfying local Ricci curvature lower bounds in a synthetic sense via
optimal transportation.
",0,0,1,0,0,0
1860,1861,Efficient tracking of a growing number of experts,"  We consider a variation on the problem of prediction with expert advice,
where new forecasters that were unknown until then may appear at each round. As
often in prediction with expert advice, designing an algorithm that achieves
near-optimal regret guarantees is straightforward, using aggregation of
experts. However, when the comparison class is sufficiently rich, for instance
when the best expert and the set of experts itself changes over time, such
strategies naively require to maintain a prohibitive number of weights
(typically exponential with the time horizon). By contrast, designing
strategies that both achieve a near-optimal regret and maintain a reasonable
number of weights is highly non-trivial. We consider three increasingly
challenging objectives (simple regret, shifting regret and sparse shifting
regret) that extend existing notions defined for a fixed expert ensemble; in
each case, we design strategies that achieve tight regret bounds, adaptive to
the parameters of the comparison class, while being computationally
inexpensive. Moreover, our algorithms are anytime, agnostic to the number of
incoming experts and completely parameter-free. Such remarkable results are
made possible thanks to two simple but highly effective recipes: first the
""abstention trick"" that comes from the specialist framework and enables to
handle the least challenging notions of regret, but is limited when addressing
more sophisticated objectives. Second, the ""muting trick"" that we introduce to
give more flexibility. We show how to combine these two tricks in order to
handle the most challenging class of comparison strategies.
",1,0,0,1,0,0
3325,3326,On the boundary between qualitative and quantitative measures of causal effects,"  Causal relationships among variables are commonly represented via directed
acyclic graphs. There are many methods in the literature to quantify the
strength of arrows in a causal acyclic graph. These methods, however, have
undesirable properties when the causal system represented by a directed acyclic
graph is degenerate. In this paper, we characterize a degenerate causal system
using multiplicity of Markov boundaries, and show that in this case, it is
impossible to quantify causal effects in a reasonable fashion. We then propose
algorithms to identify such degenerate scenarios from observed data.
Performance of our algorithms is investigated through synthetic data analysis.
",0,0,1,1,0,0
14699,14700,Weak Label Supervision for Monaural Source Separation Using Non-negative Denoising Variational Autoencoders,"  Deep learning models are very effective in source separation when there are
large amounts of labeled data available. However it is not always possible to
have carefully labeled datasets. In this paper, we propose a weak supervision
method that only uses class information rather than source signals for learning
to separate short utterance mixtures. We associate a variational autoencoder
(VAE) with each class within a non-negative model. We demonstrate that deep
convolutional VAEs provide a prior model to identify complex signals in a sound
mixture without having access to any source signal. We show that the separation
results are on par with source signal supervision.
",1,0,0,0,0,0
744,745,Artificial Intelligence Based Malware Analysis,"  Artificial intelligence methods have often been applied to perform specific
functions or tasks in the cyber-defense realm. However, as adversary methods
become more complex and difficult to divine, piecemeal efforts to understand
cyber-attacks, and malware-based attacks in particular, are not providing
sufficient means for malware analysts to understand the past, present and
future characteristics of malware.
In this paper, we present the Malware Analysis and Attributed using Genetic
Information (MAAGI) system. The underlying idea behind the MAAGI system is that
there are strong similarities between malware behavior and biological organism
behavior, and applying biologically inspired methods to corpora of malware can
help analysts better understand the ecosystem of malware attacks. Due to the
sophistication of the malware and the analysis, the MAAGI system relies heavily
on artificial intelligence techniques to provide this capability. It has
already yielded promising results over its development life, and will hopefully
inspire more integration between the artificial intelligence and cyber--defense
communities.
",1,0,0,0,0,0
17326,17327,Good Similar Patches for Image Denoising,"  Patch-based denoising algorithms like BM3D have achieved outstanding
performance. An important idea for the success of these methods is to exploit
the recurrence of similar patches in an input image to estimate the underlying
image structures. However, in these algorithms, the similar patches used for
denoising are obtained via Nearest Neighbour Search (NNS) and are sometimes not
optimal. First, due to the existence of noise, NNS can select similar patches
with similar noise patterns to the reference patch. Second, the unreliable
noisy pixels in digital images can bring a bias to the patch searching process
and result in a loss of color fidelity in the final denoising result. We
observe that given a set of good similar patches, their distribution is not
necessarily centered at the noisy reference patch and can be approximated by a
Gaussian component. Based on this observation, we present a patch searching
method that clusters similar patch candidates into patch groups using Gaussian
Mixture Model-based clustering, and selects the patch group that contains the
reference patch as the final patches for denoising. We also use an unreliable
pixel estimation algorithm to pre-process the input noisy images to further
improve the patch searching. Our experiments show that our approach can better
capture the underlying patch structures and can consistently enable the
state-of-the-art patch-based denoising algorithms, such as BM3D, LPCA and PLOW,
to better denoise images by providing them with patches found by our approach
while without modifying these algorithms.
",1,0,0,0,0,0
10623,10624,Some Formulas for Numbers of Restricted Words,"  We define a quantity $c_m(n,k)$ as a generalization of the notion of the
composition of the positive integer $n$ into $k$ parts. We proceed to derive
some known properties of this quantity. In particular, we relate two partial
Bell polynomials, in which the sequence of the variables of one polynomial is
the invert transform of the sequence of the variables of the other. We connect
the quantities $c_m(n,k)$ and $c_{m-1}(n,k)$ via Pascal matrices. We then
relate $c_m(n,k)$ with the numbers of some restricted words over a finite
alphabet. We develop a method which transfers some properties of restricted
words over an alphabet of $N$ letters to the restricted words over an alphabet
of $N+1$ letters. Several examples illustrate our findings. Note that all our
results depend solely on the initial arithmetic function $f_0$.
",0,0,1,0,0,0
10487,10488,Tensor Completion Algorithms in Big Data Analytics,"  Tensor completion is a problem of filling the missing or unobserved entries
of partially observed tensors. Due to the multidimensional character of tensors
in describing complex datasets, tensor completion algorithms and their
applications have received wide attention and achievement in areas like data
mining, computer vision, signal processing, and neuroscience. In this survey,
we provide a modern overview of recent advances in tensor completion algorithms
from the perspective of big data analytics characterized by diverse variety,
large volume, and high velocity. We characterize these advances from four
perspectives: general tensor completion algorithms, tensor completion with
auxiliary information (variety), scalable tensor completion algorithms
(volume), and dynamic tensor completion algorithms (velocity). Further, we
identify several tensor completion applications on real-world data-driven
problems and present some common experimental frameworks popularized in the
literature. Our goal is to summarize these popular methods and introduce them
to researchers and practitioners for promoting future research and
applications. We conclude with a discussion of key challenges and promising
research directions in this community for future exploration.
",1,0,0,1,0,0
18970,18971,Non-orthogonal Multiple Access for High-reliable and Low-latency V2X Communications,"  In this paper, we consider a dense vehicular communication network where each
vehicle broadcasts its safety information to its neighborhood in each
transmission period. Such applications require low latency and high
reliability, and thus, we propose a non-orthogonal multiple access scheme to
reduce the latency and to improve the packet reception probability. In the
proposed scheme, the BS performs the semi-persistent scheduling to optimize the
time scheduling and allocate frequency resources in a non-orthogonal manner
while the vehicles autonomously perform distributed power control. We formulate
the centralized scheduling and resource allocation problem as equivalent to a
multi-dimensional stable roommate matching problem, in which the users and
time/frequency resources are considered as disjoint sets of players to be
matched with each other. We then develop a novel rotation matching algorithm,
which converges to a q-exchange stable matching after a limited number of
iterations. Simulation results show that the proposed scheme outperforms the
traditional orthogonal multiple access scheme in terms of the latency and
reliability.
",1,0,0,0,0,0
13291,13292,What drives galactic magnetism?,"  We aim to use statistical analysis of a large number of various galaxies to
probe, model, and understand relations between different galaxy properties and
magnetic fields. We have compiled a sample of 55 galaxies including low-mass
dwarf and Magellanic-types, normal spirals and several massive starbursts, and
applied principal component analysis (PCA) and regression methods to assess the
impact of various galaxy properties on the observed magnetic fields. According
to PCA the global galaxy parameters (like HI, H2, and dynamical mass, star
formation rate (SFR), near-infrared luminosity, size, and rotational velocity)
are all mutually correlated and can be reduced to a single principal component.
Further PCA performed for global and intensive (not size related) properties of
galaxies (such as gas density, and surface density of the star formation rate,
SSFR), indicates that magnetic field strength B is connected mainly to the
intensive parameters, while the global parameters have only weak relationships
with B. We find that the tightest relationship of B is with SSFR, which is
described by a power-law with an index of 0.33+-0.03. The observed weaker
associations of B with galaxy dynamical mass and the rotational velocity we
interpret as indirect ones, resulting from the observed connection of the
global SFR with the available total H2 mass in galaxies. Using our sample we
constructed a diagram of B across the Hubble sequence which reveals that high
values of B are not restricted by the Hubble type. However, weaker fields
appear exclusively in later Hubble types and B as low as about 5muG is not seen
among typical spirals. The processes of generation of magnetic field in the
dwarf and Magellanic-type galaxies are similar to those in the massive spirals
and starbursts and are mainly coupled to local star-formation activity
involving the small-scale dynamo mechanism.
",0,1,0,0,0,0
139,140,Evaluating openEHR for storing computable representations of electronic health record phenotyping algorithms,"  Electronic Health Records (EHR) are data generated during routine clinical
care. EHR offer researchers unprecedented phenotypic breadth and depth and have
the potential to accelerate the pace of precision medicine at scale. A main EHR
use-case is creating phenotyping algorithms to define disease status, onset and
severity. Currently, no common machine-readable standard exists for defining
phenotyping algorithms which often are stored in human-readable formats. As a
result, the translation of algorithms to implementation code is challenging and
sharing across the scientific community is problematic. In this paper, we
evaluate openEHR, a formal EHR data specification, for computable
representations of EHR phenotyping algorithms.
",1,0,0,0,0,0
18816,18817,Analytic approximation of solutions of parabolic partial differential equations with variable coefficients,"  A complete family of solutions for the one-dimensional reaction-diffusion
equation \[ u_{xx}(x,t)-q(x)u(x,t) = u_t(x,t) \] with a coefficient $q$
depending on $x$ is constructed. The solutions represent the images of the heat
polynomials under the action of a transmutation operator. Their use allows one
to obtain an explicit solution of the noncharacteristic Cauchy problem for the
considered equation with sufficiently regular Cauchy data as well as to solve
numerically initial boundary value problems. In the paper the Dirichlet
boundary conditions are considered however the proposed method can be easily
extended onto other standard boundary conditions. The proposed numerical method
is shown to reveal good accuracy.
",0,0,1,0,0,0
12965,12966,Mechanisms of dimensionality reduction and decorrelation in deep neural networks,"  Deep neural networks are widely used in various domains. However, the nature
of computations at each layer of the deep networks is far from being well
understood. Increasing the interpretability of deep neural networks is thus
important. Here, we construct a mean-field framework to understand how compact
representations are developed across layers, not only in deterministic deep
networks with random weights but also in generative deep networks where an
unsupervised learning is carried out. Our theory shows that the deep
computation implements a dimensionality reduction while maintaining a finite
level of weak correlations between neurons for possible feature extraction.
Mechanisms of dimensionality reduction and decorrelation are unified in the
same framework. This work may pave the way for understanding how a sensory
hierarchy works.
",1,0,0,1,0,0
7171,7172,Early Experiences with Crowdsourcing Airway Annotations in Chest CT,"  Measuring airways in chest computed tomography (CT) images is important for
characterizing diseases such as cystic fibrosis, yet very time-consuming to
perform manually. Machine learning algorithms offer an alternative, but need
large sets of annotated data to perform well. We investigate whether
crowdsourcing can be used to gather airway annotations which can serve directly
for measuring the airways, or as training data for the algorithms. We generate
image slices at known locations of airways and request untrained crowd workers
to outline the airway lumen and airway wall. Our results show that the workers
are able to interpret the images, but that the instructions are too complex,
leading to many unusable annotations. After excluding unusable annotations,
quantitative results show medium to high correlations with expert measurements
of the airways. Based on this positive experience, we describe a number of
further research directions and provide insight into the challenges of
crowdsourcing in medical images from the perspective of first-time users.
",1,0,0,0,0,0
15654,15655,Berry-Esséen bounds for parameter estimation of general Gaussian processes,"  We study rates of convergence in central limit theorems for the partial sum
of squares of general Gaussian sequences, using tools from analysis on Wiener
space. No assumption of stationarity, asymptotically or otherwise, is made. The
main theoretical tool is the so-called Optimal Fourth Moment Theorem
\cite{NP2015}, which provides a sharp quantitative estimate of the total
variation distance on Wiener chaos to the normal law. The only assumptions made
on the sequence are the existence of an asymptotic variance, that a
least-squares-type estimator for this variance parameter has a bias and a
variance which can be controlled, and that the sequence's auto-correlation
function, which may exhibit long memory, has a no-worse memory than that of
fractional Brownian motion with Hurst parameter }$H<3/4$.{\ \ Our main result
is explicit, exhibiting the trade-off between bias, variance, and memory. We
apply our result to study drift parameter estimation problems for subfractional
Ornstein-Uhlenbeck and bifractional Ornstein-Uhlenbeck processes with
fixed-time-step observations. These are processes which fail to be stationary
or self-similar, but for which detailed calculations result in explicit
formulas for the estimators' asymptotic normality.
",0,0,1,1,0,0
19472,19473,Portfolio Optimization with Nondominated Priors and Unbounded Parameters,"  We consider classical Merton problem of terminal wealth maximization in
finite horizon. We assume that the drift of the stock is following
Ornstein-Uhlenbeck process and the volatility of it is following GARCH(1)
process. In particular, both mean and volatility are unbounded. We assume that
there is Knightian uncertainty on the parameters of both mean and volatility.
We take that the investor has logarithmic utility function, and solve the
corresponding utility maximization problem explicitly. To the best of our
knowledge, this is the first work on utility maximization with unbounded mean
and volatility in Knightian uncertainty under nondominated priors.
",0,0,0,0,0,1
20787,20788,Sparse Approximation of 3D Meshes using the Spectral Geometry of the Hamiltonian Operator,"  The discrete Laplace operator is ubiquitous in spectral shape analysis, since
its eigenfunctions are provably optimal in representing smooth functions
defined on the surface of the shape. Indeed, subspaces defined by its
eigenfunctions have been utilized for shape compression, treating the
coordinates as smooth functions defined on the given surface. However, surfaces
of shapes in nature often contain geometric structures for which the general
smoothness assumption may fail to hold. At the other end, some explicit mesh
compression algorithms utilize the order by which vertices that represent the
surface are traversed, a property which has been ignored in spectral
approaches. Here, we incorporate the order of vertices into an operator that
defines a novel spectral domain. We propose a method for representing 3D meshes
using the spectral geometry of the Hamiltonian operator, integrated within a
sparse approximation framework. We adapt the concept of a potential function
from quantum physics and incorporate vertex ordering information into the
potential, yielding a novel data-dependent operator. The potential function
modifies the spectral geometry of the Laplacian to focus on regions with finer
details of the given surface. By sparsely encoding the geometry of the shape
using the proposed data-dependent basis, we improve compression performance
compared to previous results that use the standard Laplacian basis and spectral
graph wavelets.
",1,0,0,0,0,0
356,357,Roche-lobe overflow in eccentric planet-star systems,"  Many giant exoplanets are found near their Roche limit and in mildly
eccentric orbits. In this study we examine the fate of such planets through
Roche-lobe overflow as a function of the physical properties of the binary
components, including the eccentricity and the asynchronicity of the rotating
planet. We use a direct three-body integrator to compute the trajectories of
the lost mass in the ballistic limit and investigate the possible outcomes. We
find three different outcomes for the mass transferred through the Lagrangian
point $L_{1}$: (i) self-accretion by the planet, (ii) direct impact on the
stellar surface, (iii) disk formation around the star. We explore the parameter
space of the three different regimes and find that at low eccentricities,
$e\lesssim 0.2$, mass overflow leads to disk formation for most systems, while
for higher eccentricities or retrograde orbits self-accretion is the only
possible outcome. We conclude that the assumption often made in previous work
that when a planet overflows its Roche lobe it is quickly disrupted and
accreted by the star is not always valid.
",0,1,0,0,0,0
16030,16031,Estimating causal effects of time-dependent exposures on a binary endpoint in a high-dimensional setting,"  Recently, the intervention calculus when the DAG is absent (IDA) method was
developed to estimate lower bounds of causal effects from observational
high-dimensional data. Originally it was introduced to assess the effect of
baseline biomarkers which do not vary over time. However, in many clinical
settings, measurements of biomarkers are repeated at fixed time points during
treatment exposure and, therefore, this method need to be extended. The purpose
of this paper is then to extend the first step of the IDA, the Peter Clarks
(PC)-algorithm, to a time-dependent exposure in the context of a binary
outcome. We generalised the PC-algorithm for taking into account the
chronological order of repeated measurements of the exposure and propose to
apply the IDA with our new version, the chronologically ordered PC-algorithm
(COPC-algorithm). A simulation study has been performed before applying the
method for estimating causal effects of time-dependent immunological biomarkers
on toxicity, death and progression in patients with metastatic melanoma. The
simulation study showed that the completed partially directed acyclic graphs
(CPDAGs) obtained using COPC-algorithm were structurally closer to the true
CPDAG than CPDAGs obtained using PC-algorithm. Also, causal effects were more
accurate when they were estimated based on CPDAGs obtained using
COPC-algorithm. Moreover, CPDAGs obtained by COPC-algorithm allowed removing
non-chronologic arrows with a variable measured at a time t pointing to a
variable measured at a time t' where t'< t. Bidirected edges were less present
in CPDAGs obtained with the COPC-algorithm, supporting the fact that there was
less variability in causal effects estimated from these CPDAGs. The
COPC-algorithm provided CPDAGs that keep the chronological structure present in
the data, thus allowed to estimate lower bounds of the causal effect of
time-dependent biomarkers.
",0,0,0,1,0,0
5724,5725,Pattern recognition techniques for Boson Sampling validation,"  The difficulty of validating large-scale quantum devices, such as Boson
Samplers, poses a major challenge for any research program that aims to show
quantum advantages over classical hardware. To address this problem, we propose
a novel data-driven approach wherein models are trained to identify common
pathologies using unsupervised machine learning methods. We illustrate this
idea by training a classifier that exploits K-means clustering to distinguish
between Boson Samplers that use indistinguishable photons from those that do
not. We train the model on numerical simulations of small-scale Boson Samplers
and then validate the pattern recognition technique on larger numerical
simulations as well as on photonic chips in both traditional Boson Sampling and
scattershot experiments. The effectiveness of such method relies on
particle-type-dependent internal correlations present in the output
distributions. This approach performs substantially better on the test data
than previous methods and underscores the ability to further generalize its
operation beyond the scope of the examples that it was trained on.
",1,0,0,0,0,0
19824,19825,Light Attenuation Length of High Quality Linear Alkyl Benzene as Liquid Scintillator Solvent for the JUNO Experiment,"  The Jiangmen Underground Neutrino Observatory (JUNO) is a multipurpose
neutrino experiment with a 20 kt liquid scintillator detector designed to
determine the neutrino mass hierarchy, and measure the neutrino oscillation
parameters. Linear alkyl benzene (LAB) will be used as the solvent for the
liquid scintillation system in the central detector of JUNO. For this purpose,
we have prepared LAB samples, and have measured their light attenuation
lengths, with one achieving a length of 25.8 m, comparable to the diameter of
the JUNO detector.
",0,1,0,0,0,0
8719,8720,Pilot system development in metre-scale laboratory discharge,"  The pilot system development in metre-scale negative laboratory discharges is
studied with ns-fast photography. The systems appear as bipolar structures in
the vicinity of the negative high-voltage electrode. They appear as a result of
a single negative streamer propagation and determine further discharge
development. Such systems possess features like glowing beads, bipolarity,
different brightness of the top and bottom parts, and mutual reconnection. A 1D
model of the ionization evolution in the spark gap is proposed. In the process
of the nonlinear development of ionization growth, the model shows features
similar to those observed. The visual similarities between high-altitude
sprites and laboratory pilots are striking and may indicate that they are two
manifestations of the same natural phenomenon.
",0,1,0,0,0,0
13612,13613,General dynamical properties of cosmological models with nonminimal kinetic coupling,"  We consider cosmological dynamics in the theory of gravity with the scalar
field possessing the nonminimal kinetic coupling to curvature given as $\eta
G^{\mu\nu}\phi_{,\mu}\phi_{,\nu}$, where $\eta$ is an arbitrary coupling
parameter, and the scalar potential $V(\phi)$ which assumed to be as general as
possible. With an appropriate dimensionless parametrization we represent the
field equations as an autonomous dynamical system which contains ultimately
only one arbitrary function $\chi (x)= 8 \pi \vert \eta \vert V(x/\sqrt{8
\pi})$ with $x=\sqrt{8 \pi}\phi$. Then, assuming the rather general properties
of $\chi(x)$, we analyze stationary points and their stability, as well as all
possible asymptotical regimes of the dynamical system. It has been shown that
for a broad class of $\chi(x)$ there exist attractors representing three
accelerated regimes of the Universe evolution, including de Sitter expansion
(or late-time inflation), the Little Rip scenario, and the Big Rip scenario. As
the specific examples, we consider a power-law potential
$V(\phi)=M^4(\phi/\phi_0)^\sigma$, Higgs-like potential
$V(\phi)=\frac{\lambda}{4}(\phi^2-\phi_0^2)^2$, and exponential potential
$V(\phi)=M^4 e^{-\phi/\phi_0}$.
",0,1,0,0,0,0
15620,15621,Learning Kolmogorov Models for Binary Random Variables,"  We summarize our recent findings, where we proposed a framework for learning
a Kolmogorov model, for a collection of binary random variables. More
specifically, we derive conditions that link outcomes of specific random
variables, and extract valuable relations from the data. We also propose an
algorithm for computing the model and show its first-order optimality, despite
the combinatorial nature of the learning problem. We apply the proposed
algorithm to recommendation systems, although it is applicable to other
scenarios. We believe that the work is a significant step toward interpretable
machine learning.
",0,0,0,1,0,0
875,876,Latent Geometry and Memorization in Generative Models,"  It can be difficult to tell whether a trained generative model has learned to
generate novel examples or has simply memorized a specific set of outputs. In
published work, it is common to attempt to address this visually, for example
by displaying a generated example and its nearest neighbor(s) in the training
set (in, for example, the L2 metric). As any generative model induces a
probability density on its output domain, we propose studying this density
directly. We first study the geometry of the latent representation and
generator, relate this to the output density, and then develop techniques to
compute and inspect the output density. As an application, we demonstrate that
""memorization"" tends to a density made of delta functions concentrated on the
memorized examples. We note that without first understanding the geometry, the
measurement would be essentially impossible to make.
",1,0,0,1,0,0
6010,6011,Simple Surveys: Response Retrieval Inspired by Recommendation Systems,"  In the last decade, the use of simple rating and comparison surveys has
proliferated on social and digital media platforms to fuel recommendations.
These simple surveys and their extrapolation with machine learning algorithms
shed light on user preferences over large and growing pools of items, such as
movies, songs and ads. Social scientists have a long history of measuring
perceptions, preferences and opinions, often over smaller, discrete item sets
with exhaustive rating or ranking surveys. This paper introduces simple surveys
for social science application. We ran experiments to compare the predictive
accuracy of both individual and aggregate comparative assessments using four
types of simple surveys: pairwise comparisons and ratings on 2, 5 and
continuous point scales in three distinct contexts: perceived Safety of Google
Streetview Images, Likeability of Artwork, and Hilarity of Animal GIFs. Across
contexts, we find that continuous scale ratings best predict individual
assessments but consume the most time and cognitive effort. Binary choice
surveys are quick and perform best to predict aggregate assessments, useful for
collective decision tasks, but poorly predict personalized preferences, for
which they are currently used by Netflix to recommend movies. Pairwise
comparisons, by contrast, perform well to predict personal assessments, but
poorly predict aggregate assessments despite being widely used to crowdsource
ideas and collective preferences. We demonstrate how findings from these
surveys can be visualized in a low-dimensional space that reveals distinct
respondent interpretations of questions asked in each context. We conclude by
reflecting on differences between sparse, incomplete simple surveys and their
traditional survey counterparts in terms of efficiency, information elicited
and settings in which knowing less about more may be critical for social
science.
",1,0,0,1,0,0
17030,17031,Non-perturbative positive Lyapunov exponent of Schrödinger equations and its applications to skew-shift,"  We first study the discrete Schrödinger equations with analytic potentials
given by a class of transformations. It is shown that if the coupling number is
large, then its logarithm equals approximately to the Lyapunov exponents. When
the transformation becomes the skew-shift, we prove that the Lyapunov exponent
is week Hölder continuous, and the spectrum satisfies Anderson Localization
and contains large intervals. Moreover, all of these conclusions are
non-perturbative.
",0,0,1,0,0,0
8604,8605,CO2 packing polymorphism under confinement in cylindrical nanopores,"  We investigate the effect of cylindrical nano-confinement on the phase
behaviour of a rigid model of carbon dioxide using both molecular dynamics and
well tempered metadynamics. To this aim we study a simplified pore model across
a parameter space comprising pore diameter, CO2-pore wall potential and CO2
density. In order to systematically identify ordering events within the pore
model we devise a generally applicable approach based on the analysis of the
distribution of intermolecular orientations. Our simulations suggest that,
while confinement in nano-pores inhibits the formation of known crystal
structures, it induces a remarkable variety of ordered packings unrelated to
their bulk counterparts, and favours the establishment of short range order in
the fluid phase. We summarise our findings by proposing a qualitative phase
diagram for this model.
",0,1,0,0,0,0
13204,13205,Inverse Reinforce Learning with Nonparametric Behavior Clustering,"  Inverse Reinforcement Learning (IRL) is the task of learning a single reward
function given a Markov Decision Process (MDP) without defining the reward
function, and a set of demonstrations generated by humans/experts. However, in
practice, it may be unreasonable to assume that human behaviors can be
explained by one reward function since they may be inherently inconsistent.
Also, demonstrations may be collected from various users and aggregated to
infer and predict user's behaviors. In this paper, we introduce the
Non-parametric Behavior Clustering IRL algorithm to simultaneously cluster
demonstrations and learn multiple reward functions from demonstrations that may
be generated from more than one behaviors. Our method is iterative: It
alternates between clustering demonstrations into different behavior clusters
and inverse learning the reward functions until convergence. It is built upon
the Expectation-Maximization formulation and non-parametric clustering in the
IRL setting. Further, to improve the computation efficiency, we remove the need
of completely solving multiple IRL problems for multiple clusters during the
iteration steps and introduce a resampling technique to avoid generating too
many unlikely clusters. We demonstrate the convergence and efficiency of the
proposed method through learning multiple driver behaviors from demonstrations
generated from a grid-world environment and continuous trajectories collected
from autonomous robot cars using the Gazebo robot simulator.
",1,0,0,0,0,0
13983,13984,Counting Quasi-Idempotent Irreducible Integral Matrices,"  Given any polynomial $p$ in $C[X]$, we show that the set of irreducible
matrices satisfying $p(A)=0$ is finite. In the specific case $p(X)=X^2-nX$, we
count the number of irreducible matrices in this set and analyze the arising
sequences and their asymptotics. Such matrices turn out to be related to
generalized compositions and generalized partitions.
",0,0,1,0,0,0
1367,1368,How hard is it to cross the room? -- Training (Recurrent) Neural Networks to steer a UAV,"  This work explores the feasibility of steering a drone with a (recurrent)
neural network, based on input from a forward looking camera, in the context of
a high-level navigation task. We set up a generic framework for training a
network to perform navigation tasks based on imitation learning. It can be
applied to both aerial and land vehicles. As a proof of concept we apply it to
a UAV (Unmanned Aerial Vehicle) in a simulated environment, learning to cross a
room containing a number of obstacles. So far only feedforward neural networks
(FNNs) have been used to train UAV control. To cope with more complex tasks, we
propose the use of recurrent neural networks (RNN) instead and successfully
train an LSTM (Long-Short Term Memory) network for controlling UAVs. Vision
based control is a sequential prediction problem, known for its highly
correlated input data. The correlation makes training a network hard,
especially an RNN. To overcome this issue, we investigate an alternative
sampling method during training, namely window-wise truncated backpropagation
through time (WW-TBPTT). Further, end-to-end training requires a lot of data
which often is not available. Therefore, we compare the performance of
retraining only the Fully Connected (FC) and LSTM control layers with networks
which are trained end-to-end. Performing the relatively simple task of crossing
a room already reveals important guidelines and good practices for training
neural control networks. Different visualizations help to explain the behavior
learned.
",1,0,0,0,0,0
12546,12547,A model bridging chimera state and explosive synchronization,"  Global and partial synchronization are the two distinctive forms of
synchronization in coupled oscillators and have been well studied in the past
decades. Recent attention on synchronization is focused on the chimera state
(CS) and explosive synchronization (ES), but little attention has been paid to
their relationship. We here study this topic by presenting a model to bridge
these two phenomena, which consists of two groups of coupled oscillators and
its coupling strength is adaptively controlled by a local order parameter. We
find that this model displays either CS or ES in two limits. In between the two
limits, this model exhibits both CS and ES, where CS can be observed for a
fixed coupling strength and ES appears when the coupling is increased
adiabatically. Moreover, we show both theoretically and numerically that there
are a variety of CS basin patterns for the case of identical oscillators,
depending on the distributions of both the initial order parameters and the
initial average phases. This model suggests a way to easily observe CS, in
contrast to others models having some (weak or strong) dependence on initial
conditions.
",0,1,0,0,0,0
5385,5386,Understanding low-temperature bulk transport in samarium hexaboride without relying on in-gap bulk states,"  We present a new model to explain the difference between the transport and
spectroscopy gaps in samarium hexaboride (SmB$_6$), which has been a mystery
for some time. We propose that SmB$_6$ can be modeled as an intrinsic
semiconductor with a depletion length that diverges at cryogenic temperatures.
In this model, we find a self-consistent solution to Poisson's equation in the
bulk, with boundary conditions based on Fermi energy pinning due to surface
charges. The solution yields band bending in the bulk; this explains the
difference between the two gaps because spectroscopic methods measure the gap
near the surface, while transport measures the average over the bulk. We also
connect the model to transport parameters, including the Hall coefficient and
thermopower, using semiclassical transport theory. The divergence of the
depletion length additionally explains the 10-12 K feature in data for these
parameters, demonstrating a crossover from bulk dominated transport above this
temperature to surface-dominated transport below this temperature. We find good
agreement between our model and a collection of transport data from 4-40 K.
This model can also be generalized to materials with similar band structure.
",0,1,0,0,0,0
17037,17038,Efficient Toxicity Prediction via Simple Features Using Shallow Neural Networks and Decision Trees,"  Toxicity prediction of chemical compounds is a grand challenge. Lately, it
achieved significant progress in accuracy but using a huge set of features,
implementing a complex blackbox technique such as a deep neural network, and
exploiting enormous computational resources. In this paper, we strongly argue
for the models and methods that are simple in machine learning characteristics,
efficient in computing resource usage, and powerful to achieve very high
accuracy levels. To demonstrate this, we develop a single task-based chemical
toxicity prediction framework using only 2D features that are less compute
intensive. We effectively use a decision tree to obtain an optimum number of
features from a collection of thousands of them. We use a shallow neural
network and jointly optimize it with decision tree taking both network
parameters and input features into account. Our model needs only a minute on a
single CPU for its training while existing methods using deep neural networks
need about 10 min on NVidia Tesla K40 GPU. However, we obtain similar or better
performance on several toxicity benchmark tasks. We also develop a cumulative
feature ranking method which enables us to identify features that can help
chemists perform prescreening of toxic compounds effectively.
",1,0,0,1,0,0
2015,2016,A general class of quasi-independence tests for left-truncated right-censored data,"  In survival studies, classical inferences for left-truncated data require
quasi-independence, a property that the joint density of truncation time and
failure time is factorizable into their marginal densities in the observable
region. The quasi-independence hypothesis is testable; many authors have
developed tests for left-truncated data with or without right-censoring. In
this paper, we propose a class of test statistics for testing the
quasi-independence which unifies the existing methods and generates new useful
statistics such as conditional Spearman's rank correlation coefficient.
Asymptotic normality of the proposed class of statistics is given. We show that
a new set of tests can be powerful under certain alternatives by theoretical
and empirical power comparison.
",0,0,0,1,0,0
19727,19728,The index of compact simple Lie groups,"  Let M be an irreducible Riemannian symmetric space. The index i(M) of M is
the minimal codimension of a (non-trivial) totally geodesic submanifold of M.
The purpose of this note is to determine the index i(M) for all irreducible
Riemannian symmetric spaces M of type (II) and (IV).
",0,0,1,0,0,0
13302,13303,Visualized Insights into the Optimization Landscape of Fully Convolutional Networks,"  Many image processing tasks involve image-to-image mapping, which can be
addressed well by fully convolutional networks (FCN) without any heavy
preprocessing. Although empirically designing and training FCNs can achieve
satisfactory results, reasons for the improvement in performance are slightly
ambiguous. Our study is to make progress in understanding their generalization
abilities through visualizing the optimization landscapes. The visualization of
objective functions is obtained by choosing a solution and projecting its
vicinity onto a 3D space. We compare three FCN-based networks (two existing
models and a new proposed in this paper for comparison) on multiple datasets.
It has been observed in practice that the connections from the pre-pooled
feature maps to the post-upsampled can achieve better results. We investigate
the cause and provide experiments to shows that the skip-layer connections in
FCN can promote flat optimization landscape, which is well known to generalize
better. Additionally, we explore the relationship between the models
generalization ability and loss surface under different batch sizes. Results
show that large-batch training makes the model converge to sharp minimizers
with chaotic vicinities while small-batch method leads the model to flat
minimizers with smooth and nearly convex regions. Our work may contribute to
insights and analysis for designing and training FCNs.
",1,0,0,1,0,0
20354,20355,"On the origin of the shallow and ""replica"" bands in FeSe monolayer superconductors","  We compare electronic structures of single FeSe layer films on SrTiO$_3$
substrate (FeSe/STO) and K$_x$Fe$_{2-y}$Se$_{2}$ superconductors obtained from
extensive LDA and LDA+DMFT calculations with the results of ARPES experiments.
It is demonstrated that correlation effects on Fe-3d states are sufficient in
principle to explain the formation of the shallow electron -- like bands at the
M(X)-point. However, in FeSe/STO these effects alone are apparently
insufficient for the simultaneous elimination of the hole -- like Fermi surface
around the $\Gamma$-point which is not observed in ARPES experiments. Detailed
comparison of ARPES detected and calculated quasiparticle bands shows
reasonable agreement between theory and experiment. Analysis of the bands with
respect to their origin and orbital composition shows, that for FeSe/STO system
the experimentally observed ""replica"" quasiparticle band at the M-point
(usually attributed to forward scattering interactions with optical phonons in
SrTiO$_3$ substrate) can be reasonably understood just as the LDA calculated
Fe-3d$_{xy}$ band, renormalized by electronic correlations. The only
manifestation of the substrate reduces to lifting the degeneracy between
Fe-3d$_{xz}$ and Fe-3d$_{yz}$ bands in the vicinity of M-point. For the case of
K$_x$Fe$_{2-y}$Se$_{2}$ most bands observed in ARPES can also be understood as
correlation renormalized Fe-3d LDA calculated bands, with overall semi --
quantitative agreement with LDA+DMFT calculations.
",0,1,0,0,0,0
20884,20885,Communication-Efficient and Decentralized Multi-Task Boosting while Learning the Collaboration Graph,"  We study the decentralized machine learning scenario where many users
collaborate to learn personalized models based on (i) their local datasets and
(ii) a similarity graph over the users' learning tasks. Our approach trains
nonlinear classifiers in a multi-task boosting manner without exchanging
personal data and with low communication costs. When background knowledge about
task similarities is not available, we propose to jointly learn the
personalized models and a sparse collaboration graph through an alternating
optimization procedure. We analyze the convergence rate, memory consumption and
communication complexity of our decentralized algorithms, and demonstrate the
benefits of our approach compared to competing techniques on synthetic and real
datasets.
",1,0,0,1,0,0
6947,6948,Geometric Biplane Graphs I: Maximal Graphs,"  We study biplane graphs drawn on a finite planar point set $S$ in general
position. This is the family of geometric graphs whose vertex set is $S$ and
can be decomposed into two plane graphs. We show that two maximal biplane
graphs---in the sense that no edge can be added while staying biplane---may
differ in the number of edges, and we provide an efficient algorithm for adding
edges to a biplane graph to make it maximal. We also study extremal properties
of maximal biplane graphs such as the maximum number of edges and the largest
maximum connectivity over $n$-element point sets.
",1,0,0,0,0,0
958,959,Boundedness of $\mathbb{Q}$-Fano varieties with degrees and alpha-invariants bounded from below,"  We show that $\mathbb{Q}$-Fano varieties of fixed dimension with
anti-canonical degrees and alpha-invariants bounded from below form a bounded
family. As a corollary, K-semistable $\mathbb{Q}$-Fano varieties of fixed
dimension with anti-canonical degrees bounded from below form a bounded family.
",0,0,1,0,0,0
1148,1149,Electron-Phonon Interaction in Ternary Rare-Earth Copper Antimonides LaCuSb2 and La(Cu0.8Ag0.2)Sb2 probed by Yanson Point-Contact Spectroscopy,"  Investigation of the electron-phonon interaction (EPI) in LaCuSb2 and
La(Cu0.8Ag0.2)Sb2 compounds by Yanson point-contact spectroscopy (PCS) has been
carried out. Point-contact spectra display a pronounced broad maximum in the
range of 10÷20 mV caused by EPI. Variation of the position of this maximum
is likely connected with anisotropic phonon spectrum in these layered
compounds. The absence of phonon features after the main maximum allows the
assessment of the Debye energy of about 40 meV. The EPI constant for the
LaCuSb2 compound was estimated to be {\lambda}=0.2+/-0.03. A zero-bias minimum
in differential resistance for the latter compound is observed for some point
contacts, which vanishes at about 6 K, pointing to the formation of
superconducting phase under point contact, while superconducting critical
temperature of the bulk sample is only 1K.
",0,1,0,0,0,0
16818,16819,Simulation chain and signal classification for acoustic neutrino detection in seawater,"  Acoustic neutrino detection is a promising approach to extend the energy
range of neutrino telescopes to energies beyond $10^{18}$\,eV. Currently
operational and planned water-Cherenkov neutrino telescopes, most notably
KM3NeT, include acoustic sensors in addition to the optical ones. These
acoustic sensors could be used as instruments for acoustic detection, while
their main purpose is the position calibration of the detection units. In this
article, a Monte Carlo simulation chain for acoustic detectors will be
presented, covering the initial interaction of the neutrino up to the signal
classification of recorded events. The ambient and transient background in the
simulation was implemented according to data recorded by the acoustic set-up
AMADEUS inside the ANTARES detector. The effects of refraction on the neutrino
signature in the detector are studied, and a classification of the recorded
events is implemented. As bipolar waveforms similar to those of the expected
neutrino signals are also emitted from other sound sources, additional features
like the geometrical shape of the propagation have to be considered for the
signal classification. This leads to a large improvement of the background
suppression by almost two orders of magnitude, since a flat cylindrical
""pancake"" propagation pattern is a distinctive feature of neutrino signals. An
overview of the simulation chain and the signal classification will be
presented and preliminary studies of the performance of the classification will
be discussed.
",0,1,0,0,0,0
2036,2037,KATE: K-Competitive Autoencoder for Text,"  Autoencoders have been successful in learning meaningful representations from
image datasets. However, their performance on text datasets has not been widely
studied. Traditional autoencoders tend to learn possibly trivial
representations of text documents due to their confounding properties such as
high-dimensionality, sparsity and power-law word distributions. In this paper,
we propose a novel k-competitive autoencoder, called KATE, for text documents.
Due to the competition between the neurons in the hidden layer, each neuron
becomes specialized in recognizing specific data patterns, and overall the
model can learn meaningful representations of textual data. A comprehensive set
of experiments show that KATE can learn better representations than traditional
autoencoders including denoising, contractive, variational, and k-sparse
autoencoders. Our model also outperforms deep generative models, probabilistic
topic models, and even word representation models (e.g., Word2Vec) in terms of
several downstream tasks such as document classification, regression, and
retrieval.
",1,0,0,1,0,0
4801,4802,Ultracold atoms in multiple-radiofrequency dressed adiabatic potentials,"  We present the first experimental demonstration of a multiple-radiofrequency
dressed potential for the configurable magnetic confinement of ultracold atoms.
We load cold $^{87}$Rb atoms into a double well potential with an adjustable
barrier height, formed by three radiofrequencies applied to atoms in a static
quadrupole magnetic field. Our multiple-radiofrequency approach gives precise
control over the double well characteristics, including the depth of individual
wells and the height of the barrier, and enables reliable transfer of atoms
between the available trapping geometries. We have characterised the
multiple-radiofrequency dressed system using radiofrequency spectroscopy,
finding good agreement with the eigenvalues numerically calculated using
Floquet theory. This method creates trapping potentials that can be
reconfigured by changing the amplitudes, polarizations and frequencies of the
applied dressing fields, and easily extended with additional dressing
frequencies.
",0,1,0,0,0,0
4134,4135,A conservative scheme for electromagnetic simulation of magnetized plasmas with kinetic electrons,"  A conservative scheme has been formulated and verified for gyrokinetic
particle simulations of electromagnetic waves and instabilities in magnetized
plasmas. An electron continuity equation derived from drift kinetic equation is
used to time advance electron density perturbation by using the perturbed
mechanical flow calculated from the parallel vector potential, and the parallel
vector potential is solved by using the perturbed canonical flow from the
perturbed distribution function. In gyrokinetic particle simulations using this
new scheme, shear Alfvén wave dispersion relation in shearless slab and
continuum damping in sheared cylinder have been recovered. The new scheme
overcomes the stringent requirement in conventional perturbative simulation
method that perpendicular grid size needs to be as small as electron
collisionless skin depth even for the long wavelength Alfvén waves. The new
scheme also avoids the problem in conventional method that an unphysically
large parallel electric field arises due to the inconsistency between
electrostatic potential calculated from the perturbed density and vector
potential calculated from the perturbed canonical flow. Finally, the
gyrokinetic particle simulations of the Alfvén waves in sheared cylinder have
superior numerical properties compared with the fluid simulations, which suffer
from numerical difficulties associated with singular mode structures.
",0,1,0,0,0,0
8426,8427,"Invariant Bianchi type I models in $f\left(R,T\right)$ Gravity","  In this paper, we search the existence of invariant solutions of Bianchi type
I space-time in the context of $f\left(R,T\right)$ gravity. The exact solution
of the Einstein's field equations are derived by using Lie point symmetry
analysis method that yield two models of invariant universe for symmetries
$X^{(1)}$ and $X^{(3)}$. The model with symmetries $X^{(1)}$ begins with big
bang singularity while the model with symmetries $X^{(3)}$ does not favour the
big bang singularity. Under this specification, we find out at set of singular
and non singular solution of Bianchi type I model which present several other
physically valid features within the framework of $f\left(R,T\right)$.
",0,1,0,0,0,0
9556,9557,The clock of chemical evolution,"  Chemical evolution is essential in understanding the origins of life. We
present a theory for the evolution of molecule masses and show that small
molecules grow by random diffusion and large molecules by a preferential
attachment process leading eventually to life's molecules. It reproduces
correctly the distribution of molecules found via mass spectroscopy for the
Murchison meteorite and estimates the start of chemical evolution back to 12.8
billion years following the birth of stars and supernovae. From the Frontier
mass between the random and preferential attachment dynamics the birth time of
molecule families can be estimated. Amino acids emerge about 165 million years
after the start of evolution. Using the scaling of reaction rates with the
distance of the molecules in space we recover correctly the few days emergence
time of amino acids in the Miller-Urey experiment. The distribution of
interstellar and extragalactic molecules are both consistent with the
evolutionary mass distribution, and their age is estimated to 108 and 65
million years after the start of evolution. From the model, we can determine
the number of different molecule compositions at the time of the creation of
Earth to be 1.6 million and the number of molecule compositions in interstellar
space to a mere 719.
",0,0,0,0,1,0
8251,8252,Thermalization in simple metals: The role of electron-phonon and phonon-phonon scatterings,"  We study the electron and phonon thermalization in simple metals excited by a
laser pulse. The thermalization is investigated numerically by solving the
Boltzmann transport equation taking into account all the relevant scattering
mechanism: the electron-electron, electron-phonon (e-ph), phonon-electron
(ph-e), and phonon-phonon (ph-ph) scatterings. In the initial stage of the
relaxation, most of the excitation energy is transferred from the electrons to
phonons through the e-ph scattering. This creates hot high-frequency phonons
due to the ph-e scatterings, followed by an energy redistribution between
phonon subsystems through the ph-ph scatterings. This yields an overshoot of
the total longitudinal-acoustic phonon energy at a time, across which a
crossover occurs from a nonequilibrium state, where the e-ph and ph-e
scatterings frequently occur, to a state, where the ph-ph scattering occurs to
reach a thermal equilibrium. This picture is quite different from the scenario
of the well-known two-temperature model (2TM). The behavior of the relaxation
dynamics is compared with those calculated by several models, including the
2TM, the four-temperature model, and nonequilibrium electron or phonon models.
The relationship between the relaxation time and the initial distribution
function is also discussed.
",0,1,0,0,0,0
2336,2337,Facets on the convex hull of $d$-dimensional Brownian and Lévy motion,"  For stationary, homogeneous Markov processes (viz., Lévy processes,
including Brownian motion) in dimension $d\geq 3$, we establish an exact
formula for the average number of $(d-1)$-dimensional facets that can be
defined by $d$ points on the process's path. This formula defines a
universality class in that it is independent of the increments' distribution,
and it admits a closed form when $d=3$, a case which is of particular interest
for applications in biophysics, chemistry and polymer science.
We also show that the asymptotical average number of facets behaves as
$\langle \mathcal{F}_T^{(d)}\rangle \sim 2\left[\ln \left( T/\Delta
t\right)\right]^{d-1}$, where $T$ is the total duration of the motion and
$\Delta t$ is the minimum time lapse separating points that define a facet.
",0,1,1,0,0,0
1518,1519,A SAT+CAS Approach to Finding Good Matrices: New Examples and Counterexamples,"  We enumerate all circulant good matrices with odd orders divisible by 3 up to
order 70. As a consequence of this we find a previously overlooked set of good
matrices of order 27 and a new set of good matrices of order 57. We also find
that circulant good matrices do not exist in the orders 51, 63, and 69, thereby
finding three new counterexamples to the conjecture that such matrices exist in
all odd orders. Additionally, we prove a new relationship between the entries
of good matrices and exploit this relationship in our enumeration algorithm.
Our method applies the SAT+CAS paradigm of combining computer algebra
functionality with modern SAT solvers to efficiently search large spaces which
are specified by both algebraic and logical constraints.
",1,0,0,0,0,0
24,25,The Knaster-Tarski theorem versus monotone nonexpansive mappings,"  Let $X$ be a partially ordered set with the property that each family of
order intervals of the form $[a,b],[a,\rightarrow )$ with the finite
intersection property has a nonempty intersection. We show that every directed
subset of $X$ has a supremum. Then we apply the above result to prove that if
$X$ is a topological space with a partial order $\preceq $ for which the order
intervals are compact, $\mathcal{F}$ a nonempty commutative family of monotone
maps from $X$ into $X$ and there exists $c\in X$ such that $c\preceq Tc$ for
every $T\in \mathcal{F}$, then the set of common fixed points of $\mathcal{F}$
is nonempty and has a maximal element. The result, specialized to the case of
Banach spaces gives a general fixed point theorem that drops almost all
assumptions from the recent results in this area. An application to the theory
of integral equations of Urysohn's type is also given.
",0,0,1,0,0,0
12689,12690,"Imaging a Central Ionized Component, a Narrow Ring, and the CO Snowline in the Multi-Gapped Disk of HD 169142","  We report Very Large Array observations at 7 mm, 9 mm, and 3 cm toward the
pre-transitional disk of the Herbig Ae star HD 169142. These observations have
allowed us to study the mm emission of this disk with the highest angular
resolution so far ($0\rlap.""12\times0\rlap.""09$, or 14 au$\times$11 au, at 7
mm). Our 7 and 9 mm images show a narrow ring of emission at a radius of
$\sim25$ au tracing the outer edge of the inner gap. This ring presents an
asymmetric morphology that could be produced by dynamical interactions between
the disk and forming planets. Additionally, the azimuthally averaged radial
intensity profiles of the 7 and 9 mm images confirm the presence of the
previously reported gap at $\sim45$ au, and reveal a new gap at $\sim85$ au. We
analyzed archival DCO$^+$(3-2) and C$^{18}$O(2-1) ALMA observations, showing
that the CO snowline is located very close to this third outer gap. This
suggests that growth and accumulation of large dust grains close to the CO
snowline could be the mechanism responsible for this proposed outer gap.
Finally, a compact source of emission is detected at 7 mm, 9 mm, and 3 cm
toward the center of the disk. Its flux density and spectral index indicate
that it is dominated by free-free emission from ionized gas, which could be
associated with either the photoionization of the inner disk, an independent
object, or an ionized jet.
",0,1,0,0,0,0
9103,9104,Achieving Privacy in the Adversarial Multi-Armed Bandit,"  In this paper, we improve the previously best known regret bound to achieve
$\epsilon$-differential privacy in oblivious adversarial bandits from
$\mathcal{O}{(T^{2/3}/\epsilon)}$ to $\mathcal{O}{(\sqrt{T} \ln T /\epsilon)}$.
This is achieved by combining a Laplace Mechanism with EXP3. We show that
though EXP3 is already differentially private, it leaks a linear amount of
information in $T$. However, we can improve this privacy by relying on its
intrinsic exponential mechanism for selecting actions. This allows us to reach
$\mathcal{O}{(\sqrt{\ln T})}$-DP, with a regret of $\mathcal{O}{(T^{2/3})}$
that holds against an adaptive adversary, an improvement from the best known of
$\mathcal{O}{(T^{3/4})}$. This is done by using an algorithm that run EXP3 in a
mini-batch loop. Finally, we run experiments that clearly demonstrate the
validity of our theoretical analysis.
",1,0,0,0,0,0
1258,1259,"Scaling up the software development process, a case study highlighting the complexities of large team software development","  Diamond Light Source is the UK's National Synchrotron Facility and as such
provides access to world class experimental services for UK and international
researchers. As a user facility, that is one that focuses on providing a good
user experience to our varied visitors, Diamond invests heavily in software
infrastructure and staff. Over 100 members of the 600 strong workforce consider
software development as a significant tool to help them achieve their primary
role. These staff work on a diverse number of different software packages,
providing support for installation and configuration, maintenance and bug
fixing, as well as additional research and development of software when
required.
This talk focuses on one of the software projects undertaken to unify and
improve the user experience of several experiments. The ""mapping project"" is a
large 2 year, multi group project targeting the collection and processing
experiments which involve scanning an X-ray beam over a sample and building up
an image of that sample, similar to the way that google maps bring together
small pieces of information to produce a full map of the world. The project
itself is divided into several work packages, ranging from teams of one to 5 or
6 in size, with varying levels of time commitment to the project. This paper
aims to explore one of these work packages as a case study, highlighting the
experiences of the project team, the methodologies employed, their outcomes,
and the lessons learnt from the experience.
",1,0,0,0,0,0
14839,14840,Probabilistic Search for Structured Data via Probabilistic Programming and Nonparametric Bayes,"  Databases are widespread, yet extracting relevant data can be difficult.
Without substantial domain knowledge, multivariate search queries often return
sparse or uninformative results. This paper introduces an approach for
searching structured data based on probabilistic programming and nonparametric
Bayes. Users specify queries in a probabilistic language that combines standard
SQL database search operators with an information theoretic ranking function
called predictive relevance. Predictive relevance can be calculated by a fast
sparse matrix algorithm based on posterior samples from CrossCat, a
nonparametric Bayesian model for high-dimensional, heterogeneously-typed data
tables. The result is a flexible search technique that applies to a broad class
of information retrieval problems, which we integrate into BayesDB, a
probabilistic programming platform for probabilistic data analysis. This paper
demonstrates applications to databases of US colleges, global macroeconomic
indicators of public health, and classic cars. We found that human evaluators
often prefer the results from probabilistic search to results from a standard
baseline.
",1,0,0,1,0,0
6661,6662,On the periodicity problem of residual r-Fubini sequences,"  For any positive integer $r$, the $r$-Fubini number with parameter $n$,
denoted by $F_{n,r}$, is equal to the number of ways that the elements of a set
with $n+r$ elements can be weak ordered such that the $r$ least elements are in
distinct orders. In this article we focus on the sequence of residues of the
$r$-Fubini numbers modulo a positive integer $s$ and show that this sequence is
periodic and then, exhibit how to calculate its period length. As an extra
result, an explicit formula for the $r$-Stirling numbers is obtained which is
frequently used in calculations.
",0,0,1,0,0,0
2573,2574,On measures of edge-uncolorability of cubic graphs: A brief survey and some new results,"  There are many hard conjectures in graph theory, like Tutte's 5-flow
conjecture, and the 5-cycle double cover conjecture, which would be true in
general if they would be true for cubic graphs. Since most of them are
trivially true for 3-edge-colorable cubic graphs, cubic graphs which are not
3-edge-colorable, often called {\em snarks}, play a key role in this context.
Here, we survey parameters measuring how far apart a non 3-edge-colorable graph
is from being 3-edge-colorable. We study their interrelation and prove some new
results. Besides getting new insight into the structure of snarks, we show that
such measures give partial results with respect to these important conjectures.
The paper closes with a list of open problems and conjectures.
",0,0,1,0,0,0
6378,6379,High brightness electron beam for radiation therapy: A new approach,"  I propose to use high brightness electron beam with 1 to 100 MeV energy as
tool to combat tumor or cancerous tissues in deep part of body. The method is
to directly deliver the electron beam to the tumor site via a small tube that
connected to a high brightness electron beam accelerator that is commonly
available around the world. Here I gave a basic scheme on the principle, I
believe other issues people raises will be solved easily for those who are
interested in solving the problems.
",0,1,0,0,0,0
14227,14228,Composition and decomposition of GANs,"  In this work, we propose a composition/decomposition framework for
adversarially training generative models on composed data - data where each
sample can be thought of as being constructed from a fixed number of
components. In our framework, samples are generated by sampling components from
component generators and feeding these components to a composition function
which combines them into a ""composed sample"". This compositional training
approach improves the modularity, extensibility and interpretability of
Generative Adversarial Networks (GANs) - providing a principled way to
incrementally construct complex models out of simpler component models, and
allowing for explicit ""division of responsibility"" between these components.
Using this framework, we define a family of learning tasks and evaluate their
feasibility on two datasets in two different data modalities (image and text).
Lastly, we derive sufficient conditions such that these compositional
generative models are identifiable. Our work provides a principled approach to
building on pre-trained generative models or for exploiting the compositional
nature of data distributions to train extensible and interpretable models.
",1,0,0,1,0,0
13903,13904,Semi-Supervised and Active Few-Shot Learning with Prototypical Networks,"  We consider the problem of semi-supervised few-shot classification where a
classifier needs to adapt to new tasks using a few labeled examples and
(potentially many) unlabeled examples. We propose a clustering approach to the
problem. The features extracted with Prototypical Networks are clustered using
$K$-means with the few labeled examples guiding the clustering process. We note
that in many real-world applications the adaptation performance can be
significantly improved by requesting the few labels through user feedback. We
demonstrate good performance of the active adaptation strategy using image
data.
",1,0,0,1,0,0
15069,15070,Efficient Probabilistic Performance Bounds for Inverse Reinforcement Learning,"  In the field of reinforcement learning there has been recent progress towards
safety and high-confidence bounds on policy performance. However, to our
knowledge, no practical methods exist for determining high-confidence policy
performance bounds in the inverse reinforcement learning setting---where the
true reward function is unknown and only samples of expert behavior are given.
We propose a sampling method based on Bayesian inverse reinforcement learning
that uses demonstrations to determine practical high-confidence upper bounds on
the $\alpha$-worst-case difference in expected return between any evaluation
policy and the optimal policy under the expert's unknown reward function. We
evaluate our proposed bound on both a standard grid navigation task and a
simulated driving task and achieve tighter and more accurate bounds than a
feature count-based baseline. We also give examples of how our proposed bound
can be utilized to perform risk-aware policy selection and risk-aware policy
improvement. Because our proposed bound requires several orders of magnitude
fewer demonstrations than existing high-confidence bounds, it is the first
practical method that allows agents that learn from demonstration to express
confidence in the quality of their learned policy.
",1,0,0,1,0,0
13250,13251,Reverse iterative volume sampling for linear regression,"  We study the following basic machine learning task: Given a fixed set of
$d$-dimensional input points for a linear regression problem, we wish to
predict a hidden response value for each of the points. We can only afford to
attain the responses for a small subset of the points that are then used to
construct linear predictions for all points in the dataset. The performance of
the predictions is evaluated by the total square loss on all responses (the
attained as well as the hidden ones). We show that a good approximate solution
to this least squares problem can be obtained from just dimension $d$ many
responses by using a joint sampling technique called volume sampling. Moreover,
the least squares solution obtained for the volume sampled subproblem is an
unbiased estimator of optimal solution based on all n responses. This
unbiasedness is a desirable property that is not shared by other common subset
selection techniques.
Motivated by these basic properties, we develop a theoretical framework for
studying volume sampling, resulting in a number of new matrix expectation
equalities and statistical guarantees which are of importance not only to least
squares regression but also to numerical linear algebra in general. Our methods
also lead to a regularized variant of volume sampling, and we propose the first
efficient algorithms for volume sampling which make this technique a practical
tool in the machine learning toolbox. Finally, we provide experimental evidence
which confirms our theoretical findings.
",0,0,0,1,0,0
13823,13824,Charge Berezinskii-Kosterlitz-Thouless transition in superconducting NbTiN films,"  A half-century after the discovery of the superconductor-insulator transition
(SIT), one of the fundamental predictions of the theory, the charge
Berezinskii-Kosterlitz-Thouless (BKT) transition that is expected to occur at
the insulating side of the SIT, has remained unobserved. The charge BKT
transition is a phenomenon dual to the vortex BKT transition, which is at the
heart of the very existence of two-dimensional superconductivity as a
zero-resistance state appearing at finite temperatures. The dual picture points
to the possibility of the existence of a superinsulating state endowed with
zero conductance at finite temperature. Here, we report the observation of the
charge BKT transition on the insulating side of the SIT, identified by the
critical behavior of the resistance. We find that the critical temperature of
the charge BKT transition depends on the magnetic field exhibiting first the
fast growth and then passing through the maximum at fields much less than the
upper critical field. Finally, we ascertain the effects of the finite
electrostatic screening length and its divergence at the magnetic field-tuned
approach to the superconductor-insulator transition.
",0,1,0,0,0,0
17619,17620,Space-time crystal and space-time group,"  Crystal structures and the Bloch theorem play a fundamental role in condensed
matter physics. We extend the static crystal to the dynamic ""space-time""
crystal characterized by the general intertwined space-time periodicities in
$D+1$ dimensions, which include both the static crystal and the Floquet crystal
as special cases. A new group structure dubbed ""space-time"" group is
constructed to describe the discrete symmetries of space-time crystal. Compared
to space and magnetic groups, space-time group is augmented by ""time-screw""
rotations and ""time-glide"" reflections involving fractional translations along
the time direction. A complete classification of the 13 space-time groups in
1+1D is performed. The Kramers-type degeneracy can arise from the glide
time-reversal symmetry without the half-integer spinor structure, which
constrains the winding number patterns of spectral dispersions. In 2+1D,
non-symmorphic space-time symmetries enforce spectral degeneracies, leading to
protected Floquet semi-metal states. Our work provides a general framework for
further studying topological properties of the $D+1$ dimensional space-time
crystal.
",0,1,0,0,0,0
15396,15397,"Nonlinear Zeeman effect, line shapes and optical pumping in electromagnetically induced transparency","  We perform Zeeman spectroscopy on a Rydberg electromagnetically induced
transparency (EIT) system in a room-temperature Cs vapor cell, in magnetic
fields up to 50~Gauss and for several polarization configurations. The magnetic
interactions of the $\vert 6S_{1/2}, F_g=4 \rangle$ ground, $\vert 6P_{3/2},
F_e=5 \rangle$ intermediate, and $\vert 33S_{1/2} \rangle$ Rydberg states that
form the ladder-type EIT system are in the linear Zeeman, quadratic Zeeman, and
the deep hyperfine Paschen-Back regimes, respectively. Starting in magnetic
fields of about 5~Gauss, the spectra develop an asymmetry that becomes
paramount in fields $\gtrsim40$~Gauss. We use a quantum Monte Carlo
wave-function approach to quantitatively model the spectra. Simulated spectra
are in good agreement with experimental data. The asymmetry in the spectra is,
in part, due to level shifts caused by the quadratic Zeeman effect, but it also
reflects the complicated interplay between optical pumping and EIT in the
magnetic field. Relevance to measurement applications is discussed. %The
simulations are also used to study optical pumping in the magnetic field and to
investigate the interplay between optical pumping and EIT, which reduces photon
scattering and optical pumping.
",0,1,0,0,0,0
13271,13272,Localizing virtual structure sheaves by cosections,"  We construct a cosection localized virtual structure sheaf when a
Deligne-Mumford stack is equipped with a perfect obstruction theory and a
cosection of the obstruction sheaf.
",0,0,1,0,0,0
17725,17726,Translations in the exponential Orlicz space with Gaussian weight,"  We study the continuity of space translations on non-parametric exponential
families based on the exponential Orlicz space with Gaussian reference density.
",0,0,1,1,0,0
7976,7977,Overcoming data scarcity with transfer learning,"  Despite increasing focus on data publication and discovery in materials
science and related fields, the global view of materials data is highly sparse.
This sparsity encourages training models on the union of multiple datasets, but
simple unions can prove problematic as (ostensibly) equivalent properties may
be measured or computed differently depending on the data source. These hidden
contextual differences introduce irreducible errors into analyses,
fundamentally limiting their accuracy. Transfer learning, where information
from one dataset is used to inform a model on another, can be an effective tool
for bridging sparse data while preserving the contextual differences in the
underlying measurements. Here, we describe and compare three techniques for
transfer learning: multi-task, difference, and explicit latent variable
architectures. We show that difference architectures are most accurate in the
multi-fidelity case of mixed DFT and experimental band gaps, while multi-task
most improves classification performance of color with band gaps. For
activation energies of steps in NO reduction, the explicit latent variable
method is not only the most accurate, but also enjoys cancellation of errors in
functions that depend on multiple tasks. These results motivate the publication
of high quality materials datasets that encode transferable information,
independent of industrial or academic interest in the particular labels, and
encourage further development and application of transfer learning methods to
materials informatics problems.
",1,0,0,1,0,0
1039,1040,Presymplectic convexity and (ir)rational polytopes,"  In this paper, we extend the Atiyah--Guillemin--Sternberg convexity theorem
and Delzant's classification of symplectic toric manifolds to presymplectic
manifolds. We also define and study the Morita equivalence of presymplectic
toric manifolds and of their corresponding framed momentum polytopes, which may
be rational or non-rational. Toric orbifolds, quasifolds and non-commutative
toric varieties may be viewed as the quotient of our presymplectic toric
manifolds by the kernel isotropy foliation of the presymplectic form.
",0,0,1,0,0,0
11945,11946,Control Capacity,"  Feedback control actively dissipates uncertainty from a dynamical system by
means of actuation. We develop a notion of ""control capacity"" that gives a
fundamental limit (in bits) on the rate at which a controller can dissipate the
uncertainty from a system, i.e. stabilize to a known fixed point. We give a
computable single-letter characterization of control capacity for memoryless
stationary scalar multiplicative actuation channels. Control capacity allows us
to answer questions of stabilizability for scalar linear systems: a system with
actuation uncertainty is stabilizable if and only if the control capacity is
larger than the log of the unstable open-loop eigenvalue.
For second-moment senses of stability, we recover the classic uncertainty
threshold principle result. However, our definition of control capacity can
quantify the stabilizability limits for any moment of stability. Our
formulation parallels the notion of Shannon's communication capacity, and thus
yields both a strong converse and a way to compute the value of
side-information in control. The results in our paper are motivated by
bit-level models for control that build on the deterministic models that are
widely used to understand information flows in wireless network information
theory.
",1,0,1,0,0,0
2275,2276,NullHop: A Flexible Convolutional Neural Network Accelerator Based on Sparse Representations of Feature Maps,"  Convolutional neural networks (CNNs) have become the dominant neural network
architecture for solving many state-of-the-art (SOA) visual processing tasks.
Even though Graphical Processing Units (GPUs) are most often used in training
and deploying CNNs, their power efficiency is less than 10 GOp/s/W for
single-frame runtime inference. We propose a flexible and efficient CNN
accelerator architecture called NullHop that implements SOA CNNs useful for
low-power and low-latency application scenarios. NullHop exploits the sparsity
of neuron activations in CNNs to accelerate the computation and reduce memory
requirements. The flexible architecture allows high utilization of available
computing resources across kernel sizes ranging from 1x1 to 7x7. NullHop can
process up to 128 input and 128 output feature maps per layer in a single pass.
We implemented the proposed architecture on a Xilinx Zynq FPGA platform and
present results showing how our implementation reduces external memory
transfers and compute time in five different CNNs ranging from small ones up to
the widely known large VGG16 and VGG19 CNNs. Post-synthesis simulations using
Mentor Modelsim in a 28nm process with a clock frequency of 500 MHz show that
the VGG19 network achieves over 450 GOp/s. By exploiting sparsity, NullHop
achieves an efficiency of 368%, maintains over 98% utilization of the MAC
units, and achieves a power efficiency of over 3TOp/s/W in a core area of
6.3mm$^2$. As further proof of NullHop's usability, we interfaced its FPGA
implementation with a neuromorphic event camera for real time interactive
demonstrations.
",1,0,0,0,0,0
9167,9168,On the evolution of galaxy spin in a cosmological hydrodynamic simulation of galaxy clusters,"  The traditional view of the morphology-spin connection is being challenged by
recent integral-field-unit observations, as the majority of early-type galaxies
are found to have a rotational component that is often as large as a dispersion
component. Mergers are often suspected to be critical in galaxy spin evolution,
yet the details of their roles are still unclear. We present the first results
on the spin evolution of galaxies in cluster environments through a
cosmological hydrodynamic simulation. Galaxies spin down globally with cosmic
evolution. Major (mass ratios > 1/4) and minor (1/4 $\geq$ mass ratios > 1/50)
mergers are important contributors to the spin down in particular in massive
galaxies. Minor mergers appear to have stronger cumulative effects than major
mergers. Surprisingly, the dominant driver of galaxy spin down seems to be
environmental effects rather than mergers. However, since multiple processes
act in combination, it is difficult to separate their individual roles. We
briefly discuss the caveats and future studies that are called for.
",0,1,0,0,0,0
20069,20070,Algebraic Aspects of Conditional Independence and Graphical Models,"  This chapter of the forthcoming Handbook of Graphical Models contains an
overview of basic theorems and techniques from algebraic geometry and how they
can be applied to the study of conditional independence and graphical models.
It also introduces binomial ideals and some ideas from real algebraic geometry.
When random variables are discrete or Gaussian, tools from computational
algebraic geometry can be used to understand implications between conditional
independence statements. This is accomplished by computing primary
decompositions of conditional independence ideals. As examples the chapter
presents in detail the graphical model of a four cycle and the intersection
axiom, a certain implication of conditional independence statements. Another
important problem in the area is to determine all constraints on a graphical
model, for example, equations determined by trek separation. The full set of
equality constraints can be determined by computing the model's vanishing
ideal. The chapter illustrates these techniques and ideas with examples from
the literature and provides references for further reading.
",0,0,1,1,0,0
15528,15529,The cauchy problem for radially symmetric homogeneous boltzmann equation with shubin class initial datum and gelfand-shilov smoothing effect,"  In this paper, we study the Cauchy problem for radially symmetric homogeneous
non-cutoff Boltzmann equation with Maxwellian molecules, the initial datum
belongs to Shubin space of the negative index which can be characterized by
spectral decomposition of the harmonic oscillators. The Shubin space of the
negative index contains the measure functions. Based on this spectral
decomposition, we construct the weak solution with Shubin class initial datum,
we also prove that the Cauchy problem enjoys Gelfand-Shilov smoothing effect,
meaning that the smoothing properties are the same as the Cauchy problem
defined by the evolution equation associated to a fractional harmonic
oscillator.
",0,0,1,0,0,0
8004,8005,Dixmier traces and residues on weak operator ideals,"  We develop the theory of modulated operators in general principal ideals of
compact operators. For Laplacian modulated operators we establish Connes' trace
formula in its local Euclidean model and a global version thereof. It expresses
Dixmier traces in terms of the vector-valued Wodzicki residue. We demonstrate
the applicability of our main results in the context of log-classical
pseudo-differential operators, studied by Lesch, and a class of operators
naturally appearing in noncommutative geometry.
",0,0,1,0,0,0
2329,2330,Correcting rural building annotations in OpenStreetMap using convolutional neural networks,"  Rural building mapping is paramount to support demographic studies and plan
actions in response to crisis that affect those areas. Rural building
annotations exist in OpenStreetMap (OSM), but their quality and quantity are
not sufficient for training models that can create accurate rural building
maps. The problems with these annotations essentially fall into three
categories: (i) most commonly, many annotations are geometrically misaligned
with the updated imagery; (ii) some annotations do not correspond to buildings
in the images (they are misannotations or the buildings have been destroyed);
and (iii) some annotations are missing for buildings in the images (the
buildings were never annotated or were built between subsequent image
acquisitions). First, we propose a method based on Markov Random Field (MRF) to
align the buildings with their annotations. The method maximizes the
correlation between annotations and a building probability map while enforcing
that nearby buildings have similar alignment vectors. Second, the annotations
with no evidence in the building probability map are removed. Third, we present
a method to detect non-annotated buildings with predefined shapes and add their
annotation. The proposed methodology shows considerable improvement in accuracy
of the OSM annotations for two regions of Tanzania and Zimbabwe, being more
accurate than state-of-the-art baselines.
",1,0,0,0,0,0
3385,3386,Information Theory of Data Privacy,"  By combining Shannon's cryptography model with an assumption to the lower
bound of adversaries' uncertainty to the queried dataset, we develop a secure
Bayesian inference-based privacy model and then in some extent answer Dwork et
al.'s question [1]: ""why Bayesian risk factors are the right measure for
privacy loss"".
This model ensures an adversary can only obtain little information of each
individual from the model's output if the adversary's uncertainty to the
queried dataset is larger than the lower bound. Importantly, the assumption to
the lower bound almost always holds, especially for big datasets. Furthermore,
this model is flexible enough to balance privacy and utility: by using four
parameters to characterize the assumption, there are many approaches to balance
privacy and utility and to discuss the group privacy and the composition
privacy properties of this model.
",1,0,0,0,0,0
16983,16984,Primordial black holes from inflaton and spectator field perturbations in a matter-dominated era,"  We study production of primordial black holes (PBHs) during an early
matter-dominated phase. As a source of perturbations, we consider either the
inflaton field with a running spectral index or a spectator field that has a
blue spectrum and thus provides a significant contribution to the PBH
production at small scales. First, we identify the region of the parameter
space where a significant fraction of the observed dark matter can be produced,
taking into account all current PBH constraints. Then, we present constraints
on the amplitude and spectral index of the spectator field as a function of the
reheating temperature. We also derive constraints on the running of the
inflaton spectral index, ${\rm d}n/{\rm d}{\rm ln}k \lesssim -0.002$, which are
comparable to those from the Planck satellite for a scenario where the
spectator field is absent.
",0,1,0,0,0,0
4285,4286,Low spin wave damping in the insulating chiral magnet Cu$_{2}$OSeO$_{3}$,"  Chiral magnets with topologically nontrivial spin order such as Skyrmions
have generated enormous interest in both fundamental and applied sciences. We
report broadband microwave spectroscopy performed on the insulating chiral
ferrimagnet Cu$_{2}$OSeO$_{3}$. For the damping of magnetization dynamics we
find a remarkably small Gilbert damping parameter of about $1\times10^{-4}$ at
5 K. This value is only a factor of 4 larger than the one reported for the best
insulating ferrimagnet yttrium iron garnet. We detect a series of sharp
resonances and attribute them to confined spin waves in the mm-sized samples.
Considering the small damping, insulating chiral magnets turn out to be
promising candidates when exploring non-collinear spin structures for high
frequency applications.
",0,1,0,0,0,0
5540,5541,Detection of an Optical Counterpart to the ALFALFA Ultra-compact High Velocity Cloud AGC 249525,"  We report on the detection at $>$98% confidence of an optical counterpart to
AGC 249525, an Ultra-Compact High Velocity Cloud (UCHVC) discovered by the
ALFALFA blind neutral hydrogen survey. UCHVCs are compact, isolated HI clouds
with properties consistent with their being nearby low-mass galaxies, but
without identified counterparts in extant optical surveys. Analysis of the
resolved stellar sources in deep $g$- and $i$-band imaging from the WIYN pODI
camera reveals a clustering of possible Red Giant Branch stars associated with
AGC 249525 at a distance of 1.64$\pm$0.45 Mpc. Matching our optical detection
with the HI synthesis map of AGC 249525 from Adams et al. (2016) shows that the
stellar overdensity is exactly coincident with the highest-density HI contour
from that study. Combining our optical photometry and the HI properties of this
object yields an absolute magnitude of $-7.1 \leq M_V \leq -4.5$, a stellar
mass between $2.2\pm0.6\times10^4 M_{\odot}$ and $3.6\pm1.0\times10^5
M_{\odot}$, and an HI to stellar mass ratio between 9 and 144. This object has
stellar properties within the observed range of gas-poor Ultra-Faint Dwarfs in
the Local Group, but is gas-dominated.
",0,1,0,0,0,0
12336,12337,Frequency responses of the K-Rb-$^{21}$Ne co-magnetometer,"  The frequency responses of the K-Rb-$^{21}$Ne co-magnetometer to magnetic
field and exotic spin dependent forces are experimentally studied and simulated
in this paper. Both the relationship between the output amplitude, the phase
shift and frequencies are studied. The responses of magnetic field are
experimentally investigated. Due to a lack of input methods, others are
numerically simulated.
",0,1,0,0,0,0
18384,18385,Online Adaptive Principal Component Analysis and Its extensions,"  We propose algorithms for online principal component analysis (PCA) and
variance minimization for adaptive settings. Previous literature has focused on
upper bounding the static adversarial regret, whose comparator is the optimal
fixed action in hindsight. However, static regret is not an appropriate metric
when the underlying environment is changing. Instead, we adopt the adaptive
regret metric from the previous literature and propose online adaptive
algorithms for PCA and variance minimization, that have sub-linear adaptive
regret guarantees. We demonstrate both theoretically and experimentally that
the proposed algorithms can adapt to the changing environments.
",1,0,0,1,0,0
13991,13992,Selective inference after likelihood- or test-based model selection in linear models,"  Statistical inference after model selection requires an inference framework
that takes the selection into account in order to be valid. Following recent
work on selective inference, we derive analytical expressions for inference
after likelihood- or test-based model selection for linear models.
",0,0,0,1,0,0
5648,5649,A Downsampled Variant of ImageNet as an Alternative to the CIFAR datasets,"  The original ImageNet dataset is a popular large-scale benchmark for training
Deep Neural Networks. Since the cost of performing experiments (e.g, algorithm
design, architecture search, and hyperparameter tuning) on the original dataset
might be prohibitive, we propose to consider a downsampled version of ImageNet.
In contrast to the CIFAR datasets and earlier downsampled versions of ImageNet,
our proposed ImageNet32$\times$32 (and its variants ImageNet64$\times$64 and
ImageNet16$\times$16) contains exactly the same number of classes and images as
ImageNet, with the only difference that the images are downsampled to
32$\times$32 pixels per image (64$\times$64 and 16$\times$16 pixels for the
variants, respectively). Experiments on these downsampled variants are
dramatically faster than on the original ImageNet and the characteristics of
the downsampled datasets with respect to optimal hyperparameters appear to
remain similar. The proposed datasets and scripts to reproduce our results are
available at this http URL and
this https URL
",1,0,0,0,0,0
6833,6834,Observational evidence of galaxy assembly bias,"  We analyze the spectra of 300,000 luminous red galaxies (LRGs) with stellar
masses $M_* \gtrsim 10^{11} M_{\odot}$ from the SDSS-III Baryon Oscillation
Spectroscopic Survey (BOSS). By studying their star-formation histories, we
find two main evolutionary paths converging into the same quiescent galaxy
population at $z\sim0.55$. Fast-growing LRGs assemble $80\%$ of their stellar
mass very early on ($z\sim5$), whereas slow-growing LRGs reach the same
evolutionary state at $z\sim1.5$. Further investigation reveals that their
clustering properties on scales of $\sim$1-30 Mpc are, at a high level of
significance, also different. Fast-growing LRGs are found to be more strongly
clustered and reside in overall denser large-scale structure environments than
slow-growing systems, for a given stellar-mass threshold. Our results imply a
dependence of clustering on stellar-mass assembly history (naturally connected
to the mass-formation history of the corresponding halos) for a homogeneous
population of similar mass and color, which constitutes a strong observational
evidence of galaxy assembly bias.
",0,1,0,0,0,0
4803,4804,Henkin constructions of models with size continuum,"  We survey the technique of constructing customized models of size continuum
in omega steps and illustrate the method by giving new proofs of mostly old
results within this rubric. One new theorem, which is joint with Saharon
Shelah, is that a pseudominimal theory has an atomic model of size continuum.
",0,0,1,0,0,0
18509,18510,Thermodynamics of a Quantum Ising system coupled to a spin bath: Zero Temperature Results,"  We study the effect of coupling a spin bath environment to a system which, at
low energies, can be modeled as a quantum Ising system. A field theoretic
formalism incorporating both thermal and quantum fluctuations is developed to
derive results for the thermodynamic properties and response functions, both
for a toy model and for the $LiHoF_4$ system, in which spin-8 electronic spins
couple to a spin-$7/2$ nuclear spin bath: the phase transition then occurs in a
system of electronuclear degrees of freedom, coupled by long-range dipolar
interactions. The quantum Ising phase transition still exists, and one
hybridized mode of the Ising and bath spins always goes soft at the transition.
",0,1,0,0,0,0
6804,6805,A fast reconstruction algorithm for geometric inverse problems using topological sensitivity analysis and Dirichlet-Neumann cost functional approach,"  This paper is concerned with the detection of objects immersed in anisotropic
media from boundary measurements. We propose an accurate approach based on the
Kohn-Vogelius formulation and the topological sensitivity analysis method. The
inverse problem is formulated as a topology optimization one minimizing an
energy like functional. A topological asymptotic expansion is derived for the
anisotropic Laplace operator. The unknown object is reconstructed using a
level-set curve of the topological gradient. The efficiency and accuracy of the
proposed algorithm are illustrated by some numerical results. MOTS-CLÉS :
Problème inverse géométrique, Laplace anisotrope, formulation de
Kohn-Vogelius, analyse de sensibilité, optimisation topologique.
",0,0,1,0,0,0
2274,2275,The Globular Cluster - Dark Matter Halo Connection,"  I present a simple phenomenological model for the observed linear scaling of
the stellar mass in old globular clusters (GCs) with $z=0$ halo mass in which
the stellar mass in GCs scales linearly with progenitor halo mass at $z=6$
above a minimum halo mass for GC formation. This model reproduces the observed
$M_{\rm GCs}-M_{\rm halo}$ relation at $z=0$ and results in a prediction for
the minimum halo mass at $z=6$ required for hosting one GC: $M_{\rm
min}(z=6)=1.07 \times 10^9\,M_{\odot}$. Translated to $z=0$, the mean threshold
mass is $M_{\rm halo}(z=0) \approx 2\times 10^{10}\,M_{\odot}$. I explore the
observability of GCs in the reionization era and their contribution to cosmic
reionization, both of which depend sensitively on the (unknown) ratio of GC
birth mass to present-day stellar mass, $\xi$. Based on current detections of
$z \gtrsim 6$ objects with $M_{1500} < -17$, values of $\xi > 10$ are strongly
disfavored; this, in turn, has potentially important implications for GC
formation scenarios. Even for low values of $\xi$, some observed high-$z$
galaxies may actually be GCs, complicating estimates of reionization-era galaxy
ultraviolet luminosity functions and constraints on dark matter models. GCs are
likely important reionization sources if $5 \lesssim \xi \lesssim 10$. I also
explore predictions for the fraction of accreted versus in situ GCs in the
local Universe and for descendants of systems at the halo mass threshold of GC
formation (dwarf galaxies). An appealing feature of the model presented here is
the ability to make predictions for GC properties based solely on dark matter
halo merger trees.
",0,1,0,0,0,0
5164,5165,Multivariate Regression with Grossly Corrupted Observations: A Robust Approach and its Applications,"  This paper studies the problem of multivariate linear regression where a
portion of the observations is grossly corrupted or is missing, and the
magnitudes and locations of such occurrences are unknown in priori. To deal
with this problem, we propose a new approach by explicitly consider the error
source as well as its sparseness nature. An interesting property of our
approach lies in its ability of allowing individual regression output elements
or tasks to possess their unique noise levels. Moreover, despite working with a
non-smooth optimization problem, our approach still guarantees to converge to
its optimal solution. Experiments on synthetic data demonstrate the
competitiveness of our approach compared with existing multivariate regression
models. In addition, empirically our approach has been validated with very
promising results on two exemplar real-world applications: The first concerns
the prediction of \textit{Big-Five} personality based on user behaviors at
social network sites (SNSs), while the second is 3D human hand pose estimation
from depth images. The implementation of our approach and comparison methods as
well as the involved datasets are made publicly available in support of the
open-source and reproducible research initiatives.
",1,0,0,1,0,0
20913,20914,Fast dose optimization for rotating shield brachytherapy,"  Purpose: To provide a fast computational method, based on the proximal graph
solver (POGS) - a convex optimization solver using the alternating direction
method of multipliers (ADMM), for calculating an optimal treatment plan in
rotating shield brachytherapy (RSBT). RSBT treatment planning has more degrees
of freedom than conventional high-dose-rate brachytherapy (HDR-BT) due to the
addition of emission direction, and this necessitates a fast optimization
technique to enable clinical usage. // Methods: The multi-helix RSBT (H-RSBT)
delivery technique was considered with five representative cervical cancer
patients. Treatment plans were generated for all patients using the POGS method
and the previously considered commercial solver IBM CPLEX. The rectum, bladder,
sigmoid, high-risk clinical target volume (HR-CTV), and HR-CTV boundary were
the structures considered in our optimization problem, called the asymmetric
dose-volume optimization with smoothness control. Dose calculation resolution
was 1x1x3 mm^3 for all cases. The H-RSBT applicator has 6 helices, with 33.3 mm
of translation along the applicator per helical rotation and 1.7 mm spacing
between dwell positions, yielding 17.5 degree emission angle spacing per 5 mm
along the applicator.// Results: For each patient, HR-CTV D90, HR-CTV D100,
rectum D2cc, sigmoid D2cc, and bladder D2cc matched within 1% for CPLEX and
POGS. Also, we obtained similar EQD2 figures between CPLEX and POGS. POGS was
around 18 times faster than CPLEX. Over all patients, total optimization times
were 32.1-65.4 seconds for CPLEX and 2.1-3.9 seconds for POGS. // Conclusions:
POGS substantially reduced treatment plan optimization time around 18 times for
RSBT with similar HR-CTV D90, OAR D2cc values, and EQD2 figure relative to
CPLEX, which is significant progress toward clinical translation of RSBT. POGS
is also applicable to conventional HDR-BT.
",0,1,1,0,0,0
8997,8998,A Data-Driven MHD Model of the Global Solar Corona within Multi-Scale Fluid-Kinetic Simulation Suite (MS-FLUKSS),"  We have developed a data-driven magnetohydrodynamic (MHD) model of the global
solar corona which uses characteristically-consistent boundary conditions (BCs)
at the inner boundary. Our global solar corona model can be driven by different
observational data including Solar Dynamics Observatory/Helioseismic and
Magnetic Imager (SDO/HMI) synoptic vector magnetograms together with the
horizontal velocity data in the photosphere obtained by the time-distance
helioseismology method, and the line-of-sight (LOS) magnetogram data obtained
by HMI, Solar and Heliospheric Observatory/Michelson Doppler Imager (SOHO/MDI),
National Solar Observatory/Global Oscillation Network Group (NSO/GONG) and
Wilcox Solar Observatory (WSO). We implemented our model in the Multi-Scale
Fluid-Kinetic Simulation Suite (MS-FLUKSS) - a suite of adaptive mesh
refinement (AMR) codes built upon the Chombo AMR framework developed at the
Lawrence Berkeley National Laboratory. We present an overview of our model,
characteristic BCs, and two results we obtained using our model: A benchmark
test of relaxation of a dipole field using characteristic BCs, and relaxation
of an initial PFSS field driven by HMI LOS magnetogram data, and horizontal
velocity data obtained by the time-distance helioseismology method using a set
of non-characteristic BCs.
",0,1,0,0,0,0
11811,11812,Decomposition theorems for asymptotic property C and property A,"  We combine aspects of the notions of finite decomposition complexity and
asymptotic property C into a notion that we call finite APC-decomposition
complexity. Any space with finite decomposition complexity has finite
APC-decomposition complexity and any space with asymptotic property C has
finite APC-decomposition complexity. Moreover, finite APC-decomposition
complexity implies property A for metric spaces. We also show that finite
APC-decomposition complexity is preserved by direct products of groups and
spaces, amalgamated products of groups, and group extensions, among other
constructions.
",0,0,1,0,0,0
7636,7637,Orbital-Free Density-Functional Theory Simulations of Displacement Cascade in Aluminum,"  Here, we report orbital-free density-functional theory (OF DFT) molecular
dynamics simulations of the displacement cascade in aluminum. The electronic
effect is our main concern. The displacement threshold energies are calculated
using OF DFT and classical molecular dynamics (MD) and the comparison reveals
the role of charge bridge. Compared to MD simulation, the displacement spike
from OF DFT has a lower peak and shorter duration time, which is attributed to
the effect of electronic damping. The charge density profiles clearly display
the existence of depleted zones, vacancy and interstitial clusters. And it is
found that the energy exchanges between ions and electrons are mainly
contributed by the kinetic energies.
",0,1,0,0,0,0
6873,6874,Gross-Hopkins Duals of Higher Real K-theory Spectra,"  We determine the Gross-Hopkins duals of certain higher real K-theory spectra.
More specifically, let p be an odd prime, and consider the Morava E-theory
spectrum of height n=p-1. It is known, in the expert circles, that for certain
finite subgroups G of the Morava stabilizer group, the homotopy fixed point
spectra E_n^{hG} are Gross-Hopkins self-dual up to a shift. In this paper, we
determine the shift for those finite subgroups G which contain p-torsion. This
generalizes previous results for n=2 and p=3.
",0,0,1,0,0,0
17324,17325,Econometric modelling and forecasting of intraday electricity prices,"  In the following paper we analyse the ID$_3$-Price on German Intraday
Continuous Electricity Market using an econometric time series model. A
multivariate approach is conducted for hourly and quarter-hourly products
separately. We estimate the model using lasso and elastic net techniques and
perform an out-of-sample very short-term forecasting study. The model's
performance is compared with benchmark models and is discussed in detail.
Forecasting results provide new insights to the German Intraday Continuous
Electricity Market regarding its efficiency and to the ID$_3$-Price behaviour.
The supplementary materials are available online.
",0,0,0,0,0,1
3409,3410,"How big was Galileo's impact? Percussion in the Sixth Day of the ""Two New Sciences""","  The Giornata Sesta about the Force of Percussion is a relatively less known
Chapter from the Galileo's masterpiece ""Discourse about Two New Sciences"". It
was first published lately (1718), long after the first edition of the Two New
Sciences (1638) and Galileo's death (1642). The Giornata Sesta focuses on how
to quantify the percussion force caused by a body in movement, and describes a
very interesting experiment known as ""the two-bucket experiment"". In this
paper, we review this experiment reported by Galileo, develop a steady-state
theoretical model, and solve its transient form numerically; additionally, we
report the results from one real simplified analogous experiment. Finally, we
discuss the conclusions drawn by Galileo -- correct, despite a probably
unnoticeable imbalance --, showing that he did not report the thrust force
component in his setup -- which would be fundamental for the correct
calculation of the percussion force.
",0,1,0,0,0,0
2813,2814,Probing the Interatomic Potential of Solids by Strong-Field Nonlinear Phononics,"  Femtosecond optical pulses at mid-infrared frequencies have opened up the
nonlinear control of lattice vibrations in solids. So far, all applications
have relied on second order phonon nonlinearities, which are dominant at field
strengths near 1 MVcm-1. In this regime, nonlinear phononics can transiently
change the average lattice structure, and with it the functionality of a
material. Here, we achieve an order-of-magnitude increase in field strength,
and explore higher-order lattice nonlinearities. We drive up to five phonon
harmonics of the A1 mode in LiNbO3. Phase-sensitive measurements of atomic
trajectories in this regime are used to experimentally reconstruct the
interatomic potential and to benchmark ab-initio calculations for this
material. Tomography of the Free Energy surface by high-order nonlinear
phononics will impact many aspects of materials research, including the study
of classical and quantum phase transitions.
",0,1,0,0,0,0
12690,12691,Viconmavlink: A software tool for indoor positioning using a motion capture system,"  Motion capture is a widely-used technology in robotics research thanks to its
precise posi tional measurements with real-time performance. This paper
presents ViconMAVLink, a cross-platform open-source software tool that provides
indoor positioning services to networked robots. ViconMAVLink converts Vicon
motion capture data into proper pose and motion data formats and send
localization information to robots using the MAVLink protocol. The software is
a convenient tool for mobile robotics researchers to conduct experiments in a
controlled indoor environment.
",1,0,0,0,0,0
10617,10618,A Microphotonic Astrocomb,"  One of the essential prerequisites for detection of Earth-like extra-solar
planets or direct measurements of the cosmological expansion is the accurate
and precise wavelength calibration of astronomical spectrometers. It has
already been realized that the large number of exactly known optical
frequencies provided by laser frequency combs ('astrocombs') can significantly
surpass conventionally used hollow-cathode lamps as calibration light sources.
A remaining challenge, however, is generation of frequency combs with lines
resolvable by astronomical spectrometers. Here we demonstrate an astrocomb
generated via soliton formation in an on-chip microphotonic resonator
('microresonator') with a resolvable line spacing of 23.7 GHz. This comb is
providing wavelength calibration on the 10 cm/s radial velocity level on the
GIANO-B high-resolution near-infrared spectrometer. As such, microresonator
frequency combs have the potential of providing broadband wavelength
calibration for the next-generation of astronomical instruments in
planet-hunting and cosmological research.
",0,1,0,0,0,0
433,434,AirSim: High-Fidelity Visual and Physical Simulation for Autonomous Vehicles,"  Developing and testing algorithms for autonomous vehicles in real world is an
expensive and time consuming process. Also, in order to utilize recent advances
in machine intelligence and deep learning we need to collect a large amount of
annotated training data in a variety of conditions and environments. We present
a new simulator built on Unreal Engine that offers physically and visually
realistic simulations for both of these goals. Our simulator includes a physics
engine that can operate at a high frequency for real-time hardware-in-the-loop
(HITL) simulations with support for popular protocols (e.g. MavLink). The
simulator is designed from the ground up to be extensible to accommodate new
types of vehicles, hardware platforms and software protocols. In addition, the
modular design enables various components to be easily usable independently in
other projects. We demonstrate the simulator by first implementing a quadrotor
as an autonomous vehicle and then experimentally comparing the software
components with real-world flights.
",1,0,0,0,0,0
6261,6262,A Multi-task Deep Learning Architecture for Maritime Surveillance using AIS Data Streams,"  In a world of global trading, maritime safety, security and efficiency are
crucial issues. We propose a multi-task deep learning framework for vessel
monitoring using Automatic Identification System (AIS) data streams. We combine
recurrent neural networks with latent variable modeling and an embedding of AIS
messages to a new representation space to jointly address key issues to be
dealt with when considering AIS data streams: massive amount of streaming data,
noisy data and irregular timesampling. We demonstrate the relevance of the
proposed deep learning framework on real AIS datasets for a three-task setting,
namely trajectory reconstruction, anomaly detection and vessel type
identification.
",0,0,0,1,0,0
16807,16808,An Agent-Based Approach for Optimizing Modular Vehicle Fleet Operation,"  Modularity in military vehicle designs enables on-base assembly, disassembly,
and reconfiguration of vehicles, which can be beneficial in promoting fleet
adaptability and life cycle cost savings. To properly manage the fleet
operation and to control the resupply, demand prediction, and scheduling
process, this paper illustrates an agent-based approach customized for highly
modularized military vehicle fleets and studies the feasibility and flexibility
of modularity for various mission scenarios. Given deterministic field demands
with operation stochasticity, we compare the performance of a modular fleet to
a conventional fleet in equivalent operation strategies and also compare fleet
performance driven by heuristic rules and optimization. Several indicators are
selected to quantify the fleet performance, including operation costs, total
resupplied resources, and fleet readiness.
When the model is implemented for military Joint Tactical Transport System
(JTTS) mission, our results indicate that fleet modularity can reduce total
resource supplies without significant losses in fleet readiness. The benefits
of fleet modularity can also be amplified through a real-time optimized
operation strategy. To highlight the feasibility of fleet modularity, a
parametric study is performed to show the impacts from working capacity on
modular fleet performance. Finally, we provide practical suggestions of modular
vehicle designs based on the analysis and other possible usage.
",1,0,0,0,0,0
4889,4890,Deep Bayesian Active Learning with Image Data,"  Even though active learning forms an important pillar of machine learning,
deep learning tools are not prevalent within it. Deep learning poses several
difficulties when used in an active learning setting. First, active learning
(AL) methods generally rely on being able to learn and update models from small
amounts of data. Recent advances in deep learning, on the other hand, are
notorious for their dependence on large amounts of data. Second, many AL
acquisition functions rely on model uncertainty, yet deep learning methods
rarely represent such model uncertainty. In this paper we combine recent
advances in Bayesian deep learning into the active learning framework in a
practical way. We develop an active learning framework for high dimensional
data, a task which has been extremely challenging so far, with very sparse
existing literature. Taking advantage of specialised models such as Bayesian
convolutional neural networks, we demonstrate our active learning techniques
with image data, obtaining a significant improvement on existing active
learning approaches. We demonstrate this on both the MNIST dataset, as well as
for skin cancer diagnosis from lesion images (ISIC2016 task).
",1,0,0,1,0,0
20589,20590,Performance Measurements of Supercomputing and Cloud Storage Solutions,"  Increasing amounts of data from varied sources, particularly in the fields of
machine learning and graph analytics, are causing storage requirements to grow
rapidly. A variety of technologies exist for storing and sharing these data,
ranging from parallel file systems used by supercomputers to distributed block
storage systems found in clouds. Relatively few comparative measurements exist
to inform decisions about which storage systems are best suited for particular
tasks. This work provides these measurements for two of the most popular
storage technologies: Lustre and Amazon S3. Lustre is an open-source, high
performance, parallel file system used by many of the largest supercomputers in
the world. Amazon's Simple Storage Service, or S3, is part of the Amazon Web
Services offering, and offers a scalable, distributed option to store and
retrieve data from anywhere on the Internet. Parallel processing is essential
for achieving high performance on modern storage systems. The performance tests
used span the gamut of parallel I/O scenarios, ranging from single-client,
single-node Amazon S3 and Lustre performance to a large-scale, multi-client
test designed to demonstrate the capabilities of a modern storage appliance
under heavy load. These results show that, when parallel I/O is used correctly
(i.e., many simultaneous read or write processes), full network bandwidth
performance is achievable and ranged from 10 gigabits/s over a 10 GigE S3
connection to 0.35 terabits/s using Lustre on a 1200 port 10 GigE switch. These
results demonstrate that S3 is well-suited to sharing vast quantities of data
over the Internet, while Lustre is well-suited to processing large quantities
of data locally.
",1,1,0,0,0,0
8952,8953,Coverage characteristics of self-repelling random walks in mobile ad-hoc networks,"  A self-repelling random walk of a token on a graph is one in which at each
step, the token moves to a neighbor that has been visited least often (with
ties broken randomly). The properties of self-repelling random walks have been
analyzed for two dimensional lattices and these walks have been shown to
exhibit a remarkable uniformity with which they visit nodes in a graph. In this
paper, we extend this analysis to self-repelling random walks on mobile
networks in which the underlying graph itself is temporally evolving. Using
network simulations in ns-3, we characterize the number of times each node is
visited from the start until all nodes have been visited at least once. We
evaluate under different mobility models and on networks ranging from 100 to
1000 nodes. Our results show that until about 85% coverage, duplicate visits
are very rare highlighting the efficiency with which a majority of nodes in the
network can be visited. Even at 100% coverage, the exploration overhead (the
ratio of number of steps to number of unique visited nodes) remains low and
under 2. Our analysis shows that self-repelling random walks are effective,
structure-free tools for data aggregation in mobile ad-hoc networks.
",1,0,0,0,0,0
14078,14079,Deep Self-Paced Learning for Person Re-Identification,"  Person re-identification (Re-ID) usually suffers from noisy samples with
background clutter and mutual occlusion, which makes it extremely difficult to
distinguish different individuals across the disjoint camera views. In this
paper, we propose a novel deep self-paced learning (DSPL) algorithm to
alleviate this problem, in which we apply a self-paced constraint and symmetric
regularization to help the relative distance metric training the deep neural
network, so as to learn the stable and discriminative features for person
Re-ID. Firstly, we propose a soft polynomial regularizer term which can derive
the adaptive weights to samples based on both the training loss and model age.
As a result, the high-confidence fidelity samples will be emphasized and the
low-confidence noisy samples will be suppressed at early stage of the whole
training process. Such a learning regime is naturally implemented under a
self-paced learning (SPL) framework, in which samples weights are adaptively
updated based on both model age and sample loss using an alternative
optimization method. Secondly, we introduce a symmetric regularizer term to
revise the asymmetric gradient back-propagation derived by the relative
distance metric, so as to simultaneously minimize the intra-class distance and
maximize the inter-class distance in each triplet unit. Finally, we build a
part-based deep neural network, in which the features of different body parts
are first discriminately learned in the lower convolutional layers and then
fused in the higher fully connected layers. Experiments on several benchmark
datasets have demonstrated the superior performance of our method as compared
with the state-of-the-art approaches.
",1,0,0,0,0,0
20427,20428,Imbalanced Malware Images Classification: a CNN based Approach,"  Deep convolutional neural networks (CNNs) can be applied to malware binary
detection through images classification. The performance, however, is degraded
due to the imbalance of malware families (classes). To mitigate this issue, we
propose a simple yet effective weighted softmax loss which can be employed as
the final layer of deep CNNs. The original softmax loss is weighted, and the
weight value can be determined according to class size. A scaling parameter is
also included in computing the weight. Proper selection of this parameter has
been studied and an empirical option is given. The weighted loss aims at
alleviating the impact of data imbalance in an end-to-end learning fashion. To
validate the efficacy, we deploy the proposed weighted loss in a pre-trained
deep CNN model and fine-tune it to achieve promising results on malware images
classification. Extensive experiments also indicate that the new loss function
can fit other typical CNNs with an improved classification performance.
",1,0,0,1,0,0
7332,7333,On discrimination between two close distribution tails,"  The goodness-of-fit test for discrimination of two tail distribution using
higher order statistics is proposed. The consistency of proposed test is proved
for two different alternatives. We do not assume belonging the corresponding
distribution function to a maximum domain of attraction.
",0,0,1,1,0,0
20523,20524,Fisher Information and Natural Gradient Learning of Random Deep Networks,"  A deep neural network is a hierarchical nonlinear model transforming input
signals to output signals. Its input-output relation is considered to be
stochastic, being described for a given input by a parameterized conditional
probability distribution of outputs. The space of parameters consisting of
weights and biases is a Riemannian manifold, where the metric is defined by the
Fisher information matrix. The natural gradient method uses the steepest
descent direction in a Riemannian manifold, so it is effective in learning,
avoiding plateaus. It requires inversion of the Fisher information matrix,
however, which is practically impossible when the matrix has a huge number of
dimensions. Many methods for approximating the natural gradient have therefore
been introduced. The present paper uses statistical neurodynamical method to
reveal the properties of the Fisher information matrix in a net of random
connections under the mean field approximation. We prove that the Fisher
information matrix is unit-wise block diagonal supplemented by small order
terms of off-block-diagonal elements, which provides a justification for the
quasi-diagonal natural gradient method by Y. Ollivier. A unitwise
block-diagonal Fisher metrix reduces to the tensor product of the Fisher
information matrices of single units. We further prove that the Fisher
information matrix of a single unit has a simple reduced form, a sum of a
diagonal matrix and a rank 2 matrix of weight-bias correlations. We obtain the
inverse of Fisher information explicitly. We then have an explicit form of the
natural gradient, without relying on the numerical matrix inversion, which
drastically speeds up stochastic gradient learning.
",0,0,0,1,0,0
18565,18566,Toward Finding Latent Cities with Non-Negative Matrix Factorization,"  In the last decade, digital footprints have been used to cluster population
activity into functional areas of cities.
However, a key aspect has been overlooked: we experience our cities not only
by performing activities at specific destinations, but also by moving from one
place to another.
In this paper, we propose to analyze and cluster the city based on how people
move through it. Particularly, we introduce Mobilicities, automatically
generated travel patterns inferred from mobile phone network data using NMF, a
matrix factorization model.
We evaluate our method in a large city and we find that mobilicities reveal
latent but at the same time interpretable mobility structures of the city. Our
results provide evidence on how clustering and visualization of aggregated
phone logs could be used in planning systems to interactively analyze city
structure and population activity.
",1,0,0,0,0,0
4893,4894,Solving Non-parametric Inverse Problem in Continuous Markov Random Field using Loopy Belief Propagation,"  In this paper, we address the inverse problem, or the statistical machine
learning problem, in Markov random fields with a non-parametric pair-wise
energy function with continuous variables. The inverse problem is formulated by
maximum likelihood estimation. The exact treatment of maximum likelihood
estimation is intractable because of two problems: (1) it includes the
evaluation of the partition function and (2) it is formulated in the form of
functional optimization. We avoid Problem (1) by using Bethe approximation.
Bethe approximation is an approximation technique equivalent to the loopy
belief propagation. Problem (2) can be solved by using orthonormal function
expansion. Orthonormal function expansion can reduce a functional optimization
problem to a function optimization problem. Our method can provide an analytic
form of the solution of the inverse problem within the framework of Bethe
approximation.
",1,1,0,1,0,0
14272,14273,Vulnerability to pandemics in a rapidly urbanizing society,"  We examine salient trends of influenza pandemics in Australia, a rapidly
urbanizing nation. To do so, we implement state-of-the-art influenza
transmission and progression models within a large-scale stochastic computer
simulation, generated using comprehensive Australian census datasets from 2006,
2011, and 2016. Our results offer the first simulation-based investigation of a
population's sensitivity to pandemics across multiple historical time points,
and highlight three significant trends in pandemic patterns over the years:
increased peak prevalence, faster spreading rates, and decreasing
spatiotemporal bimodality. We attribute these pandemic trends to increases in
two key quantities indicative of urbanization: population fraction residing in
major cities, and international air traffic. In addition, we identify features
of the pandemic's geographic spread that can only be attributed to changes in
the commuter mobility network. The generic nature of our model and the ubiquity
of urbanization trends around the world make it likely for our results to be
applicable in other rapidly urbanizing nations.
",0,0,0,0,1,0
16735,16736,Feature Selection Facilitates Learning Mixtures of Discrete Product Distributions,"  Feature selection can facilitate the learning of mixtures of discrete random
variables as they arise, e.g. in crowdsourcing tasks. Intuitively, not all
workers are equally reliable but, if the less reliable ones could be
eliminated, then learning should be more robust. By analogy with Gaussian
mixture models, we seek a low-order statistical approach, and here introduce an
algorithm based on the (pairwise) mutual information. This induces an order
over workers that is well structured for the `one coin' model. More generally,
it is justified by a goodness-of-fit measure and is validated empirically.
Improvement in real data sets can be substantial.
",1,0,0,1,0,0
4510,4511,Ranks of rational points of the Jacobian varieties of hyperelliptic curves,"  In this paper, we obtain bounds for the Mordell-Weil ranks over cyclotomic
extensions of a wide range of abelian varieties defined over a number field $F$
whose primes above $p$ are totally ramified over $F/\mathbb{Q}$. We assume that
the abelian varieties may have good non-ordinary reduction at those primes. Our
work is a generalization of \cite{Kim}, in which the second author generalized
Perrin-Riou's Iwasawa theory for elliptic curves over $\mathbb{Q}$ with
supersingular reduction (\cite{Perrin-Riou}) to elliptic curves defined over
the above-mentioned number field $F$. On top of non-ordinary reduction and the
ramification of the field $F$, we deal with the additional difficulty that the
dimensions of the abelian varieties can be any number bigger than 1 which
causes a variety of issues. As a result, we obtain bounds for the ranks over
cyclotomic extensions $\mathbb{Q}(\mu_{p^{\max(M,N)+n}})$ of the Jacobian
varieties of {\it ramified} hyperelliptic curves $y^{2p^M}=x^{3p^N}+ax^{p^N}+b$
among others.
",0,0,1,0,0,0
13814,13815,Dark matter in the Reticulum II dSph: a radio search,"  We present a deep radio search in the Reticulum II dwarf spheroidal (dSph)
galaxy performed with the Australia Telescope Compact Array. Observations were
conducted at 16 cm wavelength, with an rms sensitivity of 0.01 mJy/beam, and
with the goal of searching for synchrotron emission induced by annihilation or
decay of weakly interacting massive particles (WIMPs). Data were complemented
with observations on large angular scales taken with the KAT-7 telescope. We
find no evidence for a diffuse emission from the dSph and we derive competitive
bounds on the WIMP properties. In addition, we detect more than 200 new
background radio sources. Among them, we show there are two compelling
candidates for being the radio counterpart of the possible gamma-ray emission
reported by other groups using Fermi-LAT data.
",0,1,0,0,0,0
17203,17204,Thread-Modular Static Analysis for Relaxed Memory Models,"  We propose a memory-model-aware static program analysis method for accurately
analyzing the behavior of concurrent software running on processors with weak
consistency models such as x86-TSO, SPARC-PSO, and SPARC-RMO. At the center of
our method is a unified framework for deciding the feasibility of inter-thread
interferences to avoid propagating spurious data flows during static analysis
and thus boost the performance of the static analyzer. We formulate the
checking of interference feasibility as a set of Datalog rules which are both
efficiently solvable and general enough to capture a range of hardware-level
memory models. Compared to existing techniques, our method can significantly
reduce the number of bogus alarms as well as unsound proofs. We implemented the
method and evaluated it on a large set of multithreaded C programs. Our
experiments showthe method significantly outperforms state-of-the-art
techniques in terms of accuracy with only moderate run-time overhead.
",1,0,0,0,0,0
229,230,General notions of regression depth function,"  As a measure for the centrality of a point in a set of multivariate data,
statistical depth functions play important roles in multivariate analysis,
because one may conveniently construct descriptive as well as inferential
procedures relying on them. Many depth notions have been proposed in the
literature to fit to different applications. However, most of them are mainly
developed for the location setting. In this paper, we discuss the possibility
of extending some of them into the regression setting. A general concept of
regression depth function is also provided.
",0,0,0,1,0,0
12148,12149,Linear Convergence of a Frank-Wolfe Type Algorithm over Trace-Norm Balls,"  We propose a rank-$k$ variant of the classical Frank-Wolfe algorithm to solve
convex optimization over a trace-norm ball. Our algorithm replaces the top
singular-vector computation ($1$-SVD) in Frank-Wolfe with a top-$k$
singular-vector computation ($k$-SVD), which can be done by repeatedly applying
$1$-SVD $k$ times. Alternatively, our algorithm can be viewed as a rank-$k$
restricted version of projected gradient descent. We show that our algorithm
has a linear convergence rate when the objective function is smooth and
strongly convex, and the optimal solution has rank at most $k$. This improves
the convergence rate and the total time complexity of the Frank-Wolfe method
and its variants.
",1,0,0,1,0,0
8223,8224,Distribution System Voltage Control under Uncertainties using Tractable Chance Constraints,"  Voltage control plays an important role in the operation of electricity
distribution networks, especially with high penetration of distributed energy
resources. These resources introduce significant and fast varying
uncertainties. In this paper, we focus on reactive power compensation to
control voltage in the presence of uncertainties. We adopt a chance constraint
approach that accounts for arbitrary correlations between renewable resources
at each of the buses. We show how the problem can be solved efficiently using
historical samples via a stochastic quasi gradient method. We also show that
this optimization problem is convex for a wide variety of probabilistic
distributions. Compared to conventional per-bus chance constraints, our
formulation is more robust to uncertainty and more computationally tractable.
We illustrate the results using standard IEEE distribution test feeders.
",1,0,1,1,0,0
11696,11697,Chondrule Accretion with a Growing Protoplanet,"  Chondrules are primitive materials in the Solar System. They are formed in
the first about 3 Myr of the Solar System's history. This timescale is longer
than that of Mars formation, and it is conceivable that protoplanets,
planetesimals and chondrules might have existed simultaneously in the solar
nebula. Due to protoplanets perturbation on the planetesimal dynamics and
chondrule accretion on them, all the formed chondrules are unlikely to be
accreted by planetesimals. We investigate the amount of chondrules accreted by
planetesimals in such a condition. We assume that a protoplanet is in
oligarchic growth, and we perform analytical calculations of chondrule
accretion both by a protoplanet and by planetesimals. Through the oligarchic
growth stage, planetesimals accrete about half of the formed chondrules. The
smallest planetesimals get the largest amount of the chondrules, compared with
the amount accreted by more massive planetesimals. We perform a parameter study
and find that this fraction is not largely changed for a wide range of
parameter sets.
",0,1,0,0,0,0
17840,17841,Architecture of Text Mining Application in Analyzing Public Sentiments of West Java Governor Election using Naive Bayes Classification,"  The selection of West Java governor is one event that seizes the attention of
the public is no exception to social media users. Public opinion on a
prospective regional leader can help predict electability and tendency of
voters. Data that can be used by the opinion mining process can be obtained
from Twitter. Because the data is very varied form and very unstructured, it
must be managed and uninformed using data pre-processing techniques into
semi-structured data. This semi-structured information is followed by a
classification stage to categorize the opinion into negative or positive
opinions. The research methodology uses a literature study where the research
will examine previous research on a similar topic. The purpose of this study is
to find the right architecture to develop it into the application of twitter
opinion mining to know public sentiments toward the election of the governor of
west java. The result of this research is that Twitter opinion mining is part
of text mining where opinions in Twitter if they want to be classified, must go
through the preprocessing text stage first. The preprocessing step required
from twitter data is cleansing, case folding, POS Tagging and stemming. The
resulting text mining architecture is an architecture that can be used for text
mining research with different topics.
",1,0,0,0,0,0
17379,17380,Quantitative estimates of the surface habitability of Kepler-452b,"  Kepler-452b is currently the best example of an Earth-size planet in the
habitable zone of a sun-like star, a type of planet whose number of detections
is expected to increase in the future. Searching for biosignatures in the
supposedly thin atmospheres of these planets is a challenging goal that
requires a careful selection of the targets. Under the assumption of a
rocky-dominated nature for Kepler-452b, we considered it as a test case to
calculate a temperature-dependent habitability index, $h_{050}$, designed to
maximize the potential presence of biosignature-producing activity (Silva et
al.\ 2016). The surface temperature has been computed for a broad range of
climate factors using a climate model designed for terrestrial-type exoplanets
(Vladilo et al.\ 2015). After fixing the planetary data according to the
experimental results (Jenkins et al.\ 2015), we changed the surface gravity,
CO$_2$ abundance, surface pressure, orbital eccentricity, rotation period, axis
obliquity and ocean fraction within the range of validity of our model. For
most choices of parameters we find habitable solutions with $h_{050}>0.2$ only
for CO$_2$ partial pressure $p_\mathrm{CO_2} \lesssim 0.04$\,bar. At this
limiting value of CO$_2$ abundance the planet is still habitable if the total
pressure is $p \lesssim 2$\,bar. In all cases the habitability drops for
eccentricity $e \gtrsim 0.3$. Changes of rotation period and obliquity affect
the habitability through their impact on the equator-pole temperature
difference rather than on the mean global temperature. We calculated the
variation of $h_{050}$ resulting from the luminosity evolution of the host star
for a wide range of input parameters. Only a small combination of parameters
yield habitability-weighted lifetimes $\gtrsim 2$\,Gyr, sufficiently long to
develop atmospheric biosignatures still detectable at the present time.
",0,1,0,0,0,0
3653,3654,"Quantum variance on quaternion algebras, II","  A method for determining quantum variance asymptotics on compact quotients
attached to non-split quaternion algebras is developed in general and applied
to ""microlocal lifts"" in the non-archimedean setting. The results obtained are
in the spirit of recent work of Sarnak--Zhao.
The arguments involve a careful analytic study of the theta correspondence,
the interplay between additive and multiplicative harmonic analysis on
quaternion algebras, the equidistribution of translates of elementary theta
functions, and the Rallis inner product formula.
",0,0,1,0,0,0
357,358,A short-orbit spectrometer for low-energy pion detection in electroproduction experiments at MAMI,"  A new Short-Orbit Spectrometer (SOS) has been constructed and installed
within the experimental facility of the A1 collaboration at Mainz Microtron
(MAMI), with the goal to detect low-energy pions. It is equipped with a
Browne-Buechner magnet and a detector system consisting of two helium-ethane
based drift chambers and a scintillator telescope made of five layers. The
detector system allows detection of pions in the momentum range of 50 - 147
MeV/c, which corresponds to 8.7 - 63 MeV kinetic energy. The spectrometer can
be placed at a distance range of 54 - 66 cm from the target center. Two
collimators are available for the measurements, one having 1.8 msr aperture and
the other having 7 msr aperture. The Short-Orbit Spectrometer has been
successfully calibrated and used in coincidence measurements together with the
standard magnetic spectrometers of the A1 collaboration.
",0,1,0,0,0,0
7135,7136,Learning to Communicate: A Machine Learning Framework for Heterogeneous Multi-Agent Robotic Systems,"  We present a machine learning framework for multi-agent systems to learn both
the optimal policy for maximizing the rewards and the encoding of the high
dimensional visual observation. The encoding is useful for sharing local visual
observations with other agents under communication resource constraints. The
actor-encoder encodes the raw images and chooses an action based on local
observations and messages sent by the other agents. The machine learning agent
generates not only an actuator command to the physical device, but also a
communication message to the other agents. We formulate a reinforcement
learning problem, which extends the action space to consider the communication
action as well. The feasibility of the reinforcement learning framework is
demonstrated using a 3D simulation environment with two collaborating agents.
The environment provides realistic visual observations to be used and shared
between the two agents.
",1,0,0,0,0,0
10850,10851,Propagation of regularity in $L^p$-spaces for Kolmogorov type hypoelliptic operators,"  Consider the following Kolmogorov type hypoelliptic operator $$ \mathscr
L_t:=\mbox{$\sum_{j=2}^n$}x_j\cdot\nabla_{x_{j-1}}+{\rm Tr} (a_t
\cdot\nabla^2_{x_n}), $$ where $n\geq 2$, $x=(x_1,\cdots,x_n)\in(\mathbb R^d)^n
=\mathbb R^{nd}$ and $a_t$ is a time-dependent constant symmetric $d\times
d$-matrix that is uniformly elliptic and bounded.. Let $\{\mathcal T_{s,t};
t\geq s\}$ be the time-dependent semigroup associated with $\mathscr L_t$; that
is, $\partial_s {\mathcal T}_{s, t} f = - {\mathscr L}_s {\mathcal T}_{s, t}f$.
For any $p\in(1,\infty)$, we show that there is a constant $C=C(p,n,d)>0$ such
that for any $f(t, x)\in L^p(\mathbb R \times \mathbb R^{nd})=L^p(\mathbb
R^{1+nd})$ and every $\lambda \geq 0$, $$
\left\|\Delta_{x_j}^{{1}/{(1+2(n-j)})}\int^{\infty}_0 e^{-\lambda t} {\mathcal
T}_{s, s+t }f(t+s, x)dt\right\|_p\leq C\|f\|_p,\quad j=1,\cdots, n, $$ where
$\|\cdot\|_p$ is the usual $L^p$-norm in $L^p(\mathbb R^{1+nd}; d s\times d
x)$. To show this type of estimates, we first study the propagation of
regularity in $L^2$-space from variable $x_n$ to $x_1$ for the solution of the
transport equation $\partial_t u+\sum_{j=2}^nx_j\cdot\nabla_{x_{j-1}} u=f$.
",0,0,1,0,0,0
6545,6546,Localization and Stationary Phase Approximation on Supermanifolds,"  Given an odd vector field $Q$ on a supermanifold $M$ and a $Q$-invariant
density $\mu$ on $M$, under certain compactness conditions on $Q$, the value of
the integral $\int_{M}\mu$ is determined by the value of $\mu$ on any
neighborhood of the vanishing locus $N$ of $Q$. We present a formula for the
integral in the case where $N$ is a subsupermanifold which is appropriately
non-degenerate with respect to $Q$.
In the process, we discuss the linear algebra necessary to express our result
in a coordinate independent way. We also extend stationary phase approximation
and the Morse-Bott Lemma to supermanifolds.
",0,0,1,0,0,0
3090,3091,(Un)predictability of strong El Niño events,"  The El Niño-Southern Oscillation (ENSO) is a mode of interannual
variability in the coupled equatorial Pacific coupled atmosphere/ocean system.
El Niño describes a state in which sea surface temperatures in the eastern
Pacific increase and upwelling of colder, deep waters diminishes. El Niño
events typically peak in boreal winter, but their strength varies irregularly
on decadal time scales. There were exceptionally strong El Niño events in
1982-83, 1997-98 and 2015-16 that affected weather on a global scale. Widely
publicized forecasts in 2014 predicted that the 2015-16 event would occur a
year earlier. Predicting the strength of El Niño is a matter of practical
concern due to its effects on hydroclimate and agriculture around the world.
This paper discusses the frequency and regularity of strong El Niño events in
the context of chaotic dynamical systems. We discover a mechanism that limits
their predictability in a conceptual ""recharge oscillator"" model of ENSO. Weak
seasonal forcing or noise in this model can induce irregular switching between
an oscillatory state that has strong El Niño events and a chaotic state that
lacks strong events, In this regime, the timing of strong El Niño events on
decadal time scales is unpredictable.
",0,1,0,0,0,0
15407,15408,Coherent long-distance displacement of individual electron spins,"  Controlling nanocircuits at the single electron spin level is a possible
route for large-scale quantum information processing. In this context,
individual electron spins have been identified as versatile quantum information
carriers to interconnect different nodes of a spin-based semiconductor quantum
circuit. Despite important experimental efforts to control the electron
displacement over long distances, keeping the electron spin coherence after
transfer remained up to now elusive. Here we demonstrate that individual
electron spins can be displaced coherently over a distance of 5 micrometers.
This displacement is realized on a closed path made of three tunnel-coupled
lateral quantum dots. Using fast quantum dot control, the electrons tunnel from
one dot to another at a speed approaching 100 m/s. We find that the spin
coherence length is 8 times longer than expected from the electron spin
coherence without displacement. Such an enhanced spin coherence points at a
process similar to motional narrowing observed in nuclear magnetic resonance
experiments6. The demonstrated coherent displacement will enable long-range
interaction between distant spin-qubits and will open the route towards
non-abelian and holonomic manipulation of a single electron spin.
",0,1,0,0,0,0
20161,20162,The downward directed grounds hypothesis and very large cardinals,"  A transitive model $M$ of ZFC is called a ground if the universe $V$ is a set
forcing extension of $M$. We show that the grounds of $V$ are downward
set-directed. Consequently, we establish some fundamental theorems on the
forcing method and the set-theoretic geology. For instance, (1) the mantle, the
intersection of all grounds, must be a model of ZFC. (2) $V$ has only set many
grounds if and only if the mantle is a ground. We also show that if the
universe has some very large cardinal, then the mantle must be a ground.
",0,0,1,0,0,0
9101,9102,On the Universality of Invariant Networks,"  Constraining linear layers in neural networks to respect symmetry
transformations from a group $G$ is a common design principle for invariant
networks that has found many applications in machine learning.
In this paper, we consider a fundamental question that has received little
attention to date: Can these networks approximate any (continuous) invariant
function?
We tackle the rather general case where $G\leq S_n$ (an arbitrary subgroup of
the symmetric group) that acts on $\mathbb{R}^n$ by permuting coordinates. This
setting includes several recent popular invariant networks. We present two main
results: First, $G$-invariant networks are universal if high-order tensors are
allowed. Second, there are groups $G$ for which higher-order tensors are
unavoidable for obtaining universality.
$G$-invariant networks consisting of only first-order tensors are of special
interest due to their practical value. We conclude the paper by proving a
necessary condition for the universality of $G$-invariant networks that
incorporate only first-order tensors. Lastly, we propose a conjecture stating
that this condition is also sufficient.
",1,0,0,1,0,0
20525,20526,Parallel mean curvature surfaces in four-dimensional homogeneous spaces,"  We survey different classification results for surfaces with parallel mean
curvature immersed into some Riemannian homogeneous four-manifolds, including
real and complex space forms, and product spaces. We provide a common framework
for this problem, with special attention to the existence of holomorphic
quadratic differentials on such surfaces. The case of spheres with parallel
mean curvature is also explained in detail, as well as the state-of-the-art
advances in the general problem.
",0,0,1,0,0,0
3879,3880,Penetrating a Social Network: The Follow-back Problem,"  Modern threats have emerged from the prevalence of social networks. Hostile
actors, such as extremist groups or foreign governments, utilize these networks
to run propaganda campaigns with different aims. For extremists, these
campaigns are designed for recruiting new members or inciting violence. For
foreign governments, the aim may be to create instability in rival nations.
Proper social network counter-measures are needed to combat these threats. Here
we present one important counter-measure: penetrating social networks. This
means making target users connect with or follow agents deployed in the social
network. Once such connections are established with the targets, the agents can
influence them by sharing content which counters the influence campaign. In
this work we study how to penetrate a social network, which we call the
follow-back problem. The goal here is to find a policy that maximizes the
number of targets that follow the agent.
We conduct an empirical study to understand what behavioral and network
features affect the probability of a target following an agent. We find that
the degree of the target and the size of the mutual neighborhood of the agent
and target in the network affect this probability. Based on our empirical
findings, we then propose a model for targets following an agent. Using this
model, we solve the follow-back problem exactly on directed acyclic graphs and
derive a closed form expression for the expected number of follows an agent
receives under the optimal policy. We then formulate the follow-back problem on
an arbitrary graph as an integer program. To evaluate our integer program based
policies, we conduct simulations on real social network topologies in Twitter.
We find that our polices result in more effective network penetration, with
significant increases in the expected number of targets that follow the agent.
",1,0,0,1,0,0
904,905,A Simple Convolutional Generative Network for Next Item Recommendation,"  Convolutional Neural Networks (CNNs) have been recently introduced in the
domain of session-based next item recommendation. An ordered collection of past
items the user has interacted with in a session (or sequence) are embedded into
a 2-dimensional latent matrix, and treated as an image. The convolution and
pooling operations are then applied to the mapped item embeddings. In this
paper, we first examine the typical session-based CNN recommender and show that
both the generative model and network architecture are suboptimal when modeling
long-range dependencies in the item sequence. To address the issues, we
introduce a simple, but very effective generative model that is capable of
learning high-level representation from both short- and long-range item
dependencies. The network architecture of the proposed model is formed of a
stack of \emph{holed} convolutional layers, which can efficiently increase the
receptive fields without relying on the pooling operation. Another contribution
is the effective use of residual block structure in recommender systems, which
can ease the optimization for much deeper networks. The proposed generative
model attains state-of-the-art accuracy with less training time in the next
item recommendation task. It accordingly can be used as a powerful
recommendation baseline to beat in future, especially when there are long
sequences of user feedback.
",0,0,0,1,0,0
2672,2673,BARCHAN: Blob Alignment for Robust CHromatographic ANalysis,"  Comprehensive Two dimensional gas chromatography (GCxGC) plays a central role
into the elucidation of complex samples. The automation of the identification
of peak areas is of prime interest to obtain a fast and repeatable analysis of
chromatograms. To determine the concentration of compounds or pseudo-compounds,
templates of blobs are defined and superimposed on a reference chromatogram.
The templates then need to be modified when different chromatograms are
recorded. In this study, we present a chromatogram and template alignment
method based on peak registration called BARCHAN. Peaks are identified using a
robust mathematical morphology tool. The alignment is performed by a
probabilistic estimation of a rigid transformation along the first dimension,
and a non-rigid transformation in the second dimension, taking into account
noise, outliers and missing peaks in a fully automated way. Resulting aligned
chromatograms and masks are presented on two datasets. The proposed algorithm
proves to be fast and reliable. It significantly reduces the time to results
for GCxGC analysis.
",1,1,0,0,0,0
5561,5562,Fast Generation for Convolutional Autoregressive Models,"  Convolutional autoregressive models have recently demonstrated
state-of-the-art performance on a number of generation tasks. While fast,
parallel training methods have been crucial for their success, generation is
typically implemented in a naïve fashion where redundant computations are
unnecessarily repeated. This results in slow generation, making such models
infeasible for production environments. In this work, we describe a method to
speed up generation in convolutional autoregressive models. The key idea is to
cache hidden states to avoid redundant computation. We apply our fast
generation method to the Wavenet and PixelCNN++ models and achieve up to
$21\times$ and $183\times$ speedups respectively.
",1,0,0,1,0,0
6695,6696,A Van-Der-Waals picture for metabolic networks from MaxEnt modeling: inherent bistability and elusive coexistence,"  In this work maximum entropy distributions in the space of steady states of
metabolic networks are defined upon constraining the first and second moment of
the growth rate. Inherent bistability of fast and slow phenotypes, akin to a
Van-Der Waals picture, emerges upon considering control on the average growth
(optimization/repression) and its fluctuations (heterogeneity). This is applied
to the carbon catabolic core of E.coli where it agrees with some stylized facts
on the persisters phenotype and it provides a quantitative map with metabolic
fluxes, opening for the possibility to detect coexistence from flux data.
Preliminary analysis on data for E.Coli cultures in standard conditions shows,
on the other hand, degeneracy for the inferred parameters that extend in the
coexistence region.
",0,1,0,0,0,0
10110,10111,Multichannel Linear Prediction for Blind Reverberant Audio Source Separation,"  A class of methods based on multichannel linear prediction (MCLP) can achieve
effective blind dereverberation of a source, when the source is observed with a
microphone array. We propose an inventive use of MCLP as a pre-processing step
for blind source separation with a microphone array. We show theoretically
that, under certain assumptions, such pre-processing reduces the original blind
reverberant source separation problem to a non-reverberant one, which in turn
can be effectively tackled using existing methods. We demonstrate our claims
using real recordings obtained with an eight-microphone circular array in
reverberant environments.
",1,0,0,0,0,0
3868,3869,Embedded eigenvalues of generalized Schrödinger operators,"  We provide examples of operators $T(D)+V$ with decaying potentials that have
embedded eigenvalues. The decay of the potential depends on the curvature of
the Fermi surfaces of constant kinetic energy $T$. We make the connection to
counterexamples in Fourier restriction theory.
",0,0,1,0,0,0
15372,15373,Modal clustering asymptotics with applications to bandwidth selection,"  Density-based clustering relies on the idea of linking groups to some
specific features of the probability distribution underlying the data. The
reference to a true, yet unknown, population structure allows to frame the
clustering problem in a standard inferential setting, where the concept of
ideal population clustering is defined as the partition induced by the true
density function. The nonparametric formulation of this approach, known as
modal clustering, draws a correspondence between the groups and the domains of
attraction of the density modes. Operationally, a nonparametric density
estimate is required and a proper selection of the amount of smoothing,
governing the shape of the density and hence possibly the modal structure, is
crucial to identify the final partition. In this work, we address the issue of
density estimation for modal clustering from an asymptotic perspective. A
natural and easy to interpret metric to measure the distance between
density-based partitions is discussed, its asymptotic approximation explored,
and employed to study the problem of bandwidth selection for nonparametric
modal clustering.
",0,0,0,1,0,0
16687,16688,HARP: Hierarchical Representation Learning for Networks,"  We present HARP, a novel method for learning low dimensional embeddings of a
graph's nodes which preserves higher-order structural features. Our proposed
method achieves this by compressing the input graph prior to embedding it,
effectively avoiding troublesome embedding configurations (i.e. local minima)
which can pose problems to non-convex optimization. HARP works by finding a
smaller graph which approximates the global structure of its input. This
simplified graph is used to learn a set of initial representations, which serve
as good initializations for learning representations in the original, detailed
graph. We inductively extend this idea, by decomposing a graph in a series of
levels, and then embed the hierarchy of graphs from the coarsest one to the
original graph. HARP is a general meta-strategy to improve all of the
state-of-the-art neural algorithms for embedding graphs, including DeepWalk,
LINE, and Node2vec. Indeed, we demonstrate that applying HARP's hierarchical
paradigm yields improved implementations for all three of these methods, as
evaluated on both classification tasks on real-world graphs such as DBLP,
BlogCatalog, CiteSeer, and Arxiv, where we achieve a performance gain over the
original implementations by up to 14% Macro F1.
",1,0,0,0,0,0
7592,7593,"Long-range dynamical magnetic order and spin tunneling in the cooperative paramagnetic states of the pyrochlore analogous spinel antiferromagnets CdYb2X4 (X = S, Se)","  Magnetic systems with spins sitting on a lattice of corner sharing regular
tetrahedra have been particularly prolific for the discovery of new magnetic
states for the last two decades. The pyrochlore compounds have offered the
playground for these studies, while little attention has been comparatively
devoted to other compounds where the rare earth R occupies the same
sub-lattice, e.g. the spinel chalcogenides CdR2X4 (X = S, Se). Here we report
measurements performed on powder samples of this series with R = Yb using
specific heat, magnetic susceptibility, neutron diffraction and
muon-spin-relaxation measurements. The two compounds are found to be
magnetically similar. They long-range order into structures described by the
\Gamma_5 irreducible representation. The magnitude of the magnetic moment at
low temperature is 0.77 (1) and 0.62 (1) mu_B for X = S and Se, respectively.
Persistent spin dynamics is present in the ordered states. The spontaneous
field at the muon site is anomalously small, suggesting magnetic moment
fragmentation. A double spin-flip tunneling relaxation mechanism is suggested
in the cooperative paramagnetic state up to 10 K. The magnetic space groups
into which magnetic moments of systems of corner-sharing regular tetrahedra
order are provided for a number of insulating compounds characterized by null
propagation wavevectors.
",0,1,0,0,0,0
18425,18426,Non Uniform On Chip Power Delivery Network Synthesis Methodology,"  In this paper, we proposed a non-uniform power delivery network (PDN)
synthesis methodology. It first constructs initial PDN using uniform approach.
Then preliminary power integrity analysis is performed to derive IR-safe
candidate window. Congestion map is obtained based global route congestion
estimation. A self-adaptive non-uniform PDN synthesis is then performed to
globally and locally optimize PDN over selected regions. The PDN synthesis is
congestion-driven and IR- guarded. Experimental results show significant timing
important in trade-off small PDN length reduction with no EM/IR impact. We
further explored potential power savings using our non-uniform PDN synthesis
methodology.
",1,0,0,0,0,0
7617,7618,Finite-Size Effects in Non-Neutral Two-Dimensional Coulomb Fluids,"  Thermodynamic potential of a neutral two-dimensional (2D) Cou\-lomb fluid,
confined to a large domain with a smooth boundary, exhibits at any (inverse)
temperature $\beta$ a logarithmic finite-size correction term whose universal
prefactor depends only on the Euler number of the domain and the conformal
anomaly number $c=-1$. A minimal free boson conformal field theory, which is
equivalent to the 2D symmetric two-component plasma of elementary $\pm e$
charges at coupling constant $\Gamma=\beta e^2$, was studied in the past. It
was shown that creating a non-neutrality by spreading out a charge $Q e$ at
infinity modifies the anomaly number to $c(Q,\Gamma) = - 1 + 3\Gamma Q^2$.
Here, we study the effect of non-neutrality on the finite-size expansion of the
free energy for another Coulomb fluid, namely the 2D one-component plasma
(jellium) composed of identical pointlike $e$-charges in a homogeneous
background surface charge density. For the disk geometry of the confining
domain we find that the non-neutrality induces the same change of the anomaly
number in the finite-size expansion. We derive this result first at the
free-fermion coupling $\Gamma\equiv\beta e^2=2$ and then, by using a mapping of
the 2D one-component plasma onto an anticommuting field theory formulated on a
chain, for an arbitrary coupling constant.
",0,1,0,0,0,0
3288,3289,New constraints on time-dependent variations of fundamental constants using Planck data,"  Observations of the CMB today allow us to answer detailed questions about the
properties of our Universe, targeting both standard and non-standard physics.
In this paper, we study the effects of varying fundamental constants (i.e., the
fine-structure constant, $\alpha_{\rm EM}$, and electron rest mass, $m_{\rm
e}$) around last scattering using the recombination codes CosmoRec and
Recfast++. We approach the problem in a pedagogical manner, illustrating the
importance of various effects on the free electron fraction, Thomson visibility
function and CMB power spectra, highlighting various degeneracies. We
demonstrate that the simpler Recfast++ treatment (based on a three-level atom
approach) can be used to accurately represent the full computation of CosmoRec.
We also include explicit time-dependent variations using a phenomenological
power-law description. We reproduce previous Planck 2013 results in our
analysis. Assuming constant variations relative to the standard values, we find
the improved constraints $\alpha_{\rm EM}/\alpha_{\rm EM,0}=0.9993\pm 0.0025$
(CMB only) and $m_{\rm e}/m_{\rm e,0}= 1.0039 \pm 0.0074$ (including BAO) using
Planck 2015 data. For a redshift-dependent variation, $\alpha_{\rm
EM}(z)=\alpha_{\rm EM}(z_0)\,[(1+z)/1100]^p$ with $\alpha_{\rm
EM}(z_0)\equiv\alpha_{\rm EM,0}$ at $z_0=1100$, we obtain $p=0.0008\pm 0.0025$.
Allowing simultaneous variations of $\alpha_{\rm EM}(z_0)$ and $p$ yields
$\alpha_{\rm EM}(z_0)/\alpha_{\rm EM,0} = 0.9998\pm 0.0036$ and $p = 0.0006\pm
0.0036$. We also discuss combined limits on $\alpha_{\rm EM}$ and $m_{\rm e}$.
Our analysis shows that existing data is not only sensitive to the value of the
fundamental constants around recombination but also its first time derivative.
This suggests that a wider class of varying fundamental constant models can be
probed using the CMB.
",0,1,0,0,0,0
7902,7903,"Crystal field excitations and magnons: their roles in oxyselenides Pr2O2M2OSe2 (M = Mn, Fe)","  We present the results of neutron scattering experiments to study the crystal
and magnetic structures of the Mott-insulating transition metal oxyselenides
Pr2O2M2OSe2 (M = Mn, Fe). The structural role of the non-Kramers Pr3+ ion is
investigated and analysis of Pr3+ crystal field excitations performed.
Long-range order of Pr3+ moments in Pr2O2Fe2OSe2 can be induced by an applied
magnetic field.
",0,1,0,0,0,0
29,30,Density large deviations for multidimensional stochastic hyperbolic conservation laws,"  We investigate the density large deviation function for a multidimensional
conservation law in the vanishing viscosity limit, when the probability
concentrates on weak solutions of a hyperbolic conservation law conservation
law. When the conductivity and dif-fusivity matrices are proportional, i.e. an
Einstein-like relation is satisfied, the problem has been solved in [4]. When
this proportionality does not hold, we compute explicitly the large deviation
function for a step-like density profile, and we show that the associated
optimal current has a non trivial structure. We also derive a lower bound for
the large deviation function, valid for a general weak solution, and leave the
general large deviation function upper bound as a conjecture.
",0,1,1,0,0,0
16313,16314,A Polylogarithm Solution to the Epsilon--Delta Problem,"  Let $f$ be a continuous real function defined in a subset of the real line.
The standard definition of continuity at a point $x$ allow us to correlate any
given epsilon with a (possibly depending of $x$) delta value. This pairing is
known as the epsilon--delta relation of $f$. In this work, we demonstrate the
existence of a privileged choice of delta in the sense that it is continuous,
invertible, maximal and it is the solution of a simple functional equation. We
also introduce an algorithm that can be used to numerically calculate this map
in polylogarithm time, proving the computability of the epsilon--delta
relation. Finally, some examples are analyzed in order to showcase the accuracy
and effectiveness of these methods, even when the explicit formula for the
aforementioned privileged function is unknown due to the lack of analytical
tools for solving the functional equation.
",0,0,1,0,0,0
177,178,SU-RUG at the CoNLL-SIGMORPHON 2017 shared task: Morphological Inflection with Attentional Sequence-to-Sequence Models,"  This paper describes the Stockholm University/University of Groningen
(SU-RUG) system for the SIGMORPHON 2017 shared task on morphological
inflection. Our system is based on an attentional sequence-to-sequence neural
network model using Long Short-Term Memory (LSTM) cells, with joint training of
morphological inflection and the inverse transformation, i.e. lemmatization and
morphological analysis. Our system outperforms the baseline with a large
margin, and our submission ranks as the 4th best team for the track we
participate in (task 1, high-resource).
",1,0,0,0,0,0
632,633,Learning Models from Data with Measurement Error: Tackling Underreporting,"  Measurement error in observational datasets can lead to systematic bias in
inferences based on these datasets. As studies based on observational data are
increasingly used to inform decisions with real-world impact, it is critical
that we develop a robust set of techniques for analyzing and adjusting for
these biases. In this paper we present a method for estimating the distribution
of an outcome given a binary exposure that is subject to underreporting. Our
method is based on a missing data view of the measurement error problem, where
the true exposure is treated as a latent variable that is marginalized out of a
joint model. We prove three different conditions under which the outcome
distribution can still be identified from data containing only error-prone
observations of the exposure. We demonstrate this method on synthetic data and
analyze its sensitivity to near violations of the identifiability conditions.
Finally, we use this method to estimate the effects of maternal smoking and
opioid use during pregnancy on childhood obesity, two import problems from
public health. Using the proposed method, we estimate these effects using only
subject-reported drug use data and substantially refine the range of estimates
generated by a sensitivity analysis-based approach. Further, the estimates
produced by our method are consistent with existing literature on both the
effects of maternal smoking and the rate at which subjects underreport smoking.
",1,0,0,1,0,0
3402,3403,Limiting Behaviour of the Teichmüller Harmonic Map Flow,"  In this paper we study the Teichmüller harmonic map flow as introduced by
Rupflin and Topping [15]. It evolves pairs of maps and metrics $(u,g)$ into
branched minimal immersions, or equivalently into weakly conformal harmonic
maps, where $u$ maps from a fixed closed surface $M$ with metric $g$ to a
general target manifold $N$. It arises naturally as a gradient flow for the
Dirichlet energy functional viewed as acting on equivalence classes of such
pairs, obtained from the invariance under diffeomorphisms and conformal changes
of the domain metric.
In the construction of a suitable inner product for the gradient flow a
choice of relative weight of the map tangent directions and metric tangent
directions is made, which manifests itself in the appearance of a coupling
constant $\eta$ in the flow equations.
We study limits of the flow as $\eta$ approaches 0, corresponding to slowing
down the evolution of the metric.
We first show that given a smooth harmonic map flow on a fixed time interval,
the Teichmüller harmonic map flows starting at the same initial data converge
uniformly to the underlying harmonic map flow when $\eta \downarrow 0$.
Next we consider a rescaling of time, which increases the speed of the map
evolution while evolving the metric at a constant rate. We show that under
appropriate topological assumptions, in the limit the rescaled flows converge
to a unique flow through harmonic maps with the metric evolving in the
direction of the real part of the Hopf differential.
",0,0,1,0,0,0
4496,4497,"Explicit minimisation of a convex quadratic under a general quadratic constraint: a global, analytic approach","  A novel approach is introduced to a very widely occurring problem, providing
a complete, explicit resolution of it: minimisation of a convex quadratic under
a general quadratic, equality or inequality, constraint. Completeness comes via
identification of a set of mutually exclusive and exhaustive special cases.
Explicitness, via algebraic expressions for each solution set. Throughout,
underlying geometry illuminates and informs algebraic development. In
particular, centrally to this new approach, affine equivalence is exploited to
re-express the same problem in simpler coordinate systems. Overall, the
analysis presented provides insight into the diverse forms taken both by the
problem itself and its solution set, showing how each may be intrinsically
unstable. Comparisons of this global, analytic approach with the, intrinsically
complementary, local, computational approach of (generalised) trust region
methods point to potential synergies between them. Points of contact with
simultaneous diagonalisation results are noted.
",0,0,1,1,0,0
9508,9509,Secular Orbit Evolution in Systems with a Strong External Perturber - A Simple and Accurate Model,"  We present a semi-analytical correction to the seminal solution for the
secular motion of a planet's orbit under gravitational influence of an external
perturber derived by Heppenheimer (1978). A comparison between analytical
predictions and numerical simulations allows us to determine corrective factors
for the secular frequency and forced eccentricity in the co-planar restricted
three-body problem. The correction is given in the form of a polynomial
function of the system's parameters that can be applied to first-order forced
eccentricity and secular frequency estimates. The resulting secular equations
are simple, straight forward to use and improve the fidelity of Heppenheimer's
solution well beyond higher-order models. The quality and convergence of the
corrected secular equations are tested for a wide range of parameters and
limits of its applicability are given.
",0,1,0,0,0,0
5932,5933,Modelling wave-induced sea ice breakup in the marginal ice zone,"  A model of ice floe breakup under ocean wave forcing in the marginal ice zone
(MIZ) is proposed to investigate how floe size distribution (FSD) evolves under
repeated wave breakup events. A three-dimensional linear model of ocean wave
scattering by a finite array of compliant circular ice floes is coupled to a
flexural failure model, which breaks a floe into two floes provided the
two-dimensional stress field satisfies a breakup criterion. A closed-feedback
loop algorithm is devised, which (i)~solves wave scattering problem for a given
FSD under time-harmonic plane wave forcing, (ii)~computes the stress field in
all the floes, (iii)~fractures the floes satisfying the breakup criterion and
(iv)~generates an updated FSD, initialising the geometry for the next iteration
of the loop.The FSD after 50 breakup events is uni-modal and near normal, or
bi-modal. Multiple scattering is found to enhance breakup for long waves and
thin ice, but to reduce breakup for short waves and thick ice. A breakup front
marches forward in the latter regime, as wave-induced fracture weakens the ice
cover allowing waves to travel deeper into the MIZ.
",0,1,0,0,0,0
18269,18270,Moving Horizon Estimation for ARMAX process with t-Distribution Noise,"  In this paper, instead of the usual Gaussian noise assumption,
$t$-distribution noise is assumed. A Maximum Likelihood Estimator using the
most recent N measurements is proposed for the Auto-Regressive-Moving-Average
with eXogenous input (ARMAX) process with this assumption. The proposed
estimator is robust to outliers because the `thick tail' of the t-distribution
reduces the effect of large errors in the likelihood function. Instead of
solving the resulting nonlinear estimator numerically, the Influence Function
is used to formulate a computationally efficient recursive solution, which
reduces to the traditional Moving Horizon Estimator when the noise is Gaussian.
The formula for the variance of the estimate is derived. This formula shows
explicitly how the variance of the estimate is affected by the number of
measurements and noise variance. The simulation results show that the proposed
estimator has smaller variance and is more robust to outliers than the Moving
Window Least-Squares Estimator. For the same accuracy, the proposed estimator
is an order of magnitude faster than the particle filter.
",1,0,0,0,0,0
15194,15195,Variable Selection for Highly Correlated Predictors,"  Penalty-based variable selection methods are powerful in selecting relevant
covariates and estimating coefficients simultaneously. However, variable
selection could fail to be consistent when covariates are highly correlated.
The partial correlation approach has been adopted to solve the problem with
correlated covariates. Nevertheless, the restrictive range of partial
correlation is not effective for capturing signal strength for relevant
covariates. In this paper, we propose a new Semi-standard PArtial Covariance
(SPAC) which is able to reduce correlation effects from other predictors while
incorporating the magnitude of coefficients. The proposed SPAC variable
selection facilitates choosing covariates which have direct association with
the response variable, via utilizing dependency among covariates. We show that
the proposed method with the Lasso penalty (SPAC-Lasso) enjoys strong sign
consistency in both finite-dimensional and high-dimensional settings under
regularity conditions. Simulation studies and the `HapMap' gene data
application show that the proposed method outperforms the traditional Lasso,
adaptive Lasso, SCAD, and Peter-Clark-simple (PC-simple) methods for highly
correlated predictors.
",0,0,1,1,0,0
6268,6269,Spatial localization for nonlinear dynamical stochastic models for excitable media,"  Nonlinear dynamical stochastic models are ubiquitous in different areas.
Excitable media models are typical examples with large state dimensions. Their
statistical properties are often of great interest but are also very
challenging to compute. In this article, a theoretical framework to understand
the spatial localization for a large class of stochastically coupled nonlinear
systems in high dimensions is developed. Rigorous mathematical theories show
the covariance decay behavior due to both local and nonlocal effects, which
result from the diffusion and the mean field interaction, respectively. The
analysis is based on a comparison with an appropriate linear surrogate model,
of which the covariance propagation can be computed explicitly. Two important
applications of these theoretical results are discussed. They are the spatial
averaging strategy for efficiently sampling the covariance matrix and the
localization technique in data assimilation. Test examples of a surrogate
linear model and a stochastically coupled FitzHugh-Nagumo model for excitable
media are adopted to validate the theoretical results. The latter is also used
for a systematical study of the spatial averaging strategy in efficiently
sampling the covariance matrix in different dynamical regimes.
",0,0,1,1,0,0
5657,5658,Taxonomy Induction using Hypernym Subsequences,"  We propose a novel, semi-supervised approach towards domain taxonomy
induction from an input vocabulary of seed terms. Unlike all previous
approaches, which typically extract direct hypernym edges for terms, our
approach utilizes a novel probabilistic framework to extract hypernym
subsequences. Taxonomy induction from extracted subsequences is cast as an
instance of the minimumcost flow problem on a carefully designed directed
graph. Through experiments, we demonstrate that our approach outperforms
stateof- the-art taxonomy induction approaches across four languages.
Importantly, we also show that our approach is robust to the presence of noise
in the input vocabulary. To the best of our knowledge, no previous approaches
have been empirically proven to manifest noise-robustness in the input
vocabulary.
",1,0,0,0,0,0
15597,15598,On maxispaces of nonparametric tests,"  For the problems of nonparametric hypothesis testing we introduce the notion
of maxisets and maxispace. We point out the maxisets of $\chi^2-$tests,
Cramer-von Mises tests, tests generated $\mathbb{L}_2$- norms of kernel
estimators and tests generated quadratic forms of estimators of Fourier
coefficients. For these tests we show that, if sequence of alternatives having
given rates of convergence to hypothesis is consistent, then each altehrnative
can be broken down into the sum of two parts: a function belonging to maxiset
and orthogonal function. Sequence of functions belonging to maxiset is
consistent sequence of alternatives.
We point out asymptotically minimax tests if sets of alternatives are maxiset
with deleted ""small"" $\mathbb{L}_2$-balls.
",0,0,1,1,0,0
4444,4445,Graded super duality for general linear Lie superalgebras,"  We provide a new proof of the super duality equivalence between infinite-rank
parabolic BGG categories of general linear Lie (super) algebras conjectured by
Cheng and Wang and first proved by Cheng and Lam. We do this by establishing a
new uniqueness theorem for tensor product categorifications motivated by work
of Brundan, Losev, and Webster. Moreover we show that these BGG categories have
Koszul graded lifts and super duality can be lifted to a graded equivalence.
",0,0,1,0,0,0
8056,8057,SMAGEXP: a galaxy tool suite for transcriptomics data meta-analysis,"  Bakground: With the proliferation of available microarray and high throughput
sequencing experiments in the public domain, the use of meta-analysis methods
increases. In these experiments, where the sample size is often limited,
meta-analysis offers the possibility to considerably enhance the statistical
power and give more accurate results. For those purposes, it combines either
effect sizes or results of single studies in a appropriate manner. R packages
metaMA and metaRNASeq perform meta-analysis on microarray and NGS data,
respectively. They are not interchangeable as they rely on statistical modeling
specific to each technology.
Results: SMAGEXP (Statistical Meta-Analysis for Gene EXPression) integrates
metaMA and metaRNAseq packages into Galaxy. We aim to propose a unified way to
carry out meta-analysis of gene expression data, while taking care of their
specificities. We have developed this tool suite to analyse microarray data
from Gene Expression Omnibus (GEO) database or custom data from affymetrix
microarrays. These data are then combined to carry out meta-analysis using
metaMA package. SMAGEXP also offers to combine raw read counts from Next
Generation Sequencing (NGS) experiments using DESeq2 and metaRNASeq package. In
both cases, key values, independent from the technology type, are reported to
judge the quality of the meta-analysis. These tools are available on the Galaxy
main tool shed. Source code, help and installation instructions are available
on github.
Conclusion: The use of Galaxy offers an easy-to-use gene expression
meta-analysis tool suite based on the metaMA and metaRNASeq packages.
",0,0,0,1,1,0
20855,20856,Phase-type distributions in population genetics,"  Probability modelling for DNA sequence evolution is well established and
provides a rich framework for understanding genetic variation between samples
of individuals from one or more populations. We show that both classical and
more recent models for coalescence (with or without recombination) can be
described in terms of the so-called phase-type theory, where complicated and
tedious calculations are circumvented by the use of matrices. The application
of phase-type theory consists of describing the stochastic model as a Markov
model by appropriately setting up a state space and calculating the
corresponding intensity and reward matrices. Formulae of interest are then
expressed in terms of these aforementioned matrices. We illustrate this by a
few examples calculating the mean, variance and even higher order moments of
the site frequency spectrum in the multiple merger coalescent models, and by
analysing the mean and variance for the number of segregating sites for
multiple samples in the two-locus ancestral recombination graph. We believe
that phase-type theory has great potential as a tool for analysing probability
models in population genetics. The compact matrix notation is useful for
clarification of current models, in particular their formal manipulation
(calculation), but also for further development or extensions.
",0,0,0,1,1,0
17227,17228,Two- and three-dimensional wide-field weak lensing mass maps from the Hyper Suprime-Cam Subaru Strategic Program S16A data,"  We present wide-field (167 deg$^2$) weak lensing mass maps from the Hyper
Supreme-Cam Subaru Strategic Program (HSC-SSP). We compare these weak lensing
based dark matter maps with maps of the distribution of the stellar mass
associated with luminous red galaxies. We find a strong correlation between
these two maps with a correlation coefficient of $\rho=0.54\pm0.03$ (for a
smoothing size of $8'$). This correlation is detected even with a smaller
smoothing scale of $2'$ ($\rho=0.34\pm 0.01$). This detection is made uniquely
possible because of the high source density of the HSC-SSP weak lensing survey
($\bar{n}\sim 25$ arcmin$^{-2}$). We also present a variety of tests to
demonstrate that our maps are not significantly affected by systematic effects.
By using the photometric redshift information associated with source galaxies,
we reconstruct a three-dimensional mass map. This three-dimensional mass map is
also found to correlate with the three-dimensional galaxy mass map.
Cross-correlation tests presented in this paper demonstrate that the HSC-SSP
weak lensing mass maps are ready for further science analyses.
",0,1,0,0,0,0
18645,18646,High Performance Parallel Image Reconstruction for New Vacuum Solar Telescope,"  Many technologies have been developed to help improve spatial resolution of
observational images for ground-based solar telescopes, such as adaptive optics
(AO) systems and post-processing reconstruction. As any AO system correction is
only partial, it is indispensable to use post-processing reconstruction
techniques. In the New Vacuum Solar Telescope (NVST), speckle masking method is
used to achieve the diffraction limited resolution of the telescope. Although
the method is very promising, the computation is quite intensive, and the
amount of data is tremendous, requiring several months to reconstruct
observational data of one day on a high-end computer. To accelerate image
reconstruction, we parallelize the program package on a high performance
cluster. We describe parallel implementation details for several reconstruction
procedures. The code is written in C language using Message Passing Interface
(MPI) and optimized for parallel processing in a multi-processor environment.
We show the excellent performance of parallel implementation, and the whole
data processing speed is about 71 times faster than before. Finally, we analyze
the scalability of the code to find possible bottlenecks, and propose several
ways to further improve the parallel performance. We conclude that the
presented program is capable of executing in real-time reconstruction
applications at NVST.
",0,1,0,0,0,0
19904,19905,Solution of the Lindblad equation for spin helix states,"  Using Lindblad dynamics we study quantum spin systems with dissipative
boundary dynamics that generate a stationary nonequilibrium state with a
non-vanishing spin current that is locally conserved except at the boundaries.
We demonstrate that with suitably chosen boundary target states one can solve
the many-body Lindblad equation exactly in any dimension. As solution we obtain
pure states at any finite value of the dissipation strength and any system
size. They are characterized by a helical stationary magnetization profile and
a superdiffusive ballistic current of order one, independent of system size
even when the quantum spin system is not integrable. These results are derived
in explicit form for the one-dimensional spin-1/2 Heisenberg chain and its
higher-spin generalizations (which include for spin-1 the integrable
Zamolodchikov-Fateev model and the bi-quadratic Heisenberg chain). The
extension of the results to higher dimensions is straightforward.
",0,1,0,0,0,0
14451,14452,A Faster Solution to Smale's 17th Problem I: Real Binomial Systems,"  Suppose $F:=(f_1,\ldots,f_n)$ is a system of random $n$-variate polynomials
with $f_i$ having degree $\leq\!d_i$ and the coefficient of $x^{a_1}_1\cdots
x^{a_n}_n$ in $f_i$ being an independent complex Gaussian of mean $0$ and
variance $\frac{d_i!}{a_1!\cdots a_n!\left(d_i-\sum^n_{j=1}a_j \right)!}$.
Recent progress on Smale's 17th Problem by Lairez --- building upon seminal
work of Shub, Beltran, Pardo, Bürgisser, and Cucker --- has resulted in a
deterministic algorithm that finds a single (complex) approximate root of $F$
using just $N^{O(1)}$ arithmetic operations on average, where
$N\!:=\!\sum^n_{i=1}\frac{(n+d_i)!}{n!d_i!}$ ($=n(n+\max_i
d_i)^{O(\min\{n,\max_i d_i)\}}$) is the maximum possible total number of
monomial terms for such an $F$. However, can one go faster when the number of
terms is smaller, and we restrict to real coefficient and real roots? And can
one still maintain average-case polynomial-time with more general probability
measures?
We show the answer is yes when $F$ is instead a binomial system --- a case
whose numerical solution is a key step in polyhedral homotopy algorithms for
solving arbitrary polynomial systems. We give a deterministic algorithm that
finds a real approximate root (or correctly decides there are none) using just
$O(n^2(\log(n)+\log\max_i d_i))$ arithmetic operations on average. Furthermore,
our approach allows Gaussians with arbitrary variance. We also discuss briefly
the obstructions to maintaining average-case time polynomial in $n\log \max_i
d_i$ when $F$ has more terms.
",1,0,0,0,0,0
16325,16326,Dynamical inverse problem for Jacobi matrices,"  We consider the inverse dynamical problem for the dynamical system with
discrete time associated with the semi-infinite Jacobi matrix. We solve the
inverse problem for such a system and answer a question on the characterization
of the inverse data. As a by-product we give a necessary and sufficient
condition for the measure on the real line line to be the spectral measure of
semi-infinite discrete Schrodinger operator.
",0,0,1,0,0,0
496,497,On the self-duality of rings of integers in tame and abelian extensions,"  Let $L/K$ be a tame and Galois extension of number fields with group $G$. It
is well-known that any ambiguous ideal in $L$ is locally free over
$\mathcal{O}_KG$ (of rank one), and so it defines a class in the locally free
class group of $\mathcal{O}_KG$, where $\mathcal{O}_K$ denotes the ring of
integers of $K$. In this paper, we shall study the relationship among the
classes arising from the ring of integers $\mathcal{O}_L$ of $L$, the inverse
different $\mathfrak{D}_{L/K}^{-1}$ of $L/K$, and the square root of the
inverse different $A_{L/K}$ of $L/K$ (if it exists), in the case that $G$ is
abelian. They are naturally related because $A_{L/K}^2 =
\mathfrak{D}_{L/K}^{-1} = \mathcal{O}_L^*$, and $A_{L/K}$ is special because
$A_{L/K} = A_{L/K}^*$, where $*$ denotes dual with respect to the trace of
$L/K$.
",0,0,1,0,0,0
19814,19815,New bounds for the Probability of Causation in Mediation Analysis,"  An individual has been subjected to some exposure and has developed some
outcome. Using data on similar individuals, we wish to evaluate, for this case,
the probability that the outcome was in fact caused by the exposure. Even with
the best possible experimental data on exposure and outcome, we typically can
not identify this ""probability of causation"" exactly, but we can provide
information in the form of bounds for it. Under appropriate assumptions, these
bounds can be tightened if we can make other observations (e.g., on
non-experimental cases), measure additional variables (e.g., covariates) or
measure complete mediators. In this work we propose new bounds for the case
that a third variable mediates partially the effect of the exposure on the
outcome.
",0,0,1,1,0,0
10910,10911,Photo-realistic Facial Texture Transfer,"  Style transfer methods have achieved significant success in recent years with
the use of convolutional neural networks. However, many of these methods
concentrate on artistic style transfer with few constraints on the output image
appearance. We address the challenging problem of transferring face texture
from a style face image to a content face image in a photorealistic manner
without changing the identity of the original content image. Our framework for
face texture transfer (FaceTex) augments the prior work of MRF-CNN with a novel
facial semantic regularization that incorporates a face prior regularization
smoothly suppressing the changes around facial meso-structures (e.g eyes, nose
and mouth) and a facial structure loss function which implicitly preserves the
facial structure so that face texture can be transferred without changing the
original identity. We demonstrate results on face images and compare our
approach with recent state-of-the-art methods. Our results demonstrate superior
texture transfer because of the ability to maintain the identity of the
original face image.
",1,0,0,0,0,0
10029,10030,Involvement of Surfactant Protein D in Ebola Virus Infection Enhancement via Glycoprotein Interaction,"  Since the largest 2014-2016 Ebola virus disease outbreak in West Africa,
understanding of Ebola virus infection has improved, notably the involvement of
innate immune mediators. Amongst them, collectins are important players in the
antiviral innate immune defense. A screening of Ebola glycoprotein
(GP)-collectins interactions revealed the specific interaction of human
surfactant protein D (hSP-D), a lectin expressed in lung and liver, two
compartments where Ebola was found in vivo. Further analyses have demonstrated
an involvement of hSP-D in the enhancement of virus infection in several in
vitro models. Similar effects were observed for porcine SP-D (pSP-D). In
addition, both hSP-D and pSP-D interacted with Reston virus (RESTV) GP and
enhanced pseudoviral infection in pulmonary cells. Thus, our study reveals a
novel partner of Ebola GP that may participate to enhance viral spread.
",0,0,0,0,1,0
10257,10258,Bayesian Hypernetworks,"  We study Bayesian hypernetworks: a framework for approximate Bayesian
inference in neural networks. A Bayesian hypernetwork $\h$ is a neural network
which learns to transform a simple noise distribution, $p(\vec\epsilon) =
\N(\vec 0,\mat I)$, to a distribution $q(\pp) := q(h(\vec\epsilon))$ over the
parameters $\pp$ of another neural network (the ""primary network"")\@. We train
$q$ with variational inference, using an invertible $\h$ to enable efficient
estimation of the variational lower bound on the posterior $p(\pp | \D)$ via
sampling. In contrast to most methods for Bayesian deep learning, Bayesian
hypernets can represent a complex multimodal approximate posterior with
correlations between parameters, while enabling cheap iid sampling of~$q(\pp)$.
In practice, Bayesian hypernets can provide a better defense against
adversarial examples than dropout, and also exhibit competitive performance on
a suite of tasks which evaluate model uncertainty, including regularization,
active learning, and anomaly detection.
",1,0,0,1,0,0
3522,3523,"A Lagrangian fluctuation-dissipation relation for scalar turbulence, III. Turbulent Rayleigh-Bénard convection","  A Lagrangian fluctuation-dissipation relation has been derived in a previous
work to describe the dissipation rate of advected scalars, both passive and
active, in wall-bounded flows. We apply this relation here to develop a
Lagrangian description of thermal dissipation in turbulent Rayleigh-Bénard
convection in a right-cylindrical cell of arbitrary cross-section, with either
imposed temperature difference or imposed heat-flux at the top and bottom
walls. We obtain an exact relation between the steady-state thermal dissipation
rate and the time for passive tracer particles released at the top or bottom
wall to mix to their final uniform value near those walls. We show that an
""ultimate regime"" with the Nusselt-number scaling predicted by Spiegel (1971)
or, with a log-correction, by Kraichnan (1962) will occur at high Rayleigh
numbers, unless this near-wall mixing time is asymptotically much longer than
the free-fall time, or almost the large-scale circulation time. We suggest a
new criterion for an ultimate regime in terms of transition to turbulence of a
thermal ""mixing zone"", which is much wider than the standard thermal boundary
layer. Kraichnan-Spiegel scaling may, however, not hold if the intensity and
volume of thermal plumes decrease sufficiently rapidly with increasing Rayleigh
number. To help resolve this issue, we suggest a program to measure the
near-wall mixing time, which we argue is accessible both by laboratory
experiment and by numerical simulation.
",0,1,0,0,0,0
11790,11791,Variational Walkback: Learning a Transition Operator as a Stochastic Recurrent Net,"  We propose a novel method to directly learn a stochastic transition operator
whose repeated application provides generated samples. Traditional undirected
graphical models approach this problem indirectly by learning a Markov chain
model whose stationary distribution obeys detailed balance with respect to a
parameterized energy function. The energy function is then modified so the
model and data distributions match, with no guarantee on the number of steps
required for the Markov chain to converge. Moreover, the detailed balance
condition is highly restrictive: energy based models corresponding to neural
networks must have symmetric weights, unlike biological neural circuits. In
contrast, we develop a method for directly learning arbitrarily parameterized
transition operators capable of expressing non-equilibrium stationary
distributions that violate detailed balance, thereby enabling us to learn more
biologically plausible asymmetric neural networks and more general non-energy
based dynamical systems. The proposed training objective, which we derive via
principled variational methods, encourages the transition operator to ""walk
back"" in multi-step trajectories that start at data-points, as quickly as
possible back to the original data points. We present a series of experimental
results illustrating the soundness of the proposed approach, Variational
Walkback (VW), on the MNIST, CIFAR-10, SVHN and CelebA datasets, demonstrating
superior samples compared to earlier attempts to learn a transition operator.
We also show that although each rapid training trajectory is limited to a
finite but variable number of steps, our transition operator continues to
generate good samples well past the length of such trajectories, thereby
demonstrating the match of its non-equilibrium stationary distribution to the
data distribution. Source Code: this http URL
",1,0,0,1,0,0
230,231,Photonic topological pumping through the edges of a dynamical four-dimensional quantum Hall system,"  When a two-dimensional electron gas is exposed to a perpendicular magnetic
field and an in-plane electric field, its conductance becomes quantized in the
transverse in-plane direction: this is known as the quantum Hall (QH) effect.
This effect is a result of the nontrivial topology of the system's electronic
band structure, where an integer topological invariant known as the first Chern
number leads to the quantization of the Hall conductance. Interestingly, it was
shown that the QH effect can be generalized mathematically to four spatial
dimensions (4D), but this effect has never been realized for the obvious reason
that experimental systems are bound to three spatial dimensions. In this work,
we harness the high tunability and control offered by photonic waveguide arrays
to experimentally realize a dynamically-generated 4D QH system using a 2D array
of coupled optical waveguides. The inter-waveguide separation is constructed
such that the propagation of light along the device samples over
higher-dimensional momenta in the directions orthogonal to the two physical
dimensions, thus realizing a 2D topological pump. As a result, the device's
band structure is associated with 4D topological invariants known as second
Chern numbers which support a quantized bulk Hall response with a 4D symmetry.
In a finite-sized system, the 4D topological bulk response is carried by
localized edges modes that cross the sample as a function of of the modulated
auxiliary momenta. We directly observe this crossing through photon pumping
from edge-to-edge and corner-to-corner of our system. These are equivalent to
the pumping of charge across a 4D system from one 3D hypersurface to the
opposite one and from one 2D hyperedge to another, and serve as first
experimental realization of higher-dimensional topological physics.
",0,1,0,0,0,0
19934,19935,Emotion Intensities in Tweets,"  This paper examines the task of detecting intensity of emotion from text. We
create the first datasets of tweets annotated for anger, fear, joy, and sadness
intensities. We use a technique called best--worst scaling (BWS) that improves
annotation consistency and obtains reliable fine-grained scores. We show that
emotion-word hashtags often impact emotion intensity, usually conveying a more
intense emotion. Finally, we create a benchmark regression system and conduct
experiments to determine: which features are useful for detecting emotion
intensity, and, the extent to which two emotions are similar in terms of how
they manifest in language.
",1,0,0,0,0,0
15246,15247,On Green's proof of infinitesimal Torelli theorem for hypersurfaces,"  We prove an equivalence between the infinitesimal Torelli theorem for top
forms on a hypersurface contained inside a Grassmannian $\mathbb G$ and the
theory of adjoint volume forms presented in L. Rizzi, F. Zucconi, ""Generalized
adjoint forms on algebraic varieties"", Ann. Mat. Pura e Applicata, in press.
More precisely, via this theory and a suitable generalization of Macaulay's
theorem we show that the differential of the period map vanishes on an
infinitesimal deformation if and only if certain explicitly given twisted
volume forms go in the generalized Jacobi ideal of $X$ via the cup product
homomorphism.
",0,0,1,0,0,0
19524,19525,A Novel Stretch Energy Minimization Algorithm for Equiareal Parameterizations,"  Surface parameterizations have been widely applied to computer graphics and
digital geometry processing. In this paper, we propose a novel stretch energy
minimization (SEM) algorithm for the computation of equiareal parameterizations
of simply connected open surfaces with a very small area distortion and a
highly improved computational efficiency. In addition, the existence of
nontrivial limit points of the SEM algorithm is guaranteed under some mild
assumptions of the mesh quality. Numerical experiments indicate that the
efficiency, accuracy, and robustness of the proposed SEM algorithm outperform
other state-of-the-art algorithms. Applications of the SEM on surface remeshing
and surface registration for simply connected open surfaces are demonstrated
thereafter. Thanks to the SEM algorithm, the computations for these
applications can be carried out efficiently and robustly.
",1,0,0,0,0,0
18248,18249,Set-Based Tests for Genetic Association Using the Generalized Berk-Jones Statistic,"  Studying the effects of groups of Single Nucleotide Polymorphisms (SNPs), as
in a gene, genetic pathway, or network, can provide novel insight into complex
diseases, above that which can be gleaned from studying SNPs individually.
Common challenges in set-based genetic association testing include weak effect
sizes, correlation between SNPs in a SNP-set, and scarcity of signals, with
single-SNP effects often ranging from extremely sparse to moderately sparse in
number. Motivated by these challenges, we propose the Generalized Berk-Jones
(GBJ) test for the association between a SNP-set and outcome. The GBJ extends
the Berk-Jones (BJ) statistic by accounting for correlation among SNPs, and it
provides advantages over the Generalized Higher Criticism (GHC) test when
signals in a SNP-set are moderately sparse. We also provide an analytic p-value
calculation procedure for SNP-sets of any finite size. Using this p-value
calculation, we illustrate that the rejection region for GBJ can be described
as a compromise of those for BJ and GHC. We develop an omnibus statistic as
well, and we show that this omnibus test is robust to the degree of signal
sparsity. An additional advantage of our method is the ability to conduct
inference using individual SNP summary statistics from a genome-wide
association study. We evaluate the finite sample performance of the GBJ though
simulation studies and application to gene-level association analysis of breast
cancer risk.
",0,0,0,1,0,0
8292,8293,Label Sanitization against Label Flipping Poisoning Attacks,"  Many machine learning systems rely on data collected in the wild from
untrusted sources, exposing the learning algorithms to data poisoning.
Attackers can inject malicious data in the training dataset to subvert the
learning process, compromising the performance of the algorithm producing
errors in a targeted or an indiscriminate way. Label flipping attacks are a
special case of data poisoning, where the attacker can control the labels
assigned to a fraction of the training points. Even if the capabilities of the
attacker are constrained, these attacks have been shown to be effective to
significantly degrade the performance of the system. In this paper we propose
an efficient algorithm to perform optimal label flipping poisoning attacks and
a mechanism to detect and relabel suspicious data points, mitigating the effect
of such poisoning attacks.
",0,0,0,1,0,0
3435,3436,Noncommutative hyperbolic metrics,"  We characterize certain noncommutative domains in terms of noncommutative
holomorphic equivalence via a pseudometric that we define in purely algebraic
terms. We prove some properties of this pseudometric and provide an application
to free probability.
",0,0,1,0,0,0
4519,4520,Photon propagation through linearly active dimers,"  We provide an analytic propagator for non-Hermitian dimers showing linear
gain or losses in the quantum regime. In particular, we focus on experimentally
feasible realizations of the $\mathcal{PT}$-symmetric dimer and provide their
mean photon number and second order two-point correlation. We study the
propagation of vacuum, single photon spatially-separable, and two-photon
spatially-entangled states. We show that each configuration produces a
particular signature that might signal their possible uses as photon switches,
semi-classical intensity-tunable sources, or spatially entangled sources to
mention a few possible applications.
",0,1,0,0,0,0
11727,11728,A parity-breaking electronic nematic phase transition in the spin-orbit coupled metal Cd$_2$Re$_2$O$_7$,"  Strong electron interactions can drive metallic systems toward a variety of
well-known symmetry-broken phases, but the instabilities of correlated metals
with strong spin-orbit coupling have only recently begun to be explored. We
uncovered a multipolar nematic phase of matter in the metallic pyrochlore
Cd$_2$Re$_2$O$_7$ using spatially resolved second-harmonic optical anisotropy
measurements. Like previously discovered electronic nematic phases, this
multipolar phase spontaneously breaks rotational symmetry while preserving
translational invariance. However, it has the distinguishing property of being
odd under spatial inversion, which is allowed only in the presence of
spin-orbit coupling. By examining the critical behavior of the multipolar
nematic order parameter, we show that it drives the thermal phase transition
near 200 kelvin in Cd$_2$Re$_2$O$_7$ and induces a parity-breaking lattice
distortion as a secondary order.
",0,1,0,0,0,0
11550,11551,Quasiparticle interference in multiband superconductors with strong coupling,"  We develop a theory of the quasiparticle interference (QPI) in multiband
superconductors based on strong-coupling Eliashberg approach within the Born
approximation. In the framework of this theory, we study dependencies of the
QPI response function in the multiband superconductors with nodeless s-wave
superconductive order parameter. We pay a special attention to the difference
of the quasiparticle scattering between the bands having the same and opposite
signs of the order parameter. We show that, at the momentum values close to the
momentum transfer between two bands, the energy dependence of the quasiparticle
interference response function has three singularities. Two of these correspond
to the values of the gap functions and the third one depends on both the gaps
and the transfer momentum. We argue that only the singularity near the smallest
band gap may be used as an universal tool to distinguish between $s_{++}$ and
$s_{\pm}$ order parameters. The robustness of the sign of the response function
peak near the smaller gap value, irrespective of the change in parameters, in
both the symmetry cases is a promising feature that can be harnessed
experimentally.
",0,1,0,0,0,0
9098,9099,The sum of log-normal variates in geometric Brownian motion,"  Geometric Brownian motion (GBM) is a key model for representing
self-reproducing entities. Self-reproduction may be considered the definition
of life [5], and the dynamics it induces are of interest to those concerned
with living systems from biology to economics. Trajectories of GBM are
distributed according to the well-known log-normal density, broadening with
time. However, in many applications, what's of interest is not a single
trajectory but the sum, or average, of several trajectories. The distribution
of these objects is more complicated. Here we show two different ways of
finding their typical trajectories. We make use of an intriguing connection to
spin glasses: the expected free energy of the random energy model is an average
of log-normal variates. We make the mapping to GBM explicit and find that the
free energy result gives qualitatively correct behavior for GBM trajectories.
We then also compute the typical sum of lognormal variates using Ito calculus.
This alternative route is in close quantitative agreement with numerical work.
",0,0,0,0,0,1
13546,13547,Efficient Principal Subspace Projection of Streaming Data Through Fast Similarity Matching,"  Big data problems frequently require processing datasets in a streaming
fashion, either because all data are available at once but collectively are
larger than available memory or because the data intrinsically arrive one data
point at a time and must be processed online. Here, we introduce a
computationally efficient version of similarity matching, a framework for
online dimensionality reduction that incrementally estimates the top
K-dimensional principal subspace of streamed data while keeping in memory only
the last sample and the current iterate. To assess the performance of our
approach, we construct and make public a test suite containing both a synthetic
data generator and the infrastructure to test online dimensionality reduction
algorithms on real datasets, as well as performant implementations of our
algorithm and competing algorithms with similar aims. Among the algorithms
considered we find our approach to be competitive, performing among the best on
both synthetic and real data.
",0,0,0,1,0,0
6448,6449,Focusing light through dynamical samples using fast closed-loop wavefront optimization,"  We describe a fast closed-loop optimization wavefront shaping system able to
focus light through dynamic scattering media. A MEMS-based spatial light
modulator (SLM), a fast photodetector and FPGA electronics are combined to
implement a closed-loop optimization of a wavefront with a single mode
optimization rate of 4.1 kHz. The system performances are demonstrated by
focusing light through colloidal solutions of TiO2 particles in glycerol with
tunable temporal stability.
",0,1,0,0,0,0
6323,6324,Actions Speak Louder Than Goals: Valuing Player Actions in Soccer,"  Assessing the impact of the individual actions performed by soccer players
during games is a crucial aspect of the player recruitment process.
Unfortunately, most traditional metrics fall short in addressing this task as
they either focus on rare events like shots and goals alone or fail to account
for the context in which the actions occurred. This paper introduces a novel
advanced soccer metric for valuing any type of individual player action on the
pitch, be it with or without the ball. Our metric values each player action
based on its impact on the game outcome while accounting for the circumstances
under which the action happened. When applied to on-the-ball actions like
passes, dribbles, and shots alone, our metric identifies Argentine forward
Lionel Messi, French teenage star Kylian Mbappé, and Belgian winger Eden
Hazard as the most effective players during the 2016/2017 season.
",0,0,0,1,0,0
12874,12875,SPECTRE: Seedless Network Alignment via Spectral Centralities,"  Network alignment consists of finding a correspondence between the nodes of
two networks. From aligning proteins in computational biology, to
de-anonymization of social networks, to recognition tasks in computer vision,
this problem has applications in many diverse areas. The current approaches to
network alignment mostly focus on the case where prior information is
available, either in the form of a seed set of correctly-matched nodes or
attributes on the nodes and/or edges. Moreover, those approaches which assume
no such prior information tend to be computationally expensive and do not scale
to large-scale networks. However, many real-world networks are very large in
size, and prior information can be expensive, if not impossible, to obtain. In
this paper we introduce SPECTRE, a scalable, accurate algorithm able to solve
the network alignment problem with no prior information. SPECTRE makes use of
spectral centrality measures and percolation techniques to robustly align nodes
across networks, even if those networks exhibit only moderate correlation.
Through extensive numerical experiments, we show that SPECTRE is able to
recover high-accuracy alignments on both synthetic and real-world networks, and
outperforms other algorithms in the seedless case.
",1,0,0,0,0,0
7731,7732,How LinkedIn Economic Graph Bonds Information and Product: Applications in LinkedIn Salary,"  The LinkedIn Salary product was launched in late 2016 with the goal of
providing insights on compensation distribution to job seekers, so that they
can make more informed decisions when discovering and assessing career
opportunities. The compensation insights are provided based on data collected
from LinkedIn members and aggregated in a privacy-preserving manner. Given the
simultaneous desire for computing robust, reliable insights and for having
insights to satisfy as many job seekers as possible, a key challenge is to
reliably infer the insights at the company level when there is limited or no
data at all. We propose a two-step framework that utilizes a novel, semantic
representation of companies (Company2vec) and a Bayesian statistical model to
address this problem. Our approach makes use of the rich information present in
the LinkedIn Economic Graph, and in particular, uses the intuition that two
companies are likely to be similar if employees are very likely to transition
from one company to the other and vice versa. We compute embeddings for
companies by analyzing the LinkedIn members' company transition data using
machine learning algorithms, then compute pairwise similarities between
companies based on these embeddings, and finally incorporate company
similarities in the form of peer company groups as part of the proposed
Bayesian statistical model to predict insights at the company level. We perform
extensive validation using several different evaluation techniques, and show
that we can significantly increase the coverage of insights while, in fact,
even improving the quality of the obtained insights. For example, we were able
to compute salary insights for 35 times as many title-region-company
combinations in the U.S. as compared to previous work, corresponding to 4.9
times as many monthly active users. Finally, we highlight the lessons learned
from deployment of our system.
",1,0,0,0,0,0
16536,16537,Modeling the Multiple Sclerosis Brain Disease Using Agents: What Works and What Doesn't?,"  The human brain is one of the most complex living structures in the known
Universe. It consists of billions of neurons and synapses. Due to its intrinsic
complexity, it can be a formidable task to accurately depict brain's structure
and functionality. In the past, numerous studies have been conducted on
modeling brain disease, structure, and functionality. Some of these studies
have employed Agent-based approaches including multiagent-based simulation
models as well as brain complex networks. While these models have all been
developed using agent-based computing, however, to our best knowledge, none of
them have employed the use of Agent-Oriented Software Engineering (AOSE)
methodologies in developing the brain or disease model. This is a problem
because without due process, developed models can miss out on important
requirements. AOSE has the unique capability of merging concepts from
multiagent systems, agent-based modeling, artificial intelligence, besides
concepts from distributed systems. AOSE involves the various tested software
engineering principles in various phases of the model development ranging from
analysis, design, implementation, and testing phases. In this paper, we employ
the use of three different AOSE methodologies for modeling the Multiple
Sclerosis brain disease namely GAIA, TROPOS, and MASE. After developing the
models, we further employ the use of Exploratory Agent-based Modeling (EABM) to
develop an actual model replicating previous results as a proof of concept. The
key objective of this study is to demonstrate and explore the viability and
effectiveness of AOSE methodologies in the development of complex brain
structure and cognitive process models. Our key finding include demonstration
that AOSE methodologies can be considerably helpful in modeling various living
complex systems, in general, and the human brain, in particular.
",1,1,0,0,0,0
2008,2009,Core structure of two-dimensional Fermi gas vortices in the BEC-BCS crossover region,"  We report $T=0$ diffusion Monte Carlo results for the ground-state and vortex
excitation of unpolarized spin-1/2 fermions in a two-dimensional disk. We
investigate how vortex core structure properties behave over the BEC-BCS
crossover. We calculate the vortex excitation energy, density profiles, and
vortex core properties related to the current. We find a density suppression at
the vortex core on the BCS side of the crossover, and a depleted core on the
BEC limit. Size-effect dependencies in the disk geometry were carefully
studied.
",0,1,0,0,0,0
9226,9227,Automatic Gradient Boosting,"  Automatic machine learning performs predictive modeling with high performing
machine learning tools without human interference. This is achieved by making
machine learning applications parameter-free, i.e. only a dataset is provided
while the complete model selection and model building process is handled
internally through (often meta) optimization. Projects like Auto-WEKA and
auto-sklearn aim to solve the Combined Algorithm Selection and Hyperparameter
optimization (CASH) problem resulting in huge configuration spaces. However,
for most real-world applications, the optimization over only a few different
key learning algorithms can not only be sufficient, but also potentially
beneficial. The latter becomes apparent when one considers that models have to
be validated, explained, deployed and maintained. Here, less complex model are
often preferred, for validation or efficiency reasons, or even a strict
requirement. Automatic gradient boosting simplifies this idea one step further,
using only gradient boosting as a single learning algorithm in combination with
model-based hyperparameter tuning, threshold optimization and encoding of
categorical features. We introduce this general framework as well as a concrete
implementation called autoxgboost. It is compared to current AutoML projects on
16 datasets and despite its simplicity is able to achieve comparable results on
about half of the datasets as well as performing best on two.
",0,0,0,1,0,0
3462,3463,Spectral Properties of Continuum Fibonacci Schrödinger Operators,"  We study continuum Schrödinger operators on the real line whose potentials
are comprised of two compactly supported square-integrable functions
concatenated according to an element of the Fibonacci substitution subshift
over two letters. We show that the Hausdorff dimension of the spectrum tends to
one in the small-coupling and high-energy regimes, regardless of the shape of
the potential pieces.
",0,0,1,0,0,0
15002,15003,Radial orbit instability in systems of highly eccentric orbits: Antonov problem reviewed,"  Stationary stellar systems with radially elongated orbits are subject to
radial orbit instability -- an important phenomenon that structures galaxies.
Antonov (1973) presented a formal proof of the instability for spherical
systems in the limit of purely radial orbits. However, such spheres have highly
inhomogeneous density distributions with singularity $\sim 1/r^2$, resulting in
an inconsistency in the proof. The proof can be refined, if one considers an
orbital distribution close to purely radial, but not entirely radial, which
allows to avoid the central singularity. For this purpose we employ
non-singular analogs of generalised polytropes elaborated recently in our work
in order to derive and solve new integral equations adopted for calculation of
unstable eigenmodes in systems with nearly radial orbits. In addition, we
establish a link between our and Antonov's approaches and uncover the meaning
of infinite entities in the purely radial case. Maximum growth rates tend to
infinity as the system becomes more and more radially anisotropic. The
instability takes place both for even and odd spherical harmonics, with all
unstable modes developing rapidly, i.e. having eigenfrequencies comparable to
or greater than typical orbital frequencies. This invalidates orbital
approximation in the case of systems with all orbits very close to purely
radial.
",0,1,0,0,0,0
10408,10409,Learning to compress and search visual data in large-scale systems,"  The problem of high-dimensional and large-scale representation of visual data
is addressed from an unsupervised learning perspective. The emphasis is put on
discrete representations, where the description length can be measured in bits
and hence the model capacity can be controlled. The algorithmic infrastructure
is developed based on the synthesis and analysis prior models whose
rate-distortion properties, as well as capacity vs. sample complexity
trade-offs are carefully optimized. These models are then extended to
multi-layers, namely the RRQ and the ML-STC frameworks, where the latter is
further evolved as a powerful deep neural network architecture with fast and
sample-efficient training and discrete representations. For the developed
algorithms, three important applications are developed. First, the problem of
large-scale similarity search in retrieval systems is addressed, where a
double-stage solution is proposed leading to faster query times and shorter
database storage. Second, the problem of learned image compression is targeted,
where the proposed models can capture more redundancies from the training
images than the conventional compression codecs. Finally, the proposed
algorithms are used to solve ill-posed inverse problems. In particular, the
problems of image denoising and compressive sensing are addressed with
promising results.
",1,0,0,1,0,0
9558,9559,Approximation Algorithms for Independence and Domination on B$_1$-VPG and B$_1$-EPG Graphs,"  A graph $G$ is called B$_k$-VPG (resp., B$_k$-EPG), for some constant $k\geq
0$, if it has a string representation on a grid such that each vertex is an
orthogonal path with at most $k$ bends and two vertices are adjacent in $G$ if
and only if the corresponding strings intersect (resp., the corresponding
strings share at least one grid edge). If two adjacent strings of a B$_k$-VPG
graph intersect exactly once, then the graph is called a one-string B$_k$-VPG
graph.
In this paper, we study the Maximum Independent Set and Minimum Dominating
Set problems on B$_1$-VPG and B$_1$-EPG graphs. We first give a simple $O(\log
n)$-approximation algorithm for the Maximum Independent Set problem on
B$_1$-VPG graphs, improving the previous $O((\log n)^2)$-approximation
algorithm of Lahiri et al. (COCOA 2015). Then, we consider the Minimum
Dominating Set problem. We give an $O(1)$-approximation algorithm for this
problem on one-string B$_1$-VPG graphs, providing the first constant-factor
approximation algorithm for this problem. Moreover, we show that the Minimum
Dominating Set problem is APX-hard on B$_1$-EPG graphs, ruling out the
possibility of a PTAS unless P=NP. Finally, we give constant-factor
approximation algorithms for this problem on two non-trivial subclasses of
B$_1$-EPG graphs. To our knowledge, these are the first results for the Minimum
Dominating Set problem on B$_1$-EPG graphs, partially answering a question
posed by Epstein et al. (WADS 2013).
",1,0,0,0,0,0
9687,9688,High-power closed-cycle $^4$He cryostat with top-loading sample exchange,"  We report on the development of a versatile cryogen-free laboratory cryostat
based upon a commercial pulse tube cryocooler. It provides enough cooling power
for continuous recondensation of circulating $^4$He gas at a condensation
pressure of approximately 250~mbar. Moreover, the cryostat allows for exchange
of different cryostat-inserts as well as fast and easy ""wet"" top-loading of
samples directly into the 1 K pot with a turn-over time of less than 75~min.
Starting from room temperature and using a $^4$He cryostat-insert, a base
temperature of 1.0~K is reached within approximately seven hours and a cooling
power of 250~mW is established at 1.24~K.
",0,1,0,0,0,0
3783,3784,Auxiliary Variables in TLA+,"  Auxiliary variables are often needed for verifying that an implementation is
correct with respect to a higher-level specification. They augment the formal
description of the implementation without changing its semantics--that is, the
set of behaviors that it describes. This paper explains rules for adding
history, prophecy, and stuttering variables to TLA+ specifications, ensuring
that the augmented specification is equivalent to the original one. The rules
are explained with toy examples, and they are used to verify the correctness of
a simplified version of a snapshot algorithm due to Afek et al.
",1,0,0,0,0,0
10212,10213,Spin liquid and infinitesimal-disorder-driven cluster spin glass in the kagome lattice,"  The interplay between geometric frustration (GF) and bond disorder is studied
in the Ising kagome lattice within a cluster approach. The model considers
antiferromagnetic (AF) short-range couplings and long-range intercluster
disordered interactions. The replica formalism is used to obtain an effective
single cluster model from where the thermodynamics is analyzed by exact
diagonalization. We found that the presence of GF can introduce cluster
freezing at very low levels of disorder. The system exhibits an entropy plateau
followed by a large entropy drop close to the freezing temperature. In this
scenario, a spin-liquid (SL) behavior prevents conventional long-range order,
but an infinitesimal disorder picks out uncompensated cluster states from the
multi degenerate SL regime, potentializing the intercluster disordered coupling
and bringing the cluster spin-glass state. To summarize, our results suggest
that the SL state combined with low levels of disorder can activate small
clusters, providing hypersensitivity to the freezing process in geometrically
frustrated materials and playing a key role in the glassy stabilization. We
propose that this physical mechanism could be present in several geometrically
frustrated materials. In particular, we discuss our results in connection to
the recent experimental investigations of the Ising kagome compound
Co$_3$Mg(OH)$_6$Cl$_2$.
",0,1,0,0,0,0
15627,15628,The evolution of magnetic hot massive stars: Implementation of the quantitative influence of surface magnetic fields in modern models of stellar evolution,"  Large-scale dipolar surface magnetic fields have been detected in a fraction
of OB stars, however only few stellar evolution models of massive stars have
considered the impact of these fossil fields. We are performing 1D
hydrodynamical model calculations taking into account evolutionary consequences
of the magnetospheric-wind interactions in a simplified parametric way. Two
effects are considered: i) the global mass-loss rates are reduced due to
mass-loss quenching, and ii) the surface angular momentum loss is enhanced due
to magnetic braking. As a result of the magnetic mass-loss quenching, the mass
of magnetic massive stars remains close to their initial masses. Thus magnetic
massive stars - even at Galactic metallicity - have the potential to be
progenitors of `heavy' stellar mass black holes. Similarly, at Galactic
metallicity, the formation of pair instability supernovae is plausible with a
magnetic progenitor.
",0,1,0,0,0,0
20708,20709,Analysis and Measurement of the Transfer Matrix of a 9-cell 1.3-GHz Superconducting Cavity,"  Superconducting linacs are capable of producing intense, stable, high-quality
electron beams that have found widespread applications in science and industry.
The 9-cell 1.3-GHz superconducting standing-wave accelerating RF cavity
originally developed for $e^+/e^-$ linear-collider applications [B. Aunes, {\em
et al.} Phys. Rev. ST Accel. Beams {\bf 3}, 092001 (2000)] has been broadly
employed in various superconducting-linac designs. In this paper we discuss the
transfer matrix of such a cavity and present its measurement performed at the
Fermilab Accelerator Science and Technology (FAST) facility. The experimental
results are found to be in agreement with analytical calculations and numerical
simulations.
",0,1,0,0,0,0
14683,14684,"Sketched Ridge Regression: Optimization Perspective, Statistical Perspective, and Model Averaging","  We address the statistical and optimization impacts of the classical sketch
and Hessian sketch used to approximately solve the Matrix Ridge Regression
(MRR) problem. Prior research has quantified the effects of classical sketch on
the strictly simpler least squares regression (LSR) problem. We establish that
classical sketch has a similar effect upon the optimization properties of MRR
as it does on those of LSR: namely, it recovers nearly optimal solutions. By
contrast, Hessian sketch does not have this guarantee, instead, the
approximation error is governed by a subtle interplay between the ""mass"" in the
responses and the optimal objective value.
For both types of approximation, the regularization in the sketched MRR
problem results in significantly different statistical properties from those of
the sketched LSR problem. In particular, there is a bias-variance trade-off in
sketched MRR that is not present in sketched LSR. We provide upper and lower
bounds on the bias and variance of sketched MRR, these bounds show that
classical sketch significantly increases the variance, while Hessian sketch
significantly increases the bias. Empirically, sketched MRR solutions can have
risks that are higher by an order-of-magnitude than those of the optimal MRR
solutions.
We establish theoretically and empirically that model averaging greatly
decreases the gap between the risks of the true and sketched solutions to the
MRR problem. Thus, in parallel or distributed settings, sketching combined with
model averaging is a powerful technique that quickly obtains near-optimal
solutions to the MRR problem while greatly mitigating the increased statistical
risk incurred by sketching.
",1,0,0,1,0,0
8487,8488,GdRh$_2$Si$_2$: An exemplary tetragonal system for antiferromagnetic order with weak in-plane anisotropy,"  The anisotropy of magnetic properties commonly is introduced in textbooks
using the case of an antiferromagnetic system with Ising type anisotropy. This
model presents huge anisotropic magnetization and a pronounced metamagnetic
transition and is well-known and well-documented both, in experiments and
theory. In contrast, the case of an antiferromagnetic $X$-$Y$ system with weak
in-plane anisotropy is only poorly documented. We studied the anisotropic
magnetization of the compound GdRh$_2$Si$_2$ and found that it is a perfect
model system for such a weak-anisotropy setting because the Gd$^{3+}$ ions in
GdRh$_2$Si$_2$ have a pure spin moment of S=7/2 which orders in a simple AFM
structure with ${\bf Q} = (001)$. We observed experimentally in $M(B)$ a
continuous spin-flop transition and domain effects for field applied along the
$[100]$- and the $[110]$-direction, respectively. We applied a mean field model
for the free energy to describe our data and combine it with an Ising chain
model to account for domain effects. Our calculations reproduce the
experimental data very well. In addition, we performed magnetic X-ray
scattering and X-ray magnetic circular dichroism measurements, which confirm
the AFM propagation vector to be ${\bf Q} = (001)$ and indicate the absence of
polarization on the rhodium atoms.
",0,1,0,0,0,0
20967,20968,Contemporary machine learning: a guide for practitioners in the physical sciences,"  Machine learning is finding increasingly broad application in the physical
sciences. This most often involves building a model relationship between a
dependent, measurable output and an associated set of controllable, but
complicated, independent inputs. We present a tutorial on current techniques in
machine learning -- a jumping-off point for interested researchers to advance
their work. We focus on deep neural networks with an emphasis on demystifying
deep learning. We begin with background ideas in machine learning and some
example applications from current research in plasma physics. We discuss
supervised learning techniques for modeling complicated functions, beginning
with familiar regression schemes, then advancing to more sophisticated deep
learning methods. We also address unsupervised learning and techniques for
reducing the dimensionality of input spaces. Along the way, we describe methods
for practitioners to help ensure that their models generalize from their
training data to as-yet-unseen test data. We describe classes of tasks --
predicting scalars, handling images, fitting time-series -- and prepare the
reader to choose an appropriate technique. We finally point out some
limitations to modern machine learning and speculate on some ways that
practitioners from the physical sciences may be particularly suited to help.
",1,1,0,0,0,0
8685,8686,Boundary driven Brownian gas,"  We consider a gas of independent Brownian particles on a bounded interval in
contact with two particle reservoirs at the endpoints. Due to the Brownian
nature of the particles, infinitely many particles enter and leave the system
in each time interval. Nonetheless, the dynamics can be constructed as a Markov
process with continuous paths on a suitable space. If $\lambda_0$ and
$\lambda_1$ are the chemical potentials of the boundary reservoirs, the
stationary distribution (reversible if and only if $\lambda_0=\lambda_1$) is a
Poisson point process with intensity given by the linear interpolation between
$\lambda_0$ and $\lambda_1$. We then analyze the empirical flow that it is
defined by counting, in a time interval $[0,t]$, the net number of particles
crossing a given point $x$. In the stationary regime we identify its statistics
and show that it is given, apart an $x$ dependent correction that is bounded
for large $t$, by the difference of two independent Poisson processes with
parameters $\lambda_0$ and $\lambda_1$.
",0,0,1,0,0,0
4800,4801,On the Statistical Efficiency of Optimal Kernel Sum Classifiers,"  We propose a novel combination of optimization tools with learning theory
bounds in order to analyze the sample complexity of optimal kernel sum
classifiers. This contrasts the typical learning theoretic results which hold
for all (potentially suboptimal) classifiers. Our work also justifies
assumptions made in prior work on multiple kernel learning. As a byproduct of
our analysis, we also provide a new form of Rademacher complexity for
hypothesis classes containing only optimal classifiers.
",1,0,0,1,0,0
17348,17349,Automated Synthesis of Divide and Conquer Parallelism,"  This paper focuses on automated synthesis of divide-and-conquer parallelism,
which is a common parallel programming skeleton supported by many
cross-platform multithreaded libraries. The challenges of producing (manually
or automatically) a correct divide-and-conquer parallel program from a given
sequential code are two-fold: (1) assuming that individual worker threads
execute a code identical to the sequential code, the programmer has to provide
the extra code for dividing the tasks and combining the computation results,
and (2) sometimes, the sequential code may not be usable as is, and may need to
be modified by the programmer. We address both challenges in this paper. We
present an automated synthesis technique for the case where no modifications to
the sequential code are required, and we propose an algorithm for modifying the
sequential code to make it suitable for parallelization when some modification
is necessary. The paper presents theoretical results for when this {\em
modification} is efficiently possible, and experimental evaluation of the
technique and the quality of the produced parallel programs.
",1,0,0,0,0,0
13468,13469,Dynamical Stochastic Higher Spin Vertex Models,"  We introduce a new family of integrable stochastic processes, called
\textit{dynamical stochastic higher spin vertex models}, arising from fused
representations of Felder's elliptic quantum group $E_{\tau, \eta}
(\mathfrak{sl}_2)$. These models simultaneously generalize the stochastic
higher spin vertex models, studied by Corwin-Petrov and Borodin-Petrov, and are
dynamical in the sense of Borodin's recent stochastic interaction round-a-face
models.
We provide explicit contour integral identities for observables of these
models (when run under specific types of initial data) that characterize the
distributions of their currents. Through asymptotic analysis of these
identities in a special case, we evaluate the scaling limit for the current of
a dynamical version of a discrete-time partial exclusion process. In
particular, we show that its scaling exponent is $1 / 4$ and that its one-point
marginal converges (in a sense of moments) to that of a non-trivial random
variable, which we determine explicitly.
",0,1,1,0,0,0
545,546,Spherical Functions on Riemannian Symmetric Spaces,"  This paper deals with some simple results about spherical functions of type
$\delta$, namely new integral formulas, new results about behavior at infinity
and some facts about the related $C_\sigma$ functions.
",0,0,1,0,0,0
137,138,Zero-point spin-fluctuations of single adatoms,"  Stabilizing the magnetic signal of single adatoms is a crucial step towards
their successful usage in widespread technological applications such as
high-density magnetic data storage devices. The quantum mechanical nature of
these tiny objects, however, introduces intrinsic zero-point spin-fluctuations
that tend to destabilize the local magnetic moment of interest by dwindling the
magnetic anisotropy potential barrier even at absolute zero temperature. Here,
we elucidate the origins and quantify the effect of the fundamental ingredients
determining the magnitude of the fluctuations, namely the ($i$) local magnetic
moment, ($ii$) spin-orbit coupling and ($iii$) electron-hole Stoner
excitations. Based on a systematic first-principles study of 3d and 4d adatoms,
we demonstrate that the transverse contribution of the fluctuations is
comparable in size to the magnetic moment itself, leading to a remarkable
$\gtrsim$50$\%$ reduction of the magnetic anisotropy energy. Our analysis gives
rise to a comprehensible diagram relating the fluctuation magnitude to
characteristic features of adatoms, providing practical guidelines for
designing magnetically stable nanomagnets with minimal quantum fluctuations.
",0,1,0,0,0,0
7371,7372,Resonance fluorescence in the resolvent operator formalism,"  The Mollow spectrum for the light scattered by a driven two-level atom is
derived in the resolvent operator formalism. The derivation is based on the
construction of a master equation from the resolvent operator of the atom-field
system. We show that the natural linewidth of the excited atomic level remains
essentially unmodified, to a very good level of approximation, even in the
strong-field regime, where Rabi flopping becomes relevant inside the
self-energy loop that yields the linewidth. This ensures that the obtained
master equation and the spectrum derived matches that of Mollow.
",0,1,0,0,0,0
1746,1747,Stable absorbing boundary conditions for molecular dynamics in general domains,"  A new type of absorbing boundary conditions for molecular dynamics
simulations are presented. The exact boundary conditions for crystalline solids
with harmonic approximation are expressed as a dynamic Dirichlet- to-Neumann
(DtN) map. It connects the displacement of the atoms at the boundary to the
traction on these atoms. The DtN map is valid for a domain with general
geometry. To avoid evaluating the time convo- lution of the dynamic DtN map, we
approximate the associated kernel function by rational functions in the Laplace
domain. The parameters in the approximations are determined by interpolations.
The explicit forms of the zeroth, first, and second order approximations will
be presented. The stability of the molecular dynamics model, supplemented with
these absorbing boundary conditions is established. Two numerical simulations
are performed to demonstrate the effectiveness of the methods.
",0,1,0,0,0,0
16099,16100,Matrix Completion via Factorizing Polynomials,"  Predicting unobserved entries of a partially observed matrix has found wide
applicability in several areas, such as recommender systems, computational
biology, and computer vision. Many scalable methods with rigorous theoretical
guarantees have been developed for algorithms where the matrix is factored into
low-rank components, and embeddings are learned for the row and column
entities. While there has been recent research on incorporating explicit side
information in the low-rank matrix factorization setting, often implicit
information can be gleaned from the data, via higher-order interactions among
entities. Such implicit information is especially useful in cases where the
data is very sparse, as is often the case in real-world datasets. In this
paper, we design a method to learn embeddings in the context of recommendation
systems, using the observation that higher powers of a graph transition
probability matrix encode the probability that a random walker will hit that
node in a given number of steps. We develop a coordinate descent algorithm to
solve the resulting optimization, that makes explicit computation of the higher
order powers of the matrix redundant, preserving sparsity and making
computations efficient. Experiments on several datasets show that our method,
that can use higher order information, outperforms methods that only use
explicitly available side information, those that use only second-order
implicit information and in some cases, methods based on deep neural networks
as well.
",1,0,0,1,0,0
12976,12977,Poisson Structures and Potentials,"  We introduce a notion of weakly log-canonical Poisson structures on positive
varieties with potentials. Such a Poisson structure is log-canonical up to
terms dominated by the potential. To a compatible real form of a weakly
log-canonical Poisson variety we assign an integrable system on the product of
a certain real convex polyhedral cone (the tropicalization of the variety) and
a compact torus.
We apply this theory to the dual Poisson-Lie group $G^*$ of a
simply-connected semisimple complex Lie group $G$. We define a positive
structure and potential on $G^*$ and show that the natural Poisson-Lie
structure on $G^*$ is weakly log-canonical with respect to this positive
structure and potential.
For $K \subset G$ the compact real form, we show that the real form $K^*
\subset G^*$ is compatible and prove that the corresponding integrable system
is defined on the product of the decorated string cone and the compact torus of
dimension $\frac{1}{2}({\rm dim} \, G - {\rm rank} \, G)$.
",0,0,1,0,0,0
15366,15367,Exposing Twitter Users to Contrarian News,"  Polarized topics often spark discussion and debate on social media. Recent
studies have shown that polarized debates have a specific clustered structure
in the endorsement net- work, which indicates that users direct their
endorsements mostly to ideas they already agree with. Understanding these
polarized discussions and exposing social media users to content that broadens
their views is of paramount importance.
The contribution of this demonstration is two-fold. (i) A tool to visualize
retweet networks about controversial issues on Twitter. By using our
visualization, users can understand how polarized discussions are shaped on
Twitter, and explore the positions of the various actors. (ii) A solution to
reduce polarization of such discussions. We do so by exposing users to
information which presents a contrarian point of view. Users can visually
inspect our recommendations and understand why and how these would play out in
terms of the retweet network.
Our demo (this https URL homepage)
provides one of the first steps in developing automated tools that help users
explore, and possibly escape, their echo chambers. The ideas in the demo can
also help content providers design tools to broaden their reach to people with
different political and ideological backgrounds.
",1,0,0,0,0,0
4967,4968,Minority carrier diffusion lengths and mobilities in low-doped n-InGaAs for focal plane array applications,"  The hole diffusion length in n-InGaAs is extracted for two samples of
different doping concentrations using a set of long and thin diffused junction
diodes separated by various distances on the order of the diffusion length. The
methodology is described, including the ensuing analysis which yields diffusion
lengths between 70 - 85 um at room temperature for doping concentrations in the
range of 5 - 9 x 10^15 cm-3. The analysis also provides insight into the
minority carrier mobility which is a parameter not commonly reported in the
literature. Hole mobilities on the order of 500 - 750 cm2/Vs are reported for
the aforementioned doping range, which are comparable albeit longer than the
majority hole mobility for the same doping magnitude in p-InGaAs. A radiative
recombination coefficient of (0.5-0.2)x10^-10 cm^-3s^-1 is also extracted from
the ensuing analysis for an InGaAs thickness of 2.7 um. Preliminary evidence is
also given for both heavy and light hole diffusion. The dark current of
InP/InGaAs p-i-n photodetectors with 25 and 15 um pitches are then calibrated
to device simulations and correlated to the extracted diffusion lengths and
doping concentrations. An effective Shockley-Read-Hall lifetime of between
90-200 us provides the best fit to the dark current of these structures.
",0,1,0,0,0,0
5180,5181,Distributed methods for synchronization of orthogonal matrices over graphs,"  This paper addresses the problem of synchronizing orthogonal matrices over
directed graphs. For synchronized transformations (or matrices), composite
transformations over loops equal the identity. We formulate the synchronization
problem as a least-squares optimization problem with nonlinear constraints. The
synchronization problem appears as one of the key components in applications
ranging from 3D-localization to image registration. The main contributions of
this work can be summarized as the introduction of two novel algorithms; one
for symmetric graphs and one for graphs that are possibly asymmetric. Under
general conditions, the former has guaranteed convergence to the solution of a
spectral relaxation to the synchronization problem. The latter is stable for
small step sizes when the graph is quasi-strongly connected. The proposed
methods are verified in numerical simulations.
",1,0,1,0,0,0
578,579,Learning Graph Representations by Dendrograms,"  Hierarchical graph clustering is a common technique to reveal the multi-scale
structure of complex networks. We propose a novel metric for assessing the
quality of a hierarchical clustering. This metric reflects the ability to
reconstruct the graph from the dendrogram, which encodes the hierarchy. The
optimal representation of the graph defines a class of reducible linkages
leading to regular dendrograms by greedy agglomerative clustering.
",1,0,0,1,0,0
6135,6136,On Optimization of Radiative Dipole Body Array Coils for 7 Tesla MRI,"  In this contribution we present numerical and experimental results of a
parametric study of radiative dipole antennas in a phased array configuration
for efficient body magnetic resonance imaging at 7T via parallel transmit. For
magnetic resonance imaging (MRI) at ultrahigh fields (7T and higher) dipole
antennas are commonly used in phased arrays, particularly for body imaging
targets. This study reveals the effects of dipole positioning in the array
(elevation of dipoles above the subject and inter-dipole spacing) on their
mutual coupling, $B_1^{+}$ per unit power and $B_1^{+}$ per maximum local SAR
efficiencies as well as the RF-shimming capability. The results demonstrate the
trade-off between low maximum local SAR and sensitivity to the subject
variation and provide the working parameter range for practical body arrays
composed of recently suggested fractionated dipoles.
",0,1,0,0,0,0
17506,17507,Yamabe Solitons on three-dimensional normal almost paracontact metric manifolds,"  The purpose of the paper is to study Yamabe solitons on three-dimensional
para-Sasakian, paracosymplectic and para-Kenmotsu manifolds. Mainly, we proved
that *If the semi-Riemannian metric of a three-dimensional para-Sasakian
manifold is a Yamabe soliton, then it is of constant scalar curvature, and the
flow vector field V is Killing. In the next step, we proved that either
manifold has constant curvature -1 and reduces to an Einstein manifold, or V is
an infinitesimal automorphism of the paracontact metric structure on the
manifold. *If the semi-Riemannian metric of a three-dimensional
paracosymplectic manifold is a Yamabe soliton, then it has constant scalar
curvature. Furthermore either manifold is $\eta$-Einstein, or Ricci flat. *If
the semi-Riemannian metric on a three-dimensional para-Kenmotsu manifold is a
Yamabe soliton, then the manifold is of constant sectional curvature -1,
reduces to an Einstein manifold. Furthermore, Yamabe soliton is expanding with
$\lambda$=-6 and the vector field V is Killing. Finally, we construct examples
to illustrate the results obtained in previous sections.
",0,0,1,0,0,0
18815,18816,Localized-endemic state transition in the susceptible-infected-susceptible model on networks,"  It is a longstanding debate concerning the absence of threshold for the
susceptible-infected-susceptible spreading model on networks with localized
state. The key to resolve this controversy is the dynamical interaction
pattern, which has not been uncovered. Here we show that the interaction
driving the localized-endemic state transition is not the global interaction
between a node and all the other nodes on the network, but exists at the level
of super node composed of highly connected node and its neighbors. The internal
interactions within a super node induce localized state with limited lifetime,
while the interactions between neighboring super nodes via a path of two hops
enable them to avoid trapping in the absorbing state, marking the onset of
endemic state. The hybrid interactions render highly connected nodes
exponentially increasing infection density, which truly account for the null
threshold. These results are crucial for correctly understanding diverse
recurrent contagion phenomena
",0,1,0,0,0,0
8822,8823,Optimal bounds and extremal trajectories for time averages in nonlinear dynamical systems,"  For any quantity of interest in a system governed by ordinary differential
equations, it is natural to seek the largest (or smallest) long-time average
among solution trajectories, as well as the extremal trajectories themselves.
Upper bounds on time averages can be proved a priori using auxiliary functions,
the optimal choice of which is a convex optimization problem. We prove that the
problems of finding maximal trajectories and minimal auxiliary functions are
strongly dual. Thus, auxiliary functions provide arbitrarily sharp upper bounds
on time averages. Moreover, any nearly minimal auxiliary function provides
phase space volumes in which all nearly maximal trajectories are guaranteed to
lie. For polynomial equations, auxiliary functions can be constructed by
semidefinite programming, which we illustrate using the Lorenz system.
",0,1,1,0,0,0
1316,1317,How Generative Adversarial Networks and Their Variants Work: An Overview,"  Generative Adversarial Networks (GAN) have received wide attention in the
machine learning field for their potential to learn high-dimensional, complex
real data distribution. Specifically, they do not rely on any assumptions about
the distribution and can generate real-like samples from latent space in a
simple manner. This powerful property leads GAN to be applied to various
applications such as image synthesis, image attribute editing, image
translation, domain adaptation and other academic fields. In this paper, we aim
to discuss the details of GAN for those readers who are familiar with, but do
not comprehend GAN deeply or who wish to view GAN from various perspectives. In
addition, we explain how GAN operates and the fundamental meaning of various
objective functions that have been suggested recently. We then focus on how the
GAN can be combined with an autoencoder framework. Finally, we enumerate the
GAN variants that are applied to various tasks and other fields for those who
are interested in exploiting GAN for their research.
",1,0,0,0,0,0
14824,14825,Curriculum Dropout,"  Dropout is a very effective way of regularizing neural networks.
Stochastically ""dropping out"" units with a certain probability discourages
over-specific co-adaptations of feature detectors, preventing overfitting and
improving network generalization. Besides, Dropout can be interpreted as an
approximate model aggregation technique, where an exponential number of smaller
networks are averaged in order to get a more powerful ensemble. In this paper,
we show that using a fixed dropout probability during training is a suboptimal
choice. We thus propose a time scheduling for the probability of retaining
neurons in the network. This induces an adaptive regularization scheme that
smoothly increases the difficulty of the optimization problem. This idea of
""starting easy"" and adaptively increasing the difficulty of the learning
problem has its roots in curriculum learning and allows one to train better
models. Indeed, we prove that our optimization strategy implements a very
general curriculum scheme, by gradually adding noise to both the input and
intermediate feature representations within the network architecture.
Experiments on seven image classification datasets and different network
architectures show that our method, named Curriculum Dropout, frequently yields
to better generalization and, at worst, performs just as well as the standard
Dropout method.
",1,0,0,1,0,0
7123,7124,Look No Further: Adapting the Localization Sensory Window to the Temporal Characteristics of the Environment,"  Many localization algorithms use a spatiotemporal window of sensory
information in order to recognize spatial locations, and the length of this
window is often a sensitive parameter that must be tuned to the specifics of
the application. This letter presents a general method for environment-driven
variation of the length of the spatiotemporal window based on searching for the
most significant localization hypothesis, to use as much context as is
appropriate but not more. We evaluate this approach on benchmark datasets using
visual and Wi-Fi sensor modalities and a variety of sensory comparison
front-ends under in-order and out-of-order traversals of the environment. Our
results show that the system greatly reduces the maximum distance traveled
without localization compared to a fixed-length approach while achieving
competitive localization accuracy, and our proposed method achieves this
performance without deployment-time tuning.
",1,0,0,0,0,0
20358,20359,Motion of Massive Particles in Rindler Space and the Problem of Fall at the Centre,"  The motion of a massive particle in Rindler space has been studied and
obtained the geodesics of motion. The orbits in Rindler space are found to be
quite different from that of Schwarzschild case. The paths are not like the
Perihelion Precession type. Further we have set up the non-relativistic
Schrodinger equation for the particle in the quantum mechanical scenario in
presence of background constant gravitational field and investigated the
problem of fall of the particle at the center. This problem is also treated
classically. Unlike the conventional scenario, here the fall occurs at the
surface of a sphere of unit radius.
",0,1,0,0,0,0
9872,9873,A bound on partitioning clusters,"  Let $X$ be a finite collection of sets (or ""clusters""). We consider the
problem of counting the number of ways a cluster $A \in X$ can be partitioned
into two disjoint clusters $A_1, A_2 \in X$, thus $A = A_1 \uplus A_2$ is the
disjoint union of $A_1$ and $A_2$; this problem arises in the run time analysis
of the ASTRAL algorithm in phylogenetic reconstruction. We obtain the bound $$
| \{ (A_1,A_2,A) \in X \times X \times X: A = A_1 \uplus A_2 \} | \leq
|X|^{3/p} $$ where $|X|$ denotes the cardinality of $X$, and $p := \log_3
\frac{27}{4} = 1.73814\dots$, so that $\frac{3}{p} = 1.72598\dots$.
Furthermore, the exponent $p$ cannot be replaced by any larger quantity. This
improves upon the trivial bound of $|X|^2$. The argument relies on establishing
a one-dimensional convolution inequality that can be established by elementary
calculus combined with some numerical verification.
In a similar vein, we show that for any subset $A$ of a discrete cube
$\{0,1\}^n$, the additive energy of $A$ (the number of quadruples
$(a_1,a_2,a_3,a_4)$ in $A^4$ with $a_1+a_2=a_3+a_4$) is at most $|A|^{\log_2
6}$, and that this exponent is best possible.
",0,0,1,0,0,0
10810,10811,Quasi-ordered Rings,"  A quasi-order is a binary, reflexive and transitive relation. In the Journal
of Pure and Applied Algebra 45 (1987), S.M. Fakhruddin introduced the notion of
(totally) quasi-ordered fields and showed that each such field is either an
ordered field or else a valued field. Hence, quasi-ordered fields are very well
suited to treat ordered and valued fields simultaneously.
In this note, we will prove that the same dichotomy holds for commutative
rings with 1 as well. For that purpose we first develop an appropriate notion
of (totally) quasi-ordered rings. Our proof of the dichotomy then exploits
Fakhruddin's result that was mentioned above.
",0,0,1,0,0,0
11849,11850,Deriving mesoscopic models of collective behaviour for finite populations,"  Animal groups exhibit emergent properties that are a consequence of local
interactions. Linking individual-level behaviour to coarse-grained descriptions
of animal groups has been a question of fundamental interest. Here, we present
two complementary approaches to deriving coarse-grained descriptions of
collective behaviour at so-called mesoscopic scales, which account for the
stochasticity arising from the finite sizes of animal groups. We construct
stochastic differential equations (SDEs) for a coarse-grained variable that
describes the order/consensus within a group. The first method of construction
is based on van Kampen's system-size expansion of transition rates. The second
method employs Gillespie's chemical Langevin equations. We apply these two
methods to two microscopic models from the literature, in which organisms
stochastically interact and choose between two directions/choices of foraging.
These `binary-choice' models differ only in the types of interactions between
individuals, with one assuming simple pair-wise interactions, and the other
incorporating higher-order effects. In both cases, the derived mesoscopic SDEs
have multiplicative, or state-dependent, noise. However, the different models
demonstrate the contrasting effects of noise: increasing order in the pair-wise
interaction model, whilst reducing order in the higher-order interaction model.
Although both methods yield identical SDEs for such binary-choice, or
one-dimensional, systems, the relative tractability of the chemical Langevin
approach is beneficial in generalizations to higher-dimensions. In summary,
this book chapter provides a pedagogical review of two complementary methods to
construct mesoscopic descriptions from microscopic rules and demonstrates how
resultant multiplicative noise can have counter-intuitive effects on shaping
collective behaviour.
",0,0,0,0,1,0
18264,18265,Markov $L_2$ inequality with the Gegenbauer weight,"  For the Gegenbauer weight function $w_{\lambda}(t)=(1-t^2)^{\lambda-1/2}$,
$\lambda>-1/2$, we denote by $\Vert\cdot\Vert_{w_{\lambda}}$ the associated
$L_2$-norm, $$ \Vert
f\Vert_{w_{\lambda}}:=\Big(\int_{-1}^{1}w_{\lambda}(t)f^2(t)\,dt\Big)^{1/2}. $$
We study the Markov inequality $$ \Vert p^{\prime}\Vert_{w_{\lambda}}\leq
c_{n}(\lambda)\,\Vert p\Vert_{w_{\lambda}},\qquad p\in \mathcal{P}_n, $$ where
$\mathcal{P}_n$ is the class of algebraic polynomials of degree not exceeding
$n$. Upper and lower bounds for the best Markov constant $c_{n}(\lambda)$ are
obtained, which are valid for all $n\in \mathbb{N}$ and $\lambda>-\frac{1}{2}$.
",0,0,1,0,0,0
10961,10962,Deep Learning for Accelerated Ultrasound Imaging,"  In portable, 3-D, or ultra-fast ultrasound (US) imaging systems, there is an
increasing demand to reconstruct high quality images from limited number of
data. However, the existing solutions require either hardware changes or
computationally expansive algorithms. To overcome these limitations, here we
propose a novel deep learning approach that interpolates the missing RF data by
utilizing the sparsity of the RF data in the Fourier domain. Extensive
experimental results from sub-sampled RF data from a real US system confirmed
that the proposed method can effectively reduce the data rate without
sacrificing the image quality.
",1,0,0,1,0,0
10330,10331,Angle-resolved photoemission spectroscopy with quantum gas microscopes,"  Quantum gas microscopes are a promising tool to study interacting quantum
many-body systems and bridge the gap between theoretical models and real
materials. So far they were limited to measurements of instantaneous
correlation functions of the form $\langle \hat{O}(t) \rangle$, even though
extensions to frequency-resolved response functions $\langle \hat{O}(t)
\hat{O}(0) \rangle$ would provide important information about the elementary
excitations in a many-body system. For example, single particle spectral
functions, which are usually measured using photoemission experiments in
electron systems, contain direct information about fractionalization and the
quasiparticle excitation spectrum. Here, we propose a measurement scheme to
experimentally access the momentum and energy resolved spectral function in a
quantum gas microscope with currently available techniques. As an example for
possible applications, we numerically calculate the spectrum of a single hole
excitation in one-dimensional $t-J$ models with isotropic and anisotropic
antiferromagnetic couplings. A sharp asymmetry in the distribution of spectral
weight appears when a hole is created in an isotropic Heisenberg spin chain.
This effect slowly vanishes for anisotropic spin interactions and disappears
completely in the case of pure Ising interactions. The asymmetry strongly
depends on the total magnetization of the spin chain, which can be tuned in
experiments with quantum gas microscopes. An intuitive picture for the observed
behavior is provided by a slave-fermion mean field theory. The key properties
of the spectra are visible at currently accessible temperatures.
",0,1,0,0,0,0
10228,10229,"On strict sub-Gaussianity, optimal proxy variance and symmetry for bounded random variables","  We investigate the sub-Gaussian property for almost surely bounded random
variables. If sub-Gaussianity per se is de facto ensured by the bounded support
of said random variables, then exciting research avenues remain open. Among
these questions is how to characterize the optimal sub-Gaussian proxy variance?
Another question is how to characterize strict sub-Gaussianity, defined by a
proxy variance equal to the (standard) variance? We address the questions in
proposing conditions based on the study of functions variations. A particular
focus is given to the relationship between strict sub-Gaussianity and symmetry
of the distribution. In particular, we demonstrate that symmetry is neither
sufficient nor necessary for strict sub-Gaussianity. In contrast, simple
necessary conditions on the one hand, and simple sufficient conditions on the
other hand, for strict sub-Gaussianity are provided. These results are
illustrated via various applications to a number of bounded random variables,
including Bernoulli, beta, binomial, uniform, Kumaraswamy, and triangular
distributions.
",0,0,1,1,0,0
4672,4673,Entropy Production Rate is Maximized in Non-Contractile Actomyosin,"  The actin cytoskeleton is an active semi-flexible polymer network whose
non-equilibrium properties coordinate both stable and contractile behaviors to
maintain or change cell shape. While myosin motors drive the actin cytoskeleton
out-of-equilibrium, the role of myosin-driven active stresses in the
accumulation and dissipation of mechanical energy is unclear. To investigate
this, we synthesize an actomyosin material in vitro whose active stress content
can tune the network from stable to contractile. Each increment in activity
determines a characteristic spectrum of actin filament fluctuations which is
used to calculate the total mechanical work and the production of entropy in
the material. We find that the balance of work and entropy does not increase
monotonically and, surprisingly, the entropy production rate is maximized in
the non-contractile, stable state. Our study provides evidence that the origins
of system entropy production and activity-dependent dissipation arise from
disorder in the molecular interactions between actin and myosin
",0,0,0,0,1,0
7134,7135,Data-driven Probabilistic Atlases Capture Whole-brain Individual Variation,"  Probabilistic atlases provide essential spatial contextual information for
image interpretation, Bayesian modeling, and algorithmic processing. Such
atlases are typically constructed by grouping subjects with similar demographic
information. Importantly, use of the same scanner minimizes inter-group
variability. However, generalizability and spatial specificity of such
approaches is more limited than one might like. Inspired by Commowick
""Frankenstein's creature paradigm"" which builds a personal specific anatomical
atlas, we propose a data-driven framework to build a personal specific
probabilistic atlas under the large-scale data scheme. The data-driven
framework clusters regions with similar features using a point distribution
model to learn different anatomical phenotypes. Regional structural atlases and
corresponding regional probabilistic atlases are used as indices and targets in
the dictionary. By indexing the dictionary, the whole brain probabilistic
atlases adapt to each new subject quickly and can be used as spatial priors for
visualization and processing. The novelties of this approach are (1) it
provides a new perspective of generating personal specific whole brain
probabilistic atlases (132 regions) under data-driven scheme across sites. (2)
The framework employs the large amount of heterogeneous data (2349 images). (3)
The proposed framework achieves low computational cost since only one affine
registration and Pearson correlation operation are required for a new subject.
Our method matches individual regions better with higher Dice similarity value
when testing the probabilistic atlases. Importantly, the advantage the
large-scale scheme is demonstrated by the better performance of using
large-scale training data (1888 images) than smaller training set (720 images).
",0,0,0,1,1,0
8709,8710,Multidimensional Free Poisson Limits on Free Stochastic Integral Algebras,"  In this paper, we prove four-moment theorems for multidimensional free
Poisson limits on free Wigner chaos or the free Poisson algebra. We prove that,
under mild technical conditions, a bi-indexed sequence of free stochastic
integrals in free Wigner algebra or free Poisson algebra converges to a free
sequence of free Poisson random variables if and only if the moments with order
not greater than four of the sequence converge to the corresponding moments of
the limit sequence of random variables. Similar four-moment theorems hold when
the limit sequence is not free, but has a multidimensional free Poisson
distribution with parameters $\lambda>0$ and $\alpha=\{\alpha_i: 0\ne
\alpha_i\in \mathbb{R}, i=1, 2, \cdots\}$.
",0,0,1,0,0,0
18406,18407,WheelCon: A wheel control-based gaming platform for studying human sensorimotor control,"  Feedback control theory has been extensively implemented to theoretically
model human sensorimotor control. However, experimental platforms capable of
manipulating important components of multiple feedback loops lack development.
This paper describes the WheelCon, which is an open source platform aimed at
resolving such insufficiencies. WheelCon enables safely simulation of the
canonical sensorimotor task such as riding a mountain bike down a steep,
twisting, bumpy trail etc., with provided only a computer, standard display,
and an inexpensive gaming steering wheel with a force feedback motor. The
platform provides flexibility, as will be demonstrated in the demos provided,
so that researchers may manipulate the disturbances, delay, and quantization
(data rate) in the layered feedback loops, including a high-level advanced plan
layer and a low-level delayed reflex layer. In this paper, we illustrate
WheelCon's graphical user interface (GUI), the input and output of existing
demos, and how to design new games. In addition, we present the basic feedback
model, and we show the testing results from our demo games which align well
with prediction from the model. In short, the platform is featured as cheap,
simple to use, and flexible to program for effective sensorimotor neuroscience
research and control engineering education.
",0,0,0,0,1,0
10700,10701,A general method to describe intersystem crossing dynamics in trajectory surface hopping,"  Intersystem crossing is a radiationless process that can take place in a
molecule irradiated by UV-Vis light, thereby playing an important role in many
environmental, biological and technological processes. This paper reviews
different methods to describe intersystem crossing dynamics, paying attention
to semiclassical trajectory theories, which are especially interesting because
they can be applied to large systems with many degrees of freedom. In
particular, a general trajectory surface hopping methodology recently developed
by the authors, which is able to include non-adiabatic and spin-orbit couplings
in excited-state dynamics simulations, is explained in detail. This method,
termed SHARC, can in principle include any arbitrary coupling, what makes it
generally applicable to photophysical and photochemical problems, also those
including explicit laser fields. A step-by-step derivation of the main
equations of motion employed in surface hopping based on the fewest-switches
method of Tully, adapted for the inclusion of spin-orbit interactions, is
provided. Special emphasis is put on describing the different possible choices
of the electronic bases in which spin-orbit can be included in surface hopping,
highlighting the advantages and inconsistencies of the different approaches.
",0,1,0,0,0,0
17045,17046,Communication via FRET in Nanonetworks of Mobile Proteins,"  A practical, biologically motivated case of protein complexes (immunoglobulin
G and FcRII receptors) moving on the surface of mastcells, that are common
parts of an immunological system, is investigated. Proteins are considered as
nanomachines creating a nanonetwork. Accurate molecular models of the proteins
and the fluorophores which act as their nanoantennas are used to simulate the
communication between the nanomachines when they are close to each other. The
theory of diffusion-based Brownian motion is applied to model movements of the
proteins. It is assumed that fluorophore molecules send and receive signals
using the Forster Resonance Energy Transfer. The probability of the efficient
signal transfer and the respective bit error rate are calculated and discussed.
",0,0,0,0,1,0
14782,14783,A Unifying View of Explicit and Implicit Feature Maps for Structured Data: Systematic Studies of Graph Kernels,"  Non-linear kernel methods can be approximated by fast linear ones using
suitable explicit feature maps allowing their application to large scale
problems. To this end, explicit feature maps of kernels for vectorial data have
been extensively studied. As many real-world data is structured, various
kernels for complex data like graphs have been proposed. Indeed, many of them
directly compute feature maps. However, the kernel trick is employed when the
number of features is very large or the individual vertices of graphs are
annotated by real-valued attributes.
Can we still compute explicit feature maps efficiently under these
circumstances? Triggered by this question, we investigate how general
convolution kernels are composed from base kernels and construct corresponding
feature maps. We apply our results to widely used graph kernels and analyze for
which kernels and graph properties computation by explicit feature maps is
feasible and actually more efficient. In particular, we derive feature maps for
random walk and subgraph matching kernels and apply them to real-world graphs
with discrete labels. Thereby, our theoretical results are confirmed
experimentally by observing a phase transition when comparing running time with
respect to label diversity, walk lengths and subgraph size, respectively.
Moreover, we derive approximative, explicit feature maps for state-of-the-art
kernels supporting real-valued attributes including the GraphHopper and Graph
Invariant kernels. In extensive experiments we show that our approaches often
achieve a classification accuracy close to the exact methods based on the
kernel trick, but require only a fraction of their running time.
",1,0,0,1,0,0
4988,4989,Dust in the reionization era: ALMA observations of a $z$=8.38 Galaxy,"  We report on the detailed analysis of a gravitationally-lensed Y-band
dropout, A2744_YD4, selected from deep Hubble Space Telescope imaging in the
Frontier Field cluster Abell 2744. Band 7 observations with the Atacama Large
Millimeter Array (ALMA) indicate the proximate detection of a significant 1mm
continuum flux suggesting the presence of dust for a star-forming galaxy with a
photometric redshift of $z\simeq8$. Deep X-SHOOTER spectra confirms the high
redshift identity of A2744_YD4 via the detection of Lyman $\alpha$ emission at
a redshift $z$=8.38. The association with the ALMA detection is confirmed by
the presence of [OIII] 88$\mu$m emission at the same redshift. Although both
emission features are only significant at the 4 $\sigma$ level, we argue their
joint detection and the positional coincidence with a high redshift dropout in
the HST images confirms the physical association. Analysis of the available
photometric data and the modest gravitational magnification ($\mu\simeq2$)
indicates A2744_YD4 has a stellar mass of $\sim$ 2$\times$10$^9$ M$_{\odot}$, a
star formation rate of $\sim20$ M$_{\odot}$/yr and a dust mass of
$\sim$6$\times$10$^{6}$ M$_{\odot}$. We discuss the implications of the
formation of such a dust mass only $\simeq$200 Myr after the onset of cosmic
reionisation.
",0,1,0,0,0,0
6061,6062,A Brownian Motion Model and Extreme Belief Machine for Modeling Sensor Data Measurements,"  As the title suggests, we will describe (and justify through the presentation
of some of the relevant mathematics) prediction methodologies for sensor
measurements. This exposition will mainly be concerned with the mathematics
related to modeling the sensor measurements.
",1,0,0,0,0,0
12636,12637,Airy structures and symplectic geometry of topological recursion,"  We propose a new approach to the topological recursion of Eynard-Orantin
based on the notion of Airy structure, which we introduce in the paper. We
explain why Airy structure is a more fundamental object than the one of the
spectral curve. We explain how the concept of quantization of Airy structure
leads naturally to the formulas of topological recursion as well as their
generalizations. The notion of spectral curve is also considered in a more
general framework of Poisson surfaces endowed with foliation. We explain how
the deformation theory of spectral curves is related to Airy structures. Few
other topics (e.g. the Holomorphic Anomaly Equation) are also discussed from
the general point of view of Airy structures.
",0,0,1,0,0,0
20075,20076,Document Retrieval for Large Scale Content Analysis using Contextualized Dictionaries,"  This paper presents a procedure to retrieve subsets of relevant documents
from large text collections for Content Analysis, e.g. in social sciences.
Document retrieval for this purpose needs to take account of the fact that
analysts often cannot describe their research objective with a small set of key
terms, especially when dealing with theoretical or rather abstract research
interests. Instead, it is much easier to define a set of paradigmatic documents
which reflect topics of interest as well as targeted manner of speech. Thus, in
contrast to classic information retrieval tasks we employ manually compiled
collections of reference documents to compose large queries of several hundred
key terms, called dictionaries. We extract dictionaries via Topic Models and
also use co-occurrence data from reference collections. Evaluations show that
the procedure improves retrieval results for this purpose compared to
alternative methods of key term extraction as well as neglecting co-occurrence
data.
",1,0,0,0,0,0
12004,12005,Reduction of Second-Order Network Systems with Structure Preservation,"  This paper proposes a general framework for structure-preserving model
reduction of a secondorder network system based on graph clustering. In this
approach, vertex dynamics are captured by the transfer functions from inputs to
individual states, and the dissimilarities of vertices are quantified by the
H2-norms of the transfer function discrepancies. A greedy hierarchical
clustering algorithm is proposed to place those vertices with similar dynamics
into same clusters. Then, the reduced-order model is generated by the
Petrov-Galerkin method, where the projection is formed by the characteristic
matrix of the resulting network clustering. It is shown that the simplified
system preserves an interconnection structure, i.e., it can be again
interpreted as a second-order system evolving over a reduced graph.
Furthermore, this paper generalizes the definition of network controllability
Gramian to second-order network systems. Based on it, we develop an efficient
method to compute H2-norms and derive the approximation error between the
full-order and reduced-order models. Finally, the approach is illustrated by
the example of a small-world network.
",1,0,0,0,0,0
2946,2947,Fixed-Gain Augmented-State Tracking-Filters,"  A procedure for the design of fixed-gain tracking filters, using an
augmented-state observer with signal and interference subspaces, is proposed.
The signal subspace incorporates an integrating Newtonian model and a
second-order maneuver model that is matched to a sustained constant-g turn; the
deterministic interference model creates a Nyquist null for smoother track
estimates. The selected models provide a simple means of shaping and analyzing
the (transient and steady-state) response of tracking-filters of elevated
order.
",1,0,0,0,0,0
4825,4826,The CMS HGCAL detector for HL-LHC upgrade,"  The High Luminosity LHC (HL-LHC) will integrate 10 times more luminosity than
the LHC, posing significant challenges for radiation tolerance and event pileup
on detectors, especially for forward calorimetry, and hallmarks the issue for
future colliders. As part of its HL-LHC upgrade program, the CMS collaboration
is designing a High Granularity Calorimeter to replace the existing endcap
calorimeters. It features unprecedented transverse and longitudinal
segmentation for both electromagnetic (ECAL) and hadronic (HCAL) compartments.
This will facilitate particle-flow calorimetry, where the fine structure of
showers can be measured and used to enhance pileup rejection and particle
identification, whilst still achieving good energy resolution. The ECAL and a
large fraction of HCAL will be based on hexagonal silicon sensors of
0.5-1cm$^{2}$ cell size, with the remainder of the HCAL based on
highly-segmented scintillators with SiPM readout. The intrinsic high-precision
timing capabilities of the silicon sensors will add an extra dimension to event
reconstruction, especially in terms of pileup rejection. An overview of the
HGCAL project is presented, covering motivation, engineering design, readout
and trigger concepts, and performance (simulated and from beam tests).
",0,1,0,0,0,0
18533,18534,Seoul National University Camera II (SNUCAM-II): The New SED Camera for the Lee Sang Gak Telescope (LSGT),"  We present the characteristics and the performance of the new CCD camera
system, SNUCAM-II (Seoul National University CAMera system II) that was
installed on the Lee Sang Gak Telescope (LSGT) at the Siding Spring Observatory
in 2016. SNUCAM-II consists of a deep depletion chip covering a wide wavelength
from 0.3 {\mu}m to 1.1 {\mu}m with high sensitivity (QE at > 80% over 0.4 to
0.9 {\mu}m). It is equipped with the SDSS ugriz filters and 13 medium band
width (50 nm) filters, enabling us to study spectral energy distributions
(SEDs) of diverse objects from extragalactic sources to solar system objects.
On LSGT, SNUCAM-II offers 15.7 {\times} 15.7 arcmin field-of-view (FOV) at a
pixel scale of 0.92 arcsec and a limiting magnitude of g = 19.91 AB mag and
z=18.20 AB mag at 5{\sigma} with 180 sec exposure time for point source
detection.
",0,1,0,0,0,0
18241,18242,Evolving to Non-round Weingarten Spheres: Integer Linear Hopf Flows,"  In the 1950's Hopf gave examples of non-round convex 2-spheres in Euclidean
3-space with rotational symmetry that satisfy a linear relationship between
their principal curvatures. In this paper we investigate conditions under which
evolving a smooth rotationally symmetric sphere by a linear combination of its
radii of curvature yields a Hopf sphere. When the coefficients of the flow have
certain integer values, the fate of an initial sphere is entirely determined by
the local geometry of its isolated umbilic points. A surprising variety of
behaviours is uncovered: convergence to round spheres and non-round Hopf
spheres, as well as divergence to infinity.
The critical quantity is the rate of vanishing of the astigmatism - the
difference of the radii of curvature - at the isolated umbilic points. It is
proven that the size of this quantity versus the coefficient in the flow
function determines the fate of the evolution.
The geometric setting for the equation is Radius of Curvature space, viewed
as a pair of hyperbolic/AdS half-planes joined along their boundary, the
umbilic horizon. A rotationally symmetric sphere determines a parameterized
curve in this plane with end-points on the umbilic horizon. The slope of the
curve at the umbilic horizon is linked by the Codazzi-Mainardi equations to the
rate of vanishing of astigmatism, and for generic initial conditions can be
used to determine the outcome of the flow.
The slope can jump during the flow, and a number of examples are given:
instant jumps of the initial slope, as well as umbilic circles that contract to
points in finite time and 'pop' the slope. Finally, we present soliton-like
solutions: curves that evolve under linear flows by mutual hyperbolic/AdS
isometries (dilation and translation) of Radius of Curvature space. A
forthcoming paper will apply these geometric ideas to non-linear curvature
flows.
",0,0,1,0,0,0
3146,3147,Fast and Accurate Time Series Classification with WEASEL,"  Time series (TS) occur in many scientific and commercial applications,
ranging from earth surveillance to industry automation to the smart grids. An
important type of TS analysis is classification, which can, for instance,
improve energy load forecasting in smart grids by detecting the types of
electronic devices based on their energy consumption profiles recorded by
automatic sensors. Such sensor-driven applications are very often characterized
by (a) very long TS and (b) very large TS datasets needing classification.
However, current methods to time series classification (TSC) cannot cope with
such data volumes at acceptable accuracy; they are either scalable but offer
only inferior classification quality, or they achieve state-of-the-art
classification quality but cannot scale to large data volumes.
In this paper, we present WEASEL (Word ExtrAction for time SEries
cLassification), a novel TSC method which is both scalable and accurate. Like
other state-of-the-art TSC methods, WEASEL transforms time series into feature
vectors, using a sliding-window approach, which are then analyzed through a
machine learning classifier. The novelty of WEASEL lies in its specific method
for deriving features, resulting in a much smaller yet much more discriminative
feature set. On the popular UCR benchmark of 85 TS datasets, WEASEL is more
accurate than the best current non-ensemble algorithms at orders-of-magnitude
lower classification and training times, and it is almost as accurate as
ensemble classifiers, whose computational complexity makes them inapplicable
even for mid-size datasets. The outstanding robustness of WEASEL is also
confirmed by experiments on two real smart grid datasets, where it
out-of-the-box achieves almost the same accuracy as highly tuned,
domain-specific methods.
",1,0,0,1,0,0
18860,18861,Optimal design of a model energy conversion device,"  Fuel cells, batteries, thermochemical and other energy conversion devices
involve the transport of a number of (electro-)chemical species through
distinct materials so that they can meet and react at specified multi-material
interfaces. Therefore, morphology or arrangement of these different materials
can be critical in the performance of an energy conversion device. In this
paper, we study a model problem motivated by a solar-driven thermochemical
conversion device that splits water into hydrogen and oxygen. We formulate the
problem as a system of coupled multi-material reaction-diffusion equations
where each species diffuses selectively through a given material and where the
reaction occurs at multi-material interfaces. We express the problem of optimal
design of the material arrangement as a saddle point problem and obtain an
effective functional which shows that regions with very fine phase mixtures of
the material arise naturally. To explore this further, we introduce a
phase-field formulation of the optimal design problem, and numerically study
selected examples.
",0,1,1,0,0,0
5382,5383,One-to-One Matching of RTT and Path Changes,"  Route selection based on performance measurements is an essential task in
inter-domain Traffic Engineering. It can benefit from the detection of
significant changes in RTT measurements and the understanding on potential
causes of change. Among the extensive works on change detection methods and
their applications in various domains, few focus on RTT measurements. It is
thus unclear which approach works the best on such data.
In this paper, we present an evaluation framework for change detection on RTT
times series, consisting of: 1) a carefully labelled 34,008-hour RTT dataset as
ground truth; 2) a scoring method specifically tailored for RTT measurements.
Furthermore, we proposed a data transformation that improves the detection
performance of existing methods. Path changes are as well attended to. We fix
shortcomings of previous works by distinguishing path changes due to routing
protocols (IGP and BGP) from those caused by load balancing.
Finally, we apply our change detection methods to a large set of measurements
from RIPE Atlas. The characteristics of both RTT and path changes are analyzed;
the correlation between the two are also illustrated. We identify extremely
frequent AS path changes yet with few consequences on RTT, which has not been
reported before.
",1,0,0,0,0,0
19392,19393,"Two-Armed Bandit Problem, Data Processing, and Parallel Version of the Mirror Descent Algorithm","  We consider the minimax setup for the two-armed bandit problem as applied to
data processing if there are two alternative processing methods available with
different a priori unknown efficiencies. One should determine the most
effective method and provide its predominant application. To this end we use
the mirror descent algorithm (MDA). It is well-known that corresponding minimax
risk has the order $N^{1/2}$ with $N$ being the number of processed data. We
improve significantly the theoretical estimate of the factor using Monte-Carlo
simulations. Then we propose a parallel version of the MDA which allows
processing of data by packets in a number of stages. The usage of parallel
version of the MDA ensures that total time of data processing depends mostly on
the number of packets but not on the total number of data. It is quite
unexpectedly that the parallel version behaves unlike the ordinary one even if
the number of packets is large. Moreover, the parallel version considerably
improves control performance because it provides significantly smaller value of
the minimax risk. We explain this result by considering another parallel
modification of the MDA which behavior is close to behavior of the ordinary
version. Our estimates are based on invariant descriptions of the algorithms.
All estimates are obtained by Monte-Carlo simulations. It's worth noting that
parallel version performs well only for methods with close efficiencies. If
efficiencies differ significantly then one should use the combined algorithm
which at initial sufficiently short control horizon uses ordinary version and
then switches to the parallel version of the MDA.
",0,0,1,1,0,0
6587,6588,Principal Floquet subspaces and exponential separations of type II with applications to random delay differential equations,"  This paper deals with the study of principal Lyapunov exponents, principal
Floquet subspaces, and exponential separation for positive random linear
dynamical systems in ordered Banach spaces. The main contribution lies in the
introduction of a new type of exponential separation, called of type II,
important for its application to nonautonomous random differential equations
with delay. Under weakened assumptions, the existence of an exponential
separation of type II in an abstract general setting is shown, and an
illustration of its application to dynamical systems generated by scalar linear
random delay differential equations with finite delay is given.
",0,0,1,0,0,0
10461,10462,"Fast, precise, and widely tunable frequency control of an optical parametric oscillator referenced to a frequency comb","  Optical frequency combs (OFC) provide a convenient reference for the
frequency stabilization of continuous-wave lasers. We demonstrate a frequency
control method relying on tracking over a wide range and stabilizing the beat
note between the laser and the OFC. The approach combines fast frequency ramps
on a millisecond timescale in the entire mode-hop free tuning range of the
laser and precise stabilization to single frequencies. We apply it to a
commercially available optical parametric oscillator (OPO) and demonstrate
tuning over more than 60 GHz with a ramping speed up to 3 GHz/ms. Frequency
ramps spanning 15 GHz are performed in less than 10 ms, with the OPO instantly
relocked to the OFC after the ramp at any desired frequency. The developed
control hardware and software is able to stabilize the OPO to sub-MHz precision
and to perform sequences of fast frequency ramps automatically.
",0,1,0,0,0,0
11986,11987,Proceedings 2nd Workshop on Models for Formal Analysis of Real Systems,"  This volume contains the proceedings of MARS 2017, the second workshop on
Models for Formal Analysis of Real Systems, held on April 29, 2017 in Uppala,
Sweden, as an affiliated workshop of ETAPS 2017, the European Joint Conferences
on Theory and Practice of Software.
The workshop emphasises modelling over verification. It aims at discussing
the lessons learned from making formal methods for the verification and
analysis of realistic systems. Examples are:
(1) Which formalism is chosen, and why?
(2) Which abstractions have to be made and why?
(3) How are important characteristics of the system modelled?
(4) Were there any complications while modelling the system?
(5) Which measures were taken to guarantee the accuracy of the model?
We invited papers that present full models of real systems, which may lay the
basis for future comparison and analysis. An aim of the workshop is to present
different modelling approaches and discuss pros and cons for each of them.
Alternative formal descriptions of the systems presented at this workshop are
encouraged, which should foster the development of improved specification
formalisms.
",1,0,0,0,0,0
2653,2654,On the spectral geometry of manifolds with conic singularities,"  In the previous article we derived a detailed asymptotic expansion of the
heat trace for the Laplace-Beltrami operator on functions on manifolds with
conic singularities. In this article we investigate how the terms in the
expansion reflect the geometry of the manifold. Since the general expansion
contains a logarithmic term, its vanishing is a necessary condition for
smoothness of the manifold. In the two-dimensional case this implies that the
constant term of the expansion contains a non-local term that determines the
length of the (circular) cross section and vanishes precisely if this length
equals $2\pi$, that is, in the smooth case. We proceed to the study of higher
dimensions. In the four-dimensional case, the logarithmic term in the expansion
vanishes precisely when the cross section is a spherical space form, and we
expect that the vanishing of a further singular term will imply again
smoothness, but this is not yet clear beyond the case of cyclic space forms. In
higher dimensions the situation is naturally more difficult. We illustrate this
in the case of cross sections with constant curvature. Then the logarithmic
term becomes a polynomial in the curvature with roots that are different from
1, which necessitates more vanishing of other terms, not isolated so far.
",0,0,1,0,0,0
4347,4348,Toward Unsupervised Text Content Manipulation,"  Controlled generation of text is of high practical use. Recent efforts have
made impressive progress in generating or editing sentences with given textual
attributes (e.g., sentiment). This work studies a new practical setting of text
content manipulation. Given a structured record, such as `(PLAYER: Lebron,
POINTS: 20, ASSISTS: 10)', and a reference sentence, such as `Kobe easily
dropped 30 points', we aim to generate a sentence that accurately describes the
full content in the record, with the same writing style (e.g., wording,
transitions) of the reference. The problem is unsupervised due to lack of
parallel data in practice, and is challenging to minimally yet effectively
manipulate the text (by rewriting/adding/deleting text portions) to ensure
fidelity to the structured content. We derive a dataset from a basketball game
report corpus as our testbed, and develop a neural method with unsupervised
competing objectives and explicit content coverage constraints. Automatic and
human evaluations show superiority of our approach over competitive methods
including a strong rule-based baseline and prior approaches designed for style
transfer.
",1,0,0,0,0,0
11433,11434,Factorizations in Modules and Splitting Multiplicatively Closed Subsets,"  We introduce the concept of multiplicatively closed subsets of a commutative
ring $R$ which split an $R$-module $M$ and study factorization properties of
elements of $M$ with respect to such a set. Also we demonstrate how one can
utilize this concept to investigate factorization properties of $R$ and deduce
some Nagata type theorems relating factorization properties of $R$ to those of
its localizations, when $R$ is an integral domain.
",0,0,1,0,0,0
18891,18892,Discretization error estimates for penalty formulations of a linearized Canham-Helfrich type energy,"  This paper is concerned with minimization of a fourth-order linearized
Canham-Helfrich energy subject to Dirichlet boundary conditions on curves
inside the domain. Such problems arise in the modeling of the mechanical
interaction of biomembranes with embedded particles. There, the curve
conditions result from the imposed particle--membrane coupling. We prove
almost-$H^{\frac{5}{2}}$ regularity of the solution and then consider two
possible penalty formulations. For the combination of these penalty
formulations with a Bogner-Fox-Schmit finite element discretization we prove
discretization error estimates which are optimal in view of the solution's
reduced regularity. The error estimates are based on a general estimate for
linear penalty problems in Hilbert spaces. Finally, we illustrate the
theoretical results by numerical computations. An important feature of the
presented discretization is that it does not require to resolve the particle
boundary. This is crucial in order to avoid re-meshing if the presented problem
arises as subproblem in a model where particles are allowed to move or rotate.
",0,0,1,0,0,0
8381,8382,Parallel Simultaneous Perturbation Optimization,"  Stochastic computer simulations enable users to gain new insights into
complex physical systems. Optimization is a common problem in this context:
users seek to find model inputs that maximize the expected value of an
objective function. The objective function, however, is time-intensive to
evaluate, and cannot be directly measured. Instead, the stochastic nature of
the model means that individual realizations are corrupted by noise. More
formally, we consider the problem of optimizing the expected value of an
expensive black-box function with continuously-differentiable mean, from which
observations are corrupted by Gaussian noise. We present Parallel Simultaneous
Perturbation Optimization (PSPO), which extends a well-known stochastic
optimization algorithm, simultaneous perturbation stochastic approximation, in
several important ways. Our modifications allow the algorithm to fully take
advantage of parallel computing resources, like high-performance cloud
computing. The resulting PSPO algorithm takes fewer time-consuming iterations
to converge, automatically chooses the step size, and can vary the error
tolerance by step. Theoretical results are supported by a numerical example. To
demonstrate the performance of the algorithm, we implemented the algorithm to
maximize the pseudo-likelihood of a stochastic epidemiological model to data of
a measles outbreak.
",0,0,1,0,0,0
11881,11882,Multiscale Residual Mixture of PCA: Dynamic Dictionaries for Optimal Basis Learning,"  In this paper we are interested in the problem of learning an over-complete
basis and a methodology such that the reconstruction or inverse problem does
not need optimization. We analyze the optimality of the presented approaches,
their link to popular already known techniques s.a. Artificial Neural
Networks,k-means or Oja's learning rule. Finally, we will see that one approach
to reach the optimal dictionary is a factorial and hierarchical approach. The
derived approach lead to a formulation of a Deep Oja Network. We present
results on different tasks and present the resulting very efficient learning
algorithm which brings a new vision on the training of deep nets. Finally, the
theoretical work shows that deep frameworks are one way to efficiently have
over-complete (combinatorially large) dictionary yet allowing easy
reconstruction. We thus present the Deep Residual Oja Network (DRON). We
demonstrate that a recursive deep approach working on the residuals allow
exponential decrease of the error w.r.t. the depth.
",1,0,0,1,0,0
14377,14378,A Sufficient Condition for Nilpotency of the Nilpotent Residual of a Finite Group,"  Let $G$ be a finite group with the property that if $a,b$ are powers of
$\delta_1^*$-commutators such that $(|a|,|b|)=1$, then $|ab|=|a||b|$. We show
that $\gamma_{\infty}(G)$ is nilpotent.
",0,0,1,0,0,0
10126,10127,Topological Interference Management with Decoded Message Passing,"  The topological interference management (TIM) problem studies
partially-connected interference networks with no channel state information
except for the network topology (i.e., connectivity graph) at the transmitters.
In this paper, we consider a similar problem in the uplink cellular networks,
while message passing is enabled at the receivers (e.g., base stations), so
that the decoded messages can be routed to other receivers via backhaul links
to help further improve network performance. For this TIM problem with decoded
message passing (TIM-MP), we model the interference pattern by conflict
digraphs, connect orthogonal access to the acyclic set coloring on conflict
digraphs, and show that one-to-one interference alignment boils down to
orthogonal access because of message passing. With the aid of polyhedral
combinatorics, we identify the structural properties of certain classes of
network topologies where orthogonal access achieves the optimal
degrees-of-freedom (DoF) region in the information-theoretic sense. The
relation to the conventional index coding with simultaneous decoding is also
investigated by formulating a generalized index coding problem with successive
decoding as a result of decoded message passing. The properties of reducibility
and criticality are also studied, by which we are able to prove the linear
optimality of orthogonal access in terms of symmetric DoF for the networks up
to four users with all possible network topologies (218 instances). Practical
issues of the tradeoff between the overhead of message passing and the
achievable symmetric DoF are also discussed, in the hope of facilitating
efficient backhaul utilization.
",1,0,0,0,0,0
17191,17192,Local connectivity modulates multi-scale relaxation dynamics in a metallic glass-forming system,"  The structural description for the intriguing link between the fast
vibrational dynamics and slow diffusive dynamics in glass-forming systems is
one of the most challenging issues in physical science. Here, in a model of
metallic supercooled liquid, we find that local connectivity as an atomic-level
structural order parameter tunes the short-time vibrational excitations of the
icosahedrally coordinated particles and meanwhile modulates their long-time
relaxation dynamics changing from stretched to compressed exponentials,
denoting a dynamic transition from subdiffusive to hyperdiffusive motions of
such particles. Our result indicates that long-time dynamics has an
atomic-level structural origin which is related to the short-time dynamics,
thus suggests a structural bridge to link the fast vibrational dynamics and the
slow structural relaxation in glassy materials.
",0,1,0,0,0,0
6257,6258,SafeDrive: A Robust Lane Tracking System for Autonomous and Assisted Driving Under Limited Visibility,"  We present an approach towards robust lane tracking for assisted and
autonomous driving, particularly under poor visibility. Autonomous detection of
lane markers improves road safety, and purely visual tracking is desirable for
widespread vehicle compatibility and reducing sensor intrusion, cost, and
energy consumption. However, visual approaches are often ineffective because of
a number of factors, including but not limited to occlusion, poor weather
conditions, and paint wear-off. Our method, named SafeDrive, attempts to
improve visual lane detection approaches in drastically degraded visual
conditions without relying on additional active sensors. In scenarios where
visual lane detection algorithms are unable to detect lane markers, the
proposed approach uses location information of the vehicle to locate and access
alternate imagery of the road and attempts detection on this secondary image.
Subsequently, by using a combination of feature-based and pixel-based
alignment, an estimated location of the lane marker is found in the current
scene. We demonstrate the effectiveness of our system on actual driving data
from locations in the United States with Google Street View as the source of
alternate imagery.
",1,0,0,0,0,0
3687,3688,Impact of Traditional Sparse Optimizations on a Migratory Thread Architecture,"  Achieving high performance for sparse applications is challenging due to
irregular access patterns and weak locality. These properties preclude many
static optimizations and degrade cache performance on traditional systems. To
address these challenges, novel systems such as the Emu architecture have been
proposed. The Emu design uses light-weight migratory threads, narrow memory,
and near-memory processing capabilities to address weak locality and reduce the
total load on the memory system. Because the Emu architecture is fundamentally
different than cache based hierarchical memory systems, it is crucial to
understand the cost-benefit tradeoffs of standard sparse algorithm
optimizations on Emu hardware. In this work, we explore sparse matrix-vector
multiplication (SpMV) on the Emu architecture. We investigate the effects of
different sparse optimizations such as dense vector data layouts, work
distributions, and matrix reorderings. Our study finds that initially
distributing work evenly across the system is inadequate to maintain load
balancing over time due to the migratory nature of Emu threads. In severe
cases, matrix sparsity patterns produce hot-spots as many migratory threads
converge on a single resource. We demonstrate that known matrix reordering
techniques can improve SpMV performance on the Emu architecture by as much as
70% by encouraging more consistent load balancing. This can be compared with a
performance gain of no more than 16% on a cache-memory based system.
",1,0,0,0,0,0
15561,15562,A Simple Exponential Family Framework for Zero-Shot Learning,"  We present a simple generative framework for learning to predict previously
unseen classes, based on estimating class-attribute-gated class-conditional
distributions. We model each class-conditional distribution as an exponential
family distribution and the parameters of the distribution of each seen/unseen
class are defined as functions of the respective observed class attributes.
These functions can be learned using only the seen class data and can be used
to predict the parameters of the class-conditional distribution of each unseen
class. Unlike most existing methods for zero-shot learning that represent
classes as fixed embeddings in some vector space, our generative model
naturally represents each class as a probability distribution. It is simple to
implement and also allows leveraging additional unlabeled data from unseen
classes to improve the estimates of their class-conditional distributions using
transductive/semi-supervised learning. Moreover, it extends seamlessly to
few-shot learning by easily updating these distributions when provided with a
small number of additional labelled examples from unseen classes. Through a
comprehensive set of experiments on several benchmark data sets, we demonstrate
the efficacy of our framework.
",1,0,0,1,0,0
20617,20618,On the Complexity of Opinions and Online Discussions,"  In an increasingly polarized world, demagogues who reduce complexity down to
simple arguments based on emotion are gaining in popularity. Are opinions and
online discussions falling into demagoguery? In this work, we aim to provide
computational tools to investigate this question and, by doing so, explore the
nature and complexity of online discussions and their space of opinions,
uncovering where each participant lies.
More specifically, we present a modeling framework to construct latent
representations of opinions in online discussions which are consistent with
human judgements, as measured by online voting. If two opinions are close in
the resulting latent space of opinions, it is because humans think they are
similar. Our modeling framework is theoretically grounded and establishes a
surprising connection between opinions and voting models and the sign-rank of a
matrix. Moreover, it also provides a set of practical algorithms to both
estimate the dimension of the latent space of opinions and infer where opinions
expressed by the participants of an online discussion lie in this space.
Experiments on a large dataset from Yahoo! News, Yahoo! Finance, Yahoo! Sports,
and the Newsroom app suggest that unidimensional opinion models may often be
unable to accurately represent online discussions, provide insights into human
judgements and opinions, and show that our framework is able to circumvent
language nuances such as sarcasm or humor by relying on human judgements
instead of textual analysis.
",1,0,0,1,0,0
12047,12048,Classification of crystallization outcomes using deep convolutional neural networks,"  The Machine Recognition of Crystallization Outcomes (MARCO) initiative has
assembled roughly half a million annotated images of macromolecular
crystallization experiments from various sources and setups. Here,
state-of-the-art machine learning algorithms are trained and tested on
different parts of this data set. We find that more than 94% of the test images
can be correctly labeled, irrespective of their experimental origin. Because
crystal recognition is key to high-density screening and the systematic
analysis of crystallization experiments, this approach opens the door to both
industrial and fundamental research applications.
",0,0,0,1,1,0
4958,4959,Using Mode Connectivity for Loss Landscape Analysis,"  Mode connectivity is a recently introduced frame- work that empirically
establishes the connected- ness of minima by finding a high accuracy curve
between two independently trained models. To investigate the limits of this
setup, we examine the efficacy of this technique in extreme cases where the
input models are trained or initialized differently. We find that the procedure
is resilient to such changes. Given this finding, we propose using the
framework for analyzing loss surfaces and training trajectories more generally,
and in this direction, study SGD with cosine annealing and restarts (SGDR). We
report that while SGDR moves over barriers in its trajectory, propositions
claiming that it converges to and escapes from multiple local minima are not
substantiated by our empirical results.
",0,0,0,1,0,0
13629,13630,Controlling Stray Electric Fields on an Atom Chip for Rydberg Experiments,"  Experiments handling Rydberg atoms near surfaces must necessarily deal with
the high sensitivity of Rydberg atoms to (stray) electric fields that typically
emanate from adsorbates on the surface. We demonstrate a method to modify and
reduce the stray electric field by changing the adsorbates distribution. We use
one of the Rydberg excitation lasers to locally affect the adsorbed dipole
distribution. By adjusting the averaged exposure time we change the strength
(with the minimal value less than $0.2\,\textrm{V/cm}$ at $78\,\mu\textrm{m}$
from the chip) and even the sign of the perpendicular field component. This
technique is a useful tool for experiments handling Ryberg atoms near surfaces,
including atom chips.
",0,1,0,0,0,0
11911,11912,Customizing First Person Image Through Desired Actions,"  This paper studies a problem of inverse visual path planning: creating a
visual scene from a first person action. Our conjecture is that the spatial
arrangement of a first person visual scene is deployed to afford an action, and
therefore, the action can be inversely used to synthesize a new scene such that
the action is feasible. As a proof-of-concept, we focus on linking visual
experiences induced by walking.
A key innovation of this paper is a concept of ActionTunnel---a 3D virtual
tunnel along the future trajectory encoding what the wearer will visually
experience as moving into the scene. This connects two distinctive first person
images through similar walking paths. Our method takes a first person image
with a user defined future trajectory and outputs a new image that can afford
the future motion. The image is created by combining present and future
ActionTunnels in 3D where the missing pixels in adjoining area are computed by
a generative adversarial network. Our work can provide a travel across
different first person experiences in diverse real world scenes.
",1,0,0,0,0,0
18620,18621,DiGrad: Multi-Task Reinforcement Learning with Shared Actions,"  Most reinforcement learning algorithms are inefficient for learning multiple
tasks in complex robotic systems, where different tasks share a set of actions.
In such environments a compound policy may be learnt with shared neural network
parameters, which performs multiple tasks concurrently. However such compound
policy may get biased towards a task or the gradients from different tasks
negate each other, making the learning unstable and sometimes less data
efficient. In this paper, we propose a new approach for simultaneous training
of multiple tasks sharing a set of common actions in continuous action spaces,
which we call as DiGrad (Differential Policy Gradient). The proposed framework
is based on differential policy gradients and can accommodate multi-task
learning in a single actor-critic network. We also propose a simple heuristic
in the differential policy gradient update to further improve the learning. The
proposed architecture was tested on 8 link planar manipulator and 27 degrees of
freedom(DoF) Humanoid for learning multi-goal reachability tasks for 3 and 2
end effectors respectively. We show that our approach supports efficient
multi-task learning in complex robotic systems, outperforming related methods
in continuous action spaces.
",1,0,0,1,0,0
8541,8542,Water-based and Biocompatible 2D Crystal Inks: from Ink Formulation to All- Inkjet Printed Heterostructures,"  Fully exploiting the properties of 2D crystals requires a mass production
method able to produce heterostructures of arbitrary complexity on any
substrate, including plastic. Solution processing of graphene allows simple and
low-cost techniques such as inkjet printing to be used for device fabrication.
However, available inkjet printable formulations are still far from ideal as
they are either based on toxic solvents, have low concentration, or require
time-consuming and expensive formulation processing. In addition, none of those
formulations are suitable for thin-film heterostructure fabrication due to the
re-mixing of different 2D crystals, giving rise to uncontrolled interfaces,
which results in poor device performance and lack of reproducibility. In this
work we show a general formulation engineering approach to achieve highly
concentrated, and inkjet printable water-based 2D crystal formulations, which
also provides optimal film formation for multi-stack fabrication. We show
examples of all-inkjet printed heterostructures, such as large area arrays of
photosensors on plastic and paper and programmable logic memory devices, fully
exploiting the design flexibility of inkjet printing. Finally, dose-escalation
cytotoxicity assays in vitro also confirm the inks biocompatible character,
revealing the possibility of extending use of such 2D crystal formulations to
drug delivery and biomedical applications.
",0,1,0,0,0,0
5889,5890,Fast trimers in one-dimensional extended Fermi-Hubbard model,"  We consider a one-dimensional two component extended Fermi-Hubbard model with
nearest neighbor interactions and mass imbalance between the two species. We
study the stability of trimers, various observables for detecting them, and
expansion dynamics. We generalize the definition of the trimer gap to include
the formation of different types of clusters originating from nearest neighbor
interactions. Expansion dynamics reveal rapidly propagating trimers, with
speeds exceeding doublon propagation in strongly interacting regime. We present
a simple model for understanding this unique feature of the movement of the
trimers, and we discuss the potential for experimental realization.
",0,1,0,0,0,0
4711,4712,Decentralized P2P Energy Trading under Network Constraints in a Low-Voltage Network,"  The increasing uptake of distributed energy resources (DERs) in distribution
systems and the rapid advance of technology have established new scenarios in
the operation of low-voltage networks. In particular, recent trends in
cryptocurrencies and blockchain have led to a proliferation of peer-to-peer
(P2P) energy trading schemes, which allow the exchange of energy between the
neighbors without any intervention of a conventional intermediary in the
transactions. Nevertheless, far too little attention has been paid to the
technical constraints of the network under this scenario. A major challenge to
implementing P2P energy trading is that of ensuring that network constraints
are not violated during the energy exchange. This paper proposes a methodology
based on sensitivity analysis to assess the impact of P2P transactions on the
network and to guarantee an exchange of energy that does not violate network
constraints. The proposed method is tested on a typical UK low-voltage network.
The results show that our method ensures that energy is exchanged between users
under the P2P scheme without violating the network constraints, and that users
can still capture the economic benefits of the P2P architecture.
",1,0,0,0,0,0
5405,5406,The self-referring DNA and protein: a remark on physical and geometrical aspects,"  All known life forms are based upon a hierarchy of interwoven feedback loops,
operating over a cascade of space, time and energy scales. Among the most basic
loops are those connecting DNA and proteins. For example, in genetic networks,
DNA genes are expressed as proteins, which may bind near the same genes and
thereby control their own expression. In this molecular type of self-reference,
information is mapped from the DNA sequence to the protein and back to DNA.
There is a variety of dynamic DNA-protein self-reference loops, and the purpose
of this remark is to discuss certain geometrical and physical aspects related
to the back and forth mapping between DNA and proteins. The discussion raises
basic questions regarding the nature of DNA and proteins as self-referring
matter, which are examined in a simple toy model.
",0,0,0,0,1,0
6957,6958,Categorification of sign-skew-symmetric cluster algebras and some conjectures on g-vectors,"  Using the unfolding method given in \cite{HL}, we prove the conjectures on
sign-coherence and a recurrence formula respectively of ${\bf g}$-vectors for
acyclic sign-skew-symmetric cluster algebras. As a following consequence, the
conjecture is affirmed in the same case which states that the ${\bf g}$-vectors
of any cluster form a basis of $\mathbb Z^n$. Also, the additive
categorification of an acyclic sign-skew-symmetric cluster algebra $\mathcal
A(\Sigma)$ is given, which is realized as $(\mathcal C^{\widetilde Q},\Gamma)$
for a Frobenius $2$-Calabi-Yau category $\mathcal C^{\widetilde Q}$ constructed
from an unfolding $(Q,\Gamma)$ of the acyclic exchange matrix $B$ of $\mathcal
A(\Sigma)$.
",0,0,1,0,0,0
2429,2430,A deep Convolutional Neural Network for topology optimization with strong generalization ability,"  This paper proposes a deep Convolutional Neural Network(CNN) with strong
generalization ability for structural topology optimization. The architecture
of the neural network is made up of encoding and decoding parts, which provide
down- and up-sampling operations. In addition, a popular technique, namely
U-Net, was adopted to improve the performance of the proposed neural network.
The input of the neural network is a well-designed tensor with each channel
includes different information for the problem, and the output is the layout of
the optimal structure. To train the neural network, a large dataset is
generated by a conventional topology optimization approach, i.e. SIMP. The
performance of the proposed method was evaluated by comparing its efficiency
and accuracy with SIMP on a series of typical optimization problems. Results
show that a significant reduction in computation cost was achieved with little
sacrifice on the optimality of design solutions. Furthermore, the proposed
method can intelligently solve problems under boundary conditions not being
included in the training dataset.
",1,0,0,1,0,0
11231,11232,Bellman Gradient Iteration for Inverse Reinforcement Learning,"  This paper develops an inverse reinforcement learning algorithm aimed at
recovering a reward function from the observed actions of an agent. We
introduce a strategy to flexibly handle different types of actions with two
approximations of the Bellman Optimality Equation, and a Bellman Gradient
Iteration method to compute the gradient of the Q-value with respect to the
reward function. These methods allow us to build a differentiable relation
between the Q-value and the reward function and learn an approximately optimal
reward function with gradient methods. We test the proposed method in two
simulated environments by evaluating the accuracy of different approximations
and comparing the proposed method with existing solutions. The results show
that even with a linear reward function, the proposed method has a comparable
accuracy with the state-of-the-art method adopting a non-linear reward
function, and the proposed method is more flexible because it is defined on
observed actions instead of trajectories.
",1,0,0,0,0,0
4490,4491,Assessing the Economics of Customer-Sited Multi-Use Energy Storage,"  This paper presents an approach to assess the economics of customer-sited
energy storage systems (ESSs) which are owned and operated by a customer. The
ESSs can participate in frequency regulation and spinning reserve markets, and
are used to help the customer consume available renewable energy and reduce
electricity bill. A rolling-horizon approach is developed to optimize the
service schedule, and the resulting costs and revenues are used to assess
economics of the ESSs. The economic assessment approach is illustrated with
case studies, from which we obtain some new observations on profitability of
the customer- sited multi-use ESSs.
",0,0,1,0,0,0
15318,15319,Imaging structural transitions in organometallic molecules on Ag(100) for solar thermal energy storage,"  The use of opto-thermal molecular energy storage at the nanoscale creates new
opportunities for powering future microdevices with flexible synthetic
tailorability. Practical application of these molecular materials, however,
requires a deeper microscopic understanding of how their behavior is altered by
the presence of different types of substrates. Here we present single-molecule
resolved scanning tunneling microscopy imaging of thermally- and
optically-induced structural transitions in (fulvalene)tetracarbonyldiruthenium
molecules adsorbed onto a Ag(100) surface as a prototype system. Both the
parent complex and the photoisomer display distinct thermally-driven phase
transformations when they are in contact with a Ag(100) surface. This behavior
is consistent with the loss of carbonyl ligands due to strong molecule-surface
coupling. Ultraviolet radiation induces marked structural changes only in the
intact parent complex, thus indicating a photoisomerization reaction. These
results demonstrate how stimuli-induced structural transitions in this class of
molecule depend on the nature of the underlying substrate.
",0,1,0,0,0,0
13219,13220,Divide-and-Conquer Reinforcement Learning,"  Standard model-free deep reinforcement learning (RL) algorithms sample a new
initial state for each trial, allowing them to optimize policies that can
perform well even in highly stochastic environments. However, problems that
exhibit considerable initial state variation typically produce high-variance
gradient estimates for model-free RL, making direct policy or value function
optimization challenging. In this paper, we develop a novel algorithm that
instead partitions the initial state space into ""slices"", and optimizes an
ensemble of policies, each on a different slice. The ensemble is gradually
unified into a single policy that can succeed on the whole state space. This
approach, which we term divide-and-conquer RL, is able to solve complex tasks
where conventional deep RL methods are ineffective. Our results show that
divide-and-conquer RL greatly outperforms conventional policy gradient methods
on challenging grasping, manipulation, and locomotion tasks, and exceeds the
performance of a variety of prior methods. Videos of policies learned by our
algorithm can be viewed at this http URL
",1,0,0,0,0,0
7143,7144,Physics-Informed Regularization of Deep Neural Networks,"  This paper presents a novel physics-informed regularization method for
training of deep neural networks (DNNs). In particular, we focus on the DNN
representation for the response of a physical or biological system, for which a
set of governing laws are known. These laws often appear in the form of
differential equations, derived from first principles, empirically-validated
laws, and/or domain expertise. We propose a DNN training approach that utilizes
these known differential equations in addition to the measurement data, by
introducing a penalty term to the training loss function to penalize divergence
form the governing laws. Through three numerical examples, we will show that
the proposed regularization produces surrogates that are physically
interpretable with smaller generalization errors, when compared to other common
regularization methods.
",1,0,0,0,0,0
12016,12017,Improving End-to-End Speech Recognition with Policy Learning,"  Connectionist temporal classification (CTC) is widely used for maximum
likelihood learning in end-to-end speech recognition models. However, there is
usually a disparity between the negative maximum likelihood and the performance
metric used in speech recognition, e.g., word error rate (WER). This results in
a mismatch between the objective function and metric during training. We show
that the above problem can be mitigated by jointly training with maximum
likelihood and policy gradient. In particular, with policy learning we are able
to directly optimize on the (otherwise non-differentiable) performance metric.
We show that joint training improves relative performance by 4% to 13% for our
end-to-end model as compared to the same model learned through maximum
likelihood. The model achieves 5.53% WER on Wall Street Journal dataset, and
5.42% and 14.70% on Librispeech test-clean and test-other set, respectively.
",1,0,0,1,0,0
12583,12584,Yarkovsky Drift Detections for 159 Near-Earth Asteroids,"  The Yarkovsky effect is a thermal process acting upon the orbits of small
celestial bodies, which can cause these orbits to slowly expand or contract
with time. The effect is subtle -- typical drift rates lie near $10^{-4}$ au/My
for a $\sim$1 km diameter object -- and is thus generally difficult to measure.
However, objects with long observation intervals, as well as objects with radar
detections, serve as excellent candidates for the observation of this effect.
We analyzed both optical and radar astrometry for all numbered Near-Earth
Asteroids (NEAs), as well as several un-numbered NEAs, for the purpose of
detecting and quantifying the Yarkovsky effect. We present 159 objects with
measured drift rates. Our Yarkovsky sample is the largest published set of such
detections, and presents an opportunity to examine the physical properties of
these NEAs and the Yarkovsky effect in a statistical manner. In particular, we
confirm the Yarkovsky effect's theoretical size dependence of 1/$D$, where $D$
is diameter. We also examine the efficiency with which this effect acts on our
sample objects and find typical efficiencies of around 12%. We interpret this
efficiency with respect to the typical spin and thermal properties of objects
in our sample. We report the ratio of negative to positive drift rates in our
sample as $N_R/N_P = 2.9 \pm 0.7$ and interpret this ratio in terms of
retrograde/prograde rotators and main belt escape routes. The observed ratio
has a probability of 1 in 46 million of occurring by chance, which confirms the
presence of a non-gravitational influence. We examine how the presence of radar
data affects the strength and precision of our detections. We find that, on
average, the precision of radar+optical detections improves by a factor of
approximately 1.6 for each additional apparition with ranging data compared to
that of optical-only solutions.
",0,1,0,0,0,0
8695,8696,Laplacian Spectrum of non-commuting graphs of finite groups,"  In this paper, we compute the Laplacian spectrum of non-commuting graphs of
some classes of finite non-abelian groups. Our computations reveal that the
non-commuting graphs of all the groups considered in this paper are L-integral.
We also obtain some conditions on a group $G$ so that its non-commuting graph
is L-integral.
",0,0,1,0,0,0
18828,18829,"Distributed, scalable and gossip-free consensus optimization with application to data analysis","  Distributed algorithms for solving additive or consensus optimization
problems commonly rely on first-order or proximal splitting methods. These
algorithms generally come with restrictive assumptions and at best enjoy a
linear convergence rate. Hence, they can require many iterations or
communications among agents to converge. In many cases, however, we do not seek
a highly accurate solution for consensus problems. Based on this we propose a
controlled relaxation of the coupling in the problem which allows us to compute
an approximate solution, where the accuracy of the approximation can be
controlled by the level of relaxation. The relaxed problem can be efficiently
solved in a distributed way using a combination of primal-dual interior-point
methods (PDIPMs) and message-passing. This algorithm purely relies on
second-order methods and thus requires far fewer iterations and communications
to converge. This is illustrated in numerical experiments, showing its superior
performance compared to existing methods.
",0,0,1,0,0,0
14944,14945,On the global convergence of the Jacobi method for symmetric matrices of order 4 under parallel strategies,"  The paper analyzes special cyclic Jacobi methods for symmetric matrices of
order $4$. Only those cyclic pivot strategies that enable full parallelization
of the method are considered. These strategies, unlike the serial pivot
strategies, can force the method to be very slow or very fast within one cycle,
depending on the underlying matrix. Hence, for the global convergence proof one
has to consider two or three adjacent cycles. It is proved that for any
symmetric matrix $A$ of order~$4$ the inequality
$S(A^{[2]})\leq(1-10^{-5})S(A)$ holds, where $A^{[2]}$ results from $A$ by
applying two cycles of a particular parallel method. Here $S(A)$ stands for the
Frobenius norm of the strictly upper-triangular part of $A$. The result holds
for two special parallel strategies and implies the global convergence of the
method under all possible fully parallel strategies. It is also proved that for
every $\epsilon>0$ and $n\geq4$ there exist a symmetric matrix $A(\epsilon)$ of
order $n$ and a cyclic strategy, such that upon completion of the first cycle
of the appropriate Jacobi method the inequality $S(A^{[1]})>
(1-\epsilon)S(A(\epsilon))$ holds.
",0,0,1,0,0,0
1580,1581,FLASH: Randomized Algorithms Accelerated over CPU-GPU for Ultra-High Dimensional Similarity Search,"  We present FLASH (\textbf{F}ast \textbf{L}SH \textbf{A}lgorithm for
\textbf{S}imilarity search accelerated with \textbf{H}PC), a similarity search
system for ultra-high dimensional datasets on a single machine, that does not
require similarity computations and is tailored for high-performance computing
platforms. By leveraging a LSH style randomized indexing procedure and
combining it with several principled techniques, such as reservoir sampling,
recent advances in one-pass minwise hashing, and count based estimations, we
reduce the computational and parallelization costs of similarity search, while
retaining sound theoretical guarantees.
We evaluate FLASH on several real, high-dimensional datasets from different
domains, including text, malicious URL, click-through prediction, social
networks, etc. Our experiments shed new light on the difficulties associated
with datasets having several million dimensions. Current state-of-the-art
implementations either fail on the presented scale or are orders of magnitude
slower than FLASH. FLASH is capable of computing an approximate k-NN graph,
from scratch, over the full webspam dataset (1.3 billion nonzeros) in less than
10 seconds. Computing a full k-NN graph in less than 10 seconds on the webspam
dataset, using brute-force ($n^2D$), will require at least 20 teraflops. We
provide CPU and GPU implementations of FLASH for replicability of our results.
",1,0,0,0,0,0
18831,18832,Higher order mobile coverage control with application to localization,"  Most current results on coverage control using mobile sensors require that
one partitioned cell is associated with precisely one sensor. In this paper, we
consider a class of coverage control problems involving higher order Voronoi
partitions, motivated by applications where more than one sensor is required to
monitor and cover one cell. Such applications are frequent in scenarios
requiring the sensors to localize targets. We introduce a framework depending
on a coverage performance function incorporating higher order Voronoi cells and
then design a gradient-based controller which allows the multi-sensor system to
achieve a local equilibrium in a distributed manner. The convergence properties
are studied and related to Lloyd algorithm. We study also the extension to
coverage of a discrete set of points. In addition, we provide a number of real
world scenarios where our framework can be applied. Simulation results are also
provided to show the controller performance.
",1,0,0,0,0,0
15707,15708,On a Surprising Oversight by John S. Bell in the Proof of his Famous Theorem,"  Bell inequalities are usually derived by assuming locality and realism, and
therefore experimental violations of Bell inequalities are usually taken to
imply violations of either locality or realism, or both. But, after reviewing
an oversight by Bell, here we derive the Bell-CHSH inequality by assuming only
that Bob can measure along the directions b and b' simultaneously while Alice
measures along either a or a', and likewise Alice can measure along the
directions a and a' simultaneously while Bob measures along either b or b',
without assuming locality. The observed violations of the Bell-CHSH inequality
therefore simply verify the manifest impossibility of measuring along the
directions b and b' (or along the directions a and a') simultaneously, in any
realizable EPR-Bohm type experiment.
",0,1,0,0,0,0
9345,9346,Taming the Signal-to-Noise Problem in Lattice QCD by Phase Reweighting,"  Path integrals describing quantum many-body systems can be calculated with
Monte Carlo sampling techniques, but average quantities are often subject to
signal-to-noise ratios that degrade exponentially with time. A
phase-reweighting technique inspired by recent observations of random walk
statistics in correlation functions is proposed that allows energy levels to be
extracted from late-time correlation functions with time-independent
signal-to-noise ratios. Phase reweighting effectively includes dynamical
refinement of source magnitudes but introduces a bias associated with the
phase. This bias can be removed by performing an extrapolation, but at the
expense of re-introducing a signal-to-noise problem. Lattice Quantum
Chromodynamics calculations of the $\rho$ and nucleon masses and of the
$\Xi\Xi$ binding energy show consistency between standard results obtained
using earlier-time correlation functions and phase-reweighted results using
late-time correlation functions inaccessible to standard statistical analysis
methods.
",0,1,0,0,0,0
14260,14261,Rigidity-induced scale invariance in polymer ejection from capsid,"  While the dynamics of a fully flexible polymer ejecting a capsid through a
nanopore has been extensively studied, the ejection dynamics of semiflexible
polymers has not been properly characterized. Here we report results from
simulations of ejection dynamics of semiflexible polymers ejecting from
spherical capsids. Ejections start from strongly confined polymer conformations
of constant initial monomer density. We find that, unlike for fully flexible
polymers, for semiflexible polymers the force measured at the pore does not
show a direct relation to the instantaneous ejection velocity. The cumulative
waiting time $t(s)$, that is, the time at which a monomer $s$ exits the capsid
the last time, shows a clear change when increasing the polymer rigidity
$\kappa$. Major part of an ejecting polymer is driven out of the capsid by
internal pressure. At the final stage the polymer escapes the capsid by
diffusion. For the driven part there is a cross-over from essentially
exponential growth of $t$ with $s$ of the fully flexible polymers to a
scale-invariant form. In addition, a clear dependence of $t$ on $N_0$ was
found. These findings combined give the dependence $t(s) \propto N_0^{0.55}
s^{1.33}$ for the strongly rigid polymers. This cross-over in dynamics where
$\kappa$ acts as a control parameter is reminiscent of a phase transition. This
analogy is further enhanced by our finding a perfect data collapse of $t$ for
polymers of different $N_0$ and any constant $\kappa$.
",0,1,0,0,0,0
3426,3427,Evidence for Two Hot Jupiter Formation Paths,"  Disk migration and high-eccentricity migration are two well-studied theories
to explain the formation of hot Jupiters. The former predicts that these
planets can migrate up until the planet-star Roche separation ($a_{Roche}$) and
the latter predicts they will tidally circularize at a minimum distance of
2$a_{Roche}$. Considering long-running radial velocity and transit surveys have
identified a couple hundred hot Jupiters to date, we can revisit the classic
question of hot Jupiter formation in a data-driven manner. We approach this
problem using data from several exoplanet surveys (radial velocity, Kepler,
HAT, and WASP) allowing for either a single population or a mixture of
populations associated with these formation channels, and applying a
hierarchical Bayesian mixture model of truncated power laws of the form
$x^{\gamma-1}$ to constrain the population-level parameters of interest (e.g.,
location of inner edges, $\gamma$, mixture fractions). Within the limitations
of our chosen models, we find the current radial velocity and Kepler sample of
hot Jupiters can be well explained with a single truncated power law
distribution with a lower cutoff near 2$a_{Roche}$, a result that still holds
after a decade, and $\gamma=-0.51\pm^{0.19}_{0.20}$. However, the HAT and WASP
data show evidence for multiple populations (Bayes factor $\approx 10^{21}$).
We find that $15\pm^{9}_{6}\%$ reside in a component consistent with disk
migration ($\gamma=-0.04\pm^{0.53}_{1.27}$) and $85\pm^{6}_{9}\%$ in one
consistent with high-eccentricity migration ($\gamma=-1.38\pm^{0.32}_{0.47}$).
We find no immediately strong connections with some observed host star
properties and speculate on how future exoplanet surveys could improve upon hot
Jupiter population inference.
",0,1,0,0,0,0
13340,13341,Collective search with finite perception: transient dynamics and search efficiency,"  Motile organisms often use finite spatial perception of their surroundings to
navigate and search their habitats. Yet standard models of search are usually
based on purely local sensory information. To model how a finite perceptual
horizon affects ecological search, we propose a framework for optimal
navigation that combines concepts from random walks and optimal control theory.
We show that, while local strategies are optimal on asymptotically long and
short search times, finite perception yields faster convergence and increased
search efficiency over transient time scales relevant in biological systems.
The benefit of the finite horizon can be maintained by the searchers tuning
their response sensitivity to the length scale of the stimulant in the
environment, and is enhanced when the agents interact as a result of increased
consensus within subpopulations. Our framework sheds light on the role of
spatial perception and transients in search movement and collective sensing of
the environment.
",0,0,0,0,1,0
427,428,Birefringence induced by pp-wave modes in an electromagnetically active dynamic aether,"  In the framework of the Einstein-Maxwell-aether theory we study the
birefringence effect, which can occur in the pp-wave symmetric dynamic aether.
The dynamic aether is considered to be latently birefringent quasi-medium,
which displays this hidden property if and only if the aether motion is
non-uniform, i.e., when the aether flow is characterized by the non-vanishing
expansion, shear, vorticity or acceleration. In accordance with the
dynamo-optical scheme of description of the interaction between electromagnetic
waves and the dynamic aether, we shall model the susceptibility tensors by the
terms linear in the covariant derivative of the aether velocity four-vector.
When the pp-wave modes appear in the dynamic aether, we deal with a
gravitationally induced degeneracy removal with respect to hidden
susceptibility parameters. As a consequence, the phase velocities of
electromagnetic waves possessing orthogonal polarizations do not coincide, thus
displaying the birefringence effect. Two electromagnetic field configurations
are studied in detail: longitudinal and transversal with respect to the aether
pp-wave front. For both cases the solutions are found, which reveal anomalies
in the electromagnetic response on the action of the pp-wave aether mode.
",0,1,0,0,0,0
4960,4961,Finding Root Causes of Floating Point Error with Herbgrind,"  Floating-point arithmetic plays a central role in science, engineering, and
finance by enabling developers to approximate real arithmetic. To address
numerical issues in large floating-point applications, developers must identify
root causes, which is difficult because floating-point errors are generally
non-local, non-compositional, and non-uniform.
This paper presents Herbgrind, a tool to help developers identify and address
root causes in numerical code written in low-level C/C++ and Fortran. Herbgrind
dynamically tracks dependencies between operations and program outputs to avoid
false positives and abstracts erroneous computations to a simplified program
fragment whose improvement can reduce output error. We perform several case
studies applying Herbgrind to large, expert-crafted numerical programs and show
that it scales to applications spanning hundreds of thousands of lines,
correctly handling the low-level details of modern floating point hardware and
mathematical libraries, and tracking error across function boundaries and
through the heap.
",1,0,0,0,0,0
17541,17542,Predictive Indexing,"  There has been considerable research on automated index tuning in database
management systems (DBMSs). But the majority of these solutions tune the index
configuration by retrospectively making computationally expensive physical
design changes all at once. Such changes degrade the DBMS's performance during
the process, and have reduced utility during subsequent query processing due to
the delay between a workload shift and the associated change. A better approach
is to generate small changes that tune the physical design over time, forecast
the utility of these changes, and apply them ahead of time to maximize their
impact.
This paper presents predictive indexing that continuously improves a
database's physical design using lightweight physical design changes. It uses a
machine learning model to forecast the utility of these changes, and
continuously refines the index configuration of the database to handle evolving
workloads. We introduce a lightweight hybrid scan operator with which a DBMS
can make use of partially-built indexes for query processing. Our evaluation
shows that predictive indexing improves the throughput of a DBMS by 3.5--5.2x
compared to other state-of-the-art indexing approaches. We demonstrate that
predictive indexing works seamlessly with other lightweight automated physical
design tuning methods.
",1,0,0,0,0,0
12143,12144,Sensor Transformation Attention Networks,"  Recent work on encoder-decoder models for sequence-to-sequence mapping has
shown that integrating both temporal and spatial attention mechanisms into
neural networks increases the performance of the system substantially. In this
work, we report on the application of an attentional signal not on temporal and
spatial regions of the input, but instead as a method of switching among inputs
themselves. We evaluate the particular role of attentional switching in the
presence of dynamic noise in the sensors, and demonstrate how the attentional
signal responds dynamically to changing noise levels in the environment to
achieve increased performance on both audio and visual tasks in three
commonly-used datasets: TIDIGITS, Wall Street Journal, and GRID. Moreover, the
proposed sensor transformation network architecture naturally introduces a
number of advantages that merit exploration, including ease of adding new
sensors to existing architectures, attentional interpretability, and increased
robustness in a variety of noisy environments not seen during training.
Finally, we demonstrate that the sensor selection attention mechanism of a
model trained only on the small TIDIGITS dataset can be transferred directly to
a pre-existing larger network trained on the Wall Street Journal dataset,
maintaining functionality of switching between sensors to yield a dramatic
reduction of error in the presence of noise.
",1,0,0,0,0,0
19383,19384,Group Metrics for Graph Products of Cyclic Groups,"  We complement the characterization of the graph products of cyclic groups
$G(\Gamma, \mathfrak{p})$ admitting a Polish group topology of [9] with the
following result. Let $G = G(\Gamma, \mathfrak{p})$, then the following are
equivalent: (i) there is a metric on $\Gamma$ which induces a separable
topology in which $E_{\Gamma}$ is closed; (ii) $G(\Gamma, \mathfrak{p})$ is
embeddable into a Polish group; (iii) $G(\Gamma, \mathfrak{p})$ is embeddable
into a non-Archimedean Polish group. We also construct left-invariant separable
group ultrametrics for $G = G(\Gamma, \mathfrak{p})$ and $\Gamma$ a closed
graph on the Baire space, which is of independent interest.
",0,0,1,0,0,0
19088,19089,Magnetic Properties of Transition-Metal Adsorbed ot-Phosphorus Monolayer: A First-principles and Monte Carlo Study,"  Using the first-principles and Monte Carlo methods, here we systematically
study magnetic properties of monolayer octagonal-tetragonal phosphorus with 3d
transition-metal (TM) adatoms. Different from the puckered hexagonal black
phosphorus monolayer (phosphorene or $\alpha$-P), the octagonal-tetragonal
phase of 2D phosphorus (named as ot-P or $\epsilon$-P in this article) is
buckled with octagon-tetragon structure. Our calculations show that all TMs,
except the closed-shell Zn atom, are able to strongly bind onto monolayer
$ot$-P with significant binding energies. Local magnetic moments (up to 6
$\mu$B) on adatoms of Sc, Ti, V, Cr, Mn, Fe and Co originate from the exchange
and crystal-field splitting of TM 3d orbitals. The magnetic coupling between
localized magnetic states of adatoms is dependent on adatomic distances and
directions. Lastly, the uniformly magnetic order is investigated to screening
two-dimensional dilute ferromagnets with high Curie temperature for
applications of spintronics. It is found that ot-P with V atoms homogeneously
adsorbed at the centre of octagons with a concentration of 5% has the most
stable ferromagnetic ground state. Its Curie temperature is estimated to be 173
K using the Monte Carlo method.
",0,1,0,0,0,0
17497,17498,Unsupervised Object Discovery and Segmentation of RGBD-images,"  In this paper we introduce a system for unsupervised object discovery and
segmentation of RGBD-images. The system models the sensor noise directly from
data, allowing accurate segmentation without sensor specific hand tuning of
measurement noise models making use of the recently introduced Statistical
Inlier Estimation (SIE) method. Through a fully probabilistic formulation, the
system is able to apply probabilistic inference, enabling reliable segmentation
in previously challenging scenarios. In addition, we introduce new methods for
filtering out false positives, significantly improving the signal to noise
ratio. We show that the system significantly outperform state-of-the-art in on
a challenging real-world dataset.
",1,0,0,0,0,0
11135,11136,Trace your sources in large-scale data: one ring to find them all,"  An important preprocessing step in most data analysis pipelines aims to
extract a small set of sources that explain most of the data. Currently used
algorithms for blind source separation (BSS), however, often fail to extract
the desired sources and need extensive cross-validation. In contrast, their
rarely used probabilistic counterparts can get away with little
cross-validation and are more accurate and reliable but no simple and scalable
implementations are available. Here we present a novel probabilistic BSS
framework (DECOMPOSE) that can be flexibly adjusted to the data, is extensible
and easy to use, adapts to individual sources and handles large-scale data
through algorithmic efficiency. DECOMPOSE encompasses and generalises many
traditional BSS algorithms such as PCA, ICA and NMF and we demonstrate
substantial improvements in accuracy and robustness on artificial and real
data.
",0,0,0,1,0,0
15562,15563,Modeling Hormesis Using a Non-Monotonic Copula Method,"  This paper presents a probabilistic method for capturing non-monotonic
behavior under the biphasic dose-response regime observed in many biological
systems experiencing different types of stress. The proposed method is based on
the rolling-pin method introduced earlier to estimate highly nonlinear and
non-monotonic joint probability distributions from continuous domain data. We
show that the proposed method outperforms the conventional parametric methods
in terms of the error (namely RMSE) and it needs fewer parameters to be
estimated a priori, while offering high flexibility. The application and
performance of the proposed method are shown through an example.
",0,0,0,1,0,0
9050,9051,"Stellar Absorption Line Analysis of Local Star-Forming Galaxies: The Relation Between Stellar Mass, Metallicity, Dust Attenuation and Star Formation Rate","  We analyze the optical continuum of star-forming galaxies in SDSS by fitting
stacked spectra with stellar population synthesis models to investigate the
relation between stellar mass, stellar metallicity, dust attenuation and star
formation rate. We fit models calculated with star formation and chemical
evolution histories that are derived empirically from multi-epoch observations
of the stellar mass---star formation rate and the stellar mass---gas-phase
metallicity relations, respectively. We also fit linear combinations of single
burst models with a range of metallicities and ages. Star formation and
chemical evolution histories are unconstrained for these models. The stellar
mass---stellar metallicity relations obtained from the two methods agree with
the relation measured from individual supergiant stars in nearby galaxies.
These relations are also consistent with the relation obtained from emission
line analysis of gas-phase metallicity after accounting for systematic offsets
in the gas-phase-metallicity. We measure dust attenuation of the stellar
continuum and show that its dependence on stellar mass and star formation rate
is consistent with previously reported results derived from nebular emission
lines. However, stellar continuum attenuation is smaller than nebular emission
line attenuation. The continuum-to-nebular attenuation ratio depends on stellar
mass and is smaller in more massive galaxies. Our consistent analysis of
stellar continuum and nebular emission lines paves the way for a comprehensive
investigation of stellar metallicities of star-forming and quiescent galaxies.
",0,1,0,0,0,0
13876,13877,Extended opportunity cost model to find near equilibrium electricity prices under non-convexities,"  This paper finds near equilibrium prices for electricity markets with
nonconvexities due to binary variables, in order to reduce the market
participants' opportunity costs, such as generators' unrecovered costs. The
opportunity cost is defined as the difference between the profit when the
instructions of the market operator are followed and when the market
participants can freely make their own decisions based on the market prices. We
use the minimum complementarity approximation to the minimum total opportunity
cost (MTOC) model, from previous research, with tests on a much more realistic
unit commitment (UC) model than in previous research, including features such
as reserve requirements, ramping constraints, and minimum up and down times.
The developed model incorporates flexible price responsive demand, as in
previous research, but since not all demand is price responsive, we consider
the more realistic case that total demand is a mixture of fixed and flexible.
Another improvement over previous MTOC research is computational: whereas the
previous research had nonconvex terms among the objective function's continuous
variables, we convert the objective to an equivalent form that contains only
linear and convex quadratic terms in the continuous variables. We compare the
unit commitment model with the standard social welfare optimization version of
UC, in a series of sensitivity analyses, varying flexible demand to represent
varying degrees of future penetration of electric vehicles and smart
appliances, different ratios of generation availability, and different values
of transmission line capacities to consider possible congestion. The minimum
total opportunity cost and social welfare solutions are mostly very close in
different scenarios, except in some extreme cases.
",0,0,0,0,0,1
8584,8585,Experimental study of electron and phonon dynamics in nanoscale materials by ultrafast laser time-domain spectroscopy,"  With the rapid advances in the development of nanotechnology, nowadays, the
sizes of elementary unit, i.e. transistor, of micro- and nanoelectronic devices
are well deep into nanoscale. For the pursuit of cheaper and faster nanoscale
electronic devices, the size of transistors keeps scaling down. As the
miniaturization of the nanoelectronic devices, the electrical resistivity
increases dramatically, resulting rapid growth in the heat generation. The heat
generation and limited thermal dissipation in nanoscale materials have become a
critical problem in the development of the next generation nanoelectronic
devices. Copper (Cu) is widely used conducting material in nanoelectronic
devices, and the electron-phonon scattering is the dominant contributor to the
resistivity in Cu nanowires at room temperature. Meanwhile, phonons are the
main carriers of heat in insulators, intrinsic and lightly doped
semiconductors. The thermal transport is an ensemble of phonon transport, which
strongly depends on the phonon frequency. In addition, the phonon transport in
nanoscale materials can behave fundamentally different than in bulk materials,
because of the spatial confinement. However, the size effect on electron-phonon
scattering and frequency dependent phonon transport in nanoscale materials
remain largely unexplored, due to the lack of suitable experimental techniques.
This thesis is mainly focusing on the study of carrier dynamics and acoustic
phonon transport in nanoscale materials.
",0,1,0,0,0,0
5505,5506,Contracts as specifications for dynamical systems in driving variable form,"  This paper introduces assume/guarantee contracts on continuous-time control
systems, hereby extending contract theories for discrete systems to certain new
model classes and specifications. Contracts are regarded as formal
characterizations of control specifications, providing an alternative to
specifications in terms of dissipativity properties or set-invariance. The
framework has the potential to capture a richer class of specifications more
suitable for complex engineering systems. The proposed contracts are supported
by results that enable the verification of contract implementation and the
comparison of contracts. These results are illustrated by an example of a
vehicle following system.
",1,0,0,0,0,0
7744,7745,Efficient Antihydrogen Detection in Antimatter Physics by Deep Learning,"  Antihydrogen is at the forefront of antimatter research at the CERN
Antiproton Decelerator. Experiments aiming to test the fundamental CPT symmetry
and antigravity effects require the efficient detection of antihydrogen
annihilation events, which is performed using highly granular tracking
detectors installed around an antimatter trap. Improving the efficiency of the
antihydrogen annihilation detection plays a central role in the final
sensitivity of the experiments. We propose deep learning as a novel technique
to analyze antihydrogen annihilation data, and compare its performance with a
traditional track and vertex reconstruction method. We report that the deep
learning approach yields significant improvement, tripling event coverage while
simultaneously improving performance by over 5% in terms of Area Under Curve
(AUC).
",1,1,0,0,0,0
48,49,Attention-based Natural Language Person Retrieval,"  Following the recent progress in image classification and captioning using
deep learning, we develop a novel natural language person retrieval system
based on an attention mechanism. More specifically, given the description of a
person, the goal is to localize the person in an image. To this end, we first
construct a benchmark dataset for natural language person retrieval. To do so,
we generate bounding boxes for persons in a public image dataset from the
segmentation masks, which are then annotated with descriptions and attributes
using the Amazon Mechanical Turk. We then adopt a region proposal network in
Faster R-CNN as a candidate region generator. The cropped images based on the
region proposals as well as the whole images with attention weights are fed
into Convolutional Neural Networks for visual feature extraction, while the
natural language expression and attributes are input to Bidirectional Long
Short- Term Memory (BLSTM) models for text feature extraction. The visual and
text features are integrated to score region proposals, and the one with the
highest score is retrieved as the output of our system. The experimental
results show significant improvement over the state-of-the-art method for
generic object retrieval and this line of research promises to benefit search
in surveillance video footage.
",1,0,0,0,0,0
16547,16548,Some Remarks on the Hyperkähler Reduction,"  We consider a hyperkähler reduction and describe it via frame bundles.
Tracing the connection through the various reductions, we recover the results
of Gocho and Nakajima. In addition, we show that the fibers of such a reduction
are necessarily totally geodesic. As an independent result, we describe
O'Neill's submersion tensors on principal bundles.
",0,0,1,0,0,0
5372,5373,Phonemic and Graphemic Multilingual CTC Based Speech Recognition,"  Training automatic speech recognition (ASR) systems requires large amounts of
data in the target language in order to achieve good performance. Whereas large
training corpora are readily available for languages like English, there exists
a long tail of languages which do suffer from a lack of resources. One method
to handle data sparsity is to use data from additional source languages and
build a multilingual system. Recently, ASR systems based on recurrent neural
networks (RNNs) trained with connectionist temporal classification (CTC) have
gained substantial research interest. In this work, we extended our previous
approach towards training CTC-based systems multilingually. Our systems feature
a global phone set, based on the joint phone sets of each source language. We
evaluated the use of different language combinations as well as the addition of
Language Feature Vectors (LFVs). As contrastive experiment, we built systems
based on graphemes as well. Systems having a multilingual phone set are known
to suffer in performance compared to their monolingual counterparts. With our
proposed approach, we could reduce the gap between these mono- and multilingual
setups, using either graphemes or phonemes.
",1,0,0,0,0,0
9500,9501,Algebraic laminations for free products and arational trees,"  This work is the first step towards a description of the Gromov boundary of
the free factor graph of a free product, with applications to subgroup
classification for outer automorphisms. We extend the theory of algebraic
laminations dual to trees, as developed by Coulbois, Hilion, Lustig and
Reynolds, to the context of free products; this also gives us an opportunity to
give a unified account of this theory. We first show that any $\mathbb{R}$-tree
with dense orbits in the boundary of the corresponding outer space can be
reconstructed as a quotient of the boundary of the group by its dual
lamination. We then describe the dual lamination in terms of a band complex on
compact $\mathbb{R}$-trees (generalizing Coulbois-Hilion-Lustig's compact
heart), and we analyze this band complex using versions of the Rips machine and
of the Rauzy-Veech induction. An important output of the theory is that the
above map from the boundary of the group to the $\mathbb{R}$-tree is 2-to-1
almost everywhere.
A key point for our intended application is a unique duality result for
arational trees. It says that if two trees have a leaf in common in their dual
laminations, and if one of the trees is arational and relatively free, then
they are equivariantly homeomorphic.
This statement is an analogue of a result in the free group saying that if
two trees are dual to a common current and one of the trees is free arational,
then the two trees are equivariantly homeomorphic. However, we notice that in
the setting of free products, the continuity of the pairing between trees and
currents fails. For this reason, in all this paper, we work with laminations
rather than with currents.
",0,0,1,0,0,0
8441,8442,Local-ring network automata and the impact of hyperbolic geometry in complex network link-prediction,"  Topological link-prediction can exploit the entire network topology (global
methods) or only the neighbourhood (local methods) of the link to predict.
Global methods are believed the best. Is this common belief well-founded?
Stochastic-Block-Model (SBM) is a global method believed as one of the best
link-predictors, therefore it is considered a reference for comparison. But,
our results suggest that SBM, whose computational time is high, cannot in
general overcome the Cannistraci-Hebb (CH) network automaton model that is a
simple local-learning-rule of topological self-organization proved as the
current best local-based and parameter-free deterministic rule for
link-prediction. To elucidate the reasons of this unexpected result, we
formally introduce the notion of local-ring network automata models and their
relation with the nature of common-neighbours' definition in complex network
theory. After extensive tests, we recommend Structural-Perturbation-Method
(SPM) as the new best global method baseline. However, even SPM overall does
not outperform CH and in several evaluation frameworks we astonishingly found
the opposite. In particular, CH was the best predictor for synthetic networks
generated by the Popularity-Similarity-Optimization (PSO) model, and its
performance in PSO networks with community structure was even better than using
the original internode-hyperbolic-distance as link-predictor. Interestingly,
when tested on non-hyperbolic synthetic networks the performance of CH
significantly dropped down indicating that this rule of network
self-organization could be strongly associated to the rise of hyperbolic
geometry in complex networks. The superiority of global methods seems a
""misleading belief"" caused by a latent geometry bias of the few small networks
used as benchmark in previous studies. We propose to found a latent geometry
theory of link-prediction in complex networks.
",1,0,0,0,0,0
13749,13750,Effect of increasing disorder on domains of the two-dimensional Coulomb glass,"  We have studied a two dimensional lattice model of Coulomb glass for a wide
range of disorders at $T\sim 0$. The system was first annealed using Monte
Carlo simulation. Further minimization of the total energy of the system was
done using Baranovskii et al algorithm followed by cluster flipping to obtain
the pseudo ground states. We have shown that the energy required to create a
domain of linear size L in d dimensions is proportional to $L^{d-1}$. Using
Imry-Ma arguments given for random field Ising model, one gets critical
dimension $d_{c}\geq 2$ for Coulomb glass. The investigations of domains in the
transition region shows a discontinuity in staggered magnetization which is an
indication of a first-order type transition from charge-ordered phase to
disordered phase. The structure and nature of Random field fluctuations of the
second largest domain in Coulomb glass are inconsistent with the assumptions of
Imry and Ma as was also reported for random field Ising model. The study of
domains showed that in the transition region there were mostly two large
domains and as disorder was increased, the two large domains remained but there
were a large number of small domains. We have also studied the properties of
the second largest domain as a function of disorder. We furthermore analysed
the effect of disorder on the density of states and showed a transition from
hard gap at low disorders to a soft gap at higher disorders. At $W=2$, we have
analysed the soft gap in detail and found that the density of states deviates
slightly ($\delta\approx 1.293 \pm 0.027$) from the linear behaviour in two
dimensions. Analysis of local minima show that the pseudo ground states have
similar structure.
",0,1,0,0,0,0
20779,20780,Map Memorization and Forgetting in the IARA Autonomous Car,"  In this work, we present a novel strategy for correcting imperfections in
occupancy grid maps called map decay. The objective of map decay is to correct
invalid occupancy probabilities of map cells that are unobservable by sensors.
The strategy was inspired by an analogy between the memory architecture
believed to exist in the human brain and the maps maintained by an autonomous
vehicle. It consists in merging sensory information obtained during runtime
(online) with a priori data from a high-precision map constructed offline. In
map decay, cells observed by sensors are updated using traditional occupancy
grid mapping techniques and unobserved cells are adjusted so that their
occupancy probabilities tend to the values found in the offline map. This
strategy is grounded in the idea that the most precise information available
about an unobservable cell is the value found in the high-precision offline
map. Map decay was successfully tested and is still in use in the IARA
autonomous vehicle from Universidade Federal do Espírito Santo.
",1,0,0,0,0,0
2216,2217,Continuous Optimization of Adaptive Quadtree Structures,"  We present a novel continuous optimization method to the discrete problem of
quadtree optimization. The optimization aims at achieving a quadtree structure
with the highest mechanical stiffness, where the edges in the quadtree are
interpreted as structural elements carrying mechanical loads. We formulate
quadtree optimization as a continuous material distribution problem. The
discrete design variables (i.e., to refine or not to refine) are replaced by
continuous variables on multiple levels in the quadtree hierarchy. In discrete
quadtree optimization, a cell is only eligible for refinement if its parent
cell has been refined. We propose a continuous analogue to this dependency for
continuous multi-level design variables, and integrate it in the iterative
optimization process. Our results show that the continuously optimized quadtree
structures perform much stiffer than uniform patterns and the heuristically
optimized counterparts. We demonstrate the use of adaptive structures as
lightweight infill for 3D printed parts, where uniform geometric patterns have
been typically used in practice.
",1,0,0,0,0,0
1747,1748,Algebraic operads up to homotopy,"  This paper deals with the homotopy theory of differential graded operads. We
endow the Koszul dual category of curved conilpotent cooperads, where the
notion of quasi-isomorphism barely makes sense, with a model category structure
Quillen equivalent to that of operads. This allows us to describe the homotopy
properties of differential graded operads in a simpler and richer way, using
obstruction methods.
",0,0,1,0,0,0
15540,15541,Rule Formats for Nominal Process Calculi,"  The nominal transition systems (NTSs) of Parrow et al. describe the
operational semantics of nominal process calculi. We study NTSs in terms of the
nominal residual transition systems (NRTSs) that we introduce. We provide rule
formats for the specifications of NRTSs that ensure that the associated NRTS is
an NTS and apply them to the operational specifications of the early and late
pi-calculus. We also explore alternative specifications of the NTSs in which we
allow residuals of abstraction sort, and introduce translations between the
systems with and without residuals of abstraction sort. Our study stems from
the Nominal SOS of Cimini et al. and from earlier works in nominal sets and
nominal logic by Gabbay, Pitts and their collaborators.
",1,0,0,0,0,0
14287,14288,Characterisation of novel prototypes of monolithic HV-CMOS pixel detectors for high energy physics experiments,"  An upgrade of the ATLAS experiment for the High Luminosity phase of LHC is
planned for 2024 and foresees the replacement of the present Inner Detector
(ID) with a new Inner Tracker (ITk) completely made of silicon devices.
Depleted active pixel sensors built with the High Voltage CMOS (HV-CMOS)
technology are investigated as an option to cover large areas in the outermost
layers of the pixel detector and are especially interesting for the development
of monolithic devices which will reduce the production costs and the material
budget with respect to the present hybrid assemblies. For this purpose the
H35DEMO, a large area HV-CMOS demonstrator chip, was designed by KIT, IFAE and
University of Liverpool, and produced in AMS 350 nm CMOS technology. It
consists of four pixel matrices and additional test structures. Two of the
matrices include amplifiers and discriminator stages and are thus designed to
be operated as monolithic detectors. In these devices the signal is mainly
produced by charge drift in a small depleted volume obtained by applying a bias
voltage of the order of 100 V. Moreover, to enhance the radiation hardness of
the chip, this technology allows to enclose the electronics in the same deep
N-WELLs which are also used as collecting electrodes. In this contribution the
characterisation of H35DEMO chips and results of the very first beam test
measurements of the monolithic CMOS matrices with high energetic pions at CERN
SPS will be presented.
",0,1,0,0,0,0
5183,5184,Learning Role-based Graph Embeddings,"  Random walks are at the heart of many existing network embedding methods.
However, such algorithms have many limitations that arise from the use of
random walks, e.g., the features resulting from these methods are unable to
transfer to new nodes and graphs as they are tied to vertex identity. In this
work, we introduce the Role2Vec framework which uses the flexible notion of
attributed random walks, and serves as a basis for generalizing existing
methods such as DeepWalk, node2vec, and many others that leverage random walks.
Our proposed framework enables these methods to be more widely applicable for
both transductive and inductive learning as well as for use on graphs with
attributes (if available). This is achieved by learning functions that
generalize to new nodes and graphs. We show that our proposed framework is
effective with an average AUC improvement of 16.55% while requiring on average
853x less space than existing methods on a variety of graphs.
",1,0,0,1,0,0
10230,10231,A note on Oliver's p-group conjecture,"  Let $S$ be a $p$-group for an odd prime $p$, Oliver proposed the conjecture
that the Thompson subgroup $J(S)$ is always contained in the Oliver subgroup
$\mathfrak{X}(S)$. That means he conjectured that
$|J(S)\mathfrak{X}(S):\mathfrak{X}(S)|=1$. Let $\mathfrak{X}_1(S)$ be a
subgroup of $S$ such that $\mathfrak{X}_1(S)/\mathfrak{X}(S)$ is the center of
$S/\mathfrak{X}(S)$. In this short note, we prove that $J(S)\leq
\mathfrak{X}(S)$ if and only if $J(S)\leq \mathfrak{X}_1(S)$.
As an easy application, we prove that
$|J(S)\mathfrak{X}(S):\mathfrak{X}(S)|\neq p$.
",0,0,1,0,0,0
8268,8269,Unbiased Markov chain Monte Carlo for intractable target distributions,"  Performing numerical integration when the integrand itself cannot be
evaluated point-wise is a challenging task that arises in statistical analysis,
notably in Bayesian inference for models with intractable likelihood functions.
Markov chain Monte Carlo (MCMC) algorithms have been proposed for this setting,
such as the pseudo-marginal method for latent variable models and the exchange
algorithm for a class of undirected graphical models. As with any MCMC
algorithm, the resulting estimators are justified asymptotically in the limit
of the number of iterations, but exhibit a bias for any fixed number of
iterations due to the Markov chains starting outside of stationarity. This
""burn-in"" bias is known to complicate the use of parallel processors for MCMC
computations. We show how to use coupling techniques to generate unbiased
estimators in finite time, building on recent advances for generic MCMC
algorithms. We establish the theoretical validity of some of these procedures
by extending existing results to cover the case of polynomially ergodic Markov
chains. The efficiency of the proposed estimators is compared with that of
standard MCMC estimators, with theoretical arguments and numerical experiments
including state space models and Ising models.
",0,0,0,1,0,0
18343,18344,Does the Testing Level affect the Prevalence of Coincidental Correctness?,"  Researchers have previously shown that Coincidental Correctness (CC) is
prevalent; however, the benchmarks they used are considered inadequate
nowadays. They have also recognized the negative impact of CC on the
effectiveness of fault localization and testing. The aim of this paper is to
study Coincidental Correctness, using more realistic code, mainly from the
perspective of unit testing. This stems from the fact that the practice of unit
testing has grown tremendously in recent years due to the wide adoption of
software development processes, such as Test-Driven Development. We quantified
the presence of CC in unit testing using the Defects4J benchmark. This entailed
manually injecting two code checkers for each of the 395 defects in Defects4J:
1) a weak checker that detects weak CC tests by monitoring whether the defect
was reached; and 2) a strong checker that detects strong CC tests by monitoring
whether the defect was reached and the program has transitioned into an
infectious state. We also conducted preliminary experiments (using Defects4J,
NanoXML and JTidy) to assess the pervasiveness of CC at the unit testing level
in comparison to that at the integration and system levels. Our study showed
that unit testing is not immune to CC, as it exhibited 7.2x more strong CC
tests than failing tests and 8.3x more weak CC tests than failing tests.
However, our preliminary results suggested that it might be less prone to CC
than integration testing and system testing.
",1,0,0,0,0,0
20565,20566,Motion Planning Networks,"  Fast and efficient motion planning algorithms are crucial for many
state-of-the-art robotics applications such as self-driving cars. Existing
motion planning methods such as RRT*, A*, and D*, become ineffective as their
computational complexity increases exponentially with the dimensionality of the
motion planning problem. To address this issue, we present a neural
network-based novel planning algorithm which generates end-to-end
collision-free paths irrespective of the obstacles' geometry. The proposed
method, called MPNet (Motion Planning Network), comprises of a Contractive
Autoencoder which encodes the given workspaces directly from a point cloud
measurement, and a deep feedforward neural network which takes the workspace
encoding, start and goal configuration, and generates end-to-end feasible
motion trajectories for the robot to follow. We evaluate MPNet on multiple
planning problems such as planning of a point-mass robot, rigid-body, and 7 DOF
Baxter robot manipulators in various 2D and 3D environments. The results show
that MPNet is not only consistently computationally efficient in all 2D and 3D
environments but also show remarkable generalization to completely unseen
environments. The results also show that computation time of MPNet consistently
remains less than 1 second which is significantly lower than existing
state-of-the-art motion planning algorithms. Furthermore, through transfer
learning, the MPNet trained in one scenario (e.g., indoor living places) can
also quickly adapt to new scenarios (e.g., factory floors) with a little amount
of data.
",1,0,0,1,0,0
8863,8864,Wasserstein Variational Inference,"  This paper introduces Wasserstein variational inference, a new form of
approximate Bayesian inference based on optimal transport theory. Wasserstein
variational inference uses a new family of divergences that includes both
f-divergences and the Wasserstein distance as special cases. The gradients of
the Wasserstein variational loss are obtained by backpropagating through the
Sinkhorn iterations. This technique results in a very stable likelihood-free
training method that can be used with implicit distributions and probabilistic
programs. Using the Wasserstein variational inference framework, we introduce
several new forms of autoencoders and test their robustness and performance
against existing variational autoencoding techniques.
",0,0,0,1,0,0
14787,14788,Training Triplet Networks with GAN,"  Triplet networks are widely used models that are characterized by good
performance in classification and retrieval tasks. In this work we propose to
train a triplet network by putting it as the discriminator in Generative
Adversarial Nets (GANs). We make use of the good capability of representation
learning of the discriminator to increase the predictive quality of the model.
We evaluated our approach on Cifar10 and MNIST datasets and observed
significant improvement on the classification performance using the simple k-nn
method.
",1,0,0,1,0,0
17811,17812,OVI 6830Å Imaging Polarimetry of Symbiotic Stars,"  I present here the first results from an ongoing pilot project with the 1.6 m
telescope at the OPD, Brasil, aimed at the detection of the OVI $\lambda$6830
line via linear polarization in symbiotic stars. The main goal is to
demonstrate that OVI imaging polarimetry is an efficient technique for
discovering new symbiotic stars. The OVI $\lambda$6830 line is found in 5 out
of 9 known symbiotic stars, in which the OVI line has already been
spectroscopically confirmed, with at least 3-$\sigma$ detection. Three new
symbiotic star candidates have also been found.
",0,1,0,0,0,0
9442,9443,Multi-model ensembles for ecosystem prediction,"  When making predictions about ecosystems, we often have available a number of
different ecosystem models that attempt to represent their dynamics in a
detailed mechanistic way. Each of these can be used as simulators of
large-scale experiments and make forecasts about the fate of ecosystems under
different scenarios in order to support the development of appropriate
management strategies. However, structural differences, systematic
discrepancies and uncertainties lead to different models giving different
predictions under these scenarios. This is further complicated by the fact that
the models may not be run with the same species or functional groups, spatial
structure or time scale. Rather than simply trying to select a 'best' model, or
taking some weighted average, it is important to exploit the strengths of each
of the available models, while learning from the differences between them. To
achieve this, we construct a flexible statistical model of the relationships
between a collection or 'ensemble' of mechanistic models and their biases,
allowing for structural and parameter uncertainty and for different ways of
representing reality. Using this statistical meta-model, we can combine prior
beliefs, model estimates and direct observations using Bayesian methods, and
make coherent predictions of future outcomes under different scenarios with
robust measures of uncertainty. In this paper we present the modelling
framework and discuss results obtained using a diverse ensemble of models in
scenarios involving future changes in fishing levels. These examples illustrate
the value of our approach in predicting outcomes for possible strategies
pertaining to climate and fisheries policy aimed at improving food security and
maintaining ecosystem integrity.
",0,0,0,1,0,0
6517,6518,"Joint Routing, Scheduling and Power Control Providing Hard Deadline in Wireless Multihop Networks","  We consider optimal/efficient power allocation policies in a single/multihop
wireless network in the presence of hard end-to-end deadline delay constraints
on the transmitted packets. Such constraints can be useful for real time voice
and video. Power is consumed in only transmission of the data. We consider the
case when the power used in transmission is a convex function of the data
transmitted. We develop a computationally efficient online algorithm, which
minimizes the average power for the single hop. We model this problem as
dynamic program (DP) and obtain the optimal solution. Next, we generalize it to
the multiuser, multihop scenario when there are multiple real time streams with
different hard deadline constraints.
",1,0,0,0,0,0
15123,15124,Bounds on layer potentials with rough inputs for higher order elliptic equations,"  In this paper we establish square-function estimates on the double and single
layer potentials with rough inputs for divergence form elliptic operators, of
arbitrary even order 2m, with variable t-independent coefficients in the upper
half-space.
",0,0,1,0,0,0
14207,14208,Topological conjugacy of topological Markov shifts and Ruelle algebras,"  We will characterize topologically conjugate two-sided topological Markov
shifts $(\bar{X}_A,\bar{\sigma}_A)$ in terms of the associated asymptotic
Ruelle $C^*$-algebras ${\mathcal{R}}_A$ with its commutative $C^*$-subalgebras
$C(\bar{X}_A)$ and the canonical circle actions. We will also show that
extended Ruelle algebras ${\widetilde{\mathcal{R}}}_A$, which are purely
infinite version of the asymptotic Ruelle algebras, with its commutative
$C^*$-subalgebras $C(\bar{X}_A)$ and the canonical torus actions $\gamma^A$ are
complete invariants for topological conjugacy of two-sided topological Markov
shifts. We then have a computable topological conjugacy invariant, written in
terms of the underlying matrix, of a two-sided topological Markov shift by
using K-theory of the extended Ruelle algebra. The diagonal action of
$\gamma^A$ has a unique KMS-state on ${\widetilde{\mathcal{R}}}_A$, which is an
extension of the Parry measure on $\bar{X}_A$.
",0,0,1,0,0,0
2355,2356,Bypass Fraud Detection: Artificial Intelligence Approach,"  Telecom companies are severely damaged by bypass fraud or SIM boxing.
However, there is a shortage of published research to tackle this problem. The
traditional method of Test Call Generating is easily overcome by fraudsters and
the need for more sophisticated ways is inevitable. In this work, we are
developing intelligent algorithms that mine a huge amount of mobile operator's
data and detect the SIMs that are used to bypass international calls. This
method will make it hard for fraudsters to generate revenue and hinder their
work. Also by reducing fraudulent activities, quality of service can be
increased as well as customer satisfaction. Our technique has been evaluated
and tested on real world mobile operator data, and proved to be very efficient.
",1,0,0,0,0,0
1829,1830,Value Propagation for Decentralized Networked Deep Multi-agent Reinforcement Learning,"  We consider the networked multi-agent reinforcement learning (MARL) problem
in a fully decentralized setting, where agents learn to coordinate to achieve
the joint success. This problem is widely encountered in many areas including
traffic control, distributed control, and smart grids. We assume that the
reward function for each agent can be different and observed only locally by
the agent itself. Furthermore, each agent is located at a node of a
communication network and can exchanges information only with its neighbors.
Using softmax temporal consistency and a decentralized optimization method, we
obtain a principled and data-efficient iterative algorithm. In the first step
of each iteration, an agent computes its local policy and value gradients and
then updates only policy parameters. In the second step, the agent propagates
to its neighbors the messages based on its value function and then updates its
own value function. Hence we name the algorithm value propagation. We prove a
non-asymptotic convergence rate 1/T with the nonlinear function approximation.
To the best of our knowledge, it is the first MARL algorithm with convergence
guarantee in the control, off-policy and non-linear function approximation
setting. We empirically demonstrate the effectiveness of our approach in
experiments.
",1,0,0,1,0,0
12963,12964,Cancellable elements of the lattice of semigroup varieties,"  We completely determine all commutative semigroup varieties that are
cancellable elements of the lattice SEM of all semigroup varieties. In
particular, we prove that, for commutative varieties, the properties of being
cancellable and modular elements of SEM are equivalent.
",0,0,1,0,0,0
17489,17490,Mechanical Failure in Amorphous Solids: Scale Free Spinodal Criticality,"  The mechanical failure of amorphous media is a ubiquitous phenomenon from
material engineering to geology. It has been noticed for a long time that the
phenomenon is ""scale-free"", indicating some type of criticality. In spite of
attempts to invoke ""Self-Organized Criticality"", the physical origin of this
criticality, and also its universal nature, being quite insensitive to the
nature of microscopic interactions, remained elusive. Recently we proposed that
the precise nature of this critical behavior is manifested by a spinodal point
of a thermodynamic phase transition. Moreover, at the spinodal point there
exists a divergent correlation length which is associated with the
system-spanning instabilities (known also as shear bands) which are typical to
the mechanical yield. Demonstrating this requires the introduction of an ""order
parameter"" that is suitable for distinguishing between disordered amorphous
systems, and an associated correlation function, suitable for picking up the
growing correlation length. The theory, the order parameter, and the
correlation functions used are universal in nature and can be applied to any
amorphous solid that undergoes mechanical yield. Critical exponents for the
correlation length divergence and the system size dependence are estimated. The
phenomenon is seen at its sharpest in athermal systems, as is explained below;
in this paper we extend the discussion also to thermal systems, showing that at
sufficiently high temperatures the spinodal phenomenon is destroyed by thermal
fluctuations.
",0,1,0,0,0,0
4649,4650,Nauticle: a general-purpose particle-based simulation tool,"  Nauticle is a general-purpose simulation tool for the flexible and highly
configurable application of particle-based methods of either discrete or
continuum phenomena. It is presented that Nauticle has three distinct layers
for users and developers, then the top two layers are discussed in detail. The
paper introduces the Symbolic Form Language (SFL) of Nauticle, which
facilitates the formulation of user-defined numerical models at the top level
in text-based configuration files and provides simple application examples of
use. On the other hand, at the intermediate level, it is shown that the SFL can
be intuitively extended with new particle methods without tedious recoding or
even the knowledge of the bottom level. Finally, the efficiency of the code is
also tested through a performance benchmark.
",1,1,0,0,0,0
15681,15682,Towards Deep Learning Models Resistant to Adversarial Attacks,"  Recent work has demonstrated that neural networks are vulnerable to
adversarial examples, i.e., inputs that are almost indistinguishable from
natural data and yet classified incorrectly by the network. In fact, some of
the latest findings suggest that the existence of adversarial attacks may be an
inherent weakness of deep learning models. To address this problem, we study
the adversarial robustness of neural networks through the lens of robust
optimization. This approach provides us with a broad and unifying view on much
of the prior work on this topic. Its principled nature also enables us to
identify methods for both training and attacking neural networks that are
reliable and, in a certain sense, universal. In particular, they specify a
concrete security guarantee that would protect against any adversary. These
methods let us train networks with significantly improved resistance to a wide
range of adversarial attacks. They also suggest the notion of security against
a first-order adversary as a natural and broad security guarantee. We believe
that robustness against such well-defined classes of adversaries is an
important stepping stone towards fully resistant deep learning models.
",1,0,0,1,0,0
7956,7957,Energy Dissipation in Monolayer MoS$_2$ Electronics,"  The advancement of nanoscale electronics has been limited by energy
dissipation challenges for over a decade. Such limitations could be
particularly severe for two-dimensional (2D) semiconductors integrated with
flexible substrates or multi-layered processors, both being critical thermal
bottlenecks. To shed light into fundamental aspects of this problem, here we
report the first direct measurement of spatially resolved temperature in
functioning 2D monolayer MoS$_2$ transistors. Using Raman thermometry we
simultaneously obtain temperature maps of the device channel and its substrate.
This differential measurement reveals the thermal boundary conductance (TBC) of
the MoS$_2$ interface (14 $\pm$ 4 MWm$^-$$^2$K$^-$$^1$) is an order magnitude
larger than previously thought, yet near the low end of known solid-solid
interfaces. Our study also reveals unexpected insight into non-uniformities of
the MoS$_2$ transistors (small bilayer regions), which do not cause significant
self-heating, suggesting that such semiconductors are less sensitive to
inhomogeneity than expected. These results provide key insights into energy
dissipation of 2D semiconductors and pave the way for the future design of
energy-efficient 2D electronics.
",0,1,0,0,0,0
10195,10196,Geometry in the Courtroom,"  There has been a recent media blitz on a cohort of mathematicians valiantly
working to fix America's democratic system by combatting gerrymandering with
geometry. While statistics commonly features in the courtroom (forensics, DNA
analysis, etc.), the gerrymandering news raises a natural question: in what
other ways has pure math, specifically geometry and topology, been involved in
court cases and legal scholarship? In this survey article, we collect a few
examples with topics ranging from the Pythagorean formula to the Ham Sandwich
Theorem, and we discuss some jurists' perspectives on geometric reasoning in
the legal realm. One of our goals is to provide math educators with engaging
real-world instances of some abstract geometric concepts.
",0,0,1,0,0,0
8334,8335,Variations on known and recent cardinality bounds,"  Sapirovskii [18] proved that $|X|\leq\pi\chi(X)^{c(X)\psi(X)}$, for a regular
space $X$. We introduce the $\theta$-pseudocharacter of a Urysohn space $X$,
denoted by $\psi_\theta (X)$, and prove that the previous inequality holds for
Urysohn spaces replacing the bounds on celluarity $c(X)\leq\kappa$ and on
pseudocharacter $\psi(X)\leq\kappa$ with a bound on Urysohn cellularity
$Uc(X)\leq\kappa$ (which is a weaker conditon because $Uc(X)\leq c(X)$) and on
$\theta$-pseudocharacter $\psi_\theta (X)\leq\kappa$ respectivly (note that in
general $\psi(\cdot)\leq\psi_\theta (\cdot)$ and in the class of regular spaces
$\psi(\cdot)=\psi_\theta(\cdot)$). Further, in [6] the authors generalized the
Dissanayake and Willard's inequality: $|X|\leq 2^{aL_{c}(X)\chi(X)}$, for
Hausdorff spaces $X$ [25], in the class of $n$-Hausdorff spaces and de Groot's
result: $|X|\leq 2^{hL(X)}$, for Hausdorff spaces [11], in the class of $T_1$
spaces (see Theorems 2.22 and 2.23 in [6]). In this paper we restate Theorem
2.22 in [6] in the class of $n$-Urysohn spaces and give a variation of Theorem
2.23 in [6] using new cardinal functions, denoted by $UW(X)$, $\psi
w_\theta(X)$, $\theta\hbox{-}aL(X)$, $h\theta\hbox{-}aL(X)$,
$\theta\hbox{-}aL_c(X)$ and $\theta\hbox{-}aL_{\theta}(X)$. In [5] the authors
introduced the Hausdorff point separating weight of a space $X$ denoted by
$Hpsw(X)$ and proved a Hausdorff version of Charlesworth's inequality $|X|\leq
psw(X)^{L(X)\psi(X)}$ [7]. In this paper, we introduce the Urysohn point
separating weight of a space $X$, denoted by $Upsw(X)$, and prove that $|X|\leq
Upsw(X)^{\theta\hbox{-}aL_{c}(X)\psi(X)}$, for a Urysohn space $X$.
",0,0,1,0,0,0
18903,18904,A Dynamic Boosted Ensemble Learning Method Based on Random Forest,"  We propose a dynamic boosted ensemble learning method based on random forest
(DBRF), a novel ensemble algorithm that incorporates the notion of hard example
mining into Random Forest (RF) and thus combines the high accuracy of Boosting
algorithm with the strong generalization of Bagging algorithm. Specifically, we
propose to measure the quality of each leaf node of every decision tree in the
random forest to determine hard examples. By iteratively training and then
removing easy examples from training data, we evolve the random forest to focus
on hard examples dynamically so as to learn decision boundaries better. Data
can be cascaded through these random forests learned in each iteration in
sequence to generate predictions, thus making RF deep. We also propose to use
evolution mechanism and smart iteration mechanism to improve the performance of
the model. DBRF outperforms RF on three UCI datasets and achieved
state-of-the-art results compared to other deep models. Moreover, we show that
DBRF is also a new way of sampling and can be very useful when learning from
imbalanced data.
",0,0,0,1,0,0
19490,19491,An Improved Algorithm for E-Generalization,"  E-generalization computes common generalizations of given ground terms w.r.t.
a given equational background theory E. In 2005 [arXiv:1403.8118], we had
presented a computation approach based on standard regular tree grammar
algorithms, and a Prolog prototype implementation. In this report, we present
algorithmic improvements, prove them correct and complete, and give some
details of an efficiency-oriented implementation in C that allows us to handle
problems larger by several orders of magnitude.
",1,0,0,0,0,0
12377,12378,Scenic: Language-Based Scene Generation,"  Synthetic data has proved increasingly useful in both training and testing
machine learning models such as neural networks. The major problem in synthetic
data generation is producing meaningful data that is not simply random but
reflects properties of real-world data or covers particular cases of interest.
In this paper, we show how a probabilistic programming language can be used to
guide data synthesis by encoding domain knowledge about what data is useful.
Specifically, we focus on data sets arising from ""scenes"", configurations of
physical objects; for example, images of cars on a road. We design a
domain-specific language, Scenic, for describing ""scenarios"" that are
distributions over scenes. The syntax of Scenic makes it easy to specify
complex relationships between the positions and orientations of objects. As a
probabilistic programming language, Scenic allows assigning distributions to
features of the scene, as well as declaratively imposing hard and soft
constraints over the scene. A Scenic scenario thereby implicitly defines a
distribution over scenes, and we formulate the problem of sampling from this
distribution as ""scene improvisation"". We implement an improviser for Scenic
scenarios and apply it in a case study generating synthetic data sets for a
convolutional neural network designed to detect cars in road images. Our
experiments demonstrate the usefulness of our approach by using Scenic to
analyze and improve the performance of the network in various scenarios.
",1,0,0,0,0,0
12105,12106,Designing diagnostic platforms for analysis of disease patterns and probing disease emergence,"  The emerging era of personalized medicine relies on medical decisions,
practices, and products being tailored to the individual patient. Point-of-care
systems, at the heart of this model, play two important roles. First, they are
required for identifying subjects for optimal therapies based on their genetic
make-up and epigenetic profile. Second, they will be used for assessing the
progression of such therapies. Central to this vision is designing systems
that, with minimal user-intervention, can transduce complex signals from
biosystems in complement with clinical information to inform medical decision
within point-of-care settings. To reach our ultimate goal of developing
point-of-care systems and realizing personalized medicine, we are taking a
multistep systems-level approach towards understanding cellular processes and
biomolecular profiles, to quantify disease states and external interventions.
",0,0,0,0,1,0
1547,1548,The 2-adic complexity of a class of binary sequences with almost optimal autocorrelation,"  Pseudo-random sequences with good statistical property, such as low
autocorrelation, high linear complexity and large 2-adic complexity, have been
applied in stream cipher. In general, it is difficult to give both the linear
complexity and 2-adic complexity of a periodic binary sequence. Cai and Ding
\cite{Cai Ying} gave a class of sequences with almost optimal autocorrelation
by constructing almost difference sets. Wang \cite{Wang Qi} proved that one
type of those sequences by Cai and Ding has large linear complexity. Sun et al.
\cite{Sun Yuhua} showed that another type of sequences by Cai and Ding has also
large linear complexity. Additionally, Sun et al. also generalized the
construction by Cai and Ding using $d$-form function with difference-balanced
property. In this paper, we first give the detailed autocorrelation
distribution of the sequences was generalized from Cai and Ding \cite{Cai Ying}
by Sun et al. \cite{Sun Yuhua}. Then, inspired by the method of Hu \cite{Hu
Honggang}, we analyse their 2-adic complexity and give a lower bound on the
2-adic complexity of these sequences. Our result show that the 2-adic
complexity of these sequences is at least $N-\mathrm{log}_2\sqrt{N+1}$ and that
it reach $N-1$ in many cases, which are large enough to resist the rational
approximation algorithm (RAA) for feedback with carry shift registers (FCSRs).
",1,0,1,0,0,0
3414,3415,"On convergence rate of stochastic proximal point algorithm without strong convexity, smoothness or bounded gradients","  Significant parts of the recent learning literature on stochastic
optimization algorithms focused on the theoretical and practical behaviour of
stochastic first order schemes under different convexity properties. Due to its
simplicity, the traditional method of choice for most supervised machine
learning problems is the stochastic gradient descent (SGD) method. Many
iteration improvements and accelerations have been added to the pure SGD in
order to boost its convergence in various (strong) convexity setting. However,
the Lipschitz gradient continuity or bounded gradients assumptions are an
essential requirement for most existing stochastic first-order schemes. In this
paper novel convergence results are presented for the stochastic proximal point
algorithm in different settings. In particular, without any strong convexity,
smoothness or bounded gradients assumptions, we show that a slightly modified
quadratic growth assumption is sufficient to guarantee for the stochastic
proximal point $\mathcal{O}\left(\frac{1}{k}\right)$ convergence rate, in terms
of the distance to the optimal set. Furthermore, linear convergence is obtained
for interpolation setting, when the optimal set of expected cost is included in
the optimal sets of each functional component.
",1,0,0,1,0,0
16886,16887,Dust and Gas in Star Forming Galaxies at z~3 - Extending Galaxy Uniformity to 11.5 Billion Years,"  We present millimetre dust emission measurements of two Lyman Break Galaxies
at z~3 and construct for the first time fully sampled infrared spectral energy
distributions (SEDs), from mid-IR to the Rayleigh-Jeans tail, of individually
detected, unlensed, UV-selected, main sequence (MS) galaxies at $z=3$. The SED
modelling of the two sources confirms previous findings, based on stacked
ensembles, of an increasing mean radiation field <U> with redshift, consistent
with a rapidly decreasing gas metallicity in z > 2 galaxies. Complementing our
study with CO[3-2] emission line observations, we measure the molecular gas
mass (M_H2) reservoir of the systems using three independent approaches: 1) CO
line observations, 2) the dust to gas mass ratio vs metallicity relation and 3)
a single band, dust emission flux on the Rayleigh-Jeans side of the SED. All
techniques return consistent M_H2 estimates within a factor of ~2 or less,
yielding gas depletion time-scales (tau_dep ~ 0.35 Gyrs) and gas-to-stellar
mass ratios (M_H2/M* ~ 0.5-1) for our z~3 massive MS galaxies. The overall
properties of our galaxies are consistent with trends and relations established
at lower redshifts, extending the apparent uniformity of star-forming galaxies
over the last 11.5 billion years.
",0,1,0,0,0,0
16158,16159,An Applied Knowledge Framework to Study Complex Systems,"  The complexity of knowledge production on complex systems is well-known, but
there still lacks knowledge framework that would both account for a certain
structure of knowledge production at an epistemological level and be directly
applicable to the study and management of complex systems. We set a basis for
such a framework, by first analyzing in detail a case study of the construction
of a geographical theory of complex territorial systems, through mixed methods,
namely qualitative interview analysis and quantitative citation network
analysis. We can therethrough inductively build a framework that considers
knowledge entreprises as perspectives, with co-evolving components within
complementary knowledge domains. We finally discuss potential applications and
developments.
",1,1,0,0,0,0
5581,5582,Dirac Composite Fermion - A Particle-Hole Spinor,"  The particle-hole (PH) symmetry at half-filled Landau level requires the
relationship between the flux number N_phi and the particle number N on a
sphere to be exactly N_phi - 2(N-1) = 1. The wave functions of composite
fermions with 1/2 ""orbital spin"", which contributes to the shift ""1"" in the
N_phi and N relationship, are proposed, shown to be PH symmetric, and validated
with exact finite system results. It is shown the many-body composite electron
and composite hole wave functions at half-filling can be formed from the two
components of the same spinor wave function of a massless Dirac fermion at
zero-magnetic field. It is further shown that away from half-filling, the
many-body composite electron wave function at filling factor nu and its PH
conjugated composite hole wave function at 1-nu can be formed from the two
components of the very same spinor wave functions of a massless Dirac fermion
at non-zero magnetic field. This relationship leads to the proposal of a very
simple Dirac composite fermion effective field theory, where the two-component
Dirac fermion field is a particle-hole spinor field coupled to the same
emergent gauge field, with one field component describing the composite
electrons and the other describing the PH conjugated composite holes. As such,
the density of the Dirac spinor field is the density sum of the composite
electron and hole field components, and therefore is equal to the degeneracy of
the Lowest Landau level. On the other hand, the charge density coupled to the
external magnetic field is the density difference between the composite
electron and hole field components, and is therefore neutral at exactly
half-filling. It is shown that the proposed particle-hole spinor effective
field theory gives essentially the same electromagnetic responses as Son's
Dirac composite fermion theory does.
",0,1,0,0,0,0
4227,4228,Diversity-Sensitive Conditional Generative Adversarial Networks,"  We propose a simple yet highly effective method that addresses the
mode-collapse problem in the Conditional Generative Adversarial Network (cGAN).
Although conditional distributions are multi-modal (i.e., having many modes) in
practice, most cGAN approaches tend to learn an overly simplified distribution
where an input is always mapped to a single output regardless of variations in
latent code. To address such issue, we propose to explicitly regularize the
generator to produce diverse outputs depending on latent codes. The proposed
regularization is simple, general, and can be easily integrated into most
conditional GAN objectives. Additionally, explicit regularization on generator
allows our method to control a balance between visual quality and diversity. We
demonstrate the effectiveness of our method on three conditional generation
tasks: image-to-image translation, image inpainting, and future video
prediction. We show that simple addition of our regularization to existing
models leads to surprisingly diverse generations, substantially outperforming
the previous approaches for multi-modal conditional generation specifically
designed in each individual task.
",1,0,0,1,0,0
14110,14111,Simulation and stability analysis of oblique shock wave/boundary layer interactions at Mach 5.92,"  We investigate flow instability created by an oblique shock wave impinging on
a Mach 5.92 laminar boundary layer at a transitional Reynolds number. The
adverse pressure gradient of the oblique shock causes the boundary layer to
separate from the wall, resulting in the formation of a recirculation bubble.
For sufficiently large oblique shock angles, the recirculation bubble is
unstable to three-dimensional perturbations and the flow bifurcates from its
original laminar state. We utilize Direct Numerical Simulation (DNS) and Global
Stability Analysis (GSA) to show that this first occurs at a critical shock
angle of $\theta = 12.9^o$. At bifurcation, the least stable global mode is
non-oscillatory, and it takes place at a spanwise wavenumber $\beta=0.25$, in
good agreement with DNS results. Examination of the critical global mode
reveals that it originates from an interaction between small spanwise
corrugations at the base of the incident shock, streamwise vortices inside the
recirculation bubble, and spanwise modulation of the bubble strength. The
global mode drives the formation of long streamwise streaks downstream of the
bubble. While the streaks may be amplified by either the lift-up effect or by
Görtler instability, we show that centrifugal instability plays no role in
the upstream self-sustaining mechanism of the global mode. We employ an adjoint
solver to corroborate our physical interpretation by showing that the critical
global mode is most sensitive to base flow modifications that are entirely
contained inside the recirculation bubble.
",0,1,0,0,0,0
7006,7007,The Molecular Gas Environment in the 20 km s$^{-1}$ Cloud in the Central Molecular Zone,"  We recently reported a population of protostellar candidates in the 20 km
s$^{-1}$ cloud in the Central Molecular Zone of the Milky Way, traced by H$_2$O
masers in gravitationally bound dense cores. In this paper, we report
high-angular-resolution ($\sim$3'') molecular line studies of the environment
of star formation in this cloud. Maps of various molecular line transitions as
well as the continuum at 1.3 mm are obtained using the Submillimeter Array.
Five NH$_3$ inversion lines and the 1.3 cm continuum are observed with the Karl
G. Jansky Very Large Array. The interferometric observations are complemented
with single-dish data. We find that the CH$_3$OH, SO, and HNCO lines, which are
usually shock tracers, are better correlated spatially with the compact dust
emission from dense cores among the detected lines. These lines also show
enhancement in intensities with respect to SiO intensities toward the compact
dust emission, suggesting the presence of slow shocks or hot cores in these
regions. We find gas temperatures of $\gtrsim$100 K at 0.1-pc scales based on
RADEX modelling of the H$_2$CO and NH$_3$ lines. Although no strong
correlations between temperatures and linewidths/H$_2$O maser luminosities are
found, in high-angular-resolution maps we notice several candidate shock heated
regions offset from any dense cores, as well as signatures of localized heating
by protostars in several dense cores. Our findings suggest that at 0.1-pc
scales in this cloud star formation and strong turbulence may together affect
the chemistry and temperature of the molecular gas.
",0,1,0,0,0,0
16248,16249,An Overview of Recent Progress in Laser Wakefield Acceleration Experiments,"  The goal of this paper is to examine experimental progress in laser wakefield
acceleration over the past decade (2004-2014), and to use trends in the data to
understand some of the important physical processes. By examining a set of over
50 experiments, various trends concerning the relationship between plasma
density, accelerator length, laser power and the final electron beam en- ergy
are revealed. The data suggest that current experiments are limited by
dephasing and that current experiments typically require some pulse evolution
to reach the trapping threshold.
",0,1,0,0,0,0
17920,17921,Asymmetric Preheating,"  We study the generation of the matter-antimatter asymmetry during bosonic
preheating, focusing on the sources of the asymmetry. If the asymmetry appears
in the multiplication factor of the resonant particle production, the
matter-antimatter ratio will grow during preheating. On the other hand, if the
asymmetry does not grow during preheating, one has to find out another reason.
We consider several scenarios for the asymmetric preheating to distinguish the
sources of the asymmetry. We also discuss a new baryogenesis scenario, in which
the asymmetry is generated without introducing neither loop corrections nor
rotation of a field.
",0,1,0,0,0,0
9502,9503,Spatial Risk Measure for Max-Stable and Max-Mixture Processes,"  In this paper, we consider isotropic and stationary max-stable, inverse
max-stable and max-mixture processes $X=(X(s))\_{s\in\bR^2}$ and the damage
function $\cD\_X^{\nu}= |X|^\nu$ with $0<\nu<1/2$. We study the quantitative
behavior of a risk measure which is the variance of the average of
$\cD\_X^{\nu}$ over a region $\mathcal{A}\subset \bR^2$.} This kind of risk
measure has already been introduced and studied for \vero{some} max-stable
processes in \cite{koch2015spatial}. %\textcolor{red}{In this study, we
generalised this risk measure to be applicable for several models: asymptotic
dependence represented by max-stable, asymptotic independence represented by
inverse max-stable and mixing between of them.} We evaluated the proposed risk
measure by a simulation study.
",0,0,1,1,0,0
20874,20875,Learning Criticality in an Embodied Boltzmann Machine,"  Many biological and cognitive systems do not operate deep into one or other
regime of activity. Instead, they exploit critical surfaces poised at
transitions in their parameter space. The pervasiveness of criticality in
natural systems suggests that there may be general principles inducing this
behaviour. However, there is a lack of conceptual models explaining how
embodied agents propel themselves towards these critical points. In this paper,
we present a learning model driving an embodied Boltzmann Machine towards
critical behaviour by maximizing the heat capacity of the network. We test and
corroborate the model implementing an embodied agent in the mountain car
benchmark, controlled by a Boltzmann Machine that adjust its weights according
to the model. We find that the neural controller reaches a point of
criticality, which coincides with a transition point of the behaviour of the
agent between two regimes of behaviour, maximizing the synergistic information
between its sensors and the hidden and motor neurons. Finally, we discuss the
potential of our learning model to study the contribution of criticality to the
behaviour of embodied living systems in scenarios not necessarily constrained
by biological restrictions of the examples of criticality we find in nature.
",1,1,0,0,0,0
17220,17221,Gaia Data Release 1. Cross-match with external catalogues - Algorithm and results,"  Although the Gaia catalogue on its own will be a very powerful tool, it is
the combination of this highly accurate archive with other archives that will
truly open up amazing possibilities for astronomical research. The advanced
interoperation of archives is based on cross-matching, leaving the user with
the feeling of working with one single data archive. The data retrieval should
work not only across data archives, but also across wavelength domains. The
first step for seamless data access is the computation of the cross-match
between Gaia and external surveys. The matching of astronomical catalogues is a
complex and challenging problem both scientifically and technologically
(especially when matching large surveys like Gaia). We describe the cross-match
algorithm used to pre-compute the match of Gaia Data Release 1 (DR1) with a
selected list of large publicly available optical and IR surveys. The overall
principles of the adopted cross-match algorithm are outlined. Details are given
on the developed algorithm, including the methods used to account for position
errors, proper motions, and environment; to define the neighbours; and to
define the figure of merit used to select the most probable counterpart.
Statistics on the results are also given. The results of the cross-match are
part of the official Gaia DR1 catalogue.
",0,1,0,0,0,0
19628,19629,Differentiable Submodular Maximization,"  We consider learning of submodular functions from data. These functions are
important in machine learning and have a wide range of applications, e.g. data
summarization, feature selection and active learning. Despite their
combinatorial nature, submodular functions can be maximized approximately with
strong theoretical guarantees in polynomial time. Typically, learning the
submodular function and optimization of that function are treated separately,
i.e. the function is first learned using a proxy objective and subsequently
maximized. In contrast, we show how to perform learning and optimization
jointly. By interpreting the output of greedy maximization algorithms as
distributions over sequences of items and smoothening these distributions, we
obtain a differentiable objective. In this way, we can differentiate through
the maximization algorithms and optimize the model to work well with the
optimization algorithm. We theoretically characterize the error made by our
approach, yielding insights into the tradeoff of smoothness and accuracy. We
demonstrate the effectiveness of our approach for jointly learning and
optimizing on synthetic maximum cut data, and on real world applications such
as product recommendation and image collection summarization.
",0,0,0,1,0,0
5027,5028,On the Tropical Discs Counting on Elliptic K3 Surfaces with General Singular Fibres,"  Using Lagrangian Floer theory, we study the tropical geometry of K3 surfaces
with general singular fibres. In particular, we give the local models for the
type $I_n$, $II$, $III$ and $IV$ singular fibres in the Kodaira's
classification and generalize the correspondence theorem between open
Gromov-Witten invariants/tropical discs counting to these cases.
",0,0,1,0,0,0
1169,1170,"Comment on Photothermal radiometry parametric identifiability theory for reliable and unique nondestructive coating thickness and thermophysical measurements, J. Appl. Phys. 121(9), 095101 (2017)","  A recent paper [X. Guo, A. Mandelis, J. Tolev and K. Tang, J. Appl. Phys.,
121, 095101 (2017)] intends to demonstrate that from the photothermal
radiometry signal obtained on a coated opaque sample in 1D transfer, one should
be able to identify separately the following three parameters of the coating:
thermal diffusivity, thermal conductivity and thickness. In this comment, it is
shown that the three parameters are correlated in the considered experimental
arrangement, the identifiability criterion is in error and the thickness
inferred therefrom is not trustable.
",0,1,0,0,0,0
5292,5293,Formalizing Timing Diagram Requirements in Discrete Duration Calulus,"  Several temporal logics have been proposed to formalise timing diagram
requirements over hardware and embedded controllers. These include LTL,
discrete time MTL and the recent industry standard PSL. However, succintness
and visual structure of a timing diagram are not adequately captured by their
formulae. Interval temporal logic QDDC is a highly succint and visual notation
for specifying patterns of behaviours.
In this paper, we propose a practically useful notation called SeCeCntnl
which enhances negation free fragment of QDDC with features of nominals and
limited liveness. We show that timing diagrams can be naturally
(compositionally) and succintly formalized in SeCeCntnl as compared with PSL
and MTL. We give a linear time translation from timing diagrams to SeCeCntnl.
As our second main result, we propose a linear time translation of SeCeCntnl
into QDDC. This allows QDDC tools such as DCVALID and DCSynth to be used for
checking consistency of timing diagram requirements as well as for automatic
synthesis of property monitors and controllers. We give examples of a minepump
controller and a bus arbiter to illustrate our tools. Giving a theoretical
analysis, we show that for the proposed SeCeCntnl, the satisfiability and model
checking have elementary complexity as compared to the non-elementary
complexity for the full logic QDDC.
",1,0,0,0,0,0
14100,14101,Local asymptotic equivalence of pure quantum states ensembles and quantum Gaussian white noise,"  Quantum technology is increasingly relying on specialised statistical
inference methods for analysing quantum measurement data. This motivates the
development of ""quantum statistics"", a field that is shaping up at the overlap
of quantum physics and ""classical"" statistics. One of the less investigated
topics to date is that of statistical inference for infinite dimensional
quantum systems, which can be seen as quantum counterpart of non-parametric
statistics. In this paper we analyse the asymptotic theory of quantum
statistical models consisting of ensembles of quantum systems which are
identically prepared in a pure state. In the limit of large ensembles we
establish the local asymptotic equivalence (LAE) of this i.i.d. model to a
quantum Gaussian white noise model. We use the LAE result in order to establish
minimax rates for the estimation of pure states belonging to Hermite-Sobolev
classes of wave functions. Moreover, for quadratic functional estimation of the
same states we note an elbow effect in the rates, whereas for testing a pure
state a sharp parametric rate is attained over the nonparametric
Hermite-Sobolev class.
",0,0,1,1,0,0
20561,20562,Some new gradient estimates for two nonlinear parabolic equations under Ricci flow,"  In this paper, by maximum principle and cutoff function, we investigate
gradient estimates for positive solutions to two nonlinear parabolic equations
under Ricci flow. The related Harnack inequalities are deduced. An result about
positive solutions on closed manifolds under Ricci flow is abtained. As
applications, gradient estimates and Harnack inequalities for positive
solutions to the heat equation under Ricci flow are derived. These results in
the paper can be regard as generalizing the gradient estimates of Li-Yau, J. Y.
Li, Hamilton and Li-Xu to the Ricci flow. Our results also improve the
estimates of S. P. Liu and J. Sun to the nonlinear parabolic equation under
Ricci flow.
",0,0,1,0,0,0
426,427,Why Condorcet Consistency is Essential,"  In a single winner election with several candidates and ranked choice or
rating scale ballots, a Condorcet winner is one who wins all their two way
races by majority rule or MR. A voting system has Condorcet consistency or CC
if it names any Condorcet winner the winner. Many voting systems lack CC, but a
three step line of reasoning is used here to show why it is necessary. In step
1 we show that we can dismiss all the electoral criteria which conflict with
CC. In step 2 we point out that CC follows almost automatically if we can agree
that MR is the only acceptable system for elections with two candidates. In
step 3 we make that argument for MR. This argument itself has three parts.
First, in races with two candidates, the only well known alternatives to MR can
sometimes name as winner a candidate who is preferred over their opponent by
only one voter, with all others preferring the opponent. That is unacceptable.
Second, those same systems are also extremely susceptible to strategic
insincere voting. Third, in simulation studies using spatial models with two
candidates, the best known alternative to MR picks the best or most centrist
candidate significantly less often than MR does.
",0,0,0,1,0,0
10252,10253,Remarks on the operator-norm convergence of the Trotter product formula,"  We revise the operator-norm convergence of the Trotter product formula for a
pair {A,B} of generators of semigroups on a Banach space. Operator-norm
convergence holds true if the dominating operator A generates a holomorphic
contraction semigroup and B is a A-infinitesimally small generator of a
contraction semigroup, in particular, if B is a bounded operator. Inspired by
studies of evolution semigroups it is shown in the present paper that the
operator-norm convergence generally fails even for bounded operators B if A is
not a holomorphic generator. Moreover, it is shown that operator norm
convergence of the Trotter product formula can be arbitrary slow.
",0,0,1,0,0,0
10042,10043,PCN: Point Completion Network,"  Shape completion, the problem of estimating the complete geometry of objects
from partial observations, lies at the core of many vision and robotics
applications. In this work, we propose Point Completion Network (PCN), a novel
learning-based approach for shape completion. Unlike existing shape completion
methods, PCN directly operates on raw point clouds without any structural
assumption (e.g. symmetry) or annotation (e.g. semantic class) about the
underlying shape. It features a decoder design that enables the generation of
fine-grained completions while maintaining a small number of parameters. Our
experiments show that PCN produces dense, complete point clouds with realistic
structures in the missing regions on inputs with various levels of
incompleteness and noise, including cars from LiDAR scans in the KITTI dataset.
",1,0,0,0,0,0
18544,18545,Composite Fermions on a Torus,"  We achieve an explicit construction of the lowest Landau level (LLL)
projected wave functions for composite fermions in the periodic (torus)
geometry. To this end, we first demonstrate how the vortex attachment of the
composite fermion (CF) theory can be accomplished in the torus geometry to
produce the ""unprojected"" wave functions satisfying the correct
(quasi-)periodic boundary conditions. We then consider two methods for
projecting these wave functions into the LLL. The direct projection produces
valid wave functions but can be implemented only for very small systems. The
more powerful and more useful projection method of Jain and Kamilla fails in
the torus geometry because it does not preserve the periodic boundary
conditions and thus takes us out of the original Hilbert space. We have
succeeded in constructing a modified projection method that is consistent with
both the periodic boundary conditions and the general structure of the CF
theory. This method is valid for a large class of states of composite fermions,
called ""proper states,"" which includes the incompressible ground states at
electron filling factors $\nu=\frac{n}{2pn+ 1}$, their charged and neutral
excitations, and also the quasidegenerate ground states at arbitrary filling
factors of the form $\nu=\frac{\nu^*}{2p\nu^*+ 1}$, where $n$ and $p$ are
integers and $\nu^*$ is the CF filling factor. Comparison with exact results
known for small systems for the ground and excited states at filling factors
$\nu=1/3$, 2/5 and 3/7 demonstrates our LLL-projected wave functions to be
extremely accurate representations of the actual Coulomb eigenstates. Our
construction enables the study of large systems of composite fermions on the
torus, thereby opening the possibility of investigating numerous interesting
questions and phenomena.
",0,1,0,0,0,0
8773,8774,Disruption of Alfvénic turbulence by magnetic reconnection in a collisionless plasma,"  We calculate the disruption scale $\lambda_{\rm D}$ at which sheet-like
structures in dynamically aligned Alfvénic turbulence are destroyed by the
onset of magnetic reconnection in a low-$\beta$ collisionless plasma. The
scaling of $\lambda_{\rm D}$ depends on the order of the statistics being
considered, with more intense structures being disrupted at larger scales. The
disruption scale for the structures that dominate the energy spectrum is
$\lambda_{\rm D}\sim L_\perp^{1/9}(d_e\rho_s)^{4/9}$, where $d_e$ is the
electron inertial scale, $\rho_s$ is the ion sound scale, and $L_\perp$ is the
outer scale of the turbulence. When $\beta_e$ and $\rho_s/L_\perp$ are
sufficiently small, the scale $\lambda_{\rm D}$ is larger than $\rho_s$ and
there is a break in the energy spectrum at $\lambda_{\rm D}$, rather than at
$\rho_s$. We propose that the fluctuations produced by the disruption are
circularised flux ropes, which may have already been observed in the solar
wind. We predict the relationship between the amplitude and radius of these
structures and quantify the importance of the disruption process to the cascade
in terms of the filling fraction of undisrupted structures and the fractional
reduction of the energy contained in them at the ion sound scale $\rho_s$. Both
of these fractions depend strongly on $\beta_e$, with the disrupted structures
becoming more important at lower $\beta_e$. Finally, we predict that the energy
spectrum between $\lambda_{\rm D}$ and $\rho_s$ is steeper than $k_\perp^{-3}$,
when this range exists. Such a steep ""transition range"" is sometimes observed
in short intervals of solar-wind turbulence. The onset of collisionless
magnetic reconnection may therefore significantly affect the nature of plasma
turbulence around the ion gyroscale.
",0,1,0,0,0,0
16922,16923,Understanding News Outlets' Audience-Targeting Patterns,"  The power of the press to shape the informational landscape of a population
is unparalleled, even now in the era of democratic access to all information
outlets. However, it is known that news outlets (particularly more traditional
ones) tend to discriminate who they want to reach, and who to leave aside. In
this work, we attempt to shed some light on the audience targeting patterns of
newspapers, using the Chilean media ecosystem. First, we use the gravity model
to analyze geography as a factor in explaining audience reachability. This
shows that some newspapers are indeed driven by geographical factors (mostly
local news outlets) but some others are not (national-distribution outlets).
For those which are not, we use a regression model to study the influence of
socioeconomic and political characteristics in news outlets adoption. We
conclude that indeed larger, national-distribution news outlets target
populations based on these factors, rather than on geography or immediacy.
",1,0,0,0,0,0
515,516,Optimal Envelope Approximation in Fourier Basis with Applications in TV White Space,"  Lowpass envelope approximation of smooth continuous-variable signals are
introduced in this work. Envelope approximations are necessary when a given
signal has to be approximated always to a larger value (such as in TV white
space protection regions). In this work, a near-optimal approximate algorithm
for finding a signal's envelope, while minimizing a mean-squared cost function,
is detailed. The sparse (lowpass) signal approximation is obtained in the
linear Fourier series basis. This approximate algorithm works by discretizing
the envelope property from an infinite number of points to a large (but finite)
number of points. It is shown that this approximate algorithm is near-optimal
and can be solved by using efficient convex optimization programs available in
the literature. Simulation results are provided towards the end to gain more
insights into the analytical results presented.
",1,0,0,0,0,0
20148,20149,Restriction of Odd Degree Characters of $\mathfrak{S}_n$,"  Let $n$ and $k$ be natural numbers such that $2^k < n$. We study the
restriction to $\mathfrak{S}_{n-2^k}$ of odd-degree irreducible characters of
the symmetric group $\mathfrak{S}_n$. This analysis completes the study begun
in [Ayyer A., Prasad A., Spallone S., Sem. Lothar. Combin. 75 (2015), Art.
B75g, 13 pages] and recently developed in [Isaacs I.M., Navarro G., Olsson
J.B., Tiep P.H., J. Algebra 478 (2017), 271-282].
",0,0,1,0,0,0
20028,20029,A General Theory for Training Learning Machine,"  Though the deep learning is pushing the machine learning to a new stage,
basic theories of machine learning are still limited. The principle of
learning, the role of the a prior knowledge, the role of neuron bias, and the
basis for choosing neural transfer function and cost function, etc., are still
far from clear. In this paper, we present a general theoretical framework for
machine learning. We classify the prior knowledge into common and
problem-dependent parts, and consider that the aim of learning is to maximally
incorporate them. The principle we suggested for maximizing the former is the
design risk minimization principle, while the neural transfer function, the
cost function, as well as pretreatment of samples, are endowed with the role
for maximizing the latter. The role of the neuron bias is explained from a
different angle. We develop a Monte Carlo algorithm to establish the
input-output responses, and we control the input-output sensitivity of a
learning machine by controlling that of individual neurons. Applications of
function approaching and smoothing, pattern recognition and classification, are
provided to illustrate how to train general learning machines based on our
theory and algorithm. Our method may in addition induce new applications, such
as the transductive inference.
",1,0,0,1,0,0
14524,14525,Generalized orderless pooling performs implicit salient matching,"  Most recent CNN architectures use average pooling as a final feature encoding
step. In the field of fine-grained recognition, however, recent global
representations like bilinear pooling offer improved performance. In this
paper, we generalize average and bilinear pooling to ""alpha-pooling"", allowing
for learning the pooling strategy during training. In addition, we present a
novel way to visualize decisions made by these approaches. We identify parts of
training images having the highest influence on the prediction of a given test
image. It allows for justifying decisions to users and also for analyzing the
influence of semantic parts. For example, we can show that the higher capacity
VGG16 model focuses much more on the bird's head than, e.g., the lower-capacity
VGG-M model when recognizing fine-grained bird categories. Both contributions
allow us to analyze the difference when moving between average and bilinear
pooling. In addition, experiments show that our generalized approach can
outperform both across a variety of standard datasets.
",1,0,0,0,0,0
20964,20965,A social Network Analysis of the Operations Research/Industrial Engineering Faculty Hiring Network,"  We study the U.S. Operations Research/Industrial-Systems Engineering (ORIE)
faculty hiring network, consisting of 1,179 faculty origin and destination data
together with attribute data from 83 ORIE departments. A social network
analysis of faculty hires can reveal important patterns in an academic field,
such as the existence of a hierarchy or sociological aspects such as the
presence of communities of departments. We first statistically test for the
existence of a linear hierarchy in the network and for its steepness. We find a
near linear hierarchical order of the departments, proposing a new index for
hiring networks, which we contrast with other indicators of hierarchy,
including published rankings. A single index is not capable to capture the full
structure of a complex network, however, so we next fit a latent exponential
random graph model (ERGM) to the network, which is able to reproduce its main
observed characteristics: high incidence of self-hiring, skewed out-degree
distribution, low density and clustering. Finally, we use the latent variables
in the ERGM to simplify the network to one where faculty hires take place among
three groups of departments. We contrast our findings with those reported for
other related disciplines, Computer Science and Business.
",1,0,0,1,0,0
19822,19823,Deep reinforcement learning from human preferences,"  For sophisticated reinforcement learning (RL) systems to interact usefully
with real-world environments, we need to communicate complex goals to these
systems. In this work, we explore goals defined in terms of (non-expert) human
preferences between pairs of trajectory segments. We show that this approach
can effectively solve complex RL tasks without access to the reward function,
including Atari games and simulated robot locomotion, while providing feedback
on less than one percent of our agent's interactions with the environment. This
reduces the cost of human oversight far enough that it can be practically
applied to state-of-the-art RL systems. To demonstrate the flexibility of our
approach, we show that we can successfully train complex novel behaviors with
about an hour of human time. These behaviors and environments are considerably
more complex than any that have been previously learned from human feedback.
",1,0,0,1,0,0
4356,4357,Deep Boosted Regression for MR to CT Synthesis,"  Attenuation correction is an essential requirement of positron emission
tomography (PET) image reconstruction to allow for accurate quantification.
However, attenuation correction is particularly challenging for PET-MRI as
neither PET nor magnetic resonance imaging (MRI) can directly image tissue
attenuation properties. MRI-based computed tomography (CT) synthesis has been
proposed as an alternative to physics based and segmentation-based approaches
that assign a population-based tissue density value in order to generate an
attenuation map. We propose a novel deep fully convolutional neural network
that generates synthetic CTs in a recursive manner by gradually reducing the
residuals of the previous network, increasing the overall accuracy and
generalisability, while keeping the number of trainable parameters within
reasonable limits. The model is trained on a database of 20 pre-acquired MRI/CT
pairs and a four-fold random bootstrapped validation with a 80:20 split is
performed. Quantitative results show that the proposed framework outperforms a
state-of-the-art atlas-based approach decreasing the Mean Absolute Error (MAE)
from 131HU to 68HU for the synthetic CTs and reducing the PET reconstruction
error from 14.3% to 7.2%.
",0,0,0,1,0,0
16566,16567,Overview of Project 8 and Progress Towards Tritium Operation,"  Project 8 is a tritium endpoint neutrino mass experiment utilizing a phased
program to achieve sensitivity to the range of neutrino masses allowed by the
inverted mass hierarchy. The Cyclotron Radiation Emission Spectroscopy (CRES)
technique is employed to measure the differential energy spectrum of decay
electrons with high precision. We present an overview of the Project 8
experimental program, from first demonstration of the CRES technique to
ultimate sensitivity with an atomic tritium source. We highlight recent
advances in preparation for the first measurement of the continuous tritium
spectrum with CRES.
",0,1,0,0,0,0
9434,9435,On Constraint Qualifications of a Nonconvex Inequality,"  In this paper, we study constraint qualifications for the nonconvex
inequality defined by a proper lower semicontinuous function. These constraint
qualifications involve the generalized construction of normal cones and
subdifferentials. Several conditions for these constraint qualifications are
also provided therein. When restricted to the convex inequality, these
constraint qualifications reduce to basic constraint qualification (BCQ) and
strong BCQ studied in [SIAM J. Optim., 14(2004), 757-772] and [Math. Oper.
Res., 30 (2005), 956-965].
",0,0,1,0,0,0
18585,18586,"Refraction in exoplanet atmospheres: Photometric signatures, implications for transmission spectroscopy, and search in Kepler data","  Refraction deflects photons that pass through atmospheres, which affects
transit light curves. Refraction thus provides an avenue to probe physical
properties of exoplanet atmospheres and to constrain the presence of clouds and
hazes. In addition, an effective surface can be imposed by refraction, thereby
limiting the pressure levels probed by transmission spectroscopy. The main
objective of the paper is to model the effects of refraction on photometric
light curves for realistic planets and to explore the dependencies on
atmospheric physical parameters. We also explore under which circumstances
transmission spectra are significantly affected by refraction. Finally, we
search for refraction signatures in photometric residuals in Kepler data. We
use the model of Hui & Seager (2002) to compute deflection angles and
refraction transit light curves, allowing us to explore the parameter space of
atmospheric properties. The observational search is performed by stacking large
samples of transit light curves from Kepler. We find that out-of-transit
refraction shoulders are the most easily observable features, which can reach
peak amplitudes of ~10 parts per million (ppm) for planets around Sun-like
stars. More typical amplitudes are a few ppm or less for Jovians and at the
sub-ppm level for super-Earths. Interestingly, the signal-to-noise ratio of any
refraction residuals for planets orbiting Sun-like hosts are expected to be
similar for planets orbiting red dwarfs. We also find that the maximum depth
probed by transmission spectroscopy is not limited by refraction for weakly
lensing planets, but that the incidence of refraction can vary significantly
for strongly lensing planets. We find no signs of refraction features in the
stacked Kepler light curves, which is in agreement with our model predictions.
",0,1,0,0,0,0
865,866,Speaker Diarization using Deep Recurrent Convolutional Neural Networks for Speaker Embeddings,"  In this paper we propose a new method of speaker diarization that employs a
deep learning architecture to learn speaker embeddings. In contrast to the
traditional approaches that build their speaker embeddings using manually
hand-crafted spectral features, we propose to train for this purpose a
recurrent convolutional neural network applied directly on magnitude
spectrograms. To compare our approach with the state of the art, we collect and
release for the public an additional dataset of over 6 hours of fully annotated
broadcast material. The results of our evaluation on the new dataset and three
other benchmark datasets show that our proposed method significantly
outperforms the competitors and reduces diarization error rate by a large
margin of over 30% with respect to the baseline.
",1,0,0,0,0,0
4464,4465,On the number of circular orders on a group,"  We give a classification and complete algebraic description of groups
allowing only finitely many (left multiplication invariant) circular orders. In
particular, they are all solvable groups with a specific semi-direct product
decomposition. This allows us to also show that the space of circular orders of
any group is either finite or uncountable. As a special case and first step, we
show that the space of circular orderings of an infinite Abelian group has no
isolated points, hence is homeomorphic to a cantor set.
",0,0,1,0,0,0
8846,8847,Two-channel conduction in YbPtBi,"  We investigated transport, magnetotransport, and broadband optical properties
of the half-Heusler compound YbPtBi. Hall measurements evidence two types of
charge carriers: highly mobile electrons with a temperature-dependent
concentration and low-mobile holes; their concentration stays almost constant
within the investigated temperature range from 2.5 to 300 K. The optical
spectra (10 meV - 2.7 eV) can be naturally decomposed into contributions from
intra- and interband absorption processes, the former manifesting themselves as
two Drude bands with very different scattering rates, corresponding to the
charges with different mobilities. These results of the optical measurements
allow us to separate the contributions from electrons and holes to the total
conductivity and to implement a two-channel-conduction model for description of
the magnetotransport data. In this approach, the electron and hole mobilities
are found to be around 50000 and 10 cm$^{2}$/Vs at the lowest temperatures (2.5
K), respectively.
",0,1,0,0,0,0
11230,11231,Ergodic Theorems for Nonconventional Arrays and an Extension of the Szemeredi Theorem,"  The paper is primarily concerned with the asymptotic behavior as $N\to\infty$
of averages of nonconventional arrays having the form
$N^{-1}\sum_{n=1}^N\prod_{j=1}^\ell T^{P_j(n,N)}f_j$ where $f_j$'s are bounded
measurable functions, $T$ is an invertible measure preserving transformation
and $P_j$'s are polynomials of $n$ and $N$ taking on integer values on
integers. It turns out that when $T$ is weakly mixing and $P_j(n,N)=p_jn+q_jN$
are linear or, more generally, have the form $P_j(n,N)=P_j(n)+Q_j(N)$ for some
integer valued polynomials $P_j$ and $Q_j$ then the above averages converge in
$L^2$ but for general polynomials $P_j$ the $L^2$ convergence can be ensured
even in the case $\ell=1$ only when $T$ is strongly mixing. Studying also
weakly mixing and compact extensions and relying on Furstenberg's structure
theorem we derive an extension of Szemer\' edi's theorem saying that for any
subset of integers $\Lambda$ with positive upper density there exists a subset
$\mathcal N_\Lambda$ of positive integers having uniformly bounded gaps such
that for $N\in\mathcal N_\Lambda$ and at least $\varepsilon N,\,\varepsilon>0$
of $n$'s all numbers $p_jn+q_jN,\, j=1,...,\ell$ belong to $\Lambda$. We obtain
also a version of these results for several commuting transformations which
yields a corresponding extension of the multidimensional Szemer\' edi theorem.
",0,0,1,0,0,0
15013,15014,The AKARI IRC asteroid flux catalogue: updated diameters and albedos,"  The AKARI IRC All-sky survey provided more than twenty thousand thermal
infrared observations of over five thousand asteroids. Diameters and albedos
were obtained by fitting an empirically calibrated version of the standard
thermal model to these data. After the publication of the flux catalogue in
October 2016, our aim here is to present the AKARI IRC all-sky survey data and
discuss valuable scientific applications in the field of small-body physical
properties studies. As an example, we update the catalogue of asteroid
diameters and albedos based on AKARI using the near-Earth asteroid thermal
model (NEATM). We fit the NEATM to derive asteroid diameters and, whenever
possible, infrared beaming parameters. We obtained a total of 8097 diameters
and albedos for 5170 asteroids, and we fitted the beaming parameter for almost
two thousand of them. When it was not possible to fit the beaming parameter, we
used a straight line fit to our sample's beaming parameter-versus-phase angle
plot to set the default value for each fit individually instead of using a
single average value. Our diameters agree with stellar-occultation-based
diameters well within the accuracy expected for the model. They also match the
previous AKARI-based catalogue at phase angles lower than 50 degrees, but we
find a systematic deviation at higher phase angles, at which near-Earth and
Mars-crossing asteroids were observed. The AKARI IRC All-sky survey provides
observations at different observation geometries, rotational coverages and
aspect angles. For example, by comparing in more detail a few asteroids for
which dimensions were derived from occultations, we discuss how the multiple
observations per object may already provide three-dimensional information about
elongated objects even based on an idealised model like the NEATM.
",0,1,0,0,0,0
14019,14020,"(p,q)-webs of DIM representations, 5d N=1 instanton partition functions and qq-characters","  Instanton partition functions of $\mathcal{N}=1$ 5d Super Yang-Mills reduced
on $S^1$ can be engineered in type IIB string theory from the $(p,q)$-branes
web diagram. To this diagram is superimposed a web of representations of the
Ding-Iohara-Miki (DIM) algebra that acts on the partition function. In this
correspondence, each segment is associated to a representation, and the
(topological string) vertex is identified with the intertwiner operator
constructed by Awata, Feigin and Shiraishi. We define a new intertwiner acting
on the representation spaces of levels $(1,n)\otimes(0,m)\to(1,n+m)$, thereby
generalizing to higher rank $m$ the original construction. It allows us to use
a folded version of the usual $(p,q)$-web diagram, bringing great
simplifications to actual computations. As a result, the characterization of
Gaiotto states and vertical intertwiners, previously obtained by some of the
authors, is uplifted to operator relations acting in the Fock space of
horizontal representations. We further develop a method to build qq-characters
of linear quivers based on the horizontal action of DIM elements. While
fundamental qq-characters can be built using the coproduct, higher ones require
the introduction of a (quantum) Weyl reflection acting on tensor products of
DIM generators.
",0,0,1,0,0,0
11295,11296,Efficient Charge Collection in Coplanar Grid Radiation Detectors,"  We have modeled laser-induced transient current waveforms in radiation
coplanar grid detectors. Poisson's equation has been solved by finite element
method and currents induced by photo-generated charge were obtained using
Shockley-Ramo theorem. The spectral response on a radiation flux has been
modeled by Monte-Carlo simulations. We show 10$\times$ improved spectral
resolution of coplanar grid detector using differential signal sensing. We
model the current waveform dependence on doping, depletion width, diffusion and
detector shielding and their mutual dependence is discussed in terms of
detector optimization. The numerical simulations are successfully compared to
experimental data and further model simplifications are proposed. The space
charge below electrodes and a non-homogeneous electric field on a coplanar grid
anode are found to be the dominant contributions to laser-induced transient
current waveforms.
",0,1,0,0,0,0
17072,17073,Questions on mod p representations of reductive p-adic groups,"  This is a list of questions raised by our joint work arXiv:1412.0737 and its
sequels.
",0,0,1,0,0,0
19705,19706,rTraceroute: Réunion Traceroute Visualisation,"  Traceroute is the main tools to explore Internet path. It provides limited
information about each node along the path. However, Traceroute cannot go
further in statistics analysis, or \emph{Man-Machine Interface (MMI)}.
Indeed, there are no graphical tool that is able to draw all paths used by IP
routes. We present a new tool that can handle more than 1,000 Traceroute
results, map them, identify graphically MPLS links, get information of usage of
all routes (in percent) to improve the knowledge between countries' links.
rTraceroute want to go deeper in usage of atomic traces. In this paper, we will
discuss the concept of rTraceroute and present some example of usage.
",1,0,0,0,0,0
9837,9838,Critical behavior of a stochastic anisotropic Bak-Sneppen model,"  In this paper we present our study on the critical behavior of a stochastic
anisotropic Bak-Sneppen (saBS) model, in which a parameter $\alpha$ is
introduced to describe the interaction strength among nearest species. We
estimate the threshold fitness $f_c$ and the critical exponent $\tau_r$ by
numerically integrating a master equation for the distribution of avalanche
spatial sizes. Other critical exponents are then evaluated from previously
known scaling relations. The numerical results are in good agreement with the
counterparts yielded by the Monte Carlo simulations. Our results indicate that
all saBS models with nonzero interaction strength exhibit self-organized
criticality, and fall into the same universality class, by sharing the
universal critical exponents.
",0,1,0,0,0,0
15207,15208,On the Azuma inequality in spaces of subgaussian of rank $p$ random variables,"  For $p > 1$ let a function $\varphi_p(x) = x^2/2$ if $|x|\le 1$ and
$\varphi_p(x) = 1/p|x|^p -1/p + 1/2$ if $|x| > 1$. For a random variable $\xi$
let $\tau_{\varphi_p}(\xi)$ denote $\inf\{c\ge 0 :\;
\forall_{\lambda\in\mathbb{R}}\;
\ln\mathbb{E}\exp(\lambda\xi)\le\varphi_p(c\lambda)\}$; $\tau_{\varphi_p}$ is a
norm in a space $Sub_{\varphi_p}(\Omega) =\{\xi:
\; \tau_{\varphi_p}(\xi) <\infty\}$ of $\varphi_p$-subgaussian random
variables which we call {\it subgaussian of rank $p$ random variables}. For $p
= 2$ we have the classic subgaussian random variables. The Azuma inequality
gives an estimate on the probability of the deviations of a zero-mean
martingale $(\xi_n)_{n\ge 0}$ with bounded increments from zero. In its classic
form is assumed that $\xi_0 = 0$. In this paper it is shown a version of the
Azuma inequality under assumption that $\xi_0$ is any subgaussian of rank $p$
random variable.
",0,0,1,0,0,0
14867,14868,Two-dimensional Fourier transformations and Mordell integrals,"  Several Fourier transformations of functions of one and two variables are
evaluated and then used to derive some integral and series identities. It is
shown that certain two- dimensional Mordell integrals factorize into product of
two integrals and that the square of the absolute value of the Mordell integral
can be reduced to a single one-dimensional integral. Some connections to
elliptic functions and lattice sums are discussed.
",0,0,1,0,0,0
12603,12604,Design and Processing of Invertible Orientation Scores of 3D Images for Enhancement of Complex Vasculature,"  The enhancement and detection of elongated structures in noisy image data is
relevant for many biomedical imaging applications. To handle complex crossing
structures in 2D images, 2D orientation scores $U: \mathbb{R} ^ 2\times S ^ 1
\rightarrow \mathbb{C}$ were introduced, which already showed their use in a
variety of applications. Here we extend this work to 3D orientation scores $U:
\mathbb{R} ^ 3 \times S ^ 2\rightarrow \mathbb{C}$. First, we construct the
orientation score from a given dataset, which is achieved by an invertible
coherent state type of transform. For this transformation we introduce 3D
versions of the 2D cake-wavelets, which are complex wavelets that can
simultaneously detect oriented structures and oriented edges. Here we introduce
two types of cake-wavelets, the first uses a discrete Fourier transform, the
second is designed in the 3D generalized Zernike basis, allowing us to
calculate analytical expressions for the spatial filters. Finally, we show two
applications of the orientation score transformation. In the first application
we propose an extension of crossing-preserving coherence enhancing diffusion
via our invertible orientation scores of 3D images which we apply to real
medical image data. In the second one we develop a new tubularity measure using
3D orientation scores and apply the tubularity measure to both artificial and
real medical data.
",1,0,0,0,0,0
12462,12463,Compact Tensor Pooling for Visual Question Answering,"  Performing high level cognitive tasks requires the integration of feature
maps with drastically different structure. In Visual Question Answering (VQA)
image descriptors have spatial structures, while lexical inputs inherently
follow a temporal sequence. The recently proposed Multimodal Compact Bilinear
pooling (MCB) forms the outer products, via count-sketch approximation, of the
visual and textual representation at each spatial location. While this
procedure preserves spatial information locally, outer-products are taken
independently for each fiber of the activation tensor, and therefore do not
include spatial context. In this work, we introduce multi-dimensional sketch
({MD-sketch}), a novel extension of count-sketch to tensors. Using this new
formulation, we propose Multimodal Compact Tensor Pooling (MCT) to fully
exploit the global spatial context during bilinear pooling operations.
Contrarily to MCB, our approach preserves spatial context by directly
convolving the MD-sketch from the visual tensor features with the text vector
feature using higher order FFT. Furthermore we apply MCT incrementally at each
step of the question embedding and accumulate the multi-modal vectors with a
second LSTM layer before the final answer is chosen.
",1,0,0,0,0,0
13079,13080,RodFIter: Attitude Reconstruction from Inertial Measurement by Functional Iteration,"  Rigid motion computation or estimation is a cornerstone in numerous fields.
Attitude computation can be achieved by integrating the angular velocity
measured by gyroscopes, the accuracy of which is crucially important for the
dead-reckoning inertial navigation. The state-of-the-art attitude algorithms
have unexceptionally relied on the simplified differential equation of the
rotation vector to obtain the attitude. This paper proposes a Functional
Iteration technique with the Rodrigues vector (named the RodFIter method) to
analytically reconstruct the attitude from gyroscope measurements. The RodFIter
method is provably exact in reconstructing the incremental attitude as long as
the angular velocity is exact. Notably, the Rodrigues vector is analytically
obtained and can be used to update the attitude over the considered time
interval. The proposed method gives birth to an ultimate attitude algorithm
scheme that can be naturally extended to the general rigid motion computation.
It is extensively evaluated under the attitude coning motion and compares
favorably in accuracy with the mainstream attitude algorithms. This work is
believed having eliminated the long-standing theoretical barrier in exact
motion integration from inertial measurements.
",1,0,0,0,0,0
9549,9550,Incremental control and guidance of hybrid aircraft applied to the Cyclone tailsitter UAV,"  Hybrid unmanned aircraft, that combine hover capability with a wing for fast
and efficient forward flight, have attracted a lot of attention in recent
years. Many different designs are proposed, but one of the most promising is
the tailsitter concept. However, tailsitters are difficult to control across
the entire flight envelope, which often includes stalled flight. Additionally,
their wing surface makes them susceptible to wind gusts. In this paper, we
propose incremental nonlinear dynamic inversion control for the attitude and
position control. The result is a single, continuous controller, that is able
to track the acceleration of the vehicle across the flight envelope. The
proposed controller is implemented on the Cyclone hybrid UAV. Multiple outdoor
experiments are performed, showing that unmodeled forces and moments are
effectively compensated by the incremental control structure, and that
accelerations can be tracked across the flight envelope. Finally, we provide a
comprehensive procedure for the implementation of the controller on other types
of hybrid UAVs.
",1,0,0,0,0,0
10189,10190,Fast embedding of multilayer networks: An algorithm and application to group fMRI,"  Learning interpretable features from complex multilayer networks is a
challenging and important problem. The need for such representations is
particularly evident in multilayer networks of the brain, where nodal
characteristics may help model and differentiate regions of the brain according
to individual, cognitive task, or disease. Motivated by this problem, we
introduce the multi-node2vec algorithm, an efficient and scalable feature
engineering method that automatically learns continuous node feature
representations from multilayer networks. Multi-node2vec relies upon a
second-order random walk sampling procedure that efficiently explores the
inner- and intra- layer ties of the observed multilayer network is utilized to
identify multilayer neighborhoods. Maximum likelihood estimators of the nodal
features are identified through the use of the Skip-gram neural network model
on the collection of sampled neighborhoods. We investigate the conditions under
which multi-node2vec is an approximation of a closed-form matrix factorization
problem. We demonstrate the efficacy of multi-node2vec on a multilayer
functional brain network from resting state fMRI scans over a group of 74
healthy individuals. We find that multi-node2vec outperforms contemporary
methods on complex networks, and that multi-node2vec identifies nodal
characteristics that closely associate with the functional organization of the
brain.
",1,0,0,0,0,0
4195,4196,Feature analysis of multidisciplinary scientific collaboration patterns based on PNAS,"  The features of collaboration patterns are often considered to be different
from discipline to discipline. Meanwhile, collaborating among disciplines is an
obvious feature emerged in modern scientific research, which incubates several
interdisciplines. The features of collaborations in and among the disciplines
of biological, physical and social sciences are analyzed based on 52,803 papers
published in a multidisciplinary journal PNAS during 1999 to 2013. From those
data, we found similar transitivity and assortativity of collaboration patterns
as well as the identical distribution type of collaborators per author and that
of papers per author, namely a mixture of generalized Poisson and power-law
distributions. In addition, we found that interdisciplinary research is
undertaken by a considerable fraction of authors, not just those with many
collaborators or those with many papers. This case study provides a window for
understanding aspects of multidisciplinary and interdisciplinary collaboration
patterns.
",1,1,0,0,0,0
252,253,Software metadata: How much is enough?,"  Broad efforts are underway to capture metadata about research software and
retain it across services; notable in this regard is the CodeMeta project. What
metadata are important to have about (research) software? What metadata are
useful for searching for codes? What would you like to learn about astronomy
software? This BoF sought to gather information on metadata most desired by
researchers and users of astro software and others interested in registering,
indexing, capturing, and doing research on this software. Information from this
BoF could conceivably result in changes to the Astrophysics Source Code Library
(ASCL) or other resources for the benefit of the community or provide input
into other projects concerned with software metadata.
",1,1,0,0,0,0
8855,8856,Automatic Renal Segmentation in DCE-MRI using Convolutional Neural Networks,"  Kidney function evaluation using dynamic contrast-enhanced MRI (DCE-MRI)
images could help in diagnosis and treatment of kidney diseases of children.
Automatic segmentation of renal parenchyma is an important step in this
process. In this paper, we propose a time and memory efficient fully automated
segmentation method which achieves high segmentation accuracy with running time
in the order of seconds in both normal kidneys and kidneys with hydronephrosis.
The proposed method is based on a cascaded application of two 3D convolutional
neural networks that employs spatial and temporal information at the same time
in order to learn the tasks of localization and segmentation of kidneys,
respectively. Segmentation performance is evaluated on both normal and abnormal
kidneys with varying levels of hydronephrosis. We achieved a mean dice
coefficient of 91.4 and 83.6 for normal and abnormal kidneys of pediatric
patients, respectively.
",0,0,0,1,0,0
16387,16388,Ordered Monoids: Languages and Relations,"  We give a finite axiomatization for the variety generated by relational,
integral ordered monoids. As a corollary we get a finite axiomatization for the
language interpretation as well.
",0,0,1,0,0,0
7509,7510,Truth-Telling Mechanism for Secure Two-Way Relay Communications with Energy-Harvesting Revenue,"  This paper brings the novel idea of paying the utility to the winning agents
in terms of some physical entity in cooperative communications. Our setting is
a secret two-way communication channel where two transmitters exchange
information in the presence of an eavesdropper. The relays are selected from a
set of interested parties such that the secrecy sum rate is maximized. In
return, the selected relay nodes' energy harvesting requirements will be
fulfilled up to a certain threshold through their own payoff so that they have
the natural incentive to be selected and involved in the communication.
However, relays may exaggerate their private information in order to improve
their chance to be selected. Our objective is to develop a mechanism for relay
selection that enforces them to reveal the truth since otherwise they may be
penalized. We also propose a joint cooperative relay beamforming and transmit
power optimization scheme based on an alternating optimization approach. Note
that the problem is highly non-convex since the objective function appears as a
product of three correlated Rayleigh quotients. While a common practice in the
existing literature is to optimize the relay beamforming vector for given
transmit power via rank relaxation, we propose a second-order cone programming
(SOCP)-based approach in this paper which requires a significantly lower
computational task. The performance of the incentive control mechanism and the
optimization algorithm has been evaluated through numerical simulations.
",1,0,0,0,0,0
10557,10558,Large-Scale Sleep Condition Analysis Using Selfies from Social Media,"  Sleep condition is closely related to an individual's health. Poor sleep
conditions such as sleep disorder and sleep deprivation affect one's daily
performance, and may also cause many chronic diseases. Many efforts have been
devoted to monitoring people's sleep conditions. However, traditional
methodologies require sophisticated equipment and consume a significant amount
of time. In this paper, we attempt to develop a novel way to predict
individual's sleep condition via scrutinizing facial cues as doctors would.
Rather than measuring the sleep condition directly, we measure the
sleep-deprived fatigue which indirectly reflects the sleep condition. Our
method can predict a sleep-deprived fatigue rate based on a selfie provided by
a subject. This rate is used to indicate the sleep condition. To gain deeper
insights of human sleep conditions, we collected around 100,000 faces from
selfies posted on Twitter and Instagram, and identified their age, gender, and
race using automatic algorithms. Next, we investigated the sleep condition
distributions with respect to age, gender, and race. Our study suggests among
the age groups, fatigue percentage of the 0-20 youth and adolescent group is
the highest, implying that poor sleep condition is more prevalent in this age
group. For gender, the fatigue percentage of females is higher than that of
males, implying that more females are suffering from sleep issues than males.
Among ethnic groups, the fatigue percentage in Caucasian is the highest
followed by Asian and African American.
",1,0,0,0,0,0
4815,4816,Bridging the Gap Between Layout Pattern Sampling and Hotspot Detection via Batch Active Learning,"  Layout hotpot detection is one of the main steps in modern VLSI design. A
typical hotspot detection flow is extremely time consuming due to the
computationally expensive mask optimization and lithographic simulation. Recent
researches try to facilitate the procedure with a reduced flow including
feature extraction, training set generation and hotspot detection, where
feature extraction methods and hotspot detection engines are deeply studied.
However, the performance of hotspot detectors relies highly on the quality of
reference layout libraries which are costly to obtain and usually predetermined
or randomly sampled in previous works. In this paper, we propose an active
learning-based layout pattern sampling and hotspot detection flow, which
simultaneously optimizes the machine learning model and the training set that
aims to achieve similar or better hotspot detection performance with much
smaller number of training instances. Experimental results show that our
proposed method can significantly reduce lithography simulation overhead while
attaining satisfactory detection accuracy on designs under both DUV and EUV
lithography technologies.
",0,0,0,1,0,0
16266,16267,Entombed: An archaeological examination of an Atari 2600 game,"  The act and experience of programming is, at its heart, a fundamentally human
activity that results in the production of artifacts. When considering
programming, therefore, it would be a glaring omission to not involve people
who specialize in studying artifacts and the human activity that yields them:
archaeologists. Here we consider this with respect to computer games, the focus
of archaeology's nascent subarea of archaeogaming.
One type of archaeogaming research is digital excavation, a technical
examination of the code and techniques used in old games' implementation. We
apply that in a case study of Entombed, an Atari 2600 game released in 1982 by
US Games. The player in this game is, appropriately, an archaeologist who must
make their way through a zombie-infested maze. Maze generation is a fruitful
area for comparative retrogame archaeology, because a number of early games on
different platforms featured mazes, and their variety of approaches can be
compared. The maze in Entombed is particularly interesting: it is shaped in
part by the extensive real-time constraints of the Atari 2600 platform, and
also had to be generated efficiently and use next to no memory. We reverse
engineered key areas of the game's code to uncover its unusual maze-generation
algorithm, which we have also built a reconstruction of, and analyzed the
mysterious table that drives it. In addition, we discovered what appears to be
a 35-year-old bug in the code, as well as direct evidence of code-reuse
practices amongst game developers.
What further makes this game's development interesting is that, in an era
where video games were typically solo projects, a total of five people were
involved in various ways with Entombed. We piece together some of the backstory
of the game's development and intoxicant-fueled design using interviews to
complement our technical work.
Finally, we contextualize this example in archaeology and lay the groundwork
for a broader interdisciplinary discussion about programming, one that includes
both computer scientists and archaeologists.
",1,0,0,0,0,0
9648,9649,Stealth Attacks on the Smart Grid,"  Random attacks that jointly minimize the amount of information acquired by
the operator about the state of the grid and the probability of attack
detection are presented. The attacks minimize the information acquired by the
operator by minimizing the mutual information between the observations and the
state variables describing the grid. Simultaneously, the attacker aims to
minimize the probability of attack detection by minimizing the Kullback-Leibler
(KL) divergence between the distribution when the attack is present and the
distribution under normal operation. The resulting cost function is the
weighted sum of the mutual information and the KL divergence mentioned above.
The tradeoff between the probability of attack detection and the reduction of
mutual information is governed by the weighting parameter on the KL divergence
term in the cost function. The probability of attack detection is evaluated as
a function of the weighting parameter. A sufficient condition on the weighting
parameter is given for achieving an arbitrarily small probability of attack
detection. The attack performance is numerically assessed on the IEEE 30-Bus
and 118-Bus test systems.
",1,0,0,0,0,0
2391,2392,Life and work of Egbert Brieskorn (1936 - 2013),"  Egbert Brieskorn died on July 11, 2013, a few days after his 77th birthday.
He was an impressive personality who has left a lasting impression on all who
knew him, whether inside or outside of mathematics. Brieskorn was a great
mathematician, but his interests, his knowledge, and activities ranged far
beyond mathematics. In this contribution, which is strongly influenced by many
years of personal connectedness of the authors with Brieskorn, we try to give a
deeper insight into the life and work of Brieskorn. We illuminate both his
personal commitment to peace and the environment as well as his long-term study
of the life and work of Felix Hausdorff and the publication of Hausdorff's
collected works. However, the main focus of the article is on the presentation
of his remarkable and influential mathematical work.
",0,0,1,0,0,0
18511,18512,Search for nucleon decays with EXO-200,"  A search for instability of nucleons bound in $^{136}$Xe nuclei is reported
with 223 kg$\cdot$yr exposure of $^{136}$Xe in the EXO-200 experiment. Lifetime
limits of 3.3$\times 10^{23}$ and 1.9$\times 10^{23}$ yrs are established for
nucleon decay to $^{133}$Sb and $^{133}$Te, respectively. These are the most
stringent to date, exceeding the prior decay limits by a factor of 9 and 7,
respectively.
",0,1,0,0,0,0
5317,5318,Forward Flux Sampling Calculation of Homogeneous Nucleation Rates from Aqueous NaCl Solutions,"  We used molecular dynamics simulations and the path sampling technique known
as forward flux sampling to study homogeneous nucleation of NaCl crystals from
supersaturated aqueous solutions at 298 K and 1 bar. Nucleation rates were
obtained for a range of salt concentrations for the Joung-Cheatham NaCl force
field combined with the SPC/E water model. The calculated nucleation rates are
significantly lower than available experimental measurements. The estimates for
the nucleation rates in this work do not rely on classical nucleation theory,
but the pathways observed in the simulations suggest that the nucleation
process is better described by classical nucleation theory than an alternative
interpretation based on Ostwald's step rule, in contrast to some prior
simulations of related models. In addition to the size of NaCl nucleus, we find
that the crystallinity of a nascent cluster plays an important role in the
nucleation process. Nuclei with high crystallinity were found to have higher
growth probability and longer lifetimes, possibly because they are less exposed
to hydration water.
",0,1,0,0,0,0
8389,8390,A statistical physics approach to learning curves for the Inverse Ising problem,"  Using methods of statistical physics, we analyse the error of learning
couplings in large Ising models from independent data (the inverse Ising
problem). We concentrate on learning based on local cost functions, such as the
pseudo-likelihood method for which the couplings are inferred independently for
each spin. Assuming that the data are generated from a true Ising model, we
compute the reconstruction error of the couplings using a combination of the
replica method with the cavity approach for densely connected systems. We show
that an explicit estimator based on a quadratic cost function achieves minimal
reconstruction error, but requires the length of the true coupling vector as
prior knowledge. A simple mean field estimator of the couplings which does not
need such knowledge is asymptotically optimal, i.e. when the number of
observations is much large than the number of spins. Comparison of the theory
with numerical simulations shows excellent agreement for data generated from
two models with random couplings in the high temperature region: a model with
independent couplings (Sherrington-Kirkpatrick model), and a model where the
matrix of couplings has a Wishart distribution.
",0,1,0,1,0,0
14791,14792,Direct Optimization through $\arg \max$ for Discrete Variational Auto-Encoder,"  Reparameterization of variational auto-encoders with continuous latent spaces
is an effective method for reducing the variance of their gradient estimates.
However, using the same approach when latent variables are discrete is
problematic, due to the resulting non-differentiable objective. In this work,
we present a direct optimization method that propagates gradients through a
non-differentiable $\arg \max$ prediction operation. We apply this method to
discrete variational auto-encoders, by modeling a discrete random variable by
the $\arg \max$ function of the Gumbel-Max perturbation model.
",0,0,0,1,0,0
20468,20469,Algebraic entropy of (integrable) lattice equations and their reductions,"  We study the growth of degrees in many autonomous and non-autonomous lattice
equations defined by quad rules with corner boundary values, some of which are
known to be integrable by other characterisations. Subject to an enabling
conjecture, we prove polynomial growth for a large class of equations which
includes the Adler-Bobenko-Suris equations and Viallet's $Q_V$ and its
non-autonomous generalization. Our technique is to determine the ambient degree
growth of the projective version of the lattice equations and to conjecture the
growth of their common factors at each lattice vertex, allowing the true degree
growth to be found. The resulting degrees satisfy a linear partial difference
equation which is universal, i.e. the same for all the integrable lattice
equations considered. When we take periodic reductions of these equations,
which includes staircase initial conditions, we obtain from this linear partial
difference equation an ordinary difference equation for degrees that implies
quadratic or linear degree growth. We also study growth of degree of several
non-integrable lattice equations. Exponential growth of degrees of these
equations, and their mapping reductions, is also proved subject to a
conjecture.
",0,1,0,0,0,0
14490,14491,Logic Lectures: Gödel's Basic Logic Course at Notre Dame,"  An edited version is given of the text of Gödel's unpublished manuscript of
the notes for a course in basic logic he delivered at the University of Notre
Dame in 1939. Gödel's notes deal with what is today considered as important
logical problems par excellence, completeness, decidability, independence of
axioms, and with natural deduction too, which was all still a novelty at the
time the course was delivered. Full of regards towards beginners, the notes are
not excessively formalistic. Gödel presumably intended them just for himself,
and they are full of abbreviations. This together with some other matters (like
two versions of the same topic, and guessing the right order of the pages)
required additional effort to obtain a readable edited version. Because of the
quality of the material provided by Gödel, including also important
philosophical points, this effort should however be worthwhile. The edited
version of the text is accompanied by another version, called the source
version, which is quite close to Gödel's manuscript. It is meant to be a
record of the editorial interventions involved in producing the edited version
(in particular, how the abbreviations were disabridged), and a justification of
that later version.
",0,0,1,0,0,0
15119,15120,Identity Testing and Interpolation from High Powers of Polynomials of Large Degree over Finite Fields,"  We consider the problem of identity testing and recovering (that is,
interpolating) of a ""hidden"" monic polynomials $f$, given an oracle access to
$f(x)^e$ for $x\in\mathbb F_q$, where $\mathbb F_q$ is the finite field of $q$
elements and an extension fields access is not permitted.
The naive interpolation algorithm needs $de+1$ queries, where $d =\max\{{\rm
deg}\ f, {\rm deg }\ g\}$ and thus requires $ de<q$. For a prime $q = p$, we
design an algorithm that is asymptotically better in certain cases, especially
when $d$ is large. The algorithm is based on a result of independent interest
in spirit of additive combinatorics. It gives an upper bound on the number of
values of a rational function of large degree, evaluated on a short sequence of
consecutive integers, that belong to a small subgroup of $\mathbb F_p^*$.
",1,0,1,0,0,0
5675,5676,First measeurements in search for keV-sterile neutrino in tritium beta-decay by Troitsk nu-mass experiment,"  We present the first measurements of tritium beta-decay spectrum in the
electron energy range 16-18.6 keV. The goal is to find distortions which may
correspond to the presence of a heavy sterile neutrinos. A possible
contribution of this kind would manifest itself as a kink in the spectrum with
a similar shape but with end point shifted by the value of a heavy neutrino
mass. We set a new upper limits to the neutrino mixing matrix element U^2_{e4}
which improve existing limits by a factor from 2 to 5 in the mass range 0.1-2
keV.
",0,1,0,0,0,0
12687,12688,Thought Viruses and Asset Prices,"  We use insights from epidemiology, namely the SIR model, to study how agents
infect each other with ""investment ideas."" Once an investment idea ""goes
viral,"" equilibrium prices exhibit the typical ""fever peak,"" which is
characteristic for speculative excesses. Using our model, we identify a time
line of symptoms that indicate whether a boom is in its early or later stages.
Regarding the market's top, we find that prices start to decline while the
number of infected agents, who buy the asset, is still rising. Moreover, the
presence of fully rational agents (i) accelerates booms (ii) lowers peak prices
and (iii) produces broad, drawn-out, market tops.
",0,0,0,0,0,1
13033,13034,PPMF: A Patient-based Predictive Modeling Framework for Early ICU Mortality Prediction,"  To date, developing a good model for early intensive care unit (ICU)
mortality prediction is still challenging. This paper presents a patient based
predictive modeling framework (PPMF) to improve the performance of ICU
mortality prediction using data collected during the first 48 hours of ICU
admission. PPMF consists of three main components verifying three related
research hypotheses. The first component captures dynamic changes of patients
status in the ICU using their time series data (e.g., vital signs and
laboratory tests). The second component is a local approximation algorithm that
classifies patients based on their similarities. The third component is a
Gradient Decent wrapper that updates feature weights according to the
classification feedback. Experiments using data from MIMICIII show that PPMF
significantly outperforms: (1) the severity score systems, namely SASP III,
APACHE IV, and MPM0III, (2) the aggregation based classifiers that utilize
summarized time series, and (3) baseline feature selection methods.
",1,0,0,0,0,0
12971,12972,Alchemist: An Apache Spark <=> MPI Interface,"  The Apache Spark framework for distributed computation is popular in the data
analytics community due to its ease of use, but its MapReduce-style programming
model can incur significant overheads when performing computations that do not
map directly onto this model. One way to mitigate these costs is to off-load
computations onto MPI codes. In recent work, we introduced Alchemist, a system
for the analysis of large-scale data sets. Alchemist calls MPI-based libraries
from within Spark applications, and it has minimal coding, communication, and
memory overheads. In particular, Alchemist allows users to retain the
productivity benefits of working within the Spark software ecosystem without
sacrificing performance efficiency in linear algebra, machine learning, and
other related computations.
In this paper, we discuss the motivation behind the development of Alchemist,
and we provide a detailed overview its design and usage. We also demonstrate
the efficiency of our approach on medium-to-large data sets, using some
standard linear algebra operations, namely matrix multiplication and the
truncated singular value decomposition of a dense matrix, and we compare the
performance of Spark with that of Spark+Alchemist. These computations are run
on the NERSC supercomputer Cori Phase 1, a Cray XC40.
",0,0,0,1,0,0
19798,19799,Amplitude Mediated Chimera States with Active and Inactive Oscillators,"  The emergence and nature of amplitude mediated chimera states,
spatio-temporal patterns of co-existing coherent and incoherent regions, are
investigated for a globally coupled system of active and inactive
Ginzburg-Landau oscillators. The existence domain of such states is found to
shrink and shift in parametric space as the fraction of inactive oscillators is
increased. The role of inactive oscillators is found to be two fold - they get
activated to form a separate region of coherent oscillations and in addition
decrease the common collective frequency of the coherent regions by their
presence. The dynamical origin of these effects is delineated through a
detailed bifurcation analysis of a reduced model equation that is based on a
mean field approximation. Our results may have practical implications for the
robustness of such states in biological or physical systems where age related
deterioration in the functionality of components can occur.
",0,1,0,0,0,0
10249,10250,Self-supporting Topology Optimization for Additive Manufacturing,"  The paper presents a topology optimization approach that designs an optimal
structure, called a self-supporting structure, which is ready to be fabricated
via additive manufacturing without the usage of additional support structures.
Such supports in general have to be created during the fabricating process so
that the primary object can be manufactured layer by layer without collapse,
which is very time-consuming and waste of material.
The proposed approach resolves this problem by formulating the
self-supporting requirements as a novel explicit quadratic continuous
constraint in the topology optimization problem, or specifically, requiring the
number of unsupported elements (in terms of the sum of squares of their
densities) to be zero. Benefiting form such novel formulations, computing
sensitivity of the self-supporting constraint with respect to the design
density is straightforward, which otherwise would require lots of research
efforts in general topology optimization studies. The derived sensitivity for
each element is only linearly dependent on its sole density, which, different
from previous layer-based sensitivities, consequently allows for a parallel
implementation and possible higher convergence rate. In addition, a discrete
convolution operator is also designed to detect the unsupported elements as
involved in each step of optimization iteration, and improves the detection
process 100 times as compared with simply enumerating these elements. The
approach works for cases of general overhang angle, or general domain, and
produces an optimized structures, and their associated optimal compliance, very
close to that of the reference structure obtained without considering the
self-supporting constraint, as demonstrated by extensive 2D and 3D benchmark
examples.
",1,0,0,0,0,0
19590,19591,Sharp rates of convergence for accumulated spectrograms,"  We investigate an inverse problem in time-frequency localization: the
approximation of the symbol of a time-frequency localization operator from
partial spectral information by the method of accumulated spectrograms (the sum
of the spectrograms corresponding to large eigenvalues). We derive a sharp
bound for the rate of convergence of the accumulated spectrogram, improving on
recent results.
",0,0,1,0,0,0
2725,2726,Making Neural Programming Architectures Generalize via Recursion,"  Empirically, neural networks that attempt to learn programs from data have
exhibited poor generalizability. Moreover, it has traditionally been difficult
to reason about the behavior of these models beyond a certain level of input
complexity. In order to address these issues, we propose augmenting neural
architectures with a key abstraction: recursion. As an application, we
implement recursion in the Neural Programmer-Interpreter framework on four
tasks: grade-school addition, bubble sort, topological sort, and quicksort. We
demonstrate superior generalizability and interpretability with small amounts
of training data. Recursion divides the problem into smaller pieces and
drastically reduces the domain of each neural network component, making it
tractable to prove guarantees about the overall system's behavior. Our
experience suggests that in order for neural architectures to robustly learn
program semantics, it is necessary to incorporate a concept like recursion.
",1,0,0,0,0,0
3448,3449,Automatic sequences and generalised polynomials,"  We conjecture that bounded generalised polynomial functions cannot be
generated by finite automata, except for the trivial case when they are
ultimately periodic.
Using methods from ergodic theory, we are able to partially resolve this
conjecture, proving that any hypothetical counterexample is periodic away from
a very sparse and structured set.
In particular, we show that for a polynomial $p(n)$ with at least one
irrational coefficient (except for the constant one) and integer $m\geq 2$, the
sequence $\lfloor p(n) \rfloor \bmod{m}$ is never automatic.
We also prove that the conjecture is equivalent to the claim that the set of
powers of an integer $k\geq 2$ is not given by a generalised polynomial.
",1,0,1,0,0,0
13550,13551,Replicability Analysis for Natural Language Processing: Testing Significance with Multiple Datasets,"  With the ever-growing amounts of textual data from a large variety of
languages, domains, and genres, it has become standard to evaluate NLP
algorithms on multiple datasets in order to ensure consistent performance
across heterogeneous setups. However, such multiple comparisons pose
significant challenges to traditional statistical analysis methods in NLP and
can lead to erroneous conclusions. In this paper, we propose a Replicability
Analysis framework for a statistically sound analysis of multiple comparisons
between algorithms for NLP tasks. We discuss the theoretical advantages of this
framework over the current, statistically unjustified, practice in the NLP
literature, and demonstrate its empirical value across four applications:
multi-domain dependency parsing, multilingual POS tagging, cross-domain
sentiment classification and word similarity prediction.
",1,0,0,0,0,0
12156,12157,Batched High-dimensional Bayesian Optimization via Structural Kernel Learning,"  Optimization of high-dimensional black-box functions is an extremely
challenging problem. While Bayesian optimization has emerged as a popular
approach for optimizing black-box functions, its applicability has been limited
to low-dimensional problems due to its computational and statistical challenges
arising from high-dimensional settings. In this paper, we propose to tackle
these challenges by (1) assuming a latent additive structure in the function
and inferring it properly for more efficient and effective BO, and (2)
performing multiple evaluations in parallel to reduce the number of iterations
required by the method. Our novel approach learns the latent structure with
Gibbs sampling and constructs batched queries using determinantal point
processes. Experimental validations on both synthetic and real-world functions
demonstrate that the proposed method outperforms the existing state-of-the-art
approaches.
",1,0,1,1,0,0
271,272,Auto-Meta: Automated Gradient Based Meta Learner Search,"  Fully automating machine learning pipelines is one of the key challenges of
current artificial intelligence research, since practical machine learning
often requires costly and time-consuming human-powered processes such as model
design, algorithm development, and hyperparameter tuning. In this paper, we
verify that automated architecture search synergizes with the effect of
gradient-based meta learning. We adopt the progressive neural architecture
search \cite{liu:pnas_google:DBLP:journals/corr/abs-1712-00559} to find optimal
architectures for meta-learners. The gradient based meta-learner whose
architecture was automatically found achieved state-of-the-art results on the
5-shot 5-way Mini-ImageNet classification problem with $74.65\%$ accuracy,
which is $11.54\%$ improvement over the result obtained by the first
gradient-based meta-learner called MAML
\cite{finn:maml:DBLP:conf/icml/FinnAL17}. To our best knowledge, this work is
the first successful neural architecture search implementation in the context
of meta learning.
",0,0,0,1,0,0
11412,11413,Drop pattern resulting from the breakup of a bidimensional grid of liquid filaments,"  A rectangular grid formed by liquid filaments on a partially wetting
substrate evolves in a series of breakups leading to arrays of drops with
different shapes distributed in a rather regular bidimensional pattern. Our
study is focused on the configuration produced when two long parallel filaments
of silicone oil, which are placed upon a glass substrate previously coated with
a fluorinated solution, are crossed perpendicularly by another pair of long
parallel filaments. A remarkable feature of this kind of grids is that there
are two qualitatively different types of drops. While one set is formed at the
crossing points, the rest are consequence of the breakup of shorter filaments
formed between the crossings. Here, we analyze the main geometric features of
all types of drops, such as shape of the footprint and contact angle
distribution along the drop periphery. The formation of a series of short
filaments with similar geometric and physical properties allows us to have
simultaneously quasi identical experiments to study the subsequent breakups. We
develop a simple hydrodynamic model to predict the number of drops that results
from a filament of given initial length and width. This model is able to yield
the length intervals corresponding to a small number of drops and its
predictions are successfully compared with the experimental data as well as
with numerical simulations of the full Navier--Stokes equation that provide a
detailed time evolution of the dewetting motion of the filament till the
breakup into drops. Finally, the prediction for finite filaments is contrasted
with the existing theories for infinite ones.
",0,1,0,0,0,0
14444,14445,Inertia-Constrained Pixel-by-Pixel Nonnegative Matrix Factorisation: a Hyperspectral Unmixing Method Dealing with Intra-class Variability,"  Blind source separation is a common processing tool to analyse the
constitution of pixels of hyperspectral images. Such methods usually suppose
that pure pixel spectra (endmembers) are the same in all the image for each
class of materials. In the framework of remote sensing, such an assumption is
no more valid in the presence of intra-class variabilities due to illumination
conditions, weathering, slight variations of the pure materials, etc... In this
paper, we first describe the results of investigations highlighting intra-class
variability measured in real images. Considering these results, a new
formulation of the linear mixing model is presented leading to two new methods.
Unconstrained Pixel-by-pixel NMF (UP-NMF) is a new blind source separation
method based on the assumption of a linear mixing model, which can deal with
intra-class variability. To overcome UP-NMF limitations an extended method is
proposed, named Inertia-constrained Pixel-by-pixel NMF (IP-NMF). For each
sensed spectrum, these extended versions of NMF extract a corresponding set of
source spectra. A constraint is set to limit the spreading of each source's
estimates in IP-NMF. The methods are tested on a semi-synthetic data set built
with spectra extracted from a real hyperspectral image and then numerically
mixed. We thus demonstrate the interest of our methods for realistic source
variabilities. Finally, IP-NMF is tested on a real data set and it is shown to
yield better performance than state of the art methods.
",1,1,0,1,0,0
15832,15833,On relations between weak and strong type inequalities for maximal operators on non-doubling metric measure spaces,"  In this article we characterize all possible cases that may occur in the
relations between the sets of $p$ for which weak type $(p,p)$ and strong type
$(p,p)$ inequalities for the Hardy--Littlewood maximal operators, both centered
and non-centered, hold in the context of general metric measure spaces.
",0,0,1,0,0,0
3659,3660,The CodRep Machine Learning on Source Code Competition,"  CodRep is a machine learning competition on source code data. It is carefully
designed so that anybody can enter the competition, whether professional
researchers, students or independent scholars, without specific knowledge in
machine learning or program analysis. In particular, it aims at being a common
playground on which the machine learning and the software engineering research
communities can interact. The competition has started on April 14th 2018 and
has ended on October 14th 2018. The CodRep data is hosted at
this https URL.
",1,0,0,0,0,0
8935,8936,Evaluating Gaussian Process Metamodels and Sequential Designs for Noisy Level Set Estimation,"  We consider the problem of learning the level set for which a noisy black-box
function exceeds a given threshold.
To efficiently reconstruct the level set, we investigate Gaussian process
(GP) metamodels. Our focus is on strongly stochastic samplers, in particular
with heavy-tailed simulation noise and low signal-to-noise ratio.
To guard against noise misspecification, we assess the performance of three
variants: (i) GPs with Student-$t$ observations; (ii) Student-$t$ processes
(TPs); and (iii) classification GPs modeling the sign of the response. As a
fourth extension, we study GP surrogates with monotonicity constraints that are
relevant when the level set is known to be connected. In conjunction with these
metamodels, we analyze several acquisition functions for guiding the sequential
experimental designs, extending existing stepwise uncertainty reduction
criteria to the stochastic contour-finding context. This also motivates our
development of (approximate) updating formulas to efficiently compute such
acquisition functions. Our schemes are benchmarked by using a variety of
synthetic experiments in 1--6 dimensions. We also consider an application of
level set estimation for determining the optimal exercise policy and valuation
of Bermudan options in finance.
",0,0,0,1,0,0
4061,4062,Nucleosynthesis Predictions and High-Precision Deuterium Measurements,"  Two new high-precision measurements of the deuterium abundance from absorbers
along the line of sight to the quasar PKS1937--1009 were presented. The
absorbers have lower neutral hydrogen column densities (N(HI) $\approx$
18\,cm$^{-2}$) than for previous high-precision measurements, boding well for
further extensions of the sample due to the plenitude of low column density
absorbers. The total high-precision sample now consists of 12 measurements with
a weighted average deuterium abundance of D/H = $2.55\pm0.02\times10^{-5}$. The
sample does not favour a dipole similar to the one detected for the fine
structure constant. The increased precision also calls for improved
nucleosynthesis predictions. For that purpose we have updated the public
AlterBBN code including new reactions, updated nuclear reaction rates, and the
possibility of adding new physics such as dark matter. The standard Big Bang
Nucleosynthesis prediction of D/H = $2.456\pm0.057\times10^{-5}$ is consistent
with the observed value within 1.7 standard deviations.
",0,1,0,0,0,0
13633,13634,Geometric Analysis of Synchronization in Neuronal Networks with Global Inhibition and Coupling Delays,"  We study synaptically coupled neuronal networks to identify the role of
coupling delays in network's synchronized behaviors. We consider a network of
excitable, relaxation oscillator neurons where two distinct populations, one
excitatory and one inhibitory, are coupled and interact with each other. The
excitatory population is uncoupled, while the inhibitory population is tightly
coupled. A geometric singular perturbation analysis yields existence and
stability conditions for synchronization states under different firing patterns
between the two populations, along with formulas for the periods of such
synchronous solutions. Our results demonstrate that the presence of coupling
delays in the network promotes synchronization. Numerical simulations are
conducted to supplement and validate analytical results. We show the results
carry over to a model for spindle sleep rhythms in thalamocortical networks,
one of the biological systems which motivated our study. The analysis helps to
explain how coupling delays in either excitatory or inhibitory synapses
contribute to producing synchronized rhythms.
",0,1,0,0,0,0
18618,18619,Combinatorial Miller-Hagberg Algorithm for Randomization of Dense Networks,"  We propose a slightly revised Miller-Hagberg (MH) algorithm that efficiently
generates a random network from a given expected degree sequence. The revision
was to replace the approximated edge probability between a pair of nodes with a
combinatorically calculated edge probability that better captures the
likelihood of edge presence especially where edges are dense. The computational
complexity of this combinatorial MH algorithm is still in the same order as the
original one. We evaluated the proposed algorithm through several numerical
experiments. The results demonstrated that the proposed algorithm was
particularly good at accurately representing high-degree nodes in dense,
heterogeneous networks. This algorithm may be a useful alternative of other
more established network randomization methods, given that the data are
increasingly becoming larger and denser in today's network science research.
",1,0,0,0,0,0
6350,6351,Numerical analysis of nonlocal fracture models in Hölder space,"  In this work, we calculate the convergence rate of the finite difference
approximation for a class of nonlocal fracture models. We consider two point
force interactions characterized by a double well potential. We show the
existence of a evolving displacement field in Hölder space with Hölder
exponent $\gamma \in (0,1]$. The rate of convergence of the finite difference
approximation depends on the factor $C_s h^\gamma/\epsilon^2$ where $\epsilon$
gives the length scale of nonlocal interaction, $h$ is the discretization
length and $C_s$ is the maximum of Hölder norm of the solution and its second
derivatives during the evolution. It is shown that the rate of convergence
holds for both the forward Euler scheme as well as general single step implicit
schemes. A stability result is established for the semi-discrete approximation.
The Hölder continuous evolutions are seen to converge to a brittle fracture
evolution in the limit of vanishing nonlocality.
",0,0,1,0,0,0
18801,18802,Spin Precession Experiments for Light Axionic Dark Matter,"  Axion-like particles are promising candidates to make up the dark matter of
the universe, but it is challenging to design experiments that can detect them
over their entire allowed mass range. Dark matter in general, and in particular
axion-like particles and hidden photons, can be as light as roughly $10^{-22}
\;\rm{eV}$ ($\sim 10^{-8} \;\rm{Hz}$), with astrophysical anomalies providing
motivation for the lightest masses (""fuzzy dark matter""). We propose
experimental techniques for direct detection of axion-like dark matter in the
mass range from roughly $10^{-13} \;\rm{eV}$ ($\sim 10^2 \;\rm{Hz}$) down to
the lowest possible masses. In this range, these axion-like particles act as a
time-oscillating magnetic field coupling only to spin, inducing effects such as
a time-oscillating torque and periodic variations in the spin-precession
frequency with the frequency and direction set by fundamental physics. We show
how these signals can be measured using existing experimental technology,
including torsion pendulums, atomic magnetometers, and atom interferometry.
These experiments demonstrate a strong discovery capability, with future
iterations of these experiments capable of pushing several orders of magnitude
past current astrophysical bounds.
",0,1,0,0,0,0
19150,19151,Experimental Constraint on an Exotic Spin- and Velocity-Dependent Interaction in the Sub-meV Range of Axion Mass with a Spin-Exchange Relaxation-Free Magnetometer,"  We conducted a search for an exotic spin- and velocity-dependent interaction
for polarized electrons with an experimental approach based on a
high-sensitivity spin-exchange relaxation-free (SERF) magnetometer, which
serves as both a source of polarized electrons and a magnetic-field sensor. The
experiment aims to sensitively detect magnetic-fieldlike effects from the
exotic interaction between the polarized electrons in a SERF vapor cell and
unpolarized nucleons of a closely located solid-state mass. We report
experimental results on the interaction with 82 h of data averaging, which sets
an experimental limit on the coupling strength around $10^{-19}$ for the axion
mass $m_a \lesssim 10^{-3}$ eV, within the important axion window.
",0,1,0,0,0,0
9572,9573,Signal and Noise Statistics Oblivious Sparse Reconstruction using OMP/OLS,"  Orthogonal matching pursuit (OMP) and orthogonal least squares (OLS) are
widely used for sparse signal reconstruction in under-determined linear
regression problems. The performance of these compressed sensing (CS)
algorithms depends crucially on the \textit{a priori} knowledge of either the
sparsity of the signal ($k_0$) or noise variance ($\sigma^2$). Both $k_0$ and
$\sigma^2$ are unknown in general and extremely difficult to estimate in under
determined models. This limits the application of OMP and OLS in many practical
situations. In this article, we develop two computationally efficient
frameworks namely TF-IGP and RRT-IGP for using OMP and OLS even when $k_0$ and
$\sigma^2$ are unavailable. Both TF-IGP and RRT-IGP are analytically shown to
accomplish successful sparse recovery under the same set of restricted isometry
conditions on the design matrix required for OMP/OLS with \textit{a priori}
knowledge of $k_0$ and $\sigma^2$. Numerical simulations also indicate a highly
competitive performance of TF-IGP and RRT-IGP in comparison to OMP/OLS with
\textit{a priori} knowledge of $k_0$ and $\sigma^2$.
",0,0,0,1,0,0
20305,20306,Survivable Probability of SDN-enabled Cloud Networking with Random Physical Link Failure,"  Software-driven cloud networking is a new paradigm in orchestrating physical
resources (CPU, network bandwidth, energy, storage) allocated to network
functions, services, and applications, which is commonly modeled as a
cross-layer network. This model carries a physical network representing the
physical infrastructure, a logical network showing demands, and
logical-to-physical node/link mappings. In such networks, a single failure in
the physical network may trigger cascading failures in the logical network and
disable network services and connectivity. In this paper, we propose an
evaluation metric, survivable probability, to evaluate the reliability of such
networks under random physical link failure(s). We propose the concept of base
protecting spanning tree and prove the necessary and sufficient conditions for
its existence and relation to survivability. We then develop mathematical
programming formulations for reliable cross-layer network routing design with
the maximal reliable probability. Computation results demonstrate the viability
of our approach.
",1,0,0,0,0,0
14970,14971,Counting triangles formula for the first Chern class of a circle bundle,"  We consider the problem of the combinatorial computation of the first Chern
class of a circle bundle. N.Mnev found such a formula in terms of canonical
shellings. It represents certain invariant of a triangulation computed by
analyzing cyclic word in 3-character alphabet associated to the bundle. This
curvature is a kind of discretization of Konstevich's curvature differential
2-form.
We find a new expression of Mnev's curvature by counting triangles in a
cyclic word. Our formula is different from that of Mnev. In particular, it is
cyclically invariant by its very form. We present also some sample computations
of this invariant and also provide a small Mathematica code for the computation
of this invariant.
",0,0,1,0,0,0
8148,8149,Scalable Inference for Space-Time Gaussian Cox Processes,"  The log-Gaussian Cox process is a flexible and popular class of point pattern
models for capturing spatial and space-time dependence for point patterns.
Model fitting requires approximation of stochastic integrals which is
implemented through discretization over the domain of interest. With fine scale
discretization, inference based on Markov chain Monte Carlo is computationally
burdensome because of the cost of matrix decompositions and storage, such as
the Cholesky, for high dimensional covariance matrices associated with latent
Gaussian variables. This article addresses these computational bottlenecks by
combining two recent developments: (i) a data augmentation strategy that has
been proposed for space-time Gaussian Cox processes that is based on exact
Bayesian inference and does not require fine grid approximations for infinite
dimensional integrals, and (ii) a recently developed family of
sparsity-inducing Gaussian processes, called nearest-neighbor Gaussian
processes, to avoid expensive matrix computations. Our inference is delivered
within the fully model-based Bayesian paradigm and does not sacrifice the
richness of traditional log-Gaussian Cox processes. We apply our method to
crime event data in San Francisco and investigate the recovery of the intensity
surface.
",0,0,0,1,0,0
8448,8449,Sum-Product-Quotient Networks,"  We present a novel tractable generative model that extends Sum-Product
Networks (SPNs) and significantly boosts their power. We call it
Sum-Product-Quotient Networks (SPQNs), whose core concept is to incorporate
conditional distributions into the model by direct computation using quotient
nodes, e.g. $P(A|B) = \frac{P(A,B)}{P(B)}$. We provide sufficient conditions
for the tractability of SPQNs that generalize and relax the decomposable and
complete tractability conditions of SPNs. These relaxed conditions give rise to
an exponential boost to the expressive efficiency of our model, i.e. we prove
that there are distributions which SPQNs can compute efficiently but require
SPNs to be of exponential size. Thus, we narrow the gap in expressivity between
tractable graphical models and other Neural Network-based generative models.
",1,0,0,1,0,0
6534,6535,IP Based Traffic Recovery: An Optimal Approach using SDN Application for Data Center Network,"  With the passage of time and indulgence in Information Technology, network
management has proved its significance and has become one of the most important
and challenging task in today's era of information flow. Communication networks
implement a high level of sophistication in managing and flowing the data
through secure channels, to make it almost impossible for data loss. That is
why there are many proposed solution that are currently implemented in wide
range of network-based applications like social networks and finance
applications. The objective of this research paper is to propose a very
reliable method of data flow: Choose best path for traffic using SDN
application. This is an IP based method in which our SDN application implements
provision of best possible path by filtering the requests on base of their IP
origin. We'll distinguish among source and will provide the data flow with
lowest traffic path, thus resulting in providing us minimum chances of data
loss. A request to access our test application will be generated from host and
request from each host will be distinguished by our SDN application that will
get number of active users for all available servers and will redirect the
request to server with minimum traffic load. It will also destroy sessions of
inactive users, resulting in maintaining a best responsive channel for data
flow.
",1,0,0,0,0,0
1873,1874,Equitable neighbour-sum-distinguishing edge and total colourings,"  With any (not necessarily proper) edge $k$-colouring
$\gamma:E(G)\longrightarrow\{1,\dots,k\}$ of a graph $G$,one can associate a
vertex colouring $\sigma\_{\gamma}$ given by $\sigma\_{\gamma}(v)=\sum\_{e\ni
v}\gamma(e)$.A neighbour-sum-distinguishing edge $k$-colouring is an edge
colouring whose associated vertex colouring is proper.The
neighbour-sum-distinguishing index of a graph $G$ is then the smallest $k$ for
which $G$ admitsa neighbour-sum-distinguishing edge $k$-colouring.These notions
naturally extends to total colourings of graphs that assign colours to both
vertices and edges.We study in this paper equitable
neighbour-sum-distinguishing edge colourings andtotal colourings, that is
colourings $\gamma$ for whichthe number of elements in any two colour classes
of $\gamma$ differ by at most one.We determine the equitable
neighbour-sum-distinguishing indexof complete graphs, complete bipartite graphs
and forests,and the equitable neighbour-sum-distinguishing total chromatic
numberof complete graphs and bipartite graphs.
",1,0,1,0,0,0
4769,4770,A Channel-Based Perspective on Conjugate Priors,"  A desired closure property in Bayesian probability is that an updated
posterior distribution be in the same class of distributions --- say Gaussians
--- as the prior distribution. When the updating takes place via a statistical
model, one calls the class of prior distributions the `conjugate priors' of the
model. This paper gives (1) an abstract formulation of this notion of conjugate
prior, using channels, in a graphical language, (2) a simple abstract proof
that such conjugate priors yield Bayesian inversions, and (3) a logical
description of conjugate priors that highlights the required closure of the
priors under updating. The theory is illustrated with several standard
examples, also covering multiple updating.
",1,0,0,0,0,0
10119,10120,High-frequency approximation of the interior dirichlet-to-neumann map and applications to the transmission eigenvalues,"  We study the high-frequency behavior of the Dirichlet-to-Neumann map for an
arbitrary compact Riemannian manifold with a non-empty smooth boundary. We show
that far from the real axis it can be approximated by a simpler operator. We
use this fact to get new results concerning the location of the transmission
eigenvalues on the complex plane. In some cases we obtain optimal transmission
eigenvalue-free regions.
",0,0,1,0,0,0
8746,8747,Cohomologies of locally conformally symplectic manifolds and solvmanifolds,"  We study the Morse-Novikov cohomology and its almost-symplectic counterpart
on manifolds admitting locally conformally symplectic structures. More
precisely, we introduce lcs cohomologies and we study elliptic Hodge theory,
dualities, Hard Lefschetz Condition. We consider solvmanifolds and
Oeljeklaus-Toma manifolds. In particular, we prove that Oeljeklaus-Toma
manifolds with precisely one complex place, and under an additional arithmetic
condition, satisfy the Mostow property. This holds in particular for the Inoue
surface of type $S^0$.
",0,0,1,0,0,0
20341,20342,Infinite ergodic index of the ehrenfest wind-tree model,"  The set of all possible configurations of the Ehrenfest wind-tree model
endowed with the Hausdorff topology is a compact metric space. For a typical
configuration we show that the wind-tree dynamics has infinite ergodic index in
almost every direction. In particular some ergodic theorems can be applied to
show that if we start with a large number of initially parallel particles their
directions decorrelate as the dynamics evolve answering the question posed by
the Ehrenfests.
",0,0,1,0,0,0
591,592,The Linear Point: A cleaner cosmological standard ruler,"  We show how a characteristic length scale imprinted in the galaxy two-point
correlation function, dubbed the ""linear point"", can serve as a comoving
cosmological standard ruler. In contrast to the Baryon Acoustic Oscillation
peak location, this scale is constant in redshift and is unaffected by
non-linear effects to within $0.5$ percent precision. We measure the location
of the linear point in the galaxy correlation function of the LOWZ and CMASS
samples from the Twelfth Data Release (DR12) of the Baryon Oscillation
Spectroscopic Survey (BOSS) collaboration. We combine our linear-point
measurement with cosmic-microwave-background constraints from the Planck
satellite to estimate the isotropic-volume distance $D_{V}(z)$, without relying
on a model-template or reconstruction method. We find $D_V(0.32)=1264\pm 28$
Mpc and $D_V(0.57)=2056\pm 22$ Mpc respectively, consistent with the quoted
values from the BOSS collaboration. This remarkable result suggests that all
the distance information contained in the baryon acoustic oscillations can be
conveniently compressed into the single length associated with the linear
point.
",0,1,0,0,0,0
18293,18294,Structural Change in (Economic) Time Series,"  Methods for detecting structural changes, or change points, in time series
data are widely used in many fields of science and engineering. This chapter
sketches some basic methods for the analysis of structural changes in time
series data. The exposition is confined to retrospective methods for univariate
time series. Several recent methods for dating structural changes are compared
using a time series of oil prices spanning more than 60 years. The methods
broadly agree for the first part of the series up to the mid-1980s, for which
changes are associated with major historical events, but provide somewhat
different solutions thereafter, reflecting a gradual increase in oil prices
that is not well described by a step function. As a further illustration, 1990s
data on the volatility of the Hang Seng stock market index are reanalyzed.
",0,1,0,1,0,0
12728,12729,Explaining Transition Systems through Program Induction,"  Explaining and reasoning about processes which underlie observed black-box
phenomena enables the discovery of causal mechanisms, derivation of suitable
abstract representations and the formulation of more robust predictions. We
propose to learn high level functional programs in order to represent abstract
models which capture the invariant structure in the observed data. We introduce
the $\pi$-machine (program-induction machine) -- an architecture able to induce
interpretable LISP-like programs from observed data traces. We propose an
optimisation procedure for program learning based on backpropagation, gradient
descent and A* search. We apply the proposed method to three problems: system
identification of dynamical systems, explaining the behaviour of a DQN agent
and learning by demonstration in a human-robot interaction scenario. Our
experimental results show that the $\pi$-machine can efficiently induce
interpretable programs from individual data traces.
",1,0,0,0,0,0
4275,4276,Generative Adversarial Networks for Black-Box API Attacks with Limited Training Data,"  As online systems based on machine learning are offered to public or paid
subscribers via application programming interfaces (APIs), they become
vulnerable to frequent exploits and attacks. This paper studies adversarial
machine learning in the practical case when there are rate limitations on API
calls. The adversary launches an exploratory (inference) attack by querying the
API of an online machine learning system (in particular, a classifier) with
input data samples, collecting returned labels to build up the training data,
and training an adversarial classifier that is functionally equivalent and
statistically close to the target classifier. The exploratory attack with
limited training data is shown to fail to reliably infer the target classifier
of a real text classifier API that is available online to the public. In
return, a generative adversarial network (GAN) based on deep learning is built
to generate synthetic training data from a limited number of real training data
samples, thereby extending the training data and improving the performance of
the inferred classifier. The exploratory attack provides the basis to launch
the causative attack (that aims to poison the training process) and evasion
attack (that aims to fool the classifier into making wrong decisions) by
selecting training and test data samples, respectively, based on the confidence
scores obtained from the inferred classifier. These stealth attacks with small
footprint (using a small number of API calls) make adversarial machine learning
practical under the realistic case with limited training data available to the
adversary.
",1,0,0,1,0,0
13336,13337,SCRank: Spammer and Celebrity Ranking in Directed Social Networks,"  Many online social networks allow directed edges: Alice can unilaterally add
an ""edge"" to Bob, typically indicating interest in Bob or Bob's content,
without Bob's permission or reciprocation. In directed social networks we
observe the rise of two distinctive classes of users: celebrities who accrue
unreciprocated incoming links, and follow spammers, who generate unreciprocated
outgoing links. Identifying users in these two classes is important for abuse
detection, user and content ranking, privacy choices, and other social network
features.
In this paper we develop SCRank, an iterative algorithm to identify such
users. We analyze SCRank both theoretically and experimentally. The
spammer-celebrity definition is not amenable to analysis using standard power
iteration, so we develop a novel potential function argument to show
convergence to an approximate equilibrium point for a class of algorithms
including SCRank. We then use experimental evaluation on a real global-scale
social network and on synthetically generated graphs to observe that the
algorithm converges quickly and consistently. Using synthetic data with
built-in ground truth, we also experimentally show that the algorithm provides
a good approximation to planted celebrities and spammers.
",1,0,0,0,0,0
5399,5400,Beyond normality: Learning sparse probabilistic graphical models in the non-Gaussian setting,"  We present an algorithm to identify sparse dependence structure in continuous
and non-Gaussian probability distributions, given a corresponding set of data.
The conditional independence structure of an arbitrary distribution can be
represented as an undirected graph (or Markov random field), but most
algorithms for learning this structure are restricted to the discrete or
Gaussian cases. Our new approach allows for more realistic and accurate
descriptions of the distribution in question, and in turn better estimates of
its sparse Markov structure. Sparsity in the graph is of interest as it can
accelerate inference, improve sampling methods, and reveal important
dependencies between variables. The algorithm relies on exploiting the
connection between the sparsity of the graph and the sparsity of transport
maps, which deterministically couple one probability measure to another.
",1,0,0,1,0,0
12259,12260,A network approach to topic models,"  One of the main computational and scientific challenges in the modern age is
to extract useful information from unstructured texts. Topic models are one
popular machine-learning approach which infers the latent topical structure of
a collection of documents. Despite their success --- in particular of its most
widely used variant called Latent Dirichlet Allocation (LDA) --- and numerous
applications in sociology, history, and linguistics, topic models are known to
suffer from severe conceptual and practical problems, e.g. a lack of
justification for the Bayesian priors, discrepancies with statistical
properties of real texts, and the inability to properly choose the number of
topics. Here we obtain a fresh view on the problem of identifying topical
structures by relating it to the problem of finding communities in complex
networks. This is achieved by representing text corpora as bipartite networks
of documents and words. By adapting existing community-detection methods --
using a stochastic block model (SBM) with non-parametric priors -- we obtain a
more versatile and principled framework for topic modeling (e.g., it
automatically detects the number of topics and hierarchically clusters both the
words and documents). The analysis of artificial and real corpora demonstrates
that our SBM approach leads to better topic models than LDA in terms of
statistical model selection. More importantly, our work shows how to formally
relate methods from community detection and topic modeling, opening the
possibility of cross-fertilization between these two fields.
",1,0,0,1,0,0
8199,8200,Anisotropic mechanical and optical response and negative Poissons ratio in Mo2C nanomembranes revealed by first-principles simulations,"  Transition metal carbides include a wide variety of materials with attractive
properties that are suitable for numerous and diverse applications. Most recent
experimental advance could provide a path toward successful synthesis of
large-area and high-quality ultrathin Mo2C membranes with superconducting
properties. In the present study, we used first-principles density functional
theory calculations to explore the mechanical and optical response of
single-layer and free-standing Mo2C. Uniaxial tensile simulations along the
armchair and zigzag directions were conducted and we found that while the
elastic properties are close along various loading directions, nonlinear
regimes in stress-strain curves are considerably different. We found that Mo2C
sheets present negative Poisson's ratio and thus can be categorized as an
auxetic material. Our simulations also reveal that Mo2C films retain their
metallic electronic characteristic upon the uniaxial loading. We found that for
Mo2C nanomembranes the dielectric function becomes anisotropic along in-plane
and out-of plane directions. Our findings can be useful for the practical
application of Mo2C sheets in nanodevices.
",0,1,0,0,0,0
15041,15042,A formula goes to court: Partisan gerrymandering and the efficiency gap,"  Recently, a proposal has been advanced to detect unconstitutional partisan
gerrymandering with a simple formula called the efficiency gap. The efficiency
gap is now working its way towards a possible landmark case in the Supreme
Court. This note explores some of its mathematical properties in light of the
fact that it reduces to a straight proportional comparison of votes to seats.
Though we offer several critiques, we assess that EG can still be a useful
component of a courtroom analysis. But a famous formula can take on a life of
its own and this one will need to be watched closely.
",0,1,0,0,0,0
19146,19147,On topological fluid mechanics of non-ideal systems and virtual frozen-in dynamics,"  Euler and Navier-Stokes have variant systems with dynamical invariance of
helicity and thus (weak) topological equivalence, allowing a strong `frozen-in'
(to, or, dually, `Lie-carried' by the \textit{virtual} velocity $V$)
formulation of the vorticity with a flavor of `inverse Helmholtz theorem'. We
remark on the non-ideal (statistical) topological fluid mechanics (TFM) for (1)
the Constantin-Iyer formulation of Navier-Stokes, (2) our own extension of the
Gallavotti-Cohen type dynamical ensembles of modified Navier-Stokes with
energy-helicity constraints and (3) the Galerkin truncated Euler, as the
typical case variants with dynamical time reversibility and helicity
invariance. Ideal TFM is thus bridged with non-ideal flows. An example virtual
(Lie-)carrier of the vorticity in a Galerkin-truncated Euler system is
calculated to demonstrate the issue of determining $V$.
",0,1,0,0,0,0
10547,10548,"Spontaneous Octahedral Tilting in the Cubic Inorganic Caesium Halide Perovskites CsSnX$_3$ and CsPbX$_3$ (X = F, Cl, Br, I)","  The local crystal structures of many perovskite-structured materials deviate
from the average space group symmetry. We demonstrate, from lattice-dynamics
calculations based on quantum chemical force constants, that all the
caesium-lead and caesium-tin halide perovskites exhibit vibrational
instabilities associated with octahedral titling in their high-temperature
cubic phase. Anharmonic double-well potentials are found for zone-boundary
phonon modes in all compounds with barriers ranging from 108 to 512 meV. The
well depth is correlated with the tolerance factor and the chemistry of the
composition, but is not proportional to the imaginary harmonic phonon
frequency. We provide quantitative insights into the thermodynamic driving
forces and distinguish between dynamic and static disorder based on the
potential-energy landscape. A positive band gap deformation (spectral
blueshift) accompanies the structural distortion, with implications for
understanding the performance of these materials in applications areas
including solar cells and light-emitting diodes.
",0,1,0,0,0,0
16628,16629,Variational Bayesian Complex Network Reconstruction,"  Complex network reconstruction is a hot topic in many fields. A popular
data-driven reconstruction framework is based on lasso. However, it is found
that, in the presence of noise, it may be inefficient for lasso to determine
the network topology. This paper builds a new framework to cope with this
problem. The key idea is to employ a series of linear regression problems to
model the relationship between network nodes, and then to use an efficient
variational Bayesian method to infer the unknown coefficients. Based on the
obtained information, the network is finally reconstructed by determining
whether two nodes connect with each other or not. The numerical experiments
conducted with both synthetic and real data demonstrate that the new method
outperforms lasso with regard to both reconstruction accuracy and running
speed.
",1,0,0,0,0,0
8009,8010,Towards a Context-Aware IDE-Based Meta Search Engine for Recommendation about Programming Errors and Exceptions,"  Study shows that software developers spend about 19% of their time looking
for information in the web during software development and maintenance.
Traditional web search forces them to leave the working environment (e.g., IDE)
and look for information in the web browser. It also does not consider the
context of the problems that the developers search solutions for. The frequent
switching between web browser and the IDE is both time-consuming and
distracting, and the keyword-based traditional web search often does not help
much in problem solving. In this paper, we propose an Eclipse IDE-based web
search solution that exploits the APIs provided by three popular web search
engines-- Google, Yahoo, Bing and a popular programming Q & A site, Stack
Overflow, and captures the content-relevance, context-relevance, popularity and
search engine confidence of each candidate result against the encountered
programming problems. Experiments with 75 programming errors and exceptions
using the proposed approach show that inclusion of different types of context
information associated with a given exception can enhance the recommendation
accuracy of a given exception. Experiments both with two existing approaches
and existing web search engines confirm that our approach can perform better
than them in terms of recall, mean precision and other performance measures
with little computational cost.
",1,0,0,0,0,0
1871,1872,Transition probability of Brownian motion in the octant and its application to default modeling,"  We derive a semi-analytic formula for the transition probability of
three-dimensional Brownian motion in the positive octant with absorption at the
boundaries. Separation of variables in spherical coordinates leads to an
eigenvalue problem for the resulting boundary value problem in the two angular
components. The main theoretical result is a solution to the original problem
expressed as an expansion into special functions and an eigenvalue which has to
be chosen to allow a matching of the boundary condition. We discuss and test
several computational methods to solve a finite-dimensional approximation to
this nonlinear eigenvalue problem. Finally, we apply our results to the
computation of default probabilities and credit valuation adjustments in a
structural credit model with mutual liabilities.
",0,0,0,0,0,1
12730,12731,Word Embeddings via Tensor Factorization,"  Most popular word embedding techniques involve implicit or explicit
factorization of a word co-occurrence based matrix into low rank factors. In
this paper, we aim to generalize this trend by using numerical methods to
factor higher-order word co-occurrence based arrays, or \textit{tensors}. We
present four word embeddings using tensor factorization and analyze their
advantages and disadvantages. One of our main contributions is a novel joint
symmetric tensor factorization technique related to the idea of coupled tensor
factorization. We show that embeddings based on tensor factorization can be
used to discern the various meanings of polysemous words without being
explicitly trained to do so, and motivate the intuition behind why this works
in a way that doesn't with existing methods. We also modify an existing word
embedding evaluation metric known as Outlier Detection [Camacho-Collados and
Navigli, 2016] to evaluate the quality of the order-$N$ relations that a word
embedding captures, and show that tensor-based methods outperform existing
matrix-based methods at this task. Experimentally, we show that all of our word
embeddings either outperform or are competitive with state-of-the-art baselines
commonly used today on a variety of recent datasets. Suggested applications of
tensor factorization-based word embeddings are given, and all source code and
pre-trained vectors are publicly available online.
",1,0,0,1,0,0
1748,1749,Nonlinear Kalman Filtering with Divergence Minimization,"  We consider the nonlinear Kalman filtering problem using Kullback-Leibler
(KL) and $\alpha$-divergence measures as optimization criteria. Unlike linear
Kalman filters, nonlinear Kalman filters do not have closed form Gaussian
posteriors because of a lack of conjugacy due to the nonlinearity in the
likelihood. In this paper we propose novel algorithms to optimize the forward
and reverse forms of the KL divergence, as well as the alpha-divergence which
contains these two as limiting cases. Unlike previous approaches, our
algorithms do not make approximations to the divergences being optimized, but
use Monte Carlo integration techniques to derive unbiased algorithms for direct
optimization. We assess performance on radar and sensor tracking, and options
pricing problems, showing general improvement over the UKF and EKF, as well as
competitive performance with particle filtering.
",0,0,1,1,0,0
6031,6032,Split and Rephrase,"  We propose a new sentence simplification task (Split-and-Rephrase) where the
aim is to split a complex sentence into a meaning preserving sequence of
shorter sentences. Like sentence simplification, splitting-and-rephrasing has
the potential of benefiting both natural language processing and societal
applications. Because shorter sentences are generally better processed by NLP
systems, it could be used as a preprocessing step which facilitates and
improves the performance of parsers, semantic role labellers and machine
translation systems. It should also be of use for people with reading
disabilities because it allows the conversion of longer sentences into shorter
ones. This paper makes two contributions towards this new task. First, we
create and make available a benchmark consisting of 1,066,115 tuples mapping a
single complex sentence to a sequence of sentences expressing the same meaning.
Second, we propose five models (vanilla sequence-to-sequence to
semantically-motivated models) to understand the difficulty of the proposed
task.
",1,0,0,0,0,0
7930,7931,Exception-Based Knowledge Updates,"  Existing methods for dealing with knowledge updates differ greatly depending
on the underlying knowledge representation formalism. When Classical Logic is
used, updates are typically performed by manipulating the knowledge base on the
model-theoretic level. On the opposite side of the spectrum stand the semantics
for updating Answer-Set Programs that need to rely on rule syntax. Yet, a
unifying perspective that could embrace both these branches of research is of
great importance as it enables a deeper understanding of all involved methods
and principles and creates room for their cross-fertilisation, ripening and
further development.
This paper bridges the seemingly irreconcilable approaches to updates. It
introduces a novel monotonic characterisation of rules, dubbed RE-models, and
shows it to be a more suitable semantic foundation for rule updates than
SE-models. Then it proposes a generic scheme for specifying semantic rule
update operators, based on the idea of viewing a program as the set of sets of
RE-models of its rules; updates are performed by introducing additional
interpretations - exceptions - to the sets of RE-models of rules in the
original program. The introduced scheme is used to define rule update operators
that are closely related to both classical update principles and traditional
approaches to rules updates, and serve as a basis for a solution to the
long-standing problem of state condensing, showing how they can be equivalently
defined as binary operators on some class of logic programs.
Finally, the essence of these ideas is extracted to define an abstract
framework for exception-based update operators, viewing a knowledge base as the
set of sets of models of its elements, which can capture a wide range of both
model- and formula-based classical update operators, and thus serves as the
first firm formal ground connecting classical and rule updates.
",1,0,0,0,0,0
16472,16473,"On the isoperimetric constant, covariance inequalities and $L_p$-Poincaré inequalities in dimension one","  Firstly, we derive in dimension one a new covariance inequality of
$L_{1}-L_{\infty}$ type that characterizes the isoperimetric constant as the
best constant achieving the inequality. Secondly, we generalize our result to
$L_{p}-L_{q}$ bounds for the covariance. Consequently, we recover Cheeger's
inequality without using the co-area formula. We also prove a generalized
weighted Hardy type inequality that is needed to derive our covariance
inequalities and that is of independent interest. Finally, we explore some
consequences of our covariance inequalities for $L_{p}$-Poincaré
inequalities and moment bounds. In particular, we obtain optimal constants in
general $L_{p}$-Poincaré inequalities for measures with finite
isoperimetric constant, thus generalizing in dimension one Cheeger's
inequality, which is a $L_{p}$-Poincaré inequality for $p=2$, to any real
$p\geq 1$.
",0,0,1,1,0,0
16,17,The PdBI Arcsecond Whirlpool Survey (PAWS). The Role of Spiral Arms in Cloud and Star Formation,"  The process that leads to the formation of the bright star forming sites
observed along prominent spiral arms remains elusive. We present results of a
multi-wavelength study of a spiral arm segment in the nearby grand-design
spiral galaxy M51 that belongs to a spiral density wave and exhibits nine gas
spurs. The combined observations of the(ionized, atomic, molecular, dusty)
interstellar medium (ISM) with star formation tracers (HII regions, young
<10Myr stellar clusters) suggest (1) no variation in giant molecular cloud
(GMC) properties between arm and gas spurs, (2) gas spurs and extinction
feathers arising from the same structure with a close spatial relation between
gas spurs and ongoing/recent star formation (despite higher gas surface
densities in the spiral arm), (3) no trend in star formation age either along
the arm or along a spur, (4) evidence for strong star formation feedback in gas
spurs: (5) tentative evidence for star formation triggered by stellar feedback
for one spur, and (6) GMC associations (GMAs) being no special entities but the
result of blending of gas arm/spur cross-sections in lower resolution
observations. We conclude that there is no evidence for a coherent star
formation onset mechanism that can be solely associated to the presence of the
spiral density wave. This suggests that other (more localized) mechanisms are
important to delay star formation such that it occurs in spurs. The evidence of
star formation proceeding over several million years within individual spurs
implies that the mechanism that leads to star formation acts or is sustained
over a longer time-scale.
",0,1,0,0,0,0
3272,3273,Entanglement in topological systems,"  These lecture notes on entanglement in topological systems are part of the
48th IFF Spring School 2017 on Topological Matter: Topological Insulators,
Skyrmions and Majoranas at the Forschungszentrum Juelich, Germany. They cover a
short discussion on topologically ordered phases and review the two main tools
available for detecting topological order - the entanglement entropy and the
entanglement spectrum.
",0,1,0,0,0,0
7037,7038,Efficient enumeration of solutions produced by closure operations,"  In this paper we address the problem of generating all elements obtained by
the saturation of an initial set by some operations. More precisely, we prove
that we can generate the closure of a boolean relation (a set of boolean
vectors) by polymorphisms with a polynomial delay. Therefore we can compute
with polynomial delay the closure of a family of sets by any set of ""set
operations"": union, intersection, symmetric difference, subsets, supersets
$\dots$). To do so, we study the $Membership_{\mathcal{F}}$ problem: for a set
of operations $\mathcal{F}$, decide whether an element belongs to the closure
by $\mathcal{F}$ of a family of elements. In the boolean case, we prove that
$Membership_{\mathcal{F}}$ is in P for any set of boolean operations
$\mathcal{F}$. When the input vectors are over a domain larger than two
elements, we prove that the generic enumeration method fails, since
$Membership_{\mathcal{F}}$ is NP-hard for some $\mathcal{F}$. We also study the
problem of generating minimal or maximal elements of closures and prove that
some of them are related to well known enumeration problems such as the
enumeration of the circuits of a matroid or the enumeration of maximal
independent sets of a hypergraph. This article improves on previous works of
the same authors.
",1,0,0,0,0,0
15938,15939,Theory of Large Intrinsic Spin Hall Effect in Iridate Semimetals,"  We theoretically investigate the mechanism to generate large intrinsic spin
Hall effect in iridates or more broadly in 5d transition metal oxides with
strong spin-orbit coupling. We demonstrate such a possibility by taking the
example of orthorhombic perovskite iridate with nonsymmorphic lattice symmetry,
SrIrO$_3$, which is a three-dimensional semimetal with nodal line spectrum. It
is shown that large intrinsic spin Hall effect arises in this system via the
spin-Berry curvature originating from the nearly degenerate electronic spectra
surrounding the nodal line. This effect exists even when the nodal line is
gently gapped out, due to the persistent nearly degenerate electronic
structure, suggesting a distinct robustness. The magnitude of the spin Hall
conductivity is shown to be comparable to the best known example such as doped
topological insulators and the biggest in any transition metal oxides. To gain
further insight, we compute the intrinsic spin Hall conductivity in both of the
bulk and thin film systems. We find that the geometric confinement in thin
films leads to significant modifications of the electronic states, leading to
even bigger spin Hall conductivity in certain cases. We compare our findings
with the recent experimental report on the discovery of large spin Hall effect
in SrIrO$_3$ thin films.
",0,1,0,0,0,0
14074,14075,Onset of nonlinear structures due to eigenmode destabilization in tokamak plasmas,"  A general methodology is proposed to differentiate the likelihood of
energetic-particle-driven instabilities to produce frequency chirping or
fixed-frequency oscillations. The method employs numerically calculated
eigenstructures and multiple resonance surfaces of a given mode in the presence
of energetic ion drag and stochasticity (due to collisions and
micro-turbulence). Toroidicity-induced, reversed-shear and beta-induced
Alfven-acoustic eigenmodes are used as examples. Waves measured in experiments
are characterized and compatibility is found between the proposed criterion
predictions and the experimental observation or lack of observation of chirping
behavior of Alfvenic modes in different tokamaks. It is found that the
stochastic diffusion due to micro-turbulence can be the dominant energetic
particle detuning mechanism near the resonances in many plasma experiments, and
its strength is the key as to whether chirping solutions are likely to arise.
The proposed criterion constitutes a useful predictive tool in assessing
whether the nature of the transport for fast ion losses in fusion devices will
be dominated by convective or diffusive processes.
",0,1,0,0,0,0
14383,14384,Fixed points of morphisms among binary generalized pseudostandard words,"  We introduce a class of fixed points of primitive morphisms among aperiodic
binary generalized pseudostandard words. We conjecture that this class contains
all fixed points of primitive morphisms among aperiodic binary generalized
pseudostandard words that are not standard Sturmian words.
",0,0,1,0,0,0
9945,9946,Multiscale simulation on shearing transitions of thin-film lubrication with multi-layer molecules,"  Shearing transitions of multi-layer molecularly thin-film lubrication systems
in variations of the film-substrate coupling strength and the load are studied
by using a multiscale method. Three kinds of the interlayer slips found in
decreasing the coupling strength are in qualitative agreement with experimental
results. Although tribological behaviors are almost insensitive to the smaller
coupling strength, they and the effective film thickness are enlarged more and
more as the larger one increases. When the load increases, the tribological
behaviors are similar to those in increasing coupling strength, but the
effective film thickness is opposite.
",0,1,0,0,0,0
15113,15114,Techniques for Interpretable Machine Learning,"  Interpretable machine learning tackles the important problem that humans
cannot understand the behaviors of complex machine learning models and how
these models arrive at a particular decision. Although many approaches have
been proposed, a comprehensive understanding of the achievements and challenges
is still lacking. We provide a survey covering existing techniques to increase
the interpretability of machine learning models. We also discuss crucial issues
that the community should consider in future work such as designing
user-friendly explanations and developing comprehensive evaluation metrics to
further push forward the area of interpretable machine learning.
",0,0,0,1,0,0
6149,6150,Chalcogenide Glass-on-Graphene Photonics,"  Two-dimensional (2-D) materials are of tremendous interest to integrated
photonics given their singular optical characteristics spanning light emission,
modulation, saturable absorption, and nonlinear optics. To harness their
optical properties, these atomically thin materials are usually attached onto
prefabricated devices via a transfer process. In this paper, we present a new
route for 2-D material integration with planar photonics. Central to this
approach is the use of chalcogenide glass, a multifunctional material which can
be directly deposited and patterned on a wide variety of 2-D materials and can
simultaneously function as the light guiding medium, a gate dielectric, and a
passivation layer for 2-D materials. Besides claiming improved fabrication
yield and throughput compared to the traditional transfer process, our
technique also enables unconventional multilayer device geometries optimally
designed for enhancing light-matter interactions in the 2-D layers.
Capitalizing on this facile integration method, we demonstrate a series of
high-performance glass-on-graphene devices including ultra-broadband on-chip
polarizers, energy-efficient thermo-optic switches, as well as graphene-based
mid-infrared (mid-IR) waveguide-integrated photodetectors and modulators.
",0,1,0,0,0,0
3919,3920,Asymptotic Enumeration of Compacted Binary Trees,"  A compacted tree is a graph created from a binary tree such that repeatedly
occurring subtrees in the original tree are represented by pointers to existing
ones, and hence every subtree is unique. Such representations form a special
class of directed acyclic graphs. We are interested in the asymptotic number of
compacted trees of given size, where the size of a compacted tree is given by
the number of its internal nodes. Due to its superexponential growth this
problem poses many difficulties. Therefore we restrict our investigations to
compacted trees of bounded right height, which is the maximal number of edges
going to the right on any path from the root to a leaf.
We solve the asymptotic counting problem for this class as well as a closely
related, further simplified class.
For this purpose, we develop a calculus on exponential generating functions
for compacted trees of bounded right height and for relaxed trees of bounded
right height, which differ from compacted trees by dropping the above described
uniqueness condition. This enables us to derive a recursively defined sequence
of differential equations for the exponential generating functions. The
coefficients can then be determined by performing a singularity analysis of the
solutions of these differential equations.
Our main results are the computation of the asymptotic numbers of relaxed as
well as compacted trees of bounded right height and given size, when the size
tends to infinity.
",1,0,0,0,0,0
16484,16485,Purity and separation for oriented matroids,"  Leclerc and Zelevinsky, motivated by the study of quasi-commuting quantum
flag minors, introduced the notions of strongly separated and weakly separated
collections. These notions are closely related to the theory of cluster
algebras, to the combinatorics of the double Bruhat cells, and to the totally
positive Grassmannian.
A key feature, called the purity phenomenon, is that every maximal by
inclusion strongly (resp., weakly) separated collection of subsets in $[n]$ has
the same cardinality.
In this paper, we extend these notions and define $\mathcal{M}$-separated
collections, for any oriented matroid $\mathcal{M}$.
We show that maximal by size $\mathcal{M}$-separated collections are in
bijection with fine zonotopal tilings (if $\mathcal{M}$ is a realizable
oriented matroid), or with one-element liftings of $\mathcal{M}$ in general
position (for an arbitrary oriented matroid).
We introduce the class of pure oriented matroids for which the purity
phenomenon holds: an oriented matroid $\mathcal{M}$ is pure if
$\mathcal{M}$-separated collections form a pure simplicial complex, i.e., any
maximal by inclusion $\mathcal{M}$-separated collection is also maximal by
size.
We pay closer attention to several special classes of oriented matroids:
oriented matroids of rank $3$, graphical oriented matroids, and uniform
oriented matroids. We classify pure oriented matroids in these cases. An
oriented matroid of rank $3$ is pure if and only if it is a positroid (up to
reorienting and relabeling its ground set). A graphical oriented matroid is
pure if and only if its underlying graph is an outerplanar graph, that is, a
subgraph of a triangulation of an $n$-gon.
We give a simple conjectural characterization of pure oriented matroids by
forbidden minors and prove it for the above classes of matroids (rank $3$,
graphical, uniform).
",0,0,1,0,0,0
9690,9691,Subconvex bounds for Hecke-Maass forms on compact arithmetic quotients of semisimple Lie groups,"  Let $H$ be a semisimple algebraic group, $K$ a maximal compact subgroup of
$G:=H(\mathbb{R})$, and $\Gamma\subset H(\mathbb{Q})$ a congruence arithmetic
subgroup. In this paper, we generalize existing subconvex bounds for
Hecke-Maass forms on the locally symmetric space $\Gamma \backslash G/K$ to
corresponding bounds on the arithmetic quotient $\Gamma \backslash G$ for
cocompact lattices using the spectral function of an elliptic operator. The
bounds obtained extend known subconvex bounds for automorphic forms to
non-trivial $K$-types, yielding subconvex bounds for new classes of automorphic
representations, and constitute subconvex bounds for eigenfunctions on compact
manifolds with both positive and negative sectional curvature. We also obtain
new subconvex bounds for holomorphic modular forms in the weight aspect.
",0,0,1,0,0,0
3795,3796,"Volvox barberi flocks, forming near-optimal, two-dimensional, polydisperse lattice packings","  Volvox barberi is a multicellular green alga forming spherical colonies of
10000-50000 differentiated somatic and germ cells. Here, I show that these
colonies actively self-organize over minutes into ""flocks"" that can contain
more than 100 colonies moving and rotating collectively for hours. The colonies
in flocks form two-dimensional, irregular, ""active crystals"", with lattice
angles and colony diameters both following log-normal distributions. Comparison
with a dynamical simulation of soft spheres with diameters matched to the
Volvox samples, and a weak long-range attractive force, show that the Volvox
flocks achieve optimal random close-packing. A dye tracer in the Volvox medium
revealed large hydrodynamic vortices generated by colony and flock rotations,
providing a likely source of the forces leading to flocking and optimal
packing.
",0,0,0,0,1,0
10430,10431,Exploring Cosmic Origins with CORE: Survey requirements and mission design,"  Future observations of cosmic microwave background (CMB) polarisation have
the potential to answer some of the most fundamental questions of modern
physics and cosmology. In this paper, we list the requirements for a future CMB
polarisation survey addressing these scientific objectives, and discuss the
design drivers of the CORE space mission proposed to ESA in answer to the ""M5""
call for a medium-sized mission. The rationale and options, and the
methodologies used to assess the mission's performance, are of interest to
other future CMB mission design studies. CORE is designed as a near-ultimate
CMB polarisation mission which, for optimal complementarity with ground-based
observations, will perform the observations that are known to be essential to
CMB polarisation scienceand cannot be obtained by any other means than a
dedicated space mission.
",0,1,0,0,0,0
236,237,Tuning across the BCS-BEC crossover in the multiband superconductor Fe$_{1+y}$Se$_x$Te$_{1-x}$ : An angle-resolved photoemission study,"  The crossover from Bardeen-Cooper-Schrieffer (BCS) superconductivity to
Bose-Einstein condensation (BEC) is difficult to realize in quantum materials
because, unlike in ultracold atoms, one cannot tune the pairing interaction. We
realize the BCS-BEC crossover in a nearly compensated semimetal
Fe$_{1+y}$Se$_x$Te$_{1-x}$ by tuning the Fermi energy, $\epsilon_F$, via
chemical doping, which permits us to systematically change $\Delta /
\epsilon_F$ from 0.16 to 0.5 were $\Delta$ is the superconducting (SC) gap. We
use angle-resolved photoemission spectroscopy to measure the Fermi energy, the
SC gap and characteristic changes in the SC state electronic dispersion as the
system evolves from a BCS to a BEC regime. Our results raise important
questions about the crossover in multiband superconductors which go beyond
those addressed in the context of cold atoms.
",0,1,0,0,0,0
5380,5381,Moonshine: Distilling with Cheap Convolutions,"  Many engineers wish to deploy modern neural networks in memory-limited
settings; but the development of flexible methods for reducing memory use is in
its infancy, and there is little knowledge of the resulting cost-benefit. We
propose structural model distillation for memory reduction using a strategy
that produces a student architecture that is a simple transformation of the
teacher architecture: no redesign is needed, and the same hyperparameters can
be used. Using attention transfer, we provide Pareto curves/tables for
distillation of residual networks with four benchmark datasets, indicating the
memory versus accuracy payoff. We show that substantial memory savings are
possible with very little loss of accuracy, and confirm that distillation
provides student network performance that is better than training that student
architecture directly on data.
",1,0,0,1,0,0
14739,14740,The search for superheavy elements: Historical and philosophical perspectives,"  The heaviest of the transuranic elements known as superheavy elements (SHE)
are produced in nuclear reactions in a few specialized laboratories located in
Germany, the U.S., Russia, and Japan. The history of this branch of physical
science provides several case studies of interest to the philosophy and
sociology of modern science. The story of SHE illuminates the crucial notion of
what constitutes a chemical element, what the criteria are for discovering a
new element, and how an element is assigned a name. The story also cast light
on the sometimes uneasy relationship between physics and chemistry. It is far
from obvious that elements with Z > 110 exist in the same sense as oxygen or
sodium exists. The answers are not given by nature but by international
commissions responsible for the criteria and evaluation of discovery claims.
The works of these commissions and of SHE research in general have often been
controversial.
",0,1,0,0,0,0
20314,20315,Cellulyzer - Automated analysis and interactive visualization/simulation of select cellular processes,"  Here we report on a set of programs developed at the ZMBH Bio-Imaging
Facility for tracking real-life images of cellular processes. These programs
perform 1) automated tracking; 2) quantitative and comparative track analyses
of different images in different groups; 3) different interactive visualization
schemes; and 4) interactive realistic simulation of different cellular
processes for validation and optimal problem-specific adjustment of image
acquisition parameters (tradeoff between speed, resolution, and quality with
feedback from the very final results). The collection of programs is primarily
developed for the common bio-image analysis software ImageJ (as a single Java
Plugin). Some programs are also available in other languages (C++ and
Javascript) and may be run simply with a web-browser; even on a low-end Tablet
or Smartphone. The programs are available at
this https URL
",1,1,0,0,0,0
18858,18859,A Model Order Reduction Algorithm for Estimating the Absorption Spectrum,"  The ab initio description of the spectral interior of the absorption spectrum
poses both a theoretical and computational challenge for modern electronic
structure theory. Due to the often spectrally dense character of this domain in
the quantum propagator's eigenspectrum for medium-to-large sized systems,
traditional approaches based on the partial diagonalization of the propagator
often encounter oscillatory and stagnating convergence. Electronic structure
methods which solve the molecular response problem through the solution of
spectrally shifted linear systems, such as the complex polarization propagator,
offer an alternative approach which is agnostic to the underlying spectral
density or domain location. This generality comes at a seemingly high
computational cost associated with solving a large linear system for each
spectral shift in some discretization of the spectral domain of interest. We
present a novel, adaptive solution based on model order reduction techniques
via interpolation. Model order reduction reduces the computational complexity
of mathematical models and is ubiquitous in the simulation of dynamical
systems. The efficiency and effectiveness of the proposed algorithm in the ab
initio prediction of X-Ray absorption spectra is demonstrated using a test set
of challenging water clusters which are spectrally dense in the neighborhood of
the oxygen K-edge. Based on a single, user defined tolerance we automatically
determine the order of the reduced models and approximate the absorption
spectrum up to the given tolerance. We also illustrate that the automatically
determined model order increases logarithmically with the problem dimension,
compared to a linear increase of the number of eigenvalues within the energy
window. Furthermore, we observed that the computational cost of the proposed
algorithm only scales quadratically with respect to the problem dimension.
",1,1,0,0,0,0
8248,8249,Dipolar phonons and electronic screening in monolayer FeSe on SrTiO$_3$,"  Monolayer films of FeSe grown on SrTiO$_3$ substrates exhibit significantly
higher superconducting transition temperatures than those of bulk FeSe.
Interaction of electrons in the FeSe layer with dipolar SrTiO$_3$ phonons has
been suggested as the cause of the enhanced transition temperature. In this
paper we systematically study the coupling of SrTiO$_3$ longitudinal optical
phonons to the FeSe electron, including also electron-electron Coulomb
interactions at the random phase approximation level. We find that the
electron-phonon interaction between FeSe and SrTiO$_3$ substrate is almost
entirely screened by the electronic fluctuations in the FeSe monolayer, so that
the net electron-phonon interaction is very weak and unlikely to lead to
superconductivity.
",0,1,0,0,0,0
16811,16812,Deep scattering transform applied to note onset detection and instrument recognition,"  Automatic Music Transcription (AMT) is one of the oldest and most
well-studied problems in the field of music information retrieval. Within this
challenging research field, onset detection and instrument recognition take
important places in transcription systems, as they respectively help to
determine exact onset times of notes and to recognize the corresponding
instrument sources. The aim of this study is to explore the usefulness of
multiscale scattering operators for these two tasks on plucked string
instrument and piano music. After resuming the theoretical background and
illustrating the key features of this sound representation method, we evaluate
its performances comparatively to other classical sound representations. Using
both MIDI-driven datasets with real instrument samples and real musical pieces,
scattering is proved to outperform other sound representations for these AMT
subtasks, putting forward its richer sound representation and invariance
properties.
",1,0,0,1,0,0
10948,10949,Gradient Coding from Cyclic MDS Codes and Expander Graphs,"  Gradient coding is a technique for straggler mitigation in distributed
learning. In this paper we design novel gradient codes using tools from
classical coding theory, namely, cyclic MDS codes, which compare favourably
with existing solutions, both in the applicable range of parameters and in the
complexity of the involved algorithms. Second, we introduce an approximate
variant of the gradient coding problem, in which we settle for approximate
gradient computation instead of the exact one. This approach enables graceful
degradation, i.e., the $\ell_2$ error of the approximate gradient is a
decreasing function of the number of stragglers. Our main result is that the
normalized adjacency matrix of an expander graph can yield excellent
approximate gradient codes, and that this approach allows us to perform
significantly less computation compared to exact gradient coding. We
experimentally test our approach on Amazon EC2, and show that the
generalization error of approximate gradient coding is very close to the full
gradient while requiring significantly less computation from the workers.
",0,0,0,1,0,0
4389,4390,On the Fourth Power Moment of Fourier Coefficients of Cusp Form,"  Let $a(n)$ be the Fourier coefficients of a holomorphic cusp form of weight
$\kappa=2n\geqslant12$ for the full modular group and
$A(x)=\sum\limits_{n\leqslant x}a(n)$. In this paper, we establish an
asymptotic formula of the fourth power moment of $A(x)$ and prove that
\begin{equation*}
\int_1^TA^4(x)\mathrm{d}x=\frac{3}{64\kappa\pi^4}s_{4;2}(\tilde{a})
T^{2\kappa}+O\big(T^{2\kappa-\delta_4+\varepsilon}\big) \end{equation*} with
$\delta_4=1/8$, which improves the previous result.
",0,0,1,0,0,0
7100,7101,K-closedness of weighted Hardy spaces on the two-dimensional torus,"  It is proved that, under certain restrictions on weights, a pair of weighted
Hardy spaces on the two-dimensional torus is K-closed in the pair of the
corresponding weighted Lebesgue spaces. By now, K-closedness of Hardy spaces on
the two-dimensional torus was considered either in the case of no weights or in
the case of weights that split into a product of two functions of one variable
(the so-called ""split weights""). Here the case of certain nonsplit weights is
studied.
",0,0,1,0,0,0
15874,15875,Real-Time Object Pose Estimation with Pose Interpreter Networks,"  In this work, we introduce pose interpreter networks for 6-DoF object pose
estimation. In contrast to other CNN-based approaches to pose estimation that
require expensively annotated object pose data, our pose interpreter network is
trained entirely on synthetic pose data. We use object masks as an intermediate
representation to bridge real and synthetic. We show that when combined with a
segmentation model trained on RGB images, our synthetically trained pose
interpreter network is able to generalize to real data. Our end-to-end system
for object pose estimation runs in real-time (20 Hz) on live RGB data, without
using depth information or ICP refinement.
",1,0,0,0,0,0
19425,19426,On the nearly smooth complex spaces,"  We introduce a class of normal complex spaces having only mild sin-gularities
(close to quotient singularities) for which we generalize the notion of a
(analytic) fundamental class for an analytic cycle and also the notion of a
relative fundamental class for an analytic family of cycles. We also generalize
to these spaces the geometric intersection theory for analytic cycles with
rational positive coefficients and show that it behaves well with respect to
analytic families of cycles. We prove that this intersection theory has most of
the usual properties of the standard geometric intersection theory on complex
manifolds, but with the exception that the intersection cycle of two cycles
with positive integral coefficients that intersect properly may have rational
coefficients. AMS classification. 32 C 20-32 C 25-32 C 36.
",0,0,1,0,0,0
2327,2328,Admissibility of solution estimators for stochastic optimization,"  We look at stochastic optimization problems through the lens of statistical
decision theory. In particular, we address admissibility, in the statistical
decision theory sense, of the natural sample average estimator for a stochastic
optimization problem (which is also known as the empirical risk minimization
(ERM) rule in learning literature). It is well known that for general
stochastic optimization problems, the sample average estimator may not be
admissible. This is known as Stein's paradox in the statistics literature. We
show in this paper that for optimizing stochastic linear functions over compact
sets, the sample average estimator is admissible.
",0,0,1,1,0,0
5409,5410,Service Providers of the Sharing Economy: Who Joins and Who Benefits?,"  Many ""sharing economy"" platforms, such as Uber and Airbnb, have become
increasingly popular, providing consumers with more choices and suppliers a
chance to make profit. They, however, have also brought about emerging issues
regarding regulation, tax obligation, and impact on urban environment, and have
generated heated debates from various interest groups. Empirical studies
regarding these issues are limited, partly due to the unavailability of
relevant data. Here we aim to understand service providers of the sharing
economy, investigating who joins and who benefits, using the Airbnb market in
the United States as a case study. We link more than 211 thousand Airbnb
listings owned by 188 thousand hosts with demographic, socio-economic status
(SES), housing, and tourism characteristics. We show that income and education
are consistently the two most influential factors that are linked to the
joining of Airbnb, regardless of the form of participation or year. Areas with
lower median household income, or higher fraction of residents who have
Bachelor's and higher degrees, tend to have more hosts. However, when
considering the performance of listings, as measured by number of newly
received reviews, we find that income has a positive effect for entire-home
listings; listings located in areas with higher median household income tend to
have more new reviews. Our findings demonstrate empirically that the
disadvantage of SES-disadvantaged areas and the advantage of SES-advantaged
areas may be present in the sharing economy.
",1,0,0,0,0,0
2408,2409,Microfluidics for Chemical Synthesis: Flow Chemistry,"  Klavs F. Jensen is Warren K. Lewis Professor in Chemical Engineering and
Materials Science and Engineering at the Massachusetts Institute of Technology.
Here he describes the use of microfluidics for chemical synthesis, from the
early demonstration examples to the current efforts with automated droplet
microfluidic screening and optimization techniques.
",0,0,0,0,1,0
1019,1020,A Bag-of-Paths Node Criticality Measure,"  This work compares several node (and network) criticality measures
quantifying to which extend each node is critical with respect to the
communication flow between nodes of the network, and introduces a new measure
based on the Bag-of-Paths (BoP) framework. Network disconnection simulation
experiments show that the new BoP measure outperforms all the other measures on
a sample of Erdos-Renyi and Albert-Barabasi graphs. Furthermore, a faster
(still O(n^3)), approximate, BoP criticality relying on the Sherman-Morrison
rank-one update of a matrix is introduced for tackling larger networks. This
approximate measure shows similar performances as the original, exact, one.
",1,1,0,0,0,0
20670,20671,Parallelizing Over Artificial Neural Network Training Runs with Multigrid,"  Artificial neural networks are a popular and effective machine learning
technique. Great progress has been made parallelizing the expensive training
phase of an individual network, leading to highly specialized pieces of
hardware, many based on GPU-type architectures, and more concurrent algorithms
such as synthetic gradients. However, the training phase continues to be a
bottleneck, where the training data must be processed serially over thousands
of individual training runs. This work considers a multigrid reduction in time
(MGRIT) algorithm that is able to parallelize over the thousands of training
runs and converge to the exact same solution as traditional training would
provide. MGRIT was originally developed to provide parallelism for time
evolution problems that serially step through a finite number of time-steps.
This work recasts the training of a neural network similarly, treating neural
network training as an evolution equation that evolves the network weights from
one step to the next. Thus, this work concerns distributed computing approaches
for neural networks, but is distinct from other approaches which seek to
parallelize only over individual training runs. The work concludes with
supporting numerical results for two model problems.
",1,0,0,0,0,0
9085,9086,Spatial Random Sampling: A Structure-Preserving Data Sketching Tool,"  Random column sampling is not guaranteed to yield data sketches that preserve
the underlying structures of the data and may not sample sufficiently from
less-populated data clusters. Also, adaptive sampling can often provide
accurate low rank approximations, yet may fall short of producing descriptive
data sketches, especially when the cluster centers are linearly dependent.
Motivated by that, this paper introduces a novel randomized column sampling
tool dubbed Spatial Random Sampling (SRS), in which data points are sampled
based on their proximity to randomly sampled points on the unit sphere. The
most compelling feature of SRS is that the corresponding probability of
sampling from a given data cluster is proportional to the surface area the
cluster occupies on the unit sphere, independently from the size of the cluster
population. Although it is fully randomized, SRS is shown to provide
descriptive and balanced data representations. The proposed idea addresses a
pressing need in data science and holds potential to inspire many novel
approaches for analysis of big data.
",1,0,0,1,0,0
19634,19635,The ALMA Early Science View of FUor/EXor objects. III. The Slow and Wide Outflow of V883 Ori,"  We present Atacama Large Millimeter/ sub-millimeter Array (ALMA) observations
of V883 Ori, an FU Ori object. We describe the molecular outflow and envelope
of the system based on the $^{12}$CO and $^{13}$CO emissions, which together
trace a bipolar molecular outflow. The C$^{18}$O emission traces the rotational
motion of the circumstellar disk. From the $^{12}$CO blue-shifted emission, we
estimate a wide opening angle of $\sim$ 150$^{^{\circ}}$ for the outflow
cavities. Also, we find that the outflow is very slow (characteristic velocity
of only 0.65 km~s$^{-1}$), which is unique for an FU Ori object. We calculate
the kinematic properties of the outflow in the standard manner using the
$^{12}$CO and $^{13}$CO emissions. In addition, we present a P Cygni profile
observed in the high-resolution optical spectrum, evidence of a wind driven by
the accretion and being the cause for the particular morphology of the
outflows. We discuss the implications of our findings and the rise of these
slow outflows during and/or after the formation of a rotationally supported
disk.
",0,1,0,0,0,0
14222,14223,Loss Landscapes of Regularized Linear Autoencoders,"  Autoencoders are a deep learning model for representation learning. When
trained to minimize the Euclidean distance between the data and its
reconstruction, linear autoencoders (LAEs) learn the subspace spanned by the
top principal directions but cannot learn the principal directions themselves.
In this paper, we prove that $L_2$-regularized LAEs learn the principal
directions as the left singular vectors of the decoder, providing an extremely
simple and scalable algorithm for rank-$k$ SVD. More generally, we consider
LAEs with (i) no regularization, (ii) regularization of the composition of the
encoder and decoder, and (iii) regularization of the encoder and decoder
separately. We relate the minimum of (iii) to the MAP estimate of probabilistic
PCA and show that for all critical points the encoder and decoder are
transposes. Building on topological intuition, we smoothly parameterize the
critical manifolds for all three losses via a novel unified framework and
illustrate these results empirically. Overall, this work clarifies the
relationship between autoencoders and Bayesian models and between
regularization and orthogonality.
",1,0,0,1,0,0
5659,5660,WYS*: A DSL for Verified Secure Multi-party Computations,"  Secure multi-party computation (MPC) enables a set of mutually distrusting
parties to cooperatively compute, using a cryptographic protocol, a function
over their private data. This paper presents Wys*, a new domain-specific
language (DSL) for writing mixed-mode MPCs. Wys* is an embedded DSL hosted in
F*, a verification-oriented, effectful programming language. Wys* source
programs are essentially F* programs written in a custom MPC effect, meaning
that the programmers can use F*'s logic to verify the correctness and security
properties of their programs. To reason about the distributed runtime semantics
of these programs, we formalize a deep embedding of Wys*, also in F*. We
mechanize the necessary metatheory to prove that the properties verified for
the Wys* source programs carry over to the distributed, multi-party semantics.
Finally, we use F*'s extraction to extract an interpreter that we have proved
matches this semantics, yielding a partially verified implementation. Wys* is
the first DSL to enable formal verification of MPC programs. We have
implemented several MPC protocols in Wys*, including private set intersection,
joint median, and an MPC card dealing application, and have verified their
correctness and security.
",1,0,0,0,0,0
19301,19302,Quantifying genuine multipartite correlations and their pattern complexity,"  We propose an information-theoretic framework to quantify multipartite
correlations in classical and quantum systems, answering questions such as:
what is the amount of seven-partite correlations in a given state of ten
particles? We identify measures of genuine multipartite correlations, i.e.
statistical dependencies which cannot be ascribed to bipartite correlations,
satisfying a set of desirable properties. Inspired by ideas developed in
complexity science, we then introduce the concept of weaving to classify states
which display different correlation patterns, but cannot be distinguished by
correlation measures. The weaving of a state is defined as the weighted sum of
correlations of every order. Weaving measures are good descriptors of the
complexity of correlation structures in multipartite systems.
",0,1,0,0,0,0
7758,7759,Quaternionic Projective Bundle Theorem and Gysin Triangle in MW-Motivic Cohomology,"  In this paper, we show that the motive $HP^n$ of the quaternionic
Grassmannian (as defined by I. Panin and C. Walter) splits in the category of
effective MW-motives (as defined by B. Calmès, F. Déglise and J. Fasel).
Moreover, we extend this result to an arbitrary symplectic bundle, obtaining
the so-called quaternionic projective bundle theorem. This enables us to define
Pontryagin classes of symplectic bundles in the Chow-Witt ring.
As an application, we prove that there is a Gysin triangle in MW-motivic
cohomology in case the normal bundle is symplectic.
",0,0,1,0,0,0
10689,10690,cGAN-based Manga Colorization Using a Single Training Image,"  The Japanese comic format known as Manga is popular all over the world. It is
traditionally produced in black and white, and colorization is time consuming
and costly. Automatic colorization methods generally rely on greyscale values,
which are not present in manga. Furthermore, due to copyright protection,
colorized manga available for training is scarce. We propose a manga
colorization method based on conditional Generative Adversarial Networks
(cGAN). Unlike previous cGAN approaches that use many hundreds or thousands of
training images, our method requires only a single colorized reference image
for training, avoiding the need of a large dataset. Colorizing manga using
cGANs can produce blurry results with artifacts, and the resolution is limited.
We therefore also propose a method of segmentation and color-correction to
mitigate these issues. The final results are sharp, clear, and in high
resolution, and stay true to the character's original color scheme.
",1,0,0,0,0,0
6970,6971,Self-organization and the Maximum Empower Principle in the Framework of max-plus Algebra,"  Self-organization is a process where order of a whole system arises out of
local interactions between small components of a system.
Emergy, defined as the amount of (solar) energy used to make a product or a
service, is becoming an important ecological indicator. To explain observed
self-organization of systems by emergy the Maximum Empower Principle (MEP) was
proposed initially without a mathematical formulation.
Emergy analysis is based on four rules called emergy algebra. Most of emergy
computations in steady state are in fact approximate results, which rely on
linear algebra. In such a context, a mathematical formulation of the MEP has
been proposed by Giannantoni (2002).
In 2012 Le Corre and the second author of this paper have proposed a rigorous
mathematical framework for emergy analysis. They established that the exact
computation of emergy is based on the so-called max-plus algebra and seven
coherent axioms that replace the emergy algebra. In this paper the MEP in
steady state is formalized in the context of the max-plus algebra and graph
theory. The main concepts of the paper are (a) a particular graph called
'emergy graph', (b) the notion of compatible paths of the emergy graph, and (c)
sets of compatible paths, which are called 'emergy states'. The main results of
the paper are as follows:
(1) Emergy is mathematically expressed as a maximum over all possible emergy
states. (2) The maximum is always reached by an emergy state. (3) Only prevail
emergy states for which the maximum is reached.
",1,1,0,0,0,0
17576,17577,A note on primitive $1-$normal elements over finite fields,"  Let $q$ be a prime power of a prime $p$, $n$ a positive integer and $\mathbb
F_{q^n}$ the finite field with $q^n$ elements. The $k-$normal elements over
finite fields were introduced and characterized by Huczynska et al (2013).
Under the condition that $n$ is not divisible by $p$, they obtained an
existence result on primitive $1-$normal elements of $\mathbb F_{q^n}$ over
$\mathbb F_q$ for $q>2$. In this note, we extend their result to the excluded
case $q=2$.
",0,0,1,0,0,0
15256,15257,A strongly convergent numerical scheme from Ensemble Kalman inversion,"  The Ensemble Kalman methodology in an inverse problems setting can be viewed
as an iterative scheme, which is a weakly tamed discretization scheme for a
certain stochastic differential equation (SDE). Assuming a suitable
approximation result, dynamical properties of the SDE can be rigorously pulled
back via the discrete scheme to the original Ensemble Kalman inversion.
The results of this paper make a step towards closing the gap of the missing
approximation result by proving a strong convergence result in a simplified
model of a scalar stochastic differential equation. We focus here on a toy
model with similar properties than the one arising in the context of Ensemble
Kalman filter. The proposed model can be interpreted as a single particle
filter for a linear map and thus forms the basis for further analysis. The
difficulty in the analysis arises from the formally derived limiting SDE with
non-globally Lipschitz continuous nonlinearities both in the drift and in the
diffusion. Here the standard Euler-Maruyama scheme might fail to provide a
strongly convergent numerical scheme and taming is necessary. In contrast to
the strong taming usually used, the method presented here provides a weaker
form of taming.
We present a strong convergence analysis by first proving convergence on a
domain of high probability by using a cut-off or localisation, which then
leads, combined with bounds on moments for both the SDE and the numerical
scheme, by a bootstrapping argument to strong convergence.
",0,0,1,0,0,0
8172,8173,On the selection of polynomials for the DLP algorithm,"  In this paper we characterize the set of polynomials $f\in\mathbb F_q[X]$
satisfying the following property: there exists a positive integer $d$ such
that for any positive integer $\ell$ less or equal than the degree of $f$,
there exists $t_0$ in $\mathbb F_{q^d}$ such that the polynomial $f-t_0$ has an
irreducible factor of degree $\ell$ over $\mathbb F_{q^d}[X]$. This result is
then used to progress in the last step which is needed to remove the heuristic
from one of the quasi-polynomial time algorithms for discrete logarithm
problems (DLP) in small characteristic. Our characterization allows a
construction of polynomials satisfying the wanted property.
",1,0,1,0,0,0
14067,14068,Penalty-based spatial smoothing and outlier detection for childhood obesity surveillance from electronic health records,"  Childhood obesity is associated with increased morbidity and mortality in
adulthood, leading to substantial healthcare cost. There is an urgent need to
promote early prevention and develop an accompanying surveillance system. In
this paper, we make use of electronic health records (EHRs) and construct a
penalized multi-level generalized linear model. The model provides regular
trend and outlier information simultaneously, both of which may be useful to
raise public awareness and facilitate targeted intervention. Our strategy is to
decompose the regional contribution in the model into smooth and sparse
signals, where the characteristics of the signals are encouraged by the
combination of fusion and sparse penalties imposed on the likelihood function.
In addition, we introduce a weighting scheme to account for the missingness and
potential non-representativeness arising from the EHRs data. We propose a novel
alternating minimization algorithm, which is computationally efficient, easy to
implement, and guarantees convergence. Simulation shows that the proposed
method has a superior performance compared with traditional counterparts.
Finally, we apply our method to the University of Wisconsin Population Health
Information Exchange database.
",0,0,0,1,0,0
3600,3601,Statistical Inference with Local Optima,"  We study the statistical properties of an estimator derived by applying a
gradient ascent method with multiple initializations to a multi-modal
likelihood function. We derive the population quantity that is the target of
this estimator and study the properties of confidence intervals (CIs)
constructed from asymptotic normality and the bootstrap approach. In
particular, we analyze the coverage deficiency due to finite number of random
initializations. We also investigate the CIs by inverting the likelihood ratio
test, the score test, and the Wald test, and we show that the resulting CIs may
be very different. We provide a summary of the uncertainties that we need to
consider while making inference about the population. Note that we do not
provide a solution to the problem of multiple local maxima; instead, our goal
is to investigate the effect from local maxima on the behavior of our
estimator. In addition, we analyze the performance of the EM algorithm under
random initializations and derive the coverage of a CI with a finite number of
initializations. Finally, we extend our analysis to a nonparametric mode
hunting problem.
",0,0,0,1,0,0
20044,20045,"Instanton bundles on the flag variety F(0,1,2)","  Instanton bundles on $\mathbb{P}^3$ have been at the core of the research in
Algebraic Geometry during the last thirty years. Motivated by the recent
extension of their definition to other Fano threefolds of Picard number one, we
develop the theory of instanton bundles on the complete flag variety
$F:=F(0,1,2)$ of point-lines on $\mathbb{P}^2$. After giving for them two
different monadic presentations, we use it to show that the moduli space
$MI_F(k)$ of instanton bundles of charge $k$ is a geometric GIT quotient with a
generically smooth component of dim $8k-3$. Finally we study their locus of
jumping conics.
",0,0,1,0,0,0
4598,4599,The sequential loss of allelic diversity,"  This paper gives a new flavor of what Peter Jagers and his co-authors call
`the path to extinction'. In a neutral population with constant size $N$, we
assume that each individual at time $0$ carries a distinct type, or allele. We
consider the joint dynamics of these $N$ alleles, for example the dynamics of
their respective frequencies and more plainly the nonincreasing process
counting the number of alleles remaining by time $t$. We call this process the
extinction process. We show that in the Moran model, the extinction process is
distributed as the process counting (in backward time) the number of common
ancestors to the whole population, also known as the block counting process of
the $N$-Kingman coalescent. Stimulated by this result, we investigate: (1)
whether it extends to an identity between the frequencies of blocks in the
Kingman coalescent and the frequencies of alleles in the extinction process,
both evaluated at jump times; (2) whether it extends to the general case of
$\Lambda$-Fleming-Viot processes.
",0,0,0,0,1,0
19538,19539,"Construction,sensitivity index, and synchronization speed of optimal networks","  The stability (or instability) of synchronization is important in a number of
real world systems, including the power grid, the human brain and biological
cells. For identical synchronization, the synchronizability of a network, which
can be measured by the range of coupling strength that admits stable
synchronization, can be optimized for a given number of nodes and links.
Depending on the geometric degeneracy of the Laplacian eigenvectors, optimal
networks can be classified into different sensitivity levels, which we define
as a network's sensitivity index. We introduce an efficient and explicit way to
construct optimal networks of arbitrary size over a wide range of sensitivity
and link densities. Using coupled chaotic oscillators, we study synchronization
dynamics on optimal networks, showing that cospectral optimal networks can have
drastically different speed of synchronization. Such difference in dynamical
stability is found to be closely related to the different structural
sensitivity of these networks: generally, networks with high sensitivity index
are slower to synchronize, and, surprisingly, may not synchronize at all,
despite being theoretically stable under linear stability analysis.
",0,1,1,0,0,0
17204,17205,Bidirectional Evaluation with Direct Manipulation,"  We present an evaluation update (or simply, update) algorithm for a
full-featured functional programming language, which synthesizes program
changes based on output changes. Intuitively, the update algorithm retraces the
steps of the original evaluation, rewriting the program as needed to reconcile
differences between the original and updated output values. Our approach,
furthermore, allows expert users to define custom lenses that augment the
update algorithm with more advanced or domain-specific program updates.
To demonstrate the utility of evaluation update, we implement the algorithm
in Sketch-n-Sketch, a novel direct manipulation programming system for
generating HTML documents. In Sketch-n-Sketch, the user writes an ML-style
functional program to generate HTML output. When the user directly manipulates
the output using a graphical user interface, the update algorithm reconciles
the changes. We evaluate bidirectional evaluation in Sketch-n-Sketch by
authoring ten examples comprising approximately 1400 lines of code in total.
These examples demonstrate how a variety of HTML documents and applications can
be developed and edited interactively in Sketch-n-Sketch, mitigating the
tedious edit-run-view cycle in traditional programming environments.
",1,0,0,0,0,0
157,158,Increasing the Reusability of Enforcers with Lifecycle Events,"  Runtime enforcement can be effectively used to improve the reliability of
software applications. However, it often requires the definition of ad hoc
policies and enforcement strategies, which might be expensive to identify and
implement. This paper discusses how to exploit lifecycle events to obtain
useful enforcement strategies that can be easily reused across applications,
thus reducing the cost of adoption of the runtime enforcement technology. The
paper finally sketches how this idea can be used to define libraries that can
automatically overcome problems related to applications misusing them.
",1,0,0,0,0,0
17261,17262,Adaptive Real-Time Software Defined MIMO Visible Light Communications using Spatial Multiplexing and Spatial Diversity,"  In this paper, we experimentally demonstrate a real-time software defined
multiple input multiple output (MIMO) visible light communication (VLC) system
employing link adaptation of spatial multiplexing and spatial diversity.
Real-time MIMO signal processing is implemented by using the Field Programmable
Gate Array (FPGA) based Universal Software Radio Peripheral (USRP) devices.
Software defined implantation of MIMO VLC can assist in enabling an adaptive
and reconfigurable communication system without hardware changes. We measured
the error vector magnitude (EVM), bit error rate (BER) and spectral efficiency
performance for single carrier M-QAM MIMO VLC using spatial diversity and
spatial multiplexing. Results show that spatial diversity MIMO VLC improves
error performance at the cost of spectral efficiency that spatial multiplexing
should enhance. We propose the adaptive MIMO solution that both modulation
schema and MIMO schema are dynamically adapted to the changing channel
conditions for enhancing the error performance and spectral efficiency. The
average error-free spectral efficiency of adaptive 2x2 MIMO VLC achieved 12
b/s/Hz over 2 meters indoor dynamic transmission.
",1,0,1,0,0,0
14967,14968,ARTENOLIS: Automated Reproducibility and Testing Environment for Licensed Software,"  Motivation:
Automatically testing changes to code is an essential feature of continuous
integration. For open-source code, without licensed dependencies, a variety of
continuous integration services exist. The COnstraint-Based Reconstruction and
Analysis (COBRA) Toolbox is a suite of open-source code for computational
modelling with dependencies on licensed software. A novel automated framework
of continuous integration in a semi-licensed environment is required for the
development of the COBRA Toolbox and related tools of the COBRA community.
Results:
ARTENOLIS is a general-purpose infrastructure software application that
implements continuous integration for open-source software with licensed
dependencies. It uses a master-slave framework, tests code on multiple
operating systems, and multiple versions of licensed software dependencies.
ARTENOLIS ensures the stability, integrity, and cross-platform compatibility of
code in the COBRA Toolbox and related tools.
Availability and Implementation:
The continuous integration server, core of the reproducibility and testing
infrastructure, can be freely accessed under artenolis.lcsb.uni.lu. The
continuous integration framework code is located in the /.ci directory and at
the root of the repository freely available under
github.com/opencobra/cobratoolbox.
",1,0,0,0,0,0
10476,10477,Simultaneously Learning Neighborship and Projection Matrix for Supervised Dimensionality Reduction,"  Explicitly or implicitly, most of dimensionality reduction methods need to
determine which samples are neighbors and the similarity between the neighbors
in the original highdimensional space. The projection matrix is then learned on
the assumption that the neighborhood information (e.g., the similarity) is
known and fixed prior to learning. However, it is difficult to precisely
measure the intrinsic similarity of samples in high-dimensional space because
of the curse of dimensionality. Consequently, the neighbors selected according
to such similarity might and the projection matrix obtained according to such
similarity and neighbors are not optimal in the sense of classification and
generalization. To overcome the drawbacks, in this paper we propose to let the
similarity and neighbors be variables and model them in low-dimensional space.
Both the optimal similarity and projection matrix are obtained by minimizing a
unified objective function. Nonnegative and sum-to-one constraints on the
similarity are adopted. Instead of empirically setting the regularization
parameter, we treat it as a variable to be optimized. It is interesting that
the optimal regularization parameter is adaptive to the neighbors in
low-dimensional space and has intuitive meaning. Experimental results on the
YALE B, COIL-100, and MNIST datasets demonstrate the effectiveness of the
proposed method.
",0,0,0,1,0,0
13428,13429,ProSLAM: Graph SLAM from a Programmer's Perspective,"  In this paper we present ProSLAM, a lightweight stereo visual SLAM system
designed with simplicity in mind. Our work stems from the experience gathered
by the authors while teaching SLAM to students and aims at providing a highly
modular system that can be easily implemented and understood. Rather than
focusing on the well known mathematical aspects of Stereo Visual SLAM, in this
work we highlight the data structures and the algorithmic aspects that one
needs to tackle during the design of such a system. We implemented ProSLAM
using the C++ programming language in combination with a minimal set of well
known used external libraries. In addition to an open source implementation, we
provide several code snippets that address the core aspects of our approach
directly in this paper. The results of a thorough validation performed on
standard benchmark datasets show that our approach achieves accuracy comparable
to state of the art methods, while requiring substantially less computational
resources.
",1,0,0,0,0,0
5600,5601,Manipulative Elicitation -- A New Attack on Elections with Incomplete Preferences,"  Lu and Boutilier proposed a novel approach based on ""minimax regret"" to use
classical score based voting rules in the setting where preferences can be any
partial (instead of complete) orders over the set of alternatives. We show here
that such an approach is vulnerable to a new kind of manipulation which was not
present in the classical (where preferences are complete orders) world of
voting. We call this attack ""manipulative elicitation."" More specifically, it
may be possible to (partially) elicit the preferences of the agents in a way
that makes some distinguished alternative win the election who may not be a
winner if we elicit every preference completely. More alarmingly, we show that
the related computational task is polynomial time solvable for a large class of
voting rules which includes all scoring rules, maximin, Copeland$^\alpha$ for
every $\alpha\in[0,1]$, simplified Bucklin voting rules, etc. We then show that
introducing a parameter per pair of alternatives which specifies the minimum
number of partial preferences where this pair of alternatives must be
comparable makes the related computational task of manipulative elicitation
\NPC for all common voting rules including a class of scoring rules which
includes the plurality, $k$-approval, $k$-veto, veto, and Borda voting rules,
maximin, Copeland$^\alpha$ for every $\alpha\in[0,1]$, and simplified Bucklin
voting rules. Hence, in this work, we discover a fundamental vulnerability in
using minimax regret based approach in partial preferential setting and propose
a novel way to tackle it.
",1,0,0,0,0,0
5213,5214,Efficient Correlated Topic Modeling with Topic Embedding,"  Correlated topic modeling has been limited to small model and problem sizes
due to their high computational cost and poor scaling. In this paper, we
propose a new model which learns compact topic embeddings and captures topic
correlations through the closeness between the topic vectors. Our method
enables efficient inference in the low-dimensional embedding space, reducing
previous cubic or quadratic time complexity to linear w.r.t the topic size. We
further speedup variational inference with a fast sampler to exploit sparsity
of topic occurrence. Extensive experiments show that our approach is capable of
handling model and data scales which are several orders of magnitude larger
than existing correlation results, without sacrificing modeling quality by
providing competitive or superior performance in document classification and
retrieval.
",1,0,0,1,0,0
10947,10948,Few-Shot Learning with Metric-Agnostic Conditional Embeddings,"  Learning high quality class representations from few examples is a key
problem in metric-learning approaches to few-shot learning. To accomplish this,
we introduce a novel architecture where class representations are conditioned
for each few-shot trial based on a target image. We also deviate from
traditional metric-learning approaches by training a network to perform
comparisons between classes rather than relying on a static metric comparison.
This allows the network to decide what aspects of each class are important for
the comparison at hand. We find that this flexible architecture works well in
practice, achieving state-of-the-art performance on the Caltech-UCSD birds
fine-grained classification task.
",0,0,0,1,0,0
1222,1223,Robust Bayesian Optimization with Student-t Likelihood,"  Bayesian optimization has recently attracted the attention of the automatic
machine learning community for its excellent results in hyperparameter tuning.
BO is characterized by the sample efficiency with which it can optimize
expensive black-box functions. The efficiency is achieved in a similar fashion
to the learning to learn methods: surrogate models (typically in the form of
Gaussian processes) learn the target function and perform intelligent sampling.
This surrogate model can be applied even in the presence of noise; however, as
with most regression methods, it is very sensitive to outlier data. This can
result in erroneous predictions and, in the case of BO, biased and inefficient
exploration. In this work, we present a GP model that is robust to outliers
which uses a Student-t likelihood to segregate outliers and robustly conduct
Bayesian optimization. We present numerical results evaluating the proposed
method in both artificial functions and real problems.
",1,0,0,1,0,0
20158,20159,An Empirical Analysis of Proximal Policy Optimization with Kronecker-factored Natural Gradients,"  In this technical report, we consider an approach that combines the PPO
objective and K-FAC natural gradient optimization, for which we call PPOKFAC.
We perform a range of empirical analysis on various aspects of the algorithm,
such as sample complexity, training speed, and sensitivity to batch size and
training epochs. We observe that PPOKFAC is able to outperform PPO in terms of
sample complexity and speed in a range of MuJoCo environments, while being
scalable in terms of batch size. In spite of this, it seems that adding more
epochs is not necessarily helpful for sample efficiency, and PPOKFAC seems to
be worse than its A2C counterpart, ACKTR.
",0,0,0,1,0,0
14190,14191,Morphological Simplification of Archaeological Fracture Surfaces,"  We propose to employ scale spaces of mathematical morphology to
hierarchically simplify fracture surfaces of complementarily fitting
archaeological fragments. This representation preserves contact and is
insensitive to different kinds of abrasion affecting the exact complementarity
of the original fragments. We present a pipeline for morphologically
simplifying fracture surfaces, based on their Lipschitz nature; its core is a
new embedding of fracture surfaces to simultaneously compute both closing and
opening morphological operations, using distance transforms.
",1,0,0,0,0,0
12183,12184,Imputation Approaches for Animal Movement Modeling,"  The analysis of telemetry data is common in animal ecological studies. While
the collection of telemetry data for individual animals has improved
dramatically, the methods to properly account for inherent uncertainties (e.g.,
measurement error, dependence, barriers to movement) have lagged behind. Still,
many new statistical approaches have been developed to infer unknown quantities
affecting animal movement or predict movement based on telemetry data.
Hierarchical statistical models are useful to account for some of the
aforementioned uncertainties, as well as provide population-level inference,
but they often come with an increased computational burden. For certain types
of statistical models, it is straightforward to provide inference if the latent
true animal trajectory is known, but challenging otherwise. In these cases,
approaches related to multiple imputation have been employed to account for the
uncertainty associated with our knowledge of the latent trajectory. Despite the
increasing use of imputation approaches for modeling animal movement, the
general sensitivity and accuracy of these methods have not been explored in
detail. We provide an introduction to animal movement modeling and describe how
imputation approaches may be helpful for certain types of models. We also
assess the performance of imputation approaches in a simulation study. Our
simulation study suggests that inference for model parameters directly related
to the location of an individual may be more accurate than inference for
parameters associated with higher-order processes such as velocity or
acceleration. Finally, we apply these methods to analyze a telemetry data set
involving northern fur seals (Callorhinus ursinus) in the Bering Sea.
",0,0,0,1,0,0
10672,10673,Singular branched covers of four-manifolds,"  Consider a dihedral cover $f: Y\to X$ with $X$ and $Y$ four-manifolds and $f$
branched along an oriented surface embedded in $X$ with isolated cone
singularities. We prove that only a slice knot can arise as the unique
singularity on an irregular dihedral cover $f: Y\to S^4$ if $Y$ is homotopy
equivalent to $\mathbb{CP}^2$ and construct an explicit infinite family of such
covers with $Y$ diffeomorphic to $\mathbb{CP}^2$. An obstruction to a knot
being homotopically ribbon arises in this setting, and we describe a class of
potential counter-examples to the Slice-Ribbon Conjecture.
Our tools include lifting a trisection of a singularly embedded surface in a
four-manifold $X$ to obtain a trisection of the corresponding irregular
dihedral branched cover of $X$, when such a cover exists. We also develop a
combinatorial procedure to compute, using a formula by the second author, the
contribution to the signature of the covering manifold which results from the
presence of a singularity on the branching set.
",0,0,1,0,0,0
6277,6278,The connected countable spaces of Bing and Ritter are topologically homogeneous,"  Answering a problem posed by the second author on Mathoverflow, we prove that
the connected countable Hausdorff spaces constructed by Bing and Ritter are
topologically homogeneous.
",0,0,1,0,0,0
2359,2360,The Informativeness of $k$-Means and Dimensionality Reduction for Learning Mixture Models,"  The learning of mixture models can be viewed as a clustering problem. Indeed,
given data samples independently generated from a mixture of distributions, we
often would like to find the correct target clustering of the samples according
to which component distribution they were generated from. For a clustering
problem, practitioners often choose to use the simple k-means algorithm.
k-means attempts to find an optimal clustering which minimizes the
sum-of-squared distance between each point and its cluster center. In this
paper, we provide sufficient conditions for the closeness of any optimal
clustering and the correct target clustering assuming that the data samples are
generated from a mixture of log-concave distributions. Moreover, we show that
under similar or even weaker conditions on the mixture model, any optimal
clustering for the samples with reduced dimensionality is also close to the
correct target clustering. These results provide intuition for the
informativeness of k-means (with and without dimensionality reduction) as an
algorithm for learning mixture models. We verify the correctness of our
theorems using numerical experiments and demonstrate using datasets with
reduced dimensionality significant speed ups for the time required to perform
clustering.
",1,0,0,1,0,0
3943,3944,AutoShuffleNet: Learning Permutation Matrices via an Exact Lipschitz Continuous Penalty in Deep Convolutional Neural Networks,"  ShuffleNet is a state-of-the-art light weight convolutional neural network
architecture. Its basic operations include group, channel-wise convolution and
channel shuffling. However, channel shuffling is manually designed empirically.
Mathematically, shuffling is a multiplication by a permutation matrix. In this
paper, we propose to automate channel shuffling by learning permutation
matrices in network training. We introduce an exact Lipschitz continuous
non-convex penalty so that it can be incorporated in the stochastic gradient
descent to approximate permutation at high precision. Exact permutations are
obtained by simple rounding at the end of training and are used in inference.
The resulting network, referred to as AutoShuffleNet, achieved improved
classification accuracies on CIFAR-10 and ImageNet data sets. In addition, we
found experimentally that the standard convex relaxation of permutation
matrices into stochastic matrices leads to poor performance. We prove
theoretically the exactness (error bounds) in recovering permutation matrices
when our penalty function is zero (very small). We present examples of
permutation optimization through graph matching and two-layer neural network
models where the loss functions are calculated in closed analytical form. In
the examples, convex relaxation failed to capture permutations whereas our
penalty succeeded.
",1,0,0,1,0,0
3675,3676,Robustness of persistent currents in two-dimensional Dirac systems with disorders,"  We consider two-dimensional (2D) Dirac quantum ring systems formed by the
infinite mass constraint. When an Aharonov-Bohm magnetic flux is present, e.g.,
through the center of the ring domain, persistent currents, i.e., permanent
currents without dissipation, can arise. In real materials, impurities and
defects are inevitable, raising the issue of robustness of the persistent
currents. Using localized random potential to simulate the disorders, we
investigate how the ensemble averaged current magnitude varies with the
disorder density. For comparison, we study the nonrelativistic quantum
counterpart by analyzing the solutions of the Schrödinger equation under
the same geometrical and disorder settings. We find that, for the Dirac ring
system, as the disorder density is systematically increased, the average
current decreases slowly initially and then plateaus at a finite nonzero value,
indicating remarkable robustness of the persistent currents. The physical
mechanism responsible for the robustness is the emergence of a class of
boundary states - whispering gallery modes. In contrast, in the Schrödinger
ring system, such boundary states cannot form and the currents diminish rapidly
to zero with increase in the disorder density. We develop a physical theory
based on a quasi one-dimensional approximation to understand the striking
contrast in the behaviors of the persistent currents in the Dirac and
Schrödinger rings. Our 2D Dirac ring systems with disorders can be
experimentally realized, e.g., on the surface of a topological insulator with
natural or deliberately added impurities from the fabrication process.
",0,1,0,0,0,0
11155,11156,A New Approach of Exploiting Self-Adjoint Matrix Polynomials of Large Random Matrices for Anomaly Detection and Fault Location,"  Synchronized measurements of a large power grid enable an unprecedented
opportunity to study the spatialtemporal correlations. Statistical analytics
for those massive datasets start with high-dimensional data matrices.
Uncertainty is ubiquitous in a future's power grid. These data matrices are
recognized as random matrices. This new point of view is fundamental in our
theoretical analysis since true covariance matrices cannot be estimated
accurately in a high-dimensional regime. As an alternative, we consider
large-dimensional sample covariance matrices in the asymptotic regime to
replace the true covariance matrices. The self-adjoint polynomials of
large-dimensional random matrices are studied as statistics for big data
analytics. The calculation of the asymptotic spectrum distribution (ASD) for
such a matrix polynomial is understandably challenging. This task is made
possible by a recent breakthrough in free probability, an active research
branch in random matrix theory. This is the very reason why the work of this
paper is inspired initially. The new approach is interesting in many aspects.
The mathematical reason may be most critical. The real-world problems can be
solved using this approach, however.
",0,0,0,1,0,0
5684,5685,Attacking Similarity-Based Link Prediction in Social Networks,"  Link prediction is one of the fundamental problems in computational social
science. A particularly common means to predict existence of unobserved links
is via structural similarity metrics, such as the number of common neighbors;
node pairs with higher similarity are thus deemed more likely to be linked.
However, a number of applications of link prediction, such as predicting links
in gang or terrorist networks, are adversarial, with another party incentivized
to minimize its effectiveness by manipulating observed information about the
network. We offer a comprehensive algorithmic investigation of the problem of
attacking similarity-based link prediction through link deletion, focusing on
two broad classes of such approaches, one which uses only local information
about target links, and another which uses global network information. While we
show several variations of the general problem to be NP-Hard for both local and
global metrics, we exhibit a number of well-motivated special cases which are
tractable. Additionally, we provide principled and empirically effective
algorithms for the intractable cases, in some cases proving worst-case
approximation guarantees.
",1,0,0,0,0,0
13376,13377,Flashes of Hidden Worlds at Colliders,"  (This is a general physics level overview article about hidden sectors, and
how they motivate searches for long-lived particles. Intended for publication
in Physics Today.)
Searches for new physics at the Large Hadron Collider have so far come up
empty, but we just might not be looking in the right place. Spectacular bursts
of particles appearing seemingly out of nowhere could shed light on some of
nature's most profound mysteries.
",0,1,0,0,0,0
8664,8665,Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments,"  A robot that can carry out a natural-language instruction has been a dream
since before the Jetsons cartoon series imagined a life of leisure mediated by
a fleet of attentive robot helpers. It is a dream that remains stubbornly
distant. However, recent advances in vision and language methods have made
incredible progress in closely related areas. This is significant because a
robot interpreting a natural-language navigation instruction on the basis of
what it sees is carrying out a vision and language process that is similar to
Visual Question Answering. Both tasks can be interpreted as visually grounded
sequence-to-sequence translation problems, and many of the same methods are
applicable. To enable and encourage the application of vision and language
methods to the problem of interpreting visually-grounded navigation
instructions, we present the Matterport3D Simulator -- a large-scale
reinforcement learning environment based on real imagery. Using this simulator,
which can in future support a range of embodied vision and language tasks, we
provide the first benchmark dataset for visually-grounded natural language
navigation in real buildings -- the Room-to-Room (R2R) dataset.
",1,0,0,0,0,0
512,513,Handling state space explosion in verification of component-based systems: A review,"  Component-based design is a different way of constructing systems which
offers numerous benefits, in particular, decreasing the complexity of system
design. However, deploying components into a system is a challenging and
error-prone task. Model checking is one of the reliable methods that
automatically and systematically analyse the correctness of a given system. Its
brute-force check of the state space significantly expands the level of
confidence in the system. Nevertheless, model checking is limited by a critical
problem so-called State Space Explosion (SSE). To benefit from model checking,
appropriate methods to reduce SSE, is required. In two last decades, a great
number of methods to mitigate the state space explosion have been proposed
which have many similarities, dissimilarities, and unclear concepts in some
cases. This research, firstly, aims at present a review and brief discussion of
the methods of handling SSE problem and classify them based on their
similarities, principle and characteristics. Second, it investigates the
methods for handling SSE problem in verifying Component-based system (CBS) and
provides insight into CBS verification limitations that have not been addressed
yet. The analysis in this research has revealed the patterns, specific
features, and gaps in the state-of-the-art methods. In addition, we identified
and discussed suitable methods to soften SSE problem in CBS and underlined the
key challenges for future research efforts.
",1,0,1,0,0,0
16167,16168,New insights into non-central beta distributions,"  The beta family owes its privileged status within unit interval distributions
to several relevant features such as, for example, easyness of interpretation
and versatility in modeling different types of data. However, its flexibility
at the unit interval endpoints is poor enough to prevent from properly modeling
the portions of data having values next to zero and one. Such a drawback can be
overcome by resorting to the class of the non-central beta distributions.
Indeed, the latter allows the density to take on arbitrary positive and finite
limits which have a really simple form. That said, new insights into such class
are provided in this paper. In particular, new representations and moments
expressions are derived. Moreover, its potential with respect to alternative
models is highlighted through applications to real data.
",0,0,1,1,0,0
6627,6628,Self-similar groups of type FP_{n},"  We construct new classes of self-similar groups : S-aritmetic groups, affine
groups and metabelian groups. Most of the soluble ones are finitely presented
and of type FP_{n} for appropriate n.
",0,0,1,0,0,0
12676,12677,Impact of Feature Selection on Micro-Text Classification,"  Social media datasets, especially Twitter tweets, are popular in the field of
text classification. Tweets are a valuable source of micro-text (sometimes
referred to as ""micro-blogs""), and have been studied in domains such as
sentiment analysis, recommendation systems, spam detection, clustering, among
others. Tweets often include keywords referred to as ""Hashtags"" that can be
used as labels for the tweet. Using tweets encompassing 50 labels, we studied
the impact of word versus character-level feature selection and extraction on
different learners to solve a multi-class classification task. We show that
feature extraction of simple character-level groups performs better than simple
word groups and pre-processing methods like normalizing using Porter's Stemming
and Part-of-Speech (""POS"")-Lemmatization.
",1,0,0,0,0,0
14988,14989,Dose finding for new vaccines: the role for immunostimulation/immunodynamic modelling,"  Current methods to optimize vaccine dose are purely empirically based,
whereas in the drug development field, dosing determinations use far more
advanced quantitative methodology to accelerate decision-making. Applying these
established methods in the field of vaccine development may reduce the
currently large clinical trial sample sizes, long time frames, high costs, and
ultimately have a better potential to save lives. We propose the field of
immunostimulation/immunodynamic (IS/ID) modelling, which aims to translate
mathematical frameworks used for drug dosing towards optimizing vaccine dose
decision-making. Analogous to PK/PD modelling, IS/ID modelling approaches apply
mathematical models to describe the underlying mechanisms by which the immune
response is stimulated by vaccination (IS) and the resulting measured immune
response dynamics (ID). To move IS/ID modelling forward, existing datasets and
further data on vaccine allometry and dose-dependent dynamics need to be
generated and collate, requiring a collaborative environment with input from
academia, industry, regulators, governmental and non-governmental agencies to
share modelling expertise, and connect modellers to vaccine data.
",0,0,0,0,1,0
8166,8167,Decay of Solutions to the Maxwell Equations on Schwarzschild-de Sitter Spacetimes,"  In this work, we consider solutions of the Maxwell equations on the
Schwarzschild-de Sitter family of black hole spacetimes. We prove that, in the
static region bounded by black hole and cosmological horizons, solutions of the
Maxwell equations decay to stationary Coulomb solutions at a super-polynomial
rate, with decay measured according to ingoing and outgoing null coordinates.
Our method employs a differential transformation of Maxwell tensor components
to obtain higher-order quantities satisfying a Fackerell-Ipser equation, in the
style of Chandrasekhar and the more recent work of Pasqualotto. The analysis of
the Fackerell-Ipser equation is accomplished by means of the vector field
method, with decay estimates for the higher-order quantities leading to decay
estimates for components of the Maxwell tensor.
",0,0,1,0,0,0
7286,7287,The Parameterized Complexity of Positional Games,"  We study the parameterized complexity of several positional games. Our main
result is that Short Generalized Hex is W[1]-complete parameterized by the
number of moves. This solves an open problem from Downey and Fellows'
influential list of open problems from 1999. Previously, the problem was
thought of as a natural candidate for AW[*]-completeness. Our main tool is a
new fragment of first-order logic where universally quantified variables only
occur in inequalities. We show that model-checking on arbitrary relational
structures for a formula in this fragment is W[1]-complete when parameterized
by formula size. We also consider a general framework where a positional game
is represented as a hypergraph and two players alternately pick vertices. In a
Maker-Maker game, the first player to have picked all the vertices of some
hyperedge wins the game. In a Maker-Breaker game, the first player wins if she
picks all the vertices of some hyperedge, and the second player wins otherwise.
In an Enforcer-Avoider game, the first player wins if the second player picks
all the vertices of some hyperedge, and the second player wins otherwise. Short
Maker-Maker is AW[*]-complete, whereas Short Maker-Breaker is W[1]-complete and
Short Enforcer-Avoider co-W[1]-complete parameterized by the number of moves.
This suggests a rough parameterized complexity categorization into positional
games that are complete for the first level of the W-hierarchy when the winning
configurations only depend on which vertices one player has been able to pick,
but AW[*]-completeness when the winning condition depends on which vertices
both players have picked. However, some positional games where the board and
the winning configurations are highly structured are fixed-parameter tractable.
We give another example of such a game, Short k-Connect, which is
fixed-parameter tractable when parameterized by the number of moves.
",1,0,0,0,0,0
6397,6398,General Bayesian Updating and the Loss-Likelihood Bootstrap,"  In this paper we revisit the weighted likelihood bootstrap, a method that
generates samples from an approximate Bayesian posterior of a parametric model.
We show that the same method can be derived, without approximation, under a
Bayesian nonparametric model with the parameter of interest defined as
minimising an expected negative log-likelihood under an unknown sampling
distribution. This interpretation enables us to extend the weighted likelihood
bootstrap to posterior sampling for parameters minimizing an expected loss. We
call this method the loss-likelihood bootstrap. We make a connection between
this and general Bayesian updating, which is a way of updating prior belief
distributions without needing to construct a global probability model, yet
requires the calibration of two forms of loss function. The loss-likelihood
bootstrap is used to calibrate the general Bayesian posterior by matching
asymptotic Fisher information. We demonstrate the methodology on a number of
examples.
",0,0,0,1,0,0
7833,7834,Multi-Level Network Embedding with Boosted Low-Rank Matrix Approximation,"  As opposed to manual feature engineering which is tedious and difficult to
scale, network representation learning has attracted a surge of research
interests as it automates the process of feature learning on graphs. The
learned low-dimensional node vector representation is generalizable and eases
the knowledge discovery process on graphs by enabling various off-the-shelf
machine learning tools to be directly applied. Recent research has shown that
the past decade of network embedding approaches either explicitly factorize a
carefully designed matrix to obtain the low-dimensional node vector
representation or are closely related to implicit matrix factorization, with
the fundamental assumption that the factorized node connectivity matrix is
low-rank. Nonetheless, the global low-rank assumption does not necessarily hold
especially when the factorized matrix encodes complex node interactions, and
the resultant single low-rank embedding matrix is insufficient to capture all
the observed connectivity patterns. In this regard, we propose a novel
multi-level network embedding framework BoostNE, which can learn multiple
network embedding representations of different granularity from coarse to fine
without imposing the prevalent global low-rank assumption. The proposed BoostNE
method is also in line with the successful gradient boosting method in ensemble
learning as multiple weak embeddings lead to a stronger and more effective one.
We assess the effectiveness of the proposed BoostNE framework by comparing it
with existing state-of-the-art network embedding methods on various datasets,
and the experimental results corroborate the superiority of the proposed
BoostNE network embedding framework.
",1,0,0,1,0,0
6979,6980,On (in)stabilities of perturbations in mimetic models with higher derivatives,"  Usually when applying the mimetic model to the early universe, higher
derivative terms are needed to promote the mimetic field to be dynamical.
However such models suffer from the ghost and/or the gradient instabilities and
simple extensions cannot cure this pathology. We point out in this paper that
it is possible to overcome this difficulty by considering the direct couplings
of the higher derivatives of the mimetic field to the curvature of the
spacetime.
",0,1,0,0,0,0
5558,5559,"Toward construction of a consistent field theory with Poincare covariance in terms of step-function-type basis functions showing confinement/deconfinement, mass-gap and Regge trajectory for non-pure/pure non-Abelian gauge fields","  This article is a review by the authors concerning the construction of a
Poincar${\rm \acute{e}}$ covariant (owing to spacetime continuum)
field-theoretic formalism in terms of step-function-type basis functions
without ultraviolet divergences. This formalism analytically derives
confinement/deconfinement, mass-gap and Regge trajectory for non-Abelian gauge
fields, and gives solutions for self-interacting scalar fields. Fields
propagate in spacetime continuum and fields with finite degrees of freedom
toward continuum limit have no ultraviolet divergence. Basis functions defined
in a parameter spacetime are mapped to real spacetime. The authors derive a new
solution comprised of classical fields as a vacuum and quantum fluctuations,
leading to the linear potential between the particle and antiparticle from the
Wilson loop. The Polyakov line gives finite binding energies and reveals the
deconfining property at high temperatures. The quantum action yields positive
mass from the classical fields and quantum fluctuations produces the Coulomb
potential. Pure Yang-Mills fields show the same mass-gap owing to the
particle-antiparticle pair creation. The Dirac equation under linear potential
is analytically solved in this formalism, reproducing the principal properties
of Regge trajectories at a quantum level. Further outlook mentions a
possibility of the difference between conventional continuum and present wave
functions responsible for the cosmological constant.
",0,1,0,0,0,0
19077,19078,Follow the Compressed Leader: Faster Online Learning of Eigenvectors and Faster MMWU,"  The online problem of computing the top eigenvector is fundamental to machine
learning. In both adversarial and stochastic settings, previous results (such
as matrix multiplicative weight update, follow the regularized leader, follow
the compressed leader, block power method) either achieve optimal regret but
run slow, or run fast at the expense of loosing a $\sqrt{d}$ factor in total
regret where $d$ is the matrix dimension.
We propose a $\textit{follow-the-compressed-leader (FTCL)}$ framework which
achieves optimal regret without sacrificing the running time. Our idea is to
""compress"" the matrix strategy to dimension 3 in the adversarial setting, or
dimension 1 in the stochastic setting. These respectively resolve two open
questions regarding the design of optimal and efficient algorithms for the
online eigenvector problem.
",1,0,1,1,0,0
11629,11630,Off The Beaten Lane: AI Challenges In MOBAs Beyond Player Control,"  MOBAs represent a huge segment of online gaming and are growing as both an
eSport and a casual genre. The natural starting point for AI researchers
interested in MOBAs is to develop an AI to play the game better than a human -
but MOBAs have many more challenges besides adversarial AI. In this paper we
introduce the reader to the wider context of MOBA culture, propose a range of
challenges faced by the community today, and posit concrete AI projects that
can be undertaken to begin solving them.
",1,0,0,0,0,0
9676,9677,Processes accompanying stimulated recombination of atoms,"  The phenomenon of polarization of nuclei in the process of stimulated
recombination of atoms in the field of circularly polarized laser radiation is
considered. This effect is considered for the case of the proton-electron beams
used in the method of electron cooling. An estimate is obtained for the maximum
degree of polarization of the protons on components of the hyperfine structure
of the 2s state of the hydrogen atom.
",0,1,0,0,0,0
12870,12871,A review of possible effects of cognitive biases on interpretation of rule-based machine learning models,"  This paper investigates to what extent cognitive biases may affect human
understanding of interpretable machine learning models, in particular of rules
discovered from data. Twenty cognitive biases are covered, as are possible
debiasing techniques that can be adopted by designers of machine learning
algorithms and software. Our review transfers results obtained in cognitive
psychology to the domain of machine learning, aiming to bridge the current gap
between these two areas. It needs to be followed by empirical studies
specifically aimed at the machine learning domain.
",0,0,0,1,0,0
13073,13074,Excitonic mass gap in uniaxially strained graphene,"  We study the conditions for spontaneously generating an excitonic mass gap
due to Coulomb interactions between anisotropic Dirac fermions in uniaxially
strained graphene. The mass gap equation is realized as a self-consistent
solution for the self-energy within the Hartree-Fock mean-field and static
random phase approximations. It depends not only on momentum, due to the
long-range nature of the interaction, but also on the velocity anisotropy
caused by the presence of uniaxial strain. We solve the nonlinear integral
equation self-consistently by performing large scale numerical calculations on
variable grid sizes. We evaluate the mass gap at the charge neutrality (Dirac)
point as a function of the dimensionless coupling constant and anisotropy
parameter. We also obtain the phase diagram of the critical coupling, at which
the gap becomes finite, against velocity anisotropy. Our numerical study
indicates that with an increase in uniaxial strain in graphene the strength of
critical coupling decreases, which suggests anisotropy supports formation of
excitonic mass gap in graphene.
",0,1,0,0,0,0
2844,2845,Bayesian uncertainty quantification in linear models for diffusion MRI,"  Diffusion MRI (dMRI) is a valuable tool in the assessment of tissue
microstructure. By fitting a model to the dMRI signal it is possible to derive
various quantitative features. Several of the most popular dMRI signal models
are expansions in an appropriately chosen basis, where the coefficients are
determined using some variation of least-squares. However, such approaches lack
any notion of uncertainty, which could be valuable in e.g. group analyses. In
this work, we use a probabilistic interpretation of linear least-squares
methods to recast popular dMRI models as Bayesian ones. This makes it possible
to quantify the uncertainty of any derived quantity. In particular, for
quantities that are affine functions of the coefficients, the posterior
distribution can be expressed in closed-form. We simulated measurements from
single- and double-tensor models where the correct values of several quantities
are known, to validate that the theoretically derived quantiles agree with
those observed empirically. We included results from residual bootstrap for
comparison and found good agreement. The validation employed several different
models: Diffusion Tensor Imaging (DTI), Mean Apparent Propagator MRI (MAP-MRI)
and Constrained Spherical Deconvolution (CSD). We also used in vivo data to
visualize maps of quantitative features and corresponding uncertainties, and to
show how our approach can be used in a group analysis to downweight subjects
with high uncertainty. In summary, we convert successful linear models for dMRI
signal estimation to probabilistic models, capable of accurate uncertainty
quantification.
",0,1,0,1,0,0
9411,9412,Oscillons in the presence of external potential,"  We discuss similarity between oscillons and oscillational mode in perturbed
$\phi^4$. For small depths of the perturbing potential it is difficult to
distinguish between oscillons and the mode in moderately long time evolution,
moreover one can transform one into the other by adiabatically switching on and
off the potential. Basins of attraction are presented in the parameter space
describing the potential and initial conditions.
",0,1,0,0,0,0
18859,18860,"Uniform rank gradient, cost and local-global convergence","  We analyze the rank gradient of finitely generated groups with respect to
sequences of subgroups of finite index that do not necessarily form a chain, by
connecting it to the cost of p.m.p. actions. We generalize several results that
were only known for chains before. The connection is made by the notion of
local-global convergence.
In particular, we show that for a finitely generated group $\Gamma$ with
fixed price $c$, every Farber sequence has rank gradient $c-1$. By adapting
Lackenby's trichotomy theorem to this setting, we also show that in a finitely
presented amenable group, every sequence of subgroups with index tending to
infinity has vanishing rank gradient.
",0,0,1,0,0,0
192,193,Learning from Between-class Examples for Deep Sound Recognition,"  Deep learning methods have achieved high performance in sound recognition
tasks. Deciding how to feed the training data is important for further
performance improvement. We propose a novel learning method for deep sound
recognition: Between-Class learning (BC learning). Our strategy is to learn a
discriminative feature space by recognizing the between-class sounds as
between-class sounds. We generate between-class sounds by mixing two sounds
belonging to different classes with a random ratio. We then input the mixed
sound to the model and train the model to output the mixing ratio. The
advantages of BC learning are not limited only to the increase in variation of
the training data; BC learning leads to an enlargement of Fisher's criterion in
the feature space and a regularization of the positional relationship among the
feature distributions of the classes. The experimental results show that BC
learning improves the performance on various sound recognition networks,
datasets, and data augmentation schemes, in which BC learning proves to be
always beneficial. Furthermore, we construct a new deep sound recognition
network (EnvNet-v2) and train it with BC learning. As a result, we achieved a
performance surpasses the human level.
",1,0,0,1,0,0
1506,1507,An energy method for rough partial differential equations,"  We present a well-posedness and stability result for a class of nondegenerate
linear parabolic equations driven by rough paths. More precisely, we introduce
a notion of weak solution that satisfies an intrinsic formulation of the
equation in a suitable Sobolev space of negative order. Weak solutions are then
shown to satisfy the corresponding en- ergy estimates which are deduced
directly from the equation. Existence is obtained by showing compactness of a
suitable sequence of approximate solutions whereas unique- ness relies on a
doubling of variables argument and a careful analysis of the passage to the
diagonal. Our result is optimal in the sense that the assumptions on the
deterministic part of the equation as well as the initial condition are the
same as in the classical PDEs theory.
",0,0,1,0,0,0
12527,12528,Cognition of the circle in ancient India,"  We discuss the understanding of geometry of the circle in ancient India, in
terms of enunciation of various principles, constructions, applications etc.
during various phases of history and cultural contexts.
",0,0,1,0,0,0
16011,16012,The Morphospace of Consciousness,"  We construct a complexity-based morphospace to study systems-level properties
of conscious & intelligent systems. The axes of this space label 3 complexity
types: autonomous, cognitive & social. Given recent proposals to synthesize
consciousness, a generic complexity-based conceptualization provides a useful
framework for identifying defining features of conscious & synthetic systems.
Based on current clinical scales of consciousness that measure cognitive
awareness and wakefulness, we take a perspective on how contemporary
artificially intelligent machines & synthetically engineered life forms measure
on these scales. It turns out that awareness & wakefulness can be associated to
computational & autonomous complexity respectively. Subsequently, building on
insights from cognitive robotics, we examine the function that consciousness
serves, & argue the role of consciousness as an evolutionary game-theoretic
strategy. This makes the case for a third type of complexity for describing
consciousness: social complexity. Having identified these complexity types,
allows for a representation of both, biological & synthetic systems in a common
morphospace. A consequence of this classification is a taxonomy of possible
conscious machines. We identify four types of consciousness, based on
embodiment: (i) biological consciousness, (ii) synthetic consciousness, (iii)
group consciousness (resulting from group interactions), & (iv) simulated
consciousness (embodied by virtual agents within a simulated reality). This
taxonomy helps in the investigation of comparative signatures of consciousness
across domains, in order to highlight design principles necessary to engineer
conscious machines. This is particularly relevant in the light of recent
developments at the crossroads of cognitive neuroscience, biomedical
engineering, artificial intelligence & biomimetics.
",1,1,0,0,0,0
15060,15061,Colorings with Fractional Defect,"  Consider a coloring of a graph such that each vertex is assigned a fraction
of each color, with the total amount of colors at each vertex summing to $1$.
We define the fractional defect of a vertex $v$ to be the sum of the overlaps
with each neighbor of $v$, and the fractional defect of the graph to be the
maximum of the defects over all vertices. Note that this coincides with the
usual definition of defect if every vertex is monochromatic. We provide results
on the minimum fractional defect of $2$-colorings of some graphs.
",0,0,1,0,0,0
5963,5964,A Decidable Intuitionistic Temporal Logic,"  We introduce the logic $\sf ITL^e$, an intuitionistic temporal logic based on
structures $(W,\preccurlyeq,S)$, where $\preccurlyeq$ is used to interpret
intuitionistic implication and $S$ is a $\preccurlyeq$-monotone function used
to interpret temporal modalities. Our main result is that the satisfiability
and validity problems for $\sf ITL^e$ are decidable. We prove this by showing
that the logic enjoys the strong finite model property. In contrast, we also
consider a `persistent' version of the logic, $\sf ITL^p$, whose models are
similar to Cartesian products. We prove that, unlike $\sf ITL^e$, $\sf ITL^p$
does not have the finite model property.
",0,0,1,0,0,0
6215,6216,A characterization of cellular motivic spectra,"  Let $ \alpha: \mathcal{C} \to \mathcal{D}$ be a symmetric monoidal functor
from a stable presentable symmetric monoidal $\infty$-category $\mathcal{C} $
compactly generated by the tensorunit to a stable presentable symmetric
monoidal $\infty$-category $ \mathcal{D} $ with compact tensorunit. Let $\beta:
\mathcal{D} \to \mathcal{C}$ be a right adjoint of $\alpha$ and $ \mathrm{X}:
\mathcal{B} \to \mathcal{D} $ a symmetric monoidal functor starting at a small
rigid symmetric monoidal $\infty$-category $ \mathcal{B}$. We construct a
symmetric monoidal equivalence between modules in the $\infty$-category of
functors $ \mathcal{B} \to \mathcal{C} $ over the $ \mathrm{E}_\infty$-algebra
$\beta \circ \mathrm{X} $ and the full subcategory of $\mathcal{D}$ compactly
generated by the essential image of $\mathrm{X}$. Especially for every motivic
$ \mathrm{E}_\infty$-ring spectrum $\mathrm{A}$ we obtain a symmetric monoidal
equivalence between the $\infty$-category of cellular motivic
$\mathrm{A}$-module spectra and modules in the $\infty$-category of functors
$\mathrm{QS}$ to spectra over some $ \mathrm{E}_\infty$-algebra, where
$\mathrm{QS}$ denotes the 0th space of the sphere spectrum.
",0,0,1,0,0,0
15820,15821,Interacting Fields and Flows: Magnetic Hot Jupiters,"  We present Magnetohydrodynamic (MHD) simulations of the magnetic interactions
between a solar type star and short period hot Jupiter exoplanets, using the
publicly available MHD code PLUTO. It has been predicted that emission due to
magnetic interactions such as the electron cyclotron maser instability (ECMI)
will be observable. In our simulations, a planetary outflow, due to UV
evaporation of the exoplanets atmosphere, results in the build-up of
circumplanetary material. We predict the ECMI emission and determine that the
emission is prevented from escaping from the system. This is due to the
evaporated material leading to a high plasma frequency in the vicinity of the
planet, which inhibits the ECMI process.
",0,1,0,0,0,0
12282,12283,On the conjecture of Jeśmanowicz,"  We give a survey on some results covering the last 60 years concerning
Jeśmanowicz' conjecture. Moreover, we conclude the survey with a new result
by showing that the special Diophantine equation $$(20k)^x+(99k)^y=(101k)^z$$
has no solution other than $(x,y,z)=(2,2,2)$.
",0,0,1,0,0,0
4500,4501,Hierarchical internal representation of spectral features in deep convolutional networks trained for EEG decoding,"  Recently, there is increasing interest and research on the interpretability
of machine learning models, for example how they transform and internally
represent EEG signals in Brain-Computer Interface (BCI) applications. This can
help to understand the limits of the model and how it may be improved, in
addition to possibly provide insight about the data itself. Schirrmeister et
al. (2017) have recently reported promising results for EEG decoding with deep
convolutional neural networks (ConvNets) trained in an end-to-end manner and,
with a causal visualization approach, showed that they learn to use spectral
amplitude changes in the input. In this study, we investigate how ConvNets
represent spectral features through the sequence of intermediate stages of the
network. We show higher sensitivity to EEG phase features at earlier stages and
higher sensitivity to EEG amplitude features at later stages. Intriguingly, we
observed a specialization of individual stages of the network to the classical
EEG frequency bands alpha, beta, and high gamma. Furthermore, we find first
evidence that particularly in the last convolutional layer, the network learns
to detect more complex oscillatory patterns beyond spectral phase and
amplitude, reminiscent of the representation of complex visual features in
later layers of ConvNets in computer vision tasks. Our findings thus provide
insights into how ConvNets hierarchically represent spectral EEG features in
their intermediate layers and suggest that ConvNets can exploit and might help
to better understand the compositional structure of EEG time series.
",1,0,0,1,0,0
13013,13014,Progressive and Multi-Path Holistically Nested Neural Networks for Pathological Lung Segmentation from CT Images,"  Pathological lung segmentation (PLS) is an important, yet challenging,
medical image application due to the wide variability of pathological lung
appearance and shape. Because PLS is often a pre-requisite for other imaging
analytics, methodological simplicity and generality are key factors in
usability. Along those lines, we present a bottom-up deep-learning based
approach that is expressive enough to handle variations in appearance, while
remaining unaffected by any variations in shape. We incorporate the deeply
supervised learning framework, but enhance it with a simple, yet effective,
progressive multi-path scheme, which more reliably merges outputs from
different network stages. The result is a deep model able to produce finer
detailed masks, which we call progressive holistically-nested networks
(P-HNNs). Using extensive cross-validation, our method is tested on
multi-institutional datasets comprising 929 CT scans (848 publicly available),
of pathological lungs, reporting mean dice scores of 0.985 and demonstrating
significant qualitative and quantitative improvements over state-of-the art
approaches.
",1,0,0,0,0,0
17170,17171,Surface energy of strained amorphous solids,"  Surface stress and surface energy are fundamental quantities which
characterize the interface between two materials. Although these quantities are
identical for interfaces involving only fluids, the Shuttleworth effect
demonstrates that this is not the case for most interfaces involving solids,
since their surface energies change with strain. Crystalline materials are
known to have strain dependent surface energies, but in amorphous materials,
such as polymeric glasses and elastomers, the strain dependence is debated due
to a dearth of direct measurements. Here, we utilize contact angle measurements
on strained glassy and elastomeric solids to address this matter. We show
conclusively that interfaces involving polymeric glasses exhibit strain
dependent surface energies, and give strong evidence for the absence of such a
dependence for incompressible elastomers. The results provide fundamental
insight into our understanding of the interfaces of amorphous solids and their
interaction with contacting liquids.
",0,1,0,0,0,0
10108,10109,Warped Product Space-times,"  Many classical results in relativity theory concerning spherically symmetric
space-times have easy generalizations to warped product space-times, with a
two-dimensional Lorentzian base and arbitrary dimensional Riemannian fibers. We
first give a systematic presentation of the main geometric constructions, with
emphasis on the Kodama vector field and the Hawking energy; the construction is
signature independent. This leads to proofs of general Birkhoff-type theorems
for warped product manifolds; our theorems in particular apply to situations
where the warped product manifold is not necessarily Einstein, and thus can be
applied to solutions with matter content in general relativity. Next we
specialize to the Lorentzian case and study the propagation of null expansions
under the assumption of the dominant energy condition. We prove several
non-existence results relating to the Yamabe class of the fibers, in the spirit
of the black-hole topology theorem of Hawking-Galloway-Schoen. Finally we
discuss the effect of the warped product ansatz on matter models. In particular
we construct several cosmological solutions to the Einstein-Euler equations
whose spatial geometry is generally not isotropic.
",0,0,1,0,0,0
12896,12897,Self-injective commutative rings have no nontrivial rigid ideals,"  We establish a link between trace modules and rigidity in modules over
Noetherian rings. Using the theory of trace ideals we make partial progress on
a question of Dao, and on the Auslander-Reiten conjecture over Artinian
Gorenstein rings.
",0,0,1,0,0,0
18705,18706,Subband adaptive filter trained by differential evolution for channel estimation,"  The normalized subband adaptive filter (NSAF) is widely accepted as a
preeminent adaptive filtering algorithm because of its efficiency under the
colored excitation. However, the convergence rate of NSAF is slow. To address
this drawback, in this paper, a variant of the NSAF, called the differential
evolution (DE)-NSAF (DE-NSAF), is proposed for channel estimation based on DE
strategy. It is worth noticing that there are several papers concerning
designing DE strategies for adaptive filter. But their signal models are still
the single adaptive filter model rather than the fullband adaptive filter model
considered in this paper. Thus, the problem considered in our work is quite
different from those. The proposed DE-NSAF algorithm is based on real-valued
manipulations and has fast convergence rate for searching the global solution
of optimized weight vector. Moreover, a design step of new algorithm is given
in detail. Simulation results demonstrate the improved performance of the
proposed DE-NSAF algorithm in terms of the convergence rate.
",1,0,0,0,0,0
5652,5653,Numerical study of the Kadomtsev--Petviashvili equation and dispersive shock waves,"  A detailed numerical study of the long time behaviour of dispersive shock
waves in solutions to the Kadomtsev-Petviashvili (KP) I equation is presented.
It is shown that modulated lump solutions emerge from the dispersive shock
waves. For the description of dispersive shock waves, Whitham modulation
equations for KP are obtained. It is shown that the modulation equations near
the soliton line are hyperbolic for the KPII equation while they are elliptic
for the KPI equation leading to a focusing effect and the formation of lumps.
Such a behaviour is similar to the appearance of breathers for the focusing
nonlinear Schrodinger equation in the semiclassical limit.
",0,1,1,0,0,0
17962,17963,New insight into the dynamics of rhodopsin photoisomerization from one-dimensional quantum-classical modeling,"  Characterization of the primary events involved in the $cis-trans$
photoisomerization of the rhodopsin retinal chromophore was approximated by a
minimum one-dimensional quantum-classical model. The developed mathematical
model is identical to that obtained using conventional quantum-classical
approaches, and multiparametric quantum-chemical or molecular dynamics (MD)
computations were not required. The quantum subsystem of the model includes
three electronic states for rhodopsin: (i) the ground state, (ii) the excited
state, and (iii) the primary photoproduct in the ground state. The resultant
model is in perfect agreement with experimental data in terms of the quantum
yield, the time required to reach the conical intersection and to complete the
quantum evolution, the range of the characteristic low frequencies active
within the primary events of the $11-cis$ retinal isomerization, and the
coherent character of the photoreaction. An effective redistribution of excess
energy between the vibration modes of rhodopsin was revealed by analysis of the
dissipation process. The results confirm the validity of the minimal model,
despite its one-dimensional character. The fundamental nature of the
photoreaction was therefore demonstrated using a minimum mathematical model for
the first time.
",0,1,0,0,0,0
18579,18580,Personalization in Goal-Oriented Dialog,"  The main goal of modeling human conversation is to create agents which can
interact with people in both open-ended and goal-oriented scenarios. End-to-end
trained neural dialog systems are an important line of research for such
generalized dialog models as they do not resort to any situation-specific
handcrafting of rules. However, incorporating personalization into such systems
is a largely unexplored topic as there are no existing corpora to facilitate
such work. In this paper, we present a new dataset of goal-oriented dialogs
which are influenced by speaker profiles attached to them. We analyze the
shortcomings of an existing end-to-end dialog system based on Memory Networks
and propose modifications to the architecture which enable personalization. We
also investigate personalization in dialog as a multi-task learning problem,
and show that a single model which shares features among various profiles
outperforms separate models for each profile.
",1,0,0,0,0,0
4559,4560,Learning Certifiably Optimal Rule Lists for Categorical Data,"  We present the design and implementation of a custom discrete optimization
technique for building rule lists over a categorical feature space. Our
algorithm produces rule lists with optimal training performance, according to
the regularized empirical risk, with a certificate of optimality. By leveraging
algorithmic bounds, efficient data structures, and computational reuse, we
achieve several orders of magnitude speedup in time and a massive reduction of
memory consumption. We demonstrate that our approach produces optimal rule
lists on practical problems in seconds. Our results indicate that it is
possible to construct optimal sparse rule lists that are approximately as
accurate as the COMPAS proprietary risk prediction tool on data from Broward
County, Florida, but that are completely interpretable. This framework is a
novel alternative to CART and other decision tree methods for interpretable
modeling.
",0,0,0,1,0,0
13829,13830,Inference for partial correlation when data are missing not at random,"  We introduce uncertainty regions to perform inference on partial correlations
when data are missing not at random. These uncertainty regions are shown to
have a desired asymptotic coverage. Their finite sample performance is
illustrated via simulations and real data example.
",0,0,1,1,0,0
3258,3259,Boundary feedback stabilization of a flexible wing model under unsteady aerodynamic loads,"  This paper addresses the boundary stabilization of a flexible wing model,
both in bending and twisting displacements, under unsteady aerodynamic loads,
and in presence of a store. The wing dynamics is captured by a distributed
parameter system as a coupled Euler-Bernoulli and Timoshenko beam model. The
problem is tackled in the framework of semigroup theory, and a Lyapunov-based
stability analysis is carried out to assess that the system energy, as well as
the bending and twisting displacements, decay exponentially to zero. The
effectiveness of the proposed boundary control scheme is evaluated based on
simulations.
",1,0,1,0,0,0
11689,11690,Fast Stability Scanning for Future Grid Scenario Analysis,"  Future grid scenario analysis requires a major departure from conventional
power system planning, where only a handful of most critical conditions is
typically analyzed. To capture the inter-seasonal variations in renewable
generation of a future grid scenario necessitates the use of computationally
intensive time-series analysis. In this paper, we propose a planning framework
for fast stability scanning of future grid scenarios using a novel feature
selection algorithm and a novel self-adaptive PSO-k-means clustering algorithm.
To achieve the computational speed-up, the stability analysis is performed only
on small number of representative cluster centroids instead of on the full set
of operating conditions. As a case study, we perform small-signal stability and
steady-state voltage stability scanning of a simplified model of the Australian
National Electricity Market with significant penetration of renewable
generation. The simulation results show the effectiveness of the proposed
approach. Compared to an exhaustive time series scanning, the proposed
framework reduced the computational burden up to ten times, with an acceptable
level of accuracy.
",1,0,0,1,0,0
5850,5851,Inter-Area Oscillation Damping With Non-Synchronized Wide-Area Power System Stabilizer,"  One of the major issues in an interconnected power system is the low damping
of inter-area oscillations which significantly reduces the power transfer
capability. Advances in Wide-Area Measurement System (WAMS) makes it possible
to use the information from geographical distant location to improve power
system dynamics and performances. A speed deviation based Wide-Area Power
System Stabilizer (WAPSS) is known to be effective in damping inter-area modes.
However, the involvement of wide-area signals gives rise to the problem of
time-delay, which may degrade the system performance. In general, time-stamped
synchronized signals from Phasor Data Concentrator (PDC) are used for WAPSS, in
which delays are introduced in both local and remote signals. One can opt for a
feedback of remote signal only from PDC and uses the local signal as it is
available, without time synchronization. This paper utilizes configurations of
time-matched synchronized and nonsychronized feedback and provides the
guidelines to design the controller. The controllers are synthesized using
$H_\infty$ control with regional pole placement for ensuring adequate dynamic
performance. To show the effectiveness of the proposed approach, two power
system models have been used for the simulations. It is shown that the
controllers designed based on the nonsynchronized signals are more robust to
time time delay variations than the controllers using synchronized signal.
",1,0,0,0,0,0
8731,8732,PDE-Net: Learning PDEs from Data,"  In this paper, we present an initial attempt to learn evolution PDEs from
data. Inspired by the latest development of neural network designs in deep
learning, we propose a new feed-forward deep network, called PDE-Net, to
fulfill two objectives at the same time: to accurately predict dynamics of
complex systems and to uncover the underlying hidden PDE models. The basic idea
of the proposed PDE-Net is to learn differential operators by learning
convolution kernels (filters), and apply neural networks or other machine
learning methods to approximate the unknown nonlinear responses. Comparing with
existing approaches, which either assume the form of the nonlinear response is
known or fix certain finite difference approximations of differential
operators, our approach has the most flexibility by learning both differential
operators and the nonlinear responses. A special feature of the proposed
PDE-Net is that all filters are properly constrained, which enables us to
easily identify the governing PDE models while still maintaining the expressive
and predictive power of the network. These constrains are carefully designed by
fully exploiting the relation between the orders of differential operators and
the orders of sum rules of filters (an important concept originated from
wavelet theory). We also discuss relations of the PDE-Net with some existing
networks in computer vision such as Network-In-Network (NIN) and Residual
Neural Network (ResNet). Numerical experiments show that the PDE-Net has the
potential to uncover the hidden PDE of the observed dynamics, and predict the
dynamical behavior for a relatively long time, even in a noisy environment.
",1,0,0,1,0,0
5643,5644,Statistical inference methods for cumulative incidence function curves at a fixed point in time,"  Competing risks data arise frequently in clinical trials. When the
proportional subdistribution hazard assumption is violated or two cumulative
incidence function (CIF) curves cross, rather than comparing the overall
treatment effects, researchers may be interested in focusing on a comparison of
clinical utility at some fixed time points. This paper extend a series of tests
that are constructed based on a pseudo-value regression technique or different
transformation functions for CIFs and their variances based on Gaynor's or
Aalen's work, and the differences among CIFs at a given time point are
compared.
",0,0,0,1,0,0
7901,7902,ELICA: An Automated Tool for Dynamic Extraction of Requirements Relevant Information,"  Requirements elicitation requires extensive knowledge and deep understanding
of the problem domain where the final system will be situated. However, in many
software development projects, analysts are required to elicit the requirements
from an unfamiliar domain, which often causes communication barriers between
analysts and stakeholders. In this paper, we propose a requirements ELICitation
Aid tool (ELICA) to help analysts better understand the target application
domain by dynamic extraction and labeling of requirements-relevant knowledge.
To extract the relevant terms, we leverage the flexibility and power of
Weighted Finite State Transducers (WFSTs) in dynamic modeling of natural
language processing tasks. In addition to the information conveyed through
text, ELICA captures and processes non-linguistic information about the
intention of speakers such as their confidence level, analytical tone, and
emotions. The extracted information is made available to the analysts as a set
of labeled snippets with highlighted relevant terms which can also be exported
as an artifact of the Requirements Engineering (RE) process. The application
and usefulness of ELICA are demonstrated through a case study. This study shows
how pre-existing relevant information about the application domain and the
information captured during an elicitation meeting, such as the conversation
and stakeholders' intentions, can be captured and used to support analysts
achieving their tasks.
",1,0,0,1,0,0
9339,9340,Kulish-Sklyanin type models: integrability and reductions,"  We start with a Riemann-Hilbert problem (RHP) related to a BD.I-type
symmetric spaces $SO(2r+1)/S(O(2r-2s +1)\otimes O(2s))$, $s\geq 1$. We consider
two Riemann-Hilbert problems: the first formulated on the real axis
$\mathbb{R}$ in the complex $\lambda$-plane; the second one is formulated on
$\mathbb{R} \oplus i\mathbb{R}$. The first RHP for $s=1$ allows one to solve
the Kulish-Sklyanin (KS) model; the second RHP is relevant for a new type of KS
model. An important example for nontrivial deep reductions of KS model is
given. Its effect on the scattering matrix is formulated. In particular we
obtain new 2-component NLS equations. Finally, using the Wronskian relations we
demonstrate that the inverse scattering method for KS models may be understood
as a generalized Fourier transforms. Thus we have a tool to derive all their
fundamental properties, including the hierarchy of equations and the hierarchy
of their Hamiltonian structures.
",0,1,0,0,0,0
8284,8285,A Cluster Elastic Net for Multivariate Regression,"  We propose a method for estimating coefficients in multivariate regression
when there is a clustering structure to the response variables. The proposed
method includes a fusion penalty, to shrink the difference in fitted values
from responses in the same cluster, and an L1 penalty for simultaneous variable
selection and estimation. The method can be used when the grouping structure of
the response variables is known or unknown. When the clustering structure is
unknown the method will simultaneously estimate the clusters of the response
and the regression coefficients. Theoretical results are presented for the
penalized least squares case, including asymptotic results allowing for p >> n.
We extend our method to the setting where the responses are binomial variables.
We propose a coordinate descent algorithm for both the normal and binomial
likelihood, which can easily be extended to other generalized linear model
(GLM) settings. Simulations and data examples from business operations and
genomics are presented to show the merits of both the least squares and
binomial methods.
",0,0,0,1,0,0
5890,5891,A Geometric Approach for Real-time Monitoring of Dynamic Large Scale Graphs: AS-level graphs illustrated,"  The monitoring of large dynamic networks is a major chal- lenge for a wide
range of application. The complexity stems from properties of the underlying
graphs, in which slight local changes can lead to sizable variations of global
prop- erties, e.g., under certain conditions, a single link cut that may be
overlooked during monitoring can result in splitting the graph into two
disconnected components. Moreover, it is often difficult to determine whether a
change will propagate globally or remain local. Traditional graph theory
measure such as the centrality or the assortativity of the graph are not
satisfying to characterize global properties of the graph. In this paper, we
tackle the problem of real-time monitoring of dynamic large scale graphs by
developing a geometric approach that leverages notions of geometric curvature
and recent development in graph embeddings using Ollivier-Ricci curvature [47].
We illustrate the use of our method by consid- ering the practical case of
monitoring dynamic variations of global Internet using topology changes
information provided by combining several BGP feeds. In particular, we use our
method to detect major events and changes via the geometry of the embedding of
the graph.
",1,0,0,1,0,0
19561,19562,Fast Spectral Ranking for Similarity Search,"  Despite the success of deep learning on representing images for particular
object retrieval, recent studies show that the learned representations still
lie on manifolds in a high dimensional space. This makes the Euclidean nearest
neighbor search biased for this task. Exploring the manifolds online remains
expensive even if a nearest neighbor graph has been computed offline. This work
introduces an explicit embedding reducing manifold search to Euclidean search
followed by dot product similarity search. This is equivalent to linear graph
filtering of a sparse signal in the frequency domain. To speed up online
search, we compute an approximate Fourier basis of the graph offline. We
improve the state of art on particular object retrieval datasets including the
challenging Instre dataset containing small objects. At a scale of 10^5 images,
the offline cost is only a few hours, while query time is comparable to
standard similarity search.
",1,0,0,0,0,0
879,880,A Topologist's View of Kinematic Maps and Manipulation Complexity,"  In this paper we combine a survey of the most important topological
properties of kinematic maps that appear in robotics, with the exposition of
some basic results regarding the topological complexity of a map. In
particular, we discuss mechanical devices that consist of rigid parts connected
by joints and show how the geometry of the joints determines the forward
kinematic map that relates the configuration of joints with the pose of the
end-effector of the device. We explain how to compute the dimension of the
joint space and describe topological obstructions for a kinematic map to be a
fibration or to admit a continuous section. In the second part of the paper we
define the complexity of a continuous map and show how the concept can be
viewed as a measure of the difficulty to find a robust manipulation plan for a
given mechanical device. We also derive some basic estimates for the complexity
and relate it to the degree of instability of a manipulation plan.
",1,0,1,0,0,0
1197,1198,Mathematical modeling of Zika disease in pregnant women and newborns with microcephaly in Brazil,"  We propose a new mathematical model for the spread of Zika virus. Special
attention is paid to the transmission of microcephaly. Numerical simulations
show the accuracy of the model with respect to the Zika outbreak occurred in
Brazil.
",0,0,1,0,0,0
9516,9517,Voice Conversion from Unaligned Corpora using Variational Autoencoding Wasserstein Generative Adversarial Networks,"  Building a voice conversion (VC) system from non-parallel speech corpora is
challenging but highly valuable in real application scenarios. In most
situations, the source and the target speakers do not repeat the same texts or
they may even speak different languages. In this case, one possible, although
indirect, solution is to build a generative model for speech. Generative models
focus on explaining the observations with latent variables instead of learning
a pairwise transformation function, thereby bypassing the requirement of speech
frame alignment. In this paper, we propose a non-parallel VC framework with a
variational autoencoding Wasserstein generative adversarial network (VAW-GAN)
that explicitly considers a VC objective when building the speech model.
Experimental results corroborate the capability of our framework for building a
VC system from unaligned data, and demonstrate improved conversion quality.
",1,0,0,0,0,0
16214,16215,"Space-time Constructivism vs. Modal Provincialism: Or, How Special Relativistic Theories Needn't Show Minkowski Chronogeometry","  In 1835 Lobachevski entertained the possibility of multiple (rival)
geometries. This idea has reappeared on occasion (e.g., Poincaré) but
didn't become key in space-time foundations prior to Brown's \emph{Physical
Relativity} (at the end, the interpretive key to the book). A crucial
difference between his constructivism and orthodox ""space-time realism"" is
modal scope. Constructivism applies to all local classical field theories,
including those with multiple geometries. But the orthodox view provincially
assumes a unique geometry, as familiar theories (Newton, Special Relativity,
Nordström, and GR) have. They serve as the orthodox ""canon."" Their
historical roles suggest a story of inevitable progress. Physics literature
after c. 1920 is relevant to orthodoxy mostly as commentary on the canon, which
closed in the 1910s. The orthodox view explains the behavior of matter as the
manifestation of the real space-time geometry, which works within the canon.
The orthodox view, Whiggish history, and the canon relate symbiotically.
If one considers a theory outside the canon, space-time realism sheds little
light on matter's behavior. Worse, it gives the wrong answer when applied to an
example arguably in the canon, massive scalar gravity with universal coupling.
Which is the true geometry---the flat metric from the Poincaré symmetry,
the conformally flat metric exhibited by material rods and clocks, or both---or
is the question bad? How does space-time realism explain that all matter fields
see the same curved geometry, given so many ways to mix and match?
Constructivist attention to dynamical details is vindicated; geometrical
shortcuts disappoint. The more exhaustive exploration of relativistic field
theories (especially massive) in particle physics is an underused resource for
foundations.
",0,1,0,0,0,0
7110,7111,Mahonian STAT on rearrangement class of words,"  In 2000, Babson and Steingrímsson generalized the notion of permutation
patterns to the so-called vincular patterns, and they showed that many Mahonian
statistics can be expressed as sums of vincular pattern occurrence statistics.
STAT is one of such Mahonian statistics discoverd by them. In 2016, Kitaev and
the third author introduced a words analogue of STAT and proved a joint
equidistribution result involving two sextuple statistics on the whole set of
words with fixed length and alphabet. Moreover, their computer experiments
hinted at a finer involution on $R(w)$, the rearrangement class of a given word
$w$. We construct such an involution in this paper, which yields a comparable
joint equidistribution between two sextuple statistics over $R(w)$. Our
involution builds on Burstein's involution and Foata-Schützenberger's
involution that utilizes the celebrated RSK algorithm.
",1,0,0,0,0,0
4454,4455,Sleep Stage Classification Based on Multi-level Feature Learning and Recurrent Neural Networks via Wearable Device,"  This paper proposes a practical approach for automatic sleep stage
classification based on a multi-level feature learning framework and Recurrent
Neural Network (RNN) classifier using heart rate and wrist actigraphy derived
from a wearable device. The feature learning framework is designed to extract
low- and mid-level features. Low-level features capture temporal and frequency
domain properties and mid-level features learn compositions and structural
information of signals. Since sleep staging is a sequential problem with
long-term dependencies, we take advantage of RNNs with Bidirectional Long
Short-Term Memory (BLSTM) architectures for sequence data learning. To simulate
the actual situation of daily sleep, experiments are conducted with a resting
group in which sleep is recorded in resting state, and a comprehensive group in
which both resting sleep and non-resting sleep are included.We evaluate the
algorithm based on an eight-fold cross validation to classify five sleep stages
(W, N1, N2, N3, and REM). The proposed algorithm achieves weighted precision,
recall and F1 score of 58.0%, 60.3%, and 58.2% in the resting group and 58.5%,
61.1%, and 58.5% in the comprehensive group, respectively. Various comparison
experiments demonstrate the effectiveness of feature learning and BLSTM. We
further explore the influence of depth and width of RNNs on performance. Our
method is specially proposed for wearable devices and is expected to be
applicable for long-term sleep monitoring at home. Without using too much prior
domain knowledge, our method has the potential to generalize sleep disorder
detection.
",1,0,0,1,0,0
3014,3015,Learning Deep CNN Denoiser Prior for Image Restoration,"  Model-based optimization methods and discriminative learning methods have
been the two dominant strategies for solving various inverse problems in
low-level vision. Typically, those two kinds of methods have their respective
merits and drawbacks, e.g., model-based optimization methods are flexible for
handling different inverse problems but are usually time-consuming with
sophisticated priors for the purpose of good performance; in the meanwhile,
discriminative learning methods have fast testing speed but their application
range is greatly restricted by the specialized task. Recent works have revealed
that, with the aid of variable splitting techniques, denoiser prior can be
plugged in as a modular part of model-based optimization methods to solve other
inverse problems (e.g., deblurring). Such an integration induces considerable
advantage when the denoiser is obtained via discriminative learning. However,
the study of integration with fast discriminative denoiser prior is still
lacking. To this end, this paper aims to train a set of fast and effective CNN
(convolutional neural network) denoisers and integrate them into model-based
optimization method to solve other inverse problems. Experimental results
demonstrate that the learned set of denoisers not only achieve promising
Gaussian denoising results but also can be used as prior to deliver good
performance for various low-level vision applications.
",1,0,0,0,0,0
9975,9976,ArchiveWeb: collaboratively extending and exploring web archive collections - How would you like to work with your collections?,"  Curated web archive collections contain focused digital content which is
collected by archiving organizations, groups, and individuals to provide a
representative sample covering specific topics and events to preserve them for
future exploration and analysis. In this paper, we discuss how to best support
collaborative construction and exploration of these collections through the
ArchiveWeb system. ArchiveWeb has been developed using an iterative
evaluation-driven design-based research approach, with considerable user
feedback at all stages. The first part of this paper describes the important
insights we gained from our initial requirements engineering phase during the
first year of the project and the main functionalities of the current
ArchiveWeb system for searching, constructing, exploring, and discussing web
archive collections. The second part summarizes the feedback we received on
this version from archiving organizations and libraries, as well as our
corresponding plans for improving and extending the system for the next
release.
",1,0,0,0,0,0
13611,13612,First Detection of Equatorial Dark Dust Lane in a Protostellar Disk at Submillimeter Wavelength,"  In the earliest (so-called ""Class 0"") phase of sunlike (low-mass) star
formation, circumstellar disks are expected to form, feeding the protostars.
However, such disks are difficult to resolve spatially because of their small
sizes. Moreover, there are theoretical difficulties in producing such disks in
the earliest phase, due to the retarding effects of magnetic fields on the
rotating, collapsing material (so-called ""magnetic braking""). With the Atacama
Large Millimeter/submillimeter Array (ALMA), it becomes possible to uncover
such disks and study them in detail. HH 212 is a very young protostellar
system. With ALMA, we not only detect but also spatially resolve its disk in
dust emission at submillimeter wavelength. The disk is nearly edge-on and has a
radius of ~ 60 AU. Interestingly, it shows a prominent equatorial dark lane
sandwiched between two brighter features, due to relatively low temperature and
high optical depth near the disk midplane. For the first time, this dark lane
is seen at submillimeter wavelength, producing a ""hamburger""-shaped appearance
that is reminiscent of the scattered-light image of an edge-on disk in optical
and near infrared. Our observations open up an exciting possibility of directly
detecting and characterizing small disks around the youngest protostars through
high-resolution imaging with ALMA, which provides strong constraints on
theories of disk formation.
",0,1,0,0,0,0
102,103,On Improving the Capacity of Solving Large-scale Wireless Network Design Problems by Genetic Algorithms,"  Over the last decade, wireless networks have experienced an impressive growth
and now play a main role in many telecommunications systems. As a consequence,
scarce radio resources, such as frequencies, became congested and the need for
effective and efficient assignment methods arose. In this work, we present a
Genetic Algorithm for solving large instances of the Power, Frequency and
Modulation Assignment Problem, arising in the design of wireless networks. To
our best knowledge, this is the first Genetic Algorithm that is proposed for
such problem. Compared to previous works, our approach allows a wider
exploration of the set of power solutions, while eliminating sources of
numerical problems. The performance of the algorithm is assessed by tests over
a set of large realistic instances of a Fixed WiMAX Network.
",1,0,1,0,0,0
17092,17093,Practical Machine Learning for Cloud Intrusion Detection: Challenges and the Way Forward,"  Operationalizing machine learning based security detections is extremely
challenging, especially in a continuously evolving cloud environment.
Conventional anomaly detection does not produce satisfactory results for
analysts that are investigating security incidents in the cloud. Model
evaluation alone presents its own set of problems due to a lack of benchmark
datasets. When deploying these detections, we must deal with model compliance,
localization, and data silo issues, among many others. We pose the problem of
""attack disruption"" as a way forward in the security data science space. In
this paper, we describe the framework, challenges, and open questions
surrounding the successful operationalization of machine learning based
security detections in a cloud environment and provide some insights on how we
have addressed them.
",1,0,0,0,0,0
8409,8410,Improving Sharir and Welzl's bound on crossing-free matchings through solving a stronger recurrence,"  Sharir and Welzl [1] derived a bound on crossing-free matchings primarily
based on solving a recurrence based on the size of the matchings. We show that
the recurrence given in Lemma 2.3 in Sharir and Welzl can be improve to
$(2n-6s)\textbf{Ma}_{m}(P)\leq\frac{68}{3}(s+2)\textbf{Ma}_{m-1}(P)$ and
$(3n-7s)\textbf{Ma}_{m}(P)\leq44.5(s+2)\textbf{Ma}_{m-1}(P)$, thereby improving
the upper bound for crossing-free matchings.
",0,0,1,0,0,0
19147,19148,Topological AdS/CFT,"  We define a holographic dual to the Donaldson-Witten topological twist of
$\mathcal{N}=2$ gauge theories on a Riemannian four-manifold. This is described
by a class of asymptotically locally hyperbolic solutions to $\mathcal{N}=4$
gauged supergravity in five dimensions, with the four-manifold as conformal
boundary. Under AdS/CFT, minus the logarithm of the partition function of the
gauge theory is identified with the holographically renormalized supergravity
action. We show that the latter is independent of the metric on the boundary
four-manifold, as required for a topological theory. Supersymmetric solutions
in the bulk satisfy first order differential equations for a twisted $Sp(1)$
structure, which extends the quaternionic Kahler structure that exists on any
Riemannian four-manifold boundary. We comment on applications and extensions,
including generalizations to other topological twists.
",0,0,1,0,0,0
3197,3198,Conversion of Mersenne Twister to double-precision floating-point numbers,"  The 32-bit Mersenne Twister generator MT19937 is a widely used random number
generator. To generate numbers with more than 32 bits in bit length, and
particularly when converting into 53-bit double-precision floating-point
numbers in $[0,1)$ in the IEEE 754 format, the typical implementation
concatenates two successive 32-bit integers and divides them by a power of $2$.
In this case, the 32-bit MT19937 is optimized in terms of its equidistribution
properties (the so-called dimension of equidistribution with $v$-bit accuracy)
under the assumption that one will mainly be using 32-bit output values, and
hence the concatenation sometimes degrades the dimension of equidistribution
compared with the simple use of 32-bit outputs. In this paper, we analyze such
phenomena by investigating hidden $\mathbb{F}_2$-linear relations among the
bits of high-dimensional outputs. Accordingly, we report that MT19937 with a
specific lag set fails several statistical tests, such as the overlapping
collision test, matrix rank test, and Hamming independence test.
",1,0,0,1,0,0
15196,15197,Batch Size Influence on Performance of Graphic and Tensor Processing Units during Training and Inference Phases,"  The impact of the maximally possible batch size (for the better runtime) on
performance of graphic processing units (GPU) and tensor processing units (TPU)
during training and inference phases is investigated. The numerous runs of the
selected deep neural network (DNN) were performed on the standard MNIST and
Fashion-MNIST datasets. The significant speedup was obtained even for extremely
low-scale usage of Google TPUv2 units (8 cores only) in comparison to the quite
powerful GPU NVIDIA Tesla K80 card with the speedup up to 10x for training
stage (without taking into account the overheads) and speedup up to 2x for
prediction stage (with and without taking into account overheads). The precise
speedup values depend on the utilization level of TPUv2 units and increase with
the increase of the data volume under processing, but for the datasets used in
this work (MNIST and Fashion-MNIST with images of sizes 28x28) the speedup was
observed for batch sizes >512 images for training phase and >40 000 images for
prediction phase. It should be noted that these results were obtained without
detriment to the prediction accuracy and loss that were equal for both GPU and
TPU runs up to the 3rd significant digit for MNIST dataset, and up to the 2nd
significant digit for Fashion-MNIST dataset.
",1,0,0,0,0,0
16445,16446,A Characterisation of Open Bisimilarity using an Intuitionistic Modal Logic,"  Open bisimilarity is the original notion of bisimilarity to be introduced for
the pi-calculus that is a congruence. In open bisimilarity, free names in
processes are treated as variables that may be instantiated lazily; in contrast
to early and late bisimilarity where free names are constants. We build on the
established line of work, due to Milner, Parrow, and Walker, on classical modal
logics characterising early and late bisimilarity for the $\pi$-calculus. The
important insight is, to characterise open bisimilarity, we move to the setting
of intuitionistic modal logics. The intuitionistic modal logic introduced,
called OM, is such that modalities are closed under (respectful) substitutions,
inducing a property known as intuitionistic hereditary. Intuitionistic
hereditary reflects the lazy instantiation of names in open bisimilarity. The
soundness proof for open bisimilarity with respect to the modal logic is
mechanised in Abella. The constructive content of the completeness proof
provides an algorithm for generating distinguishing formulae, where such
formulae are useful as a certificate explaining why two processes are not open
bisimilar. We draw attention to the fact that open bisimilarity is not the only
notion of bisimilarity that is a congruence: for name-passing calculi there is
a classical/intuitionistic spectrum of bisimilarities.
",1,0,0,0,0,0
2491,2492,Structured Differential Learning for Automatic Threshold Setting,"  We introduce a technique that can automatically tune the parameters of a
rule-based computer vision system comprised of thresholds, combinational logic,
and time constants. This lets us retain the flexibility and perspicacity of a
conventionally structured system while allowing us to perform approximate
gradient descent using labeled data. While this is only a heuristic procedure,
as far as we are aware there is no other efficient technique for tuning such
systems. We describe the components of the system and the associated supervised
learning mechanism. We also demonstrate the utility of the algorithm by
comparing its performance versus hand tuning for an automotive headlight
controller. Despite having over 100 parameters, the method is able to
profitably adjust the system values given just the desired output for a number
of videos.
",0,0,0,1,0,0
11375,11376,Multiband NFC for High-Throughput Wireless Computer Vision Sensor Network,"  Vision sensors lie in the heart of computer vision. In many computer vision
applications, such as AR/VR, non-contacting near-field communication (NFC) with
high throughput is required to transfer information to algorithms. In this
work, we proposed a novel NFC system which utilizes multiple frequency bands to
achieve high throughput.
",1,0,0,0,0,0
13137,13138,User-driven mobile robot storyboarding: Learning image interest and saliency from pairwise image comparisons,"  This paper describes a novel storyboarding scheme that uses a model trained
on pairwise image comparisons to identify images likely to be of interest to a
mobile robot user. Traditional storyboarding schemes typically attempt to
summarise robot observations using predefined novelty or image quality
objectives, but we propose a user training stage that allows the incorporation
of user interest when storyboarding. Our approach dramatically reduces the
number of image comparisons required to infer image interest by applying a
Gaussian process smoothing algorithm on image features extracted using a
pre-trained convolutional neural network. As a particularly valuable
by-product, the proposed approach allows the generation of user-specific
saliency or attention maps.
",1,0,0,0,0,0
11826,11827,Designing spin and orbital exchange Hamiltonians with ultrashort electric field transients,"  We demonstrate how electric fields with arbitrary time profile can be used to
control the time-dependent parameters of spin and orbital exchange
Hamiltonians. Analytic expressions for the exchange constants are derived from
a time-dependent Schrieffer-Wolff transformation, and the validity of the
resulting effective Hamiltonian is verified for the case of a quarter-filled
two-orbital Hubbard model, by comparing to the results of a full nonequilibrium
dynamical mean-field theory simulation. The ability to manipulate Hamiltonians
with arbitrary time-dependent fields, beyond the paradigm of Floquet
engineering, opens the possibility to control intertwined spin and orbital
order using laser or THz pulses which are tailored to minimize electronic
excitations.
",0,1,0,0,0,0
7297,7298,A Nonlinear Dimensionality Reduction Framework Using Smooth Geodesics,"  Existing dimensionality reduction methods are adept at revealing hidden
underlying manifolds arising from high-dimensional data and thereby producing a
low-dimensional representation. However, the smoothness of the manifolds
produced by classic techniques over sparse and noisy data is not guaranteed. In
fact, the embedding generated using such data may distort the geometry of the
manifold and thereby produce an unfaithful embedding. Herein, we propose a
framework for nonlinear dimensionality reduction that generates a manifold in
terms of smooth geodesics that is designed to treat problems in which manifold
measurements are either sparse or corrupted by noise. Our method generates a
network structure for given high-dimensional data using a nearest neighbors
search and then produces piecewise linear shortest paths that are defined as
geodesics. Then, we fit points in each geodesic by a smoothing spline to
emphasize the smoothness. The robustness of this approach for sparse and noisy
datasets is demonstrated by the implementation of the method on synthetic and
real-world datasets.
",1,0,0,1,0,0
9664,9665,Probabilistic Reduced-Order Modeling for Stochastic Partial Differential Equations,"  We discuss a Bayesian formulation to coarse-graining (CG) of PDEs where the
coefficients (e.g. material parameters) exhibit random, fine scale variability.
The direct solution to such problems requires grids that are small enough to
resolve this fine scale variability which unavoidably requires the repeated
solution of very large systems of algebraic equations. We establish a
physically inspired, data-driven coarse-grained model which learns a low-
dimensional set of microstructural features that are predictive of the
fine-grained model (FG) response. Once learned, those features provide a sharp
distribution over the coarse scale effec- tive coefficients of the PDE that are
most suitable for prediction of the fine scale model output. This ultimately
allows to replace the computationally expensive FG by a generative proba-
bilistic model based on evaluating the much cheaper CG several times. Sparsity
enforcing pri- ors further increase predictive efficiency and reveal
microstructural features that are important in predicting the FG response.
Moreover, the model yields probabilistic rather than single-point predictions,
which enables the quantification of the unavoidable epistemic uncertainty that
is present due to the information loss that occurs during the coarse-graining
process.
",0,0,0,1,0,0
9911,9912,$Ψ$ec: A Local Spectral Exterior Calculus,"  We introduce $\Psi$ec, a local spectral exterior calculus that provides a
discretization of Cartan's exterior calculus of differential forms using
wavelet functions. Our construction consists of differential form wavelets with
flexible directional localization, between fully isotropic and curvelet- and
ridgelet-like, that provide tight frames for the spaces of $k$-forms in
$\mathbb{R}^2$ and $\mathbb{R}^3$. By construction, these wavelets satisfy the
de Rahm co-chain complex, the Hodge decomposition, and that the integral of a
$k+1$-form is a $k$-form. They also enforce Stokes' theorem for differential
forms, and we show that with a finite number of wavelet levels it is most
efficiently approximated using anisotropic curvelet- or ridgelet-like forms.
Our construction is based on the intrinsic geometric properties of the exterior
calculus in the Fourier domain. To reveal these, we extend existing results on
the Fourier transform of differential forms to a frequency domain description
of the exterior calculus, including, for example, a Parseval theorem for forms
and a description of the symbols of all important operators.
",1,0,0,0,0,0
3520,3521,The Case for Meta-Cognitive Machine Learning: On Model Entropy and Concept Formation in Deep Learning,"  Machine learning is usually defined in behaviourist terms, where external
validation is the primary mechanism of learning. In this paper, I argue for a
more holistic interpretation in which finding more probable, efficient and
abstract representations is as central to learning as performance. In other
words, machine learning should be extended with strategies to reason over its
own learning process, leading to so-called meta-cognitive machine learning. As
such, the de facto definition of machine learning should be reformulated in
these intrinsically multi-objective terms, taking into account not only the
task performance but also internal learning objectives. To this end, we suggest
a ""model entropy function"" to be defined that quantifies the efficiency of the
internal learning processes. It is conjured that the minimization of this model
entropy leads to concept formation. Besides philosophical aspects, some initial
illustrations are included to support the claims.
",1,0,0,1,0,0
14047,14048,Eternal inflation and the quantum birth of cosmic structure,"  We consider the eternal inflation scenario of the slow-roll/chaotic type with
the additional element of an objective collapse of the wave function. The
incorporation of this new agent to the traditional inflationary setting might
represent a possible solution to the quantum measurement problem during
inflation, a subject that has not reached a consensus among the community.
Specifically, it could provide an explanation for the generation of the
primordial anisotropies and inhomogeneities, starting from a perfectly
symmetric background and invoking symmetric dynamics. We adopt the continuous
spontaneous localization model, in the context of inflation, as the dynamical
reduction mechanism that generates the primordial inhomogeneities. Furthermore,
when enforcing the objective reduction mechanism, the condition for eternal
inflation can be bypassed. In particular, the collapse mechanism incites the
wave function, corresponding to the inflaton, to localize itself around the
zero mode of the field. Then the zero mode will evolve essentially unperturbed,
driving inflation to an end in any region of the Universe where inflation
occurred. Also, our approach achieves a primordial spectrum with an amplitude
and shape consistent with the one that best fits the observational data.
",0,1,0,0,0,0
17312,17313,Effects of pressure impulse and peak pressure of a shock wave on microjet velocity and the onset of cavitation in a microchannel,"  The development of needle-free injection systems utilizing high-speed
microjets is of great importance to world healthcare. It is thus crucial to
control the microjets, which are often induced by underwater shock waves. In
this contribution from fluid-mechanics point of view, we experimentally
investigate the effect of a shock wave on the velocity of a free surface
(microjet) and underwater cavitation onset in a microchannel, focusing on the
pressure impulse and peak pressure of the shock wave. The shock wave used had a
non-spherically-symmetric peak pressure distribution and a spherically
symmetric pressure impulse distribution [Tagawa et al., J. Fluid Mech., 2016,
808, 5-18]. First, we investigate the effect of the shock wave on the jet
velocity by installing a narrow tube and a hydrophone in different
configurations in a large water tank, and measuring the shock wave pressure and
the jet velocity simultaneously. The results suggest that the jet velocity
depends only on the pressure impulse of the shock wave. We then investigate the
effect of the shock wave on the cavitation onset by taking measurements in an
L-shaped microchannel. The results suggest that the probability of cavitation
onset depends only on the peak pressure of the shock wave. In addition, the jet
velocity varies according to the presence or absence of cavitation. The above
findings provide new insights for advancing a control method for high-speed
microjets.
",0,1,0,0,0,0
18156,18157,Recurrent Neural Network-based Model Predictive Control for Continuous Pharmaceutical Manufacturing,"  The pharmaceutical industry has witnessed exponential growth in transforming
operations towards continuous manufacturing to effectively achieve increased
profitability, reduced waste, and extended product range. Model Predictive
Control (MPC) can be applied for enabling this vision, in providing superior
regulation of critical quality attributes. For MPC, obtaining a workable model
is of fundamental importance, especially in the presence of complex reaction
kinetics and process dynamics. Whilst physics-based models are desirable, it is
not always practical to obtain one effective and fit-for-purpose model.
Instead, within industry, data-driven system-identification approaches have
been found to be useful and widely deployed in MPC solutions. In this work, we
demonstrated the applicability of Recurrent Neural Networks (RNNs) for MPC
applications in continuous pharmaceutical manufacturing. We have shown that
RNNs are especially well-suited for modeling dynamical systems due to their
mathematical structure and satisfactory closed-loop control performance can be
yielded for MPC in continuous pharmaceutical manufacturing.
",1,0,0,0,0,0
15355,15356,On Lebesgue Integral Quadrature,"  A new type of quadrature is developed. The Gauss quadrature, for a given
measure, finds optimal values of a function's argument (nodes) and the
corresponding weights. In contrast, the Lebesgue quadrature developed in this
paper, finds optimal values of function (value-nodes) and the corresponding
weights. The Gauss quadrature groups sums by function argument, it can be
viewed as a $n$-point discrete measure, producing the Riemann integral. The
Lebesgue quadrature groups sums by function value, it can be viewed as a
$n$-point discrete distribution, producing the Lebesgue integral.
Mathematically, the problem is reduced to a generalized eigenvalue problem:
Lebesgue quadrature value-nodes are the eigenvalues and the corresponding
weights are the square of the averaged eigenvectors. A numerical estimation of
an integral as the Lebesgue integral is especially advantageous when analyzing
irregular and stochastic processes. The approach separates the outcome
(value-nodes) and the probability of the outcome (weight). For this reason, it
is especially well-suited for the study of non--Gaussian processes. The
software implementing the theory is available from the authors.
",0,0,0,1,0,0
7276,7277,Spectroscopy of Ultra-diffuse Galaxies in the Coma Cluster,"  We present spectra of 5 ultra-diffuse galaxies (UDGs) in the vicinity of the
Coma Cluster obtained with the Multi-Object Double Spectrograph on the Large
Binocular Telescope. We confirm 4 of these as members of the cluster,
quintupling the number of spectroscopically confirmed systems. Like the
previously confirmed large (projected half light radius $>$ 4.6 kpc) UDG, DF44,
the systems we targeted all have projected half light radii $> 2.9$ kpc. As
such, we spectroscopically confirm a population of physically large UDGs in the
Coma cluster. The remaining UDG is located in the field, about $45$ Mpc behind
the cluster. We observe Balmer and Ca II H \& K absorption lines in all of our
UDG spectra. By comparing the stacked UDG spectrum against stellar population
synthesis models, we conclude that, on average, these UDGs are composed of
metal-poor stars ([Fe/H] $\lesssim -1.5$). We also discover the first UDG with
[OII] and [OIII] emission lines within a clustered environment, demonstrating
that not all cluster UDGs are devoid of gas and sources of ionizing radiation.
",0,1,0,0,0,0
16314,16315,Pricing of debt and equity in a financial network with comonotonic endowments,"  In this paper we present formulas for the valuation of debt and equity of
firms in a financial network under comonotonic endowments. We demonstrate that
the comonotonic setting provides a lower bound to the price of debt under
Eisenberg-Noe financial networks with consistent marginal endowments. Such
financial networks encode the interconnection of firms through debt claims. The
proposed pricing formulas consider the realized, endogenous, recovery rate on
debt claims. Special consideration will be given to the setting in which firms
only invest in a risk-free bond and a common risky asset following a geometric
Brownian motion.
",0,0,0,0,0,1
10827,10828,Extending the topological analysis and seeking the real-space subsystems in non-Coulombic systems with homogeneous potential energy functions,"  It is customary to conceive the interactions of all the constituents of a
molecular system, i.e. electrons and nuclei, as Coulombic. However, in a more
detailed analysis one may always find small but non-negligible non-Coulombic
interactions in molecular systems originating from the finite size of nuclei,
magnetic interactions, etc. While such small modifications of the Coulombic
interactions do not seem to alter the nature of a molecular system in real
world seriously, they are a serious obstacle for quantum chemical theories and
methodologies which their formalism is strictly confined to the Coulombic
interactions. Although the quantum theory of atoms in molecules (QTAIM) has
been formulated originally for the Coulombic systems, some recent studies have
demonstrated that apart from basin energy of an atom in a molecule, its
theoretical ingredients are not sensitive to the explicit form of the potential
energy operator. In this study, it is demonstrated that the basin energy may be
defined not only for coulombic systems but for all real-space subsystems of
those systems that are described by any member of the set of the homogeneous
potential energy functions. On the other hand, this extension opens the door
for seeking novel real-space subsystems, apart from atoms in molecules, in
non-Coulombic systems. These novel real-space subsystems call for an extended
formalism that goes beyond the orthodox QTAIM, which is not confined to the
Coulombic systems nor to the atoms in molecules as the sole real-space
subsystems. It is termed the quantum theory of real-space open subsystems
(QTROS) and its potential applications are detailed. The harmonic trap model,
containing non-interacting fermions or bosons, is considered as an example for
the QTROS analysis. The QTROS analysis of bosonic systems is particularly quite
unprecedented, not attempted before.
",0,1,0,0,0,0
5628,5629,Case Studies of Exocomets in the System of HD 10180,"  The aim of our study is to investigate the dynamics of possible comets in the
HD 10180 system. This investigation is motivated by the discovery of exocomets
in various systems, especially $\beta$ Pictoris, as well as in at least ten
other systems. Detailed theoretical studies about the formation and evolution
of star--planet systems indicate that exocomets should be quite common. Further
observational results are expected in the foreseeable future, in part due to
the availability of the Large Synoptic Survey Telescope. Nonetheless, the Solar
System represents the best studied example for comets, thus serving as a prime
motivation for investigating comets in HD 10180 as well. HD 10180 is strikingly
similar to the Sun. This system contains six confirmed planets and (at least)
two additional planets subject to final verification. In our studies, we
consider comets of different inclinations and eccentricities and find an array
of different outcomes such as encounters with planets, captures, and escapes.
Comets with relatively large eccentricities are able to enter the inner region
of the system facing early planetary encounters. Stable comets experience
long-term evolution of orbital elements, as expected. We also tried to
distinguish cometary families akin to our Solar System but no clear distinction
between possible families was found. Generally, theoretical and observational
studies of exoplanets have a large range of ramifications, involving the
origin, structure and evolution of systems as well as the proliferation of
water and prebiotic compounds to terrestrial planets, which will increase their
chances of being habitable.
",0,1,0,0,0,0
2543,2544,High-precision measurement of the proton's atomic mass,"  We report on the precise measurement of the atomic mass of a single proton
with a purpose-built Penning-trap system. With a precision of 32
parts-per-trillion our result not only improves on the current CODATA
literature value by a factor of three, but also disagrees with it at a level of
about 3 standard deviations.
",0,1,0,0,0,0
10093,10094,Localized Thermal States,"  It is believed that thermalization in closed systems of interacting particles
can occur only when the eigenstates are fully delocalized and chaotic in the
preferential (unperturbed) basis of the total Hamiltonian. Here we demonstrate
that at variance with this common belief the typical situation in the systems
with two-body inter-particle interaction is much more complicated and allows to
treat as thermal even eigenstates that are not fully delocalized. Using a
semi-analytical approach we establish the conditions for the emergence of such
thermal states in a model of randomly interacting bosons. Our numerical data
show an excellent correspondence with the predicted properties of {\it
localized thermal eigenstates}.
",0,1,0,0,0,0
14758,14759,Hyperpolarizability and operational magic wavelength in an optical lattice clock,"  Optical clocks benefit from tight atomic confinement enabling extended
interrogation times as well as Doppler- and recoil-free operation. However,
these benefits come at the cost of frequency shifts that, if not properly
controlled, may degrade clock accuracy. Numerous theoretical studies have
predicted optical lattice clock frequency shifts that scale nonlinearly with
trap depth. To experimentally observe and constrain these shifts in an
$^{171}$Yb optical lattice clock, we construct a lattice enhancement cavity
that exaggerates the light shifts. We observe an atomic temperature that is
proportional to the optical trap depth, fundamentally altering the scaling of
trap-induced light shifts and simplifying their parametrization. We identify an
""operational"" magic wavelength where frequency shifts are insensitive to
changes in trap depth. These measurements and scaling analysis constitute an
essential systematic characterization for clock operation at the $10^{-18}$
level and beyond.
",0,1,0,0,0,0
1870,1871,Structured Connectivity Augmentation,"  We initiate the algorithmic study of the following ""structured augmentation""
question: is it possible to increase the connectivity of a given graph G by
superposing it with another given graph H? More precisely, graph F is the
superposition of G and H with respect to injective mapping \phi: V(H)->V(G) if
every edge uv of F is either an edge of G, or \phi^{-1}(u)\phi^{-1}(v) is an
edge of H. We consider the following optimization problem. Given graphs G,H,
and a weight function \omega assigning non-negative weights to pairs of
vertices of V(G), the task is to find \varphi of minimum weight
\omega(\phi)=\sum_{xy\in E(H)}\omega(\phi(x)\varphi(y)) such that the edge
connectivity of the superposition F of G and H with respect to \phi is higher
than the edge connectivity of G. Our main result is the following ""dichotomy""
complexity classification. We say that a class of graphs C has bounded
vertex-cover number, if there is a constant t depending on C only such that the
vertex-cover number of every graph from C does not exceed t. We show that for
every class of graphs C with bounded vertex-cover number, the problems of
superposing into a connected graph F and to 2-edge connected graph F, are
solvable in polynomial time when H\in C. On the other hand, for any hereditary
class C with unbounded vertex-cover number, both problems are NP-hard when H\in
C. For the unweighted variants of structured augmentation problems, i.e. the
problems where the task is to identify whether there is a superposition of
graphs of required connectivity, we provide necessary and sufficient
combinatorial conditions on the existence of such superpositions. These
conditions imply polynomial time algorithms solving the unweighted variants of
the problems.
",1,0,0,0,0,0
13533,13534,Learning Generalized Reactive Policies using Deep Neural Networks,"  We present a new approach to learning for planning, where knowledge acquired
while solving a given set of planning problems is used to plan faster in
related, but new problem instances. We show that a deep neural network can be
used to learn and represent a \emph{generalized reactive policy} (GRP) that
maps a problem instance and a state to an action, and that the learned GRPs
efficiently solve large classes of challenging problem instances. In contrast
to prior efforts in this direction, our approach significantly reduces the
dependence of learning on handcrafted domain knowledge or feature selection.
Instead, the GRP is trained from scratch using a set of successful execution
traces. We show that our approach can also be used to automatically learn a
heuristic function that can be used in directed search algorithms. We evaluate
our approach using an extensive suite of experiments on two challenging
planning problem domains and show that our approach facilitates learning
complex decision making policies and powerful heuristic functions with minimal
human input. Videos of our results are available at goo.gl/Hpy4e3.
",1,0,0,0,0,0
19553,19554,Learning to Play Othello with Deep Neural Networks,"  Achieving superhuman playing level by AlphaGo corroborated the capabilities
of convolutional neural architectures (CNNs) for capturing complex spatial
patterns. This result was to a great extent due to several analogies between Go
board states and 2D images CNNs have been designed for, in particular
translational invariance and a relatively large board. In this paper, we verify
whether CNN-based move predictors prove effective for Othello, a game with
significantly different characteristics, including a much smaller board size
and complete lack of translational invariance. We compare several CNN
architectures and board encodings, augment them with state-of-the-art
extensions, train on an extensive database of experts' moves, and examine them
with respect to move prediction accuracy and playing strength. The empirical
evaluation confirms high capabilities of neural move predictors and suggests a
strong correlation between prediction accuracy and playing strength. The best
CNNs not only surpass all other 1-ply Othello players proposed to date but
defeat (2-ply) Edax, the best open-source Othello player.
",1,0,0,1,0,0
20345,20346,Sequential Inverse Approximation of a Regularized Sample Covariance Matrix,"  One of the goals in scaling sequential machine learning methods pertains to
dealing with high-dimensional data spaces. A key related challenge is that many
methods heavily depend on obtaining the inverse covariance matrix of the data.
It is well known that covariance matrix estimation is problematic when the
number of observations is relatively small compared to the number of variables.
A common way to tackle this problem is through the use of a shrinkage estimator
that offers a compromise between the sample covariance matrix and a
well-conditioned matrix, with the aim of minimizing the mean-squared error. We
derived sequential update rules to approximate the inverse shrinkage estimator
of the covariance matrix. The approach paves the way for improved large-scale
machine learning methods that involve sequential updates.
",0,0,0,1,0,0
13060,13061,Parametrizing filters of a CNN with a GAN,"  It is commonly agreed that the use of relevant invariances as a good
statistical bias is important in machine-learning. However, most approaches
that explicitly incorporate invariances into a model architecture only make use
of very simple transformations, such as translations and rotations. Hence,
there is a need for methods to model and extract richer transformations that
capture much higher-level invariances. To that end, we introduce a tool
allowing to parametrize the set of filters of a trained convolutional neural
network with the latent space of a generative adversarial network. We then show
that the method can capture highly non-linear invariances of the data by
visualizing their effect in the data space.
",1,0,0,1,0,0
10992,10993,Invariant theory of a special group action on irreducible polynomials over finite fields,"  In the past few years, an action of $\mathrm{PGL}_2(\mathbb F_q)$ on the set
of irreducible polynomials in $\mathbb F_q[x]$ has been introduced and many
questions have been discussed, such as the characterization and number of
invariant elements. In this paper, we analyze some recent works on this action
and provide full generalizations of them, yielding final theoretical results on
the characterization and number of invariant elements.
",0,0,1,0,0,0
6656,6657,Fast Compressed Self-Indexes with Deterministic Linear-Time Construction,"  We introduce a compressed suffix array representation that, on a text $T$ of
length $n$ over an alphabet of size $\sigma$, can be built in $O(n)$
deterministic time, within $O(n\log\sigma)$ bits of working space, and counts
the number of occurrences of any pattern $P$ in $T$ in time $O(|P| + \log\log_w
\sigma)$ on a RAM machine of $w=\Omega(\log n)$-bit words. This new index
outperforms all the other compressed indexes that can be built in linear
deterministic time, and some others. The only faster indexes can be built in
linear time only in expectation, or require $\Theta(n\log n)$ bits. We also
show that, by using $O(n\log\sigma)$ bits, we can build in linear time an index
that counts in time $O(|P|/\log_\sigma n + \log n(\log\log n)^2)$, which is
RAM-optimal for $w=\Theta(\log n)$ and sufficiently long patterns.
",1,0,0,0,0,0
19674,19675,"Gravitational wave, collider and dark matter signals from a scalar singlet electroweak baryogenesis","  We analyse a simple extension of the SM with just an additional scalar
singlet coupled to the Higgs boson. We discuss the possible probes for
electroweak baryogenesis in this model including collider searches,
gravitational wave and direct dark matter detection signals. We show that a
large portion of the model parameter space exists where the observation of
gravitational waves would allow detection while the indirect collider searches
would not.
",0,1,0,0,0,0
16108,16109,Transportation analysis of denoising autoencoders: a novel method for analyzing deep neural networks,"  The feature map obtained from the denoising autoencoder (DAE) is investigated
by determining transportation dynamics of the DAE, which is a cornerstone for
deep learning. Despite the rapid development in its application, deep neural
networks remain analytically unexplained, because the feature maps are nested
and parameters are not faithful. In this paper, we address the problem of the
formulation of nested complex of parameters by regarding the feature map as a
transport map. Even when a feature map has different dimensions between input
and output, we can regard it as a transportation map by considering that both
the input and output spaces are embedded in a common high-dimensional space. In
addition, the trajectory is a geometric object and thus, is independent of
parameterization. In this manner, transportation can be regarded as a universal
character of deep neural networks. By determining and analyzing the
transportation dynamics, we can understand the behavior of a deep neural
network. In this paper, we investigate a fundamental case of deep neural
networks: the DAE. We derive the transport map of the DAE, and reveal that the
infinitely deep DAE transports mass to decrease a certain quantity, such as
entropy, of the data distribution. These results though analytically simple,
shed light on the correspondence between deep neural networks and the
Wasserstein gradient flows.
",1,0,0,1,0,0
16745,16746,Arcades: A deep model for adaptive decision making in voice controlled smart-home,"  In a voice-controlled smart-home, a controller must respond not only to
user's requests but also according to the interaction context. This paper
describes Arcades, a system which uses deep reinforcement learning to extract
context from a graphical representation of home automation system and to update
continuously its behavior to the user's one. This system is robust to changes
in the environment (sensor breakdown or addition) through its graphical
representation (scale well) and the reinforcement mechanism (adapt well). The
experiments on realistic data demonstrate that this method promises to reach
long life context-aware control of smart-home.
",1,0,0,1,0,0
2070,2071,Automating Image Analysis by Annotating Landmarks with Deep Neural Networks,"  Image and video analysis is often a crucial step in the study of animal
behavior and kinematics. Often these analyses require that the position of one
or more animal landmarks are annotated (marked) in numerous images. The process
of annotating landmarks can require a significant amount of time and tedious
labor, which motivates the need for algorithms that can automatically annotate
landmarks. In the community of scientists that use image and video analysis to
study the 3D flight of animals, there has been a trend of developing more
automated approaches for annotating landmarks, yet they fall short of being
generally applicable. Inspired by the success of Deep Neural Networks (DNNs) on
many problems in the field of computer vision, we investigate how suitable DNNs
are for accurate and automatic annotation of landmarks in video datasets
representative of those collected by scientists studying animals.
Our work shows, through extensive experimentation on videos of hawkmoths,
that DNNs are suitable for automatic and accurate landmark localization. In
particular, we show that one of our proposed DNNs is more accurate than the
current best algorithm for automatic localization of landmarks on hawkmoth
videos. Moreover, we demonstrate how these annotations can be used to
quantitatively analyze the 3D flight of a hawkmoth. To facilitate the use of
DNNs by scientists from many different fields, we provide a self contained
explanation of what DNNs are, how they work, and how to apply them to other
datasets using the freely available library Caffe and supplemental code that we
provide.
",1,0,0,0,0,0
2057,2058,RelNN: A Deep Neural Model for Relational Learning,"  Statistical relational AI (StarAI) aims at reasoning and learning in noisy
domains described in terms of objects and relationships by combining
probability with first-order logic. With huge advances in deep learning in the
current years, combining deep networks with first-order logic has been the
focus of several recent studies. Many of the existing attempts, however, only
focus on relations and ignore object properties. The attempts that do consider
object properties are limited in terms of modelling power or scalability. In
this paper, we develop relational neural networks (RelNNs) by adding hidden
layers to relational logistic regression (the relational counterpart of
logistic regression). We learn latent properties for objects both directly and
through general rules. Back-propagation is used for training these models. A
modular, layer-wise architecture facilitates utilizing the techniques developed
within deep learning community to our architecture. Initial experiments on
eight tasks over three real-world datasets show that RelNNs are promising
models for relational learning.
",1,0,0,1,0,0
12824,12825,Demographics and discussion influence views on algorithmic fairness,"  The field of algorithmic fairness has highlighted ethical questions which may
not have purely technical answers. For example, different algorithmic fairness
constraints are often impossible to satisfy simultaneously, and choosing
between them requires value judgments about which people may disagree.
Achieving consensus on algorithmic fairness will be difficult unless we
understand why people disagree in the first place. Here we use a series of
surveys to investigate how two factors affect disagreement: demographics and
discussion. First, we study whether disagreement on algorithmic fairness
questions is caused partially by differences in demographic backgrounds. This
is a question of interest because computer science is demographically
non-representative. If beliefs about algorithmic fairness correlate with
demographics, and algorithm designers are demographically non-representative,
decisions made about algorithmic fairness may not reflect the will of the
population as a whole. We show, using surveys of three separate populations,
that there are gender differences in beliefs about algorithmic fairness. For
example, women are less likely to favor including gender as a feature in an
algorithm which recommends courses to students if doing so would make female
students less likely to be recommended science courses. Second, we investigate
whether people's views on algorithmic fairness can be changed by discussion and
show, using longitudinal surveys of students in two computer science classes,
that they can.
",1,0,0,0,0,0
9321,9322,Geometric comparison of phylogenetic trees with different leaf sets,"  The metric space of phylogenetic trees defined by Billera, Holmes, and
Vogtmann, which we refer to as BHV space, provides a natural geometric setting
for describing collections of trees on the same set of taxa. However, it is
sometimes necessary to analyze collections of trees on non-identical taxa sets
(i.e., with different numbers of leaves), and in this context it is not evident
how to apply BHV space. Davidson et al. recently approached this problem by
describing a combinatorial algorithm extending tree topologies to regions in
higher dimensional tree spaces, so that one can quickly compute which
topologies contain a given tree as partial data. In this paper, we refine and
adapt their algorithm to work for metric trees to give a full characterization
of the subspace of extensions of a subtree. We describe how to apply our
algorithm to define and search a space of possible supertrees and, for a
collection of tree fragments with different leaf sets, to measure their
compatibility.
",0,0,0,0,1,0
2386,2387,"Robotic frameworks, architectures and middleware comparison","  Nowadays, the construction of a complex robotic system requires a high level
of specialization in a large number of diverse scientific areas. It is
reasonable that a single researcher cannot create from scratch the entirety of
this system, as it is impossible for him to have the necessary skills in the
necessary fields. This obstacle is being surpassed with the existent robotic
frameworks. This paper tries to give an extensive review of the most famous
robotic frameworks and middleware, as well as to provide the means to
effortlessly compare them. Additionally, we try to investigate the differences
between the definitions of a robotic framework, a robotic middleware and a
robotic architecture.
",1,0,0,0,0,0
9158,9159,"Response to Comment on ""Cell nuclei have lower refractive index and mass density than cytoplasm""","  In a recent study entitled ""Cell nuclei have lower refractive index and mass
density than cytoplasm"", we provided strong evidence indicating that the
nuclear refractive index (RI) is lower than the RI of the cytoplasm for several
cell lines. In a complementary study in 2017, entitled ""Is the nuclear
refractive index lower than cytoplasm? Validation of phase measurements and
implications for light scattering technologies"", Steelman et al. observed a
lower nuclear RI also for other cell lines and ruled out methodological error
sources such as phase wrapping and scattering effects. Recently, Yurkin
composed a comment on these 2 publications, entitled ""How a phase image of a
cell with nucleus refractive index smaller than that of the cytoplasm should
look like?"", putting into question the methods used for measuring the cellular
and nuclear RI in the aforementioned publications by suggesting that a lower
nuclear RI would produce a characteristic dip in the measured phase profile in
situ. We point out the difficulty of identifying this dip in the presence of
other cell organelles, noise, or blurring due to the imaging point spread
function. Furthermore, we mitigate Yurkin's concerns regarding the ability of
the simple-transmission approximation to compare cellular and nuclear RI by
analyzing a set of phase images with a novel, scattering-based approach. We
conclude that the absence of a characteristic dip in the measured phase
profiles does not contradict the usage of the simple-transmission approximation
for the determination of the average cellular or nuclear RI. Our response can
be regarded as an addition to the response by Steelman, Eldridge and Wax. We
kindly ask the reader to attend to their thorough ascertainment prior to
reading our response.
",0,0,0,0,1,0
4448,4449,Randomness-induced quantum spin liquid on honeycomb lattice,"  We present a quantu spin liquid state in a spin-1/2 honeycomb lattice with
randomness in the exchange interaction. That is, we successfully introduce
randomness into the organic radial-based complex and realize a random-singlet
(RS) state. All magnetic and thermodynamic experimental results indicate the
liquid-like behaviors, which are consistent with those expected in the RS
state. These results demonstrate that the randomness or inhomogeneity in the
actual systems stabilize the RS state and yield liquid-like behavior.
",0,1,0,0,0,0
4840,4841,Implicit Weight Uncertainty in Neural Networks,"  Modern neural networks tend to be overconfident on unseen, noisy or
incorrectly labelled data and do not produce meaningful uncertainty measures.
Bayesian deep learning aims to address this shortcoming with variational
approximations (such as Bayes by Backprop or Multiplicative Normalising Flows).
However, current approaches have limitations regarding flexibility and
scalability. We introduce Bayes by Hypernet (BbH), a new method of variational
approximation that interprets hypernetworks as implicit distributions. It
naturally uses neural networks to model arbitrarily complex distributions and
scales to modern deep learning architectures. In our experiments, we
demonstrate that our method achieves competitive accuracies and predictive
uncertainties on MNIST and a CIFAR5 task, while being the most robust against
adversarial attacks.
",1,0,0,1,0,0
7254,7255,TLR: Transfer Latent Representation for Unsupervised Domain Adaptation,"  Domain adaptation refers to the process of learning prediction models in a
target domain by making use of data from a source domain. Many classic methods
solve the domain adaptation problem by establishing a common latent space,
which may cause the loss of many important properties across both domains. In
this manuscript, we develop a novel method, transfer latent representation
(TLR), to learn a better latent space. Specifically, we design an objective
function based on a simple linear autoencoder to derive the latent
representations of both domains. The encoder in the autoencoder aims to project
the data of both domains into a robust latent space. Besides, the decoder
imposes an additional constraint to reconstruct the original data, which can
preserve the common properties of both domains and reduce the noise that causes
domain shift. Experiments on cross-domain tasks demonstrate the advantages of
TLR over competing methods.
",0,0,0,1,0,0
6931,6932,Burst Synchronization in A Scale-Free Neuronal Network with Inhibitory Spike-Timing-Dependent Plasticity,"  We are concerned about burst synchronization (BS), related to neural
information processes in health and disease, in the Barabási-Albert
scale-free network (SFN) composed of inhibitory bursting Hindmarsh-Rose
neurons. This inhibitory neuronal population has adaptive dynamic synaptic
strengths governed by the inhibitory spike-timing-dependent plasticity (iSTDP).
In previous works without considering iSTDP, BS was found to appear in a range
of noise intensities for fixed synaptic inhibition strengths. In contrast, in
our present work, we take into consideration iSTDP and investigate its effect
on BS by varying the noise intensity. Our new main result is to find occurrence
of a Matthew effect in inhibitory synaptic plasticity: good BS gets better via
LTD, while bad BS get worse via LTP. This kind of Matthew effect in inhibitory
synaptic plasticity is in contrast to that in excitatory synaptic plasticity
where good (bad) synchronization gets better (worse) via LTP (LTD). We note
that, due to inhibition, the roles of LTD and LTP in inhibitory synaptic
plasticity are reversed in comparison with those in excitatory synaptic
plasticity. Moreover, emergences of LTD and LTP of synaptic inhibition
strengths are intensively investigated via a microscopic method based on the
distributions of time delays between the pre- and the post-synaptic burst onset
times. Finally, in the presence of iSTDP we investigate the effects of network
architecture on BS by varying the symmetric attachment degree $l^*$ and the
asymmetry parameter $\Delta l$ in the SFN.
",0,0,0,0,1,0
14127,14128,Generic coexistence of Fermi arcs and Dirac cones on the surface of time-reversal invariant Weyl semimetals,"  The hallmark of Weyl semimetals is the existence of open constant-energy
contours on their surface -- the so-called Fermi arcs -- connecting Weyl
points. Here, we show that for time-reversal symmetric realizations of Weyl
semimetals these Fermi arcs in many cases coexist with closed Fermi pockets
originating from surface Dirac cones pinned to time-reversal invariant momenta.
The existence of Fermi pockets is required for certain Fermi-arc connectivities
due to additional restrictions imposed by the six $\mathbb{Z}_2$ topological
invariants characterizing a generic time-reversal invariant Weyl semimetal. We
show that a change of the Fermi-arc connectivity generally leads to a different
topology of the surface Fermi surface, and identify the half-Heusler compound
LaPtBi under in-plane compressive strain as a material that realizes this
surface Lifshitz transition. We also discuss universal features of this
coexistence in quasi-particle interference spectra.
",0,1,0,0,0,0
13447,13448,Semantical Equivalence of the Control Flow Graph and the Program Dependence Graph,"  The program dependence graph (PDG) represents data and control dependence
between statements in a program. This paper presents an operational semantics
of program dependence graphs. Since PDGs exclude artificial order of statements
that resides in sequential programs, executions of PDGs are not unique.
However, we identified a class of PDGs that have unique final states of
executions, called deterministic PDGs. We prove that the operational semantics
of control flow graphs is equivalent to that of deterministic PDGs. The class
of deterministic PDGs properly include PDGs obtained from well-structured
programs. Thus, our operational semantics of PDGs is more general than that of
PDGs for well-structured programs, which are already established in literature.
",1,0,0,0,0,0
13561,13562,An Executable Sequential Specification for Spark Aggregation,"  Spark is a new promising platform for scalable data-parallel computation. It
provides several high-level application programming interfaces (APIs) to
perform parallel data aggregation. Since execution of parallel aggregation in
Spark is inherently non-deterministic, a natural requirement for Spark programs
is to give the same result for any execution on the same data set. We present
PureSpark, an executable formal Haskell specification for Spark aggregate
combinators. Our specification allows us to deduce the precise condition for
deterministic outcomes from Spark aggregation. We report case studies analyzing
deterministic outcomes and correctness of Spark programs.
",1,0,0,0,0,0
8942,8943,On some further properties and application of Weibull-R family of distributions,"  In this paper, we provide some new results for the Weibull-R family of
distributions (Alzaghal, Ghosh and Alzaatreh (2016)). We derive some new
structural properties of the Weibull-R family of distributions. We provide
various characterizations of the family via conditional moments, some functions
of order statistics and via record values.
",0,0,1,1,0,0
1539,1540,Search for axions in streaming dark matter,"  A new search strategy for the detection of the elusive dark matter (DM) axion
is proposed. The idea is based on streaming DM axions, whose flux might get
temporally enormously enhanced due to gravitational lensing. This can happen if
the Sun or some planet (including the Moon) is found along the direction of a
DM stream propagating towards the Earth location. The experimental requirements
to the axion haloscope are a wide-band performance combined with a fast axion
rest mass scanning mode, which are feasible. Once both conditions have been
implemented in a haloscope, the axion search can continue parasitically almost
as before. Interestingly, some new DM axion detectors are operating wide-band
by default. In order not to miss the actually unpredictable timing of a
potential short duration signal, a network of co-ordinated axion antennae is
required, preferentially distributed world-wide. The reasoning presented here
for the axions applies to some degree also to any other DM candidates like the
WIMPs.
",0,1,0,0,0,0
17339,17340,Optimal Nonparametric Inference under Quantization,"  Statistical inference based on lossy or incomplete samples is of fundamental
importance in research areas such as signal/image processing, medical image
storage, remote sensing, signal transmission. In this paper, we propose a
nonparametric testing procedure based on quantized samples. In contrast to the
classic nonparametric approach, our method lives on a coarse grid of sample
information and are simple-to-use. Under mild technical conditions, we
establish the asymptotic properties of the proposed procedures including
asymptotic null distribution of the quantization test statistic as well as its
minimax power optimality. Concrete quantizers are constructed for achieving the
minimax optimality in practical use. Simulation results and a real data
analysis are provided to demonstrate the validity and effectiveness of the
proposed test. Our work bridges the classical nonparametric inference to modern
lossy data setting.
",1,0,1,1,0,0
2198,2199,Fixed-Rank Approximation of a Positive-Semidefinite Matrix from Streaming Data,"  Several important applications, such as streaming PCA and semidefinite
programming, involve a large-scale positive-semidefinite (psd) matrix that is
presented as a sequence of linear updates. Because of storage limitations, it
may only be possible to retain a sketch of the psd matrix. This paper develops
a new algorithm for fixed-rank psd approximation from a sketch. The approach
combines the Nystrom approximation with a novel mechanism for rank truncation.
Theoretical analysis establishes that the proposed method can achieve any
prescribed relative error in the Schatten 1-norm and that it exploits the
spectral decay of the input matrix. Computer experiments show that the proposed
method dominates alternative techniques for fixed-rank psd matrix approximation
across a wide range of examples.
",1,0,0,1,0,0
18875,18876,On Compression of Unsupervised Neural Nets by Pruning Weak Connections,"  Unsupervised neural nets such as Restricted Boltzmann Machines(RBMs) and Deep
Belif Networks(DBNs), are powerful in automatic feature extraction,unsupervised
weight initialization and density estimation. In this paper,we demonstrate that
the parameters of these neural nets can be dramatically reduced without
affecting their performance. We describe a method to reduce the parameters
required by RBM which is the basic building block for deep architectures.
Further we propose an unsupervised sparse deep architectures selection
algorithm to form sparse deep neural networks.Experimental results show that
there is virtually no loss in either generative or discriminative performance.
",1,0,0,0,0,0
1065,1066,On the Privacy of the Opal Data Release: A Response,"  This document is a response to a report from the University of Melbourne on
the privacy of the Opal dataset release. The Opal dataset was released by
Data61 (CSIRO) in conjunction with the Transport for New South Wales (TfNSW).
The data consists of two separate weeks of ""tap-on/tap-off"" data of individuals
who used any of the four different modes of public transport from TfNSW: buses,
light rail, train and ferries. These taps are recorded through the smart
ticketing system, known as Opal, available in the state of New South Wales,
Australia.
",1,0,0,0,0,0
6720,6721,Imitating Driver Behavior with Generative Adversarial Networks,"  The ability to accurately predict and simulate human driving behavior is
critical for the development of intelligent transportation systems. Traditional
modeling methods have employed simple parametric models and behavioral cloning.
This paper adopts a method for overcoming the problem of cascading errors
inherent in prior approaches, resulting in realistic behavior that is robust to
trajectory perturbations. We extend Generative Adversarial Imitation Learning
to the training of recurrent policies, and we demonstrate that our model
outperforms rule-based controllers and maximum likelihood models in realistic
highway simulations. Our model both reproduces emergent behavior of human
drivers, such as lane change rate, while maintaining realistic control over
long time horizons.
",1,0,0,0,0,0
1842,1843,The Geodetic Hull Number is Hard for Chordal Graphs,"  We show the hardness of the geodetic hull number for chordal graphs.
",1,0,0,0,0,0
3396,3397,Temporal Convolution Networks for Real-Time Abdominal Fetal Aorta Analysis with Ultrasound,"  The automatic analysis of ultrasound sequences can substantially improve the
efficiency of clinical diagnosis. In this work we present our attempt to
automate the challenging task of measuring the vascular diameter of the fetal
abdominal aorta from ultrasound images. We propose a neural network
architecture consisting of three blocks: a convolutional layer for the
extraction of imaging features, a Convolution Gated Recurrent Unit (C-GRU) for
enforcing the temporal coherence across video frames and exploiting the
temporal redundancy of a signal, and a regularized loss function, called
\textit{CyclicLoss}, to impose our prior knowledge about the periodicity of the
observed signal. We present experimental evidence suggesting that the proposed
architecture can reach an accuracy substantially superior to previously
proposed methods, providing an average reduction of the mean squared error from
$0.31 mm^2$ (state-of-art) to $0.09 mm^2$, and a relative error reduction from
$8.1\%$ to $5.3\%$. The mean execution speed of the proposed approach of 289
frames per second makes it suitable for real time clinical use.
",0,0,0,1,0,0
188,189,SecureTime: Secure Multicast Time Synchronization,"  Due to the increasing dependency of critical infrastructure on synchronized
clocks, network time synchronization protocols have become an attractive target
for attackers. We identify data origin authentication as the key security
objective and suggest to employ recently proposed high-performance digital
signature schemes (Ed25519 and MQQ-SIG)) as foundation of a novel set of
security measures to secure multicast time synchronization. We conduct
experiments to verify the computational and communication efficiency for using
these signatures in the standard time synchronization protocols NTP and PTP. We
propose additional security measures to prevent replay attacks and to mitigate
delay attacks. Our proposed solutions cover 1-step mode for NTP and PTP and we
extend our security measures specifically to 2-step mode (PTP) and show that
they have no impact on time synchronization's precision.
",1,0,0,0,0,0
8264,8265,Geometric vulnerability of democratic institutions against lobbying: a sociophysics approach,"  An alternative voting scheme is proposed to fill the democratic gap between a
president elected democratically via universal suffrage (deterministic outcome,
the actual majority decides), and a president elected by one person randomly
selected from the population (probabilistic outcome depending on respective
supports). Moving from one voting agent to a group of r randomly selected
voting agents reduces the probabilistic character of the outcome. Building r
such groups, each one electing its president, to constitute a group of the
groups with the r local presidents electing a higher-level president, does
reduce further the outcome probabilistic aspect. Repeating the process n times
leads to a n-level bottom-up pyramidal structure. The hierarchy top president
is still elected with a probability but the distance from a deterministic
outcome reduces quickly with increasing n. At a critical value n_{c,r} the
outcome turns deterministic recovering the same result a universal suffrage
would yield. The scheme yields several social advantages like the distribution
of local power to the competing minority making the structure more resilient,
yet preserving the presidency allocation to the actual majority. An area is
produced around fifty percent for which the president is elected with an almost
equiprobability slightly biased in favor of the actual majority. However, our
results reveal the existence of a severe geometric vulnerability to lobbying. A
tiny lobbying group is able to kill the democratic balance by seizing the
presidency democratically. It is sufficient to complete a correlated
distribution of a few agents at the hierarchy bottom. Moreover, at the present
stage, identifying an actual killing distribution is not feasible, which sheds
a disturbing light on the devastating effect geometric lobbying can have on
democratic hierarchical institutions.
",0,1,0,0,0,0
15383,15384,Residual Squeeze VGG16,"  Deep learning has given way to a new era of machine learning, apart from
computer vision. Convolutional neural networks have been implemented in image
classification, segmentation and object detection. Despite recent advancements,
we are still in the very early stages and have yet to settle on best practices
for network architecture in terms of deep design, small in size and a short
training time. In this work, we propose a very deep neural network comprised of
16 Convolutional layers compressed with the Fire Module adapted from the
SQUEEZENET model. We also call for the addition of residual connections to help
suppress degradation. This model can be implemented on almost every neural
network model with fully incorporated residual learning. This proposed model
Residual-Squeeze-VGG16 (ResSquVGG16) trained on the large-scale MIT
Places365-Standard scene dataset. In our tests, the model performed with
accuracy similar to the pre-trained VGG16 model in Top-1 and Top-5 validation
accuracy while also enjoying a 23.86% reduction in training time and an 88.4%
reduction in size. In our tests, this model was trained from scratch.
",1,0,0,0,0,0
13827,13828,Query K-means Clustering and the Double Dixie Cup Problem,"  We consider the problem of approximate $K$-means clustering with outliers and
side information provided by same-cluster queries and possibly noisy answers.
Our solution shows that, under some mild assumptions on the smallest cluster
size, one can obtain an $(1+\epsilon)$-approximation for the optimal potential
with probability at least $1-\delta$, where $\epsilon>0$ and $\delta\in(0,1)$,
using an expected number of $O(\frac{K^3}{\epsilon \delta})$ noiseless
same-cluster queries and comparison-based clustering of complexity $O(ndK +
\frac{K^3}{\epsilon \delta})$, here, $n$ denotes the number of points and $d$
the dimension of space. Compared to a handful of other known approaches that
perform importance sampling to account for small cluster sizes, the proposed
query technique reduces the number of queries by a factor of roughly
$O(\frac{K^6}{\epsilon^3})$, at the cost of possibly missing very small
clusters. We extend this settings to the case where some queries to the oracle
produce erroneous information, and where certain points, termed outliers, do
not belong to any clusters. Our proof techniques differ from previous methods
used for $K$-means clustering analysis, as they rely on estimating the sizes of
the clusters and the number of points needed for accurate centroid estimation
and subsequent nontrivial generalizations of the double Dixie cup problem. We
illustrate the performance of the proposed algorithm both on synthetic and real
datasets, including MNIST and CIFAR $10$.
",0,0,0,1,0,0
11791,11792,Toward a language-theoretic foundation for planning and filtering,"  We address problems underlying the algorithmic question of automating the
co-design of robot hardware in tandem with its apposite software. Specifically,
we consider the impact that degradations of a robot's sensor and actuation
suites may have on the ability of that robot to complete its tasks. We
introduce a new formal structure that generalizes and consolidates a variety of
well-known structures including many forms of plans, planning problems, and
filters, into a single data structure called a procrustean graph, and give
these graph structures semantics in terms of ideas based in formal language
theory. We describe a collection of operations on procrustean graphs (both
semantics-preserving and semantics-mutating), and show how a family of
questions about the destructiveness of a change to the robot hardware can be
answered by applying these operations. We also highlight the connections
between this new approach and existing threads of research, including
combinatorial filtering, Erdmann's strategy complexes, and hybrid automata.
",1,0,0,0,0,0
7438,7439,Interpreted Formalisms for Configurations,"  Imprecise and incomplete specification of system \textit{configurations}
threatens safety, security, functionality, and other critical system properties
and uselessly enlarges the configuration spaces to be searched by configuration
engineers and auto-tuners. To address these problems, this paper introduces
\textit{interpreted formalisms based on real-world types for configurations}.
Configuration values are lifted to values of real-world types, which we
formalize as \textit{subset types} in Coq. Values of these types are dependent
pairs whose components are values of underlying Coq types and proofs of
additional properties about them. Real-world types both extend and further
constrain \textit{machine-level} configurations, enabling richer, proof-based
checking of their consistency with real-world constraints. Tactic-based proof
scripts are written once to automate the construction of proofs, if proofs
exist, for configuration fields and whole configurations. \textit{Failures to
prove} reveal real-world type errors. Evaluation is based on a case study of
combinatorial optimization of Hadoop performance by meta-heuristic search over
Hadoop configurations spaces.
",1,0,0,0,0,0
6977,6978,Spontaneous symmetry breaking as a triangular relation between pairs of Goldstone bosons and the degenerate vacuum: Interactions of D-branes,"  We formulate the Nambu-Goldstone theorem as a triangular relation between
pairs of Goldstone bosons with the degenerate vacuum. The vacuum degeneracy is
then a natural consequence of this relation. Inside the scenario of String
Theory, we then find that there is a correspondence between the way how the
$D$-branes interact and the properties of the Goldstone bosons.
",0,1,0,0,0,0
20794,20795,Off-diagonal estimates of some Bergman-type operators on tube domains over symmetric cones,"  We obtain some necessary and sufficient conditions for the boundedness of a
family of positive operators defined on symmetric cones, we then deduce
off-diagonal boundedness of associated Bergman-type operators in tube domains
over symmetric cones.
",0,0,1,0,0,0
6112,6113,Connections on parahoric torsors over curves,"  We define parahoric $\cG$--torsors for certain Bruhat--Tits group scheme
$\cG$ on a smooth complex projective curve $X$ when the weights are real, and
also define connections on them. We prove that a $\cG$--torsor is given by a
homomorphism from $\pi_1(X\setminus D)$ to a maximal compact subgroup of $G$,
where $D\, \subset\, X$ is the parabolic divisor, if and only if the torsor is
polystable.
",0,0,1,0,0,0
19841,19842,Using Task Descriptions in Lifelong Machine Learning for Improved Performance and Zero-Shot Transfer,"  Knowledge transfer between tasks can improve the performance of learned
models, but requires an accurate estimate of the inter-task relationships to
identify the relevant knowledge to transfer. These inter-task relationships are
typically estimated based on training data for each task, which is inefficient
in lifelong learning settings where the goal is to learn each consecutive task
rapidly from as little data as possible. To reduce this burden, we develop a
lifelong learning method based on coupled dictionary learning that utilizes
high-level task descriptions to model the inter-task relationships. We show
that using task descriptors improves the performance of the learned task
policies, providing both theoretical justification for the benefit and
empirical demonstration of the improvement across a variety of learning
problems. Given only the descriptor for a new task, the lifelong learner is
also able to accurately predict a model for the new task through zero-shot
learning using the coupled dictionary, eliminating the need to gather training
data before addressing the task.
",1,0,0,1,0,0
8540,8541,Anyonic Entanglement and Topological Entanglement Entropy,"  We study the properties of entanglement in two-dimensional topologically
ordered phases of matter. Such phases support anyons, quasiparticles with
exotic exchange statistics. The emergent nonlocal state spaces of anyonic
systems admit a particular form of entanglement that does not exist in
conventional quantum mechanical systems. We study this entanglement by adapting
standard notions of entropy to anyonic systems. We use the algebraic theory of
anyon models (modular tensor categories) to illustrate the nonlocal
entanglement structure of anyonic systems. Using this formalism, we present a
general method of deriving the universal topological contributions to the
entanglement entropy for general system configurations of a topological phase,
including surfaces of arbitrary genus, punctures, and quasiparticle content. We
analyze a number of examples in detail. Our results recover and extend prior
results for anyonic entanglement and the topological entanglement entropy.
",0,1,0,0,0,0
14749,14750,Zero-Delay Rate Distortion via Filtering for Vector-Valued Gaussian Sources,"  We deal with zero-delay source coding of a vector-valued Gauss-Markov source
subject to a mean-squared error (MSE) fidelity criterion characterized by the
operational zero-delay vector-valued Gaussian rate distortion function (RDF).
We address this problem by considering the nonanticipative RDF (NRDF) which is
a lower bound to the causal optimal performance theoretically attainable (OPTA)
function and operational zero-delay RDF. We recall the realization that
corresponds to the optimal ""test-channel"" of the Gaussian NRDF, when
considering a vector Gauss-Markov source subject to a MSE distortion in the
finite time horizon. Then, we introduce sufficient conditions to show existence
of solution for this problem in the infinite time horizon. For the asymptotic
regime, we use the asymptotic characterization of the Gaussian NRDF to provide
a new equivalent realization scheme with feedback which is characterized by a
resource allocation (reverse-waterfilling) problem across the dimension of the
vector source. We leverage the new realization to derive a predictive coding
scheme via lattice quantization with subtractive dither and joint memoryless
entropy coding. This coding scheme offers an upper bound to the operational
zero-delay vector-valued Gaussian RDF. When we use scalar quantization, then
for ""r"" active dimensions of the vector Gauss-Markov source the gap between the
obtained lower and theoretical upper bounds is less than or equal to 0.254r + 1
bits/vector. We further show that it is possible when we use vector
quantization, and assume infinite dimensional Gauss-Markov sources to make the
previous gap to be negligible, i.e., Gaussian NRDF approximates the operational
zero-delay Gaussian RDF. We also extend our results to vector-valued Gaussian
sources of any finite memory under mild conditions. Our theoretical framework
is demonstrated with illustrative numerical experiments.
",1,0,0,0,0,0
2992,2993,Small-dimensional representations of algebraic groups of type $A_l$,"  For $G$ an algebraic group of type $A_l$ over an algebraically closed field
of characteristic $p$, we determine all irreducible rational representations of
$G$ in defining characteristic with dimensions $\le (l+1)^s$ for $s = 3, 4$,
provided that $l > 18$, $l > 35$ respectively. We also give explicit
descriptions of the corresponding modules for $s = 3$.
",0,0,1,0,0,0
16774,16775,Stellar Abundances for Galactic Archaeology Database IV - Compilation of Stars in Dwarf Galaxies,"  We have constructed the database of stars in the local group using the
extended version of the SAGA (Stellar Abundances for Galactic Archaeology)
database that contains stars in 24 dwarf spheroidal galaxies and ultra faint
dwarfs. The new version of the database includes more than 4500 stars in the
Milky Way, by removing the previous metallicity criterion of [Fe/H] <= -2.5,
and more than 6000 stars in the local group galaxies. We examined a validity of
using a combined data set for elemental abundances. We also checked a
consistency between the derived distances to individual stars and those to
galaxies in the literature values. Using the updated database, the
characteristics of stars in dwarf galaxies are discussed. Our statistical
analyses of alpha-element abundances show that the change of the slope of the
[alpha/Fe] relative to [Fe/H] (so-called ""knee"") occurs at [Fe/H] = -1.0+-0.1
for the Milky Way. The knee positions for selected galaxies are derived by
applying the same method. Star formation history of individual galaxies are
explored using the slope of the cumulative metallicity distribution function.
Radial gradients along the four directions are inspected in six galaxies where
we find no direction dependence of metallicity gradients along the major and
minor axes. The compilation of all the available data shows a lack of CEMP-s
population in dwarf galaxies, while there may be some CEMP-no stars at [Fe/H]
<~ -3 even in the very small sample. The inspection of the relationship between
Eu and Ba abundances confirms an anomalously Ba-rich population in Fornax,
which indicates a pre-enrichment of interstellar gas with r-process elements.
We do not find any evidence of anti-correlations in O-Na and Mg-Al abundances,
which characterises the abundance trends in the Galactic globular clusters.
",0,1,0,0,0,0
14106,14107,Tidal viscosity of Enceladus,"  In the preceding paper (Efroimsky 2017), we derived an expression for the
tidal dissipation rate in a homogeneous near-spherical Maxwell body librating
in longitude. Now, by equating this expression to the outgoing energy flux due
to the vapour plumes, we estimate the mean tidal viscosity of Enceladus, under
the assumption that the Enceladean mantle behaviour is Maxwell. This method
yields a value of $\,0.24\times 10^{14}\;\mbox{Pa~s}\,$ for the mean tidal
viscosity, which is very close to the viscosity of ice near the melting point.
",0,1,0,0,0,0
3141,3142,Stimulated Raman Scattering Imposes Fundamental Limits to the Duration and Bandwidth of Temporal Cavity Solitons,"  Temporal cavity solitons (CS) are optical pulses that can persist in passive
resonators, and they play a key role in the generation of coherent
microresonator frequency combs. In resonators made of amorphous materials, such
as fused silica, they can exhibit a spectral red-shift due to stimulated Raman
scattering. Here we show that this Raman-induced self-frequency-shift imposes a
fundamental limit on the duration and bandwidth of temporal CSs. Specifically,
we theoretically predict that stimulated Raman scattering introduces a
previously unidentified Hopf bifurcation that leads to destabilization of CSs
at large pump-cavity detunings, limiting the range of detunings over which they
can exist. We have confirmed our theoretical predictions by performing
extensive experiments in several different synchronously-driven fiber ring
resonators, obtaining results in excellent agreement with numerical
simulations. Our results could have significant implications for the future
design of Kerr frequency comb systems based on amorphous microresonators.
",0,1,0,0,0,0
9318,9319,Real-time convolutional networks for sonar image classification in low-power embedded systems,"  Deep Neural Networks have impressive classification performance, but this
comes at the expense of significant computational resources at inference time.
Autonomous Underwater Vehicles use low-power embedded systems for sonar image
perception, and cannot execute large neural networks in real-time. We propose
the use of max-pooling aggressively, and we demonstrate it with a Fire-based
module and a new Tiny module that includes max-pooling in each module. By
stacking them we build networks that achieve the same accuracy as bigger ones,
while reducing the number of parameters and considerably increasing
computational performance. Our networks can classify a 96x96 sonar image with
98.8 - 99.7 accuracy on only 41 to 61 milliseconds on a Raspberry Pi 2, which
corresponds to speedups of 28.6 - 19.7.
",1,0,0,0,0,0
3827,3828,A Machine Learning Alternative to P-values,"  This paper presents an alternative approach to p-values in regression
settings. This approach, whose origins can be traced to machine learning, is
based on the leave-one-out bootstrap for prediction error. In machine learning
this is called the out-of-bag (OOB) error. To obtain the OOB error for a model,
one draws a bootstrap sample and fits the model to the in-sample data. The
out-of-sample prediction error for the model is obtained by calculating the
prediction error for the model using the out-of-sample data. Repeating and
averaging yields the OOB error, which represents a robust cross-validated
estimate of the accuracy of the underlying model. By a simple modification to
the bootstrap data involving ""noising up"" a variable, the OOB method yields a
variable importance (VIMP) index, which directly measures how much a specific
variable contributes to the prediction precision of a model. VIMP provides a
scientifically interpretable measure of the effect size of a variable, we call
the ""predictive effect size"", that holds whether the researcher's model is
correct or not, unlike the p-value whose calculation is based on the assumed
correctness of the model. We also discuss a marginal VIMP index, also easily
calculated, which measures the marginal effect of a variable, or what we call
""the discovery effect"". The OOB procedure can be applied to both parametric and
nonparametric regression models and requires only that the researcher can
repeatedly fit their model to bootstrap and modified bootstrap data. We
illustrate this approach on a survival data set involving patients with
systolic heart failure and to a simulated survival data set where the model is
incorrectly specified to illustrate its robustness to model misspecification.
",1,0,0,1,0,0
11434,11435,"ROPPERI - A TPC readout with GEMs, pads and Timepix","  The concept of a hybrid readout of a time projection chamber is presented. It
combines a GEM-based amplification and a pad-based anode plane with a pixel
chip as readout electronics. This way, a high granularity enabling to identify
electron clusters from the primary ionisation is achieved as well as
flexibility and large anode coverage. The benefits of this high granularity, in
particular for dE/dx measurements, are outlined and the current software and
hardware development status towards a proof-of-principle is given.
",0,1,0,0,0,0
19245,19246,SD-CPS: Taming the Challenges of Cyber-Physical Systems with a Software-Defined Approach,"  Cyber-Physical Systems (CPS) revolutionize various application domains with
integration and interoperability of networking, computing systems, and
mechanical devices. Due to its scale and variety, CPS faces a number of
challenges and opens up a few research questions in terms of management,
fault-tolerance, and scalability. We propose a software-defined approach
inspired by Software-Defined Networking (SDN), to address the challenges for a
wider CPS adoption. We thus design a middleware architecture for the correct
and resilient operation of CPS, to manage and coordinate the interacting
devices centrally in the cyberspace whilst not sacrificing the functionality
and performance benefits inherent to a distributed execution.
",1,0,0,0,0,0
1184,1185,Predictive Simulations for Tuning Electronic and Optical Properties of SubPc Derivatives,"  Boron subphthalocyanine chloride is an electron donor material used in small
molecule organic photovoltaics with an unusually large molecular dipole moment.
Using first-principles calculations, we investigate enhancing the electronic
and optical properties of boron subphthalocyanine chloride, by substituting the
boron and chlorine atoms with other trivalent and halogen atoms in order to
modify the molecular dipole moment. Gas phase molecular structures and
properties are predicted with hybrid functionals. Using positions and
orientations of the known compounds as the starting coordinates for these
molecules, stable crystalline structures are derived following a procedure that
involves perturbation and accurate total energy minimization. Electronic
structure and photonic properties of the predicted crystals are computed using
the GW method and the Bethe-Salpeter equation, respectively. Finally, a simple
transport model is use to demonstrate the importance of molecular dipole
moments on device performance.
",0,1,0,0,0,0
6347,6348,HOUDINI: Lifelong Learning as Program Synthesis,"  We present a neurosymbolic framework for the lifelong learning of algorithmic
tasks that mix perception and procedural reasoning. Reusing high-level concepts
across domains and learning complex procedures are key challenges in lifelong
learning. We show that a program synthesis approach that combines gradient
descent with combinatorial search over programs can be a more effective
response to these challenges than purely neural methods. Our framework, called
HOUDINI, represents neural networks as strongly typed, differentiable
functional programs that use symbolic higher-order combinators to compose a
library of neural functions. Our learning algorithm consists of: (1) a symbolic
program synthesizer that performs a type-directed search over parameterized
programs, and decides on the library functions to reuse, and the architectures
to combine them, while learning a sequence of tasks; and (2) a neural module
that trains these programs using stochastic gradient descent. We evaluate
HOUDINI on three benchmarks that combine perception with the algorithmic tasks
of counting, summing, and shortest-path computation. Our experiments show that
HOUDINI transfers high-level concepts more effectively than traditional
transfer learning and progressive neural networks, and that the typed
representation of networks significantly accelerates the search.
",1,0,0,1,0,0
7991,7992,Learning Hidden Quantum Markov Models,"  Hidden Quantum Markov Models (HQMMs) can be thought of as quantum
probabilistic graphical models that can model sequential data. We extend
previous work on HQMMs with three contributions: (1) we show how classical
hidden Markov models (HMMs) can be simulated on a quantum circuit, (2) we
reformulate HQMMs by relaxing the constraints for modeling HMMs on quantum
circuits, and (3) we present a learning algorithm to estimate the parameters of
an HQMM from data. While our algorithm requires further optimization to handle
larger datasets, we are able to evaluate our algorithm using several synthetic
datasets. We show that on HQMM generated data, our algorithm learns HQMMs with
the same number of hidden states and predictive accuracy as the true HQMMs,
while HMMs learned with the Baum-Welch algorithm require more states to match
the predictive accuracy.
",0,0,0,1,0,0
17856,17857,Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice,"  It is well known that the initialization of weights in deep neural networks
can have a dramatic impact on learning speed. For example, ensuring the mean
squared singular value of a network's input-output Jacobian is $O(1)$ is
essential for avoiding the exponential vanishing or explosion of gradients. The
stronger condition that all singular values of the Jacobian concentrate near
$1$ is a property known as dynamical isometry. For deep linear networks,
dynamical isometry can be achieved through orthogonal weight initialization and
has been shown to dramatically speed up learning; however, it has remained
unclear how to extend these results to the nonlinear setting. We address this
question by employing powerful tools from free probability theory to compute
analytically the entire singular value distribution of a deep network's
input-output Jacobian. We explore the dependence of the singular value
distribution on the depth of the network, the weight initialization, and the
choice of nonlinearity. Intriguingly, we find that ReLU networks are incapable
of dynamical isometry. On the other hand, sigmoidal networks can achieve
isometry, but only with orthogonal weight initialization. Moreover, we
demonstrate empirically that deep nonlinear networks achieving dynamical
isometry learn orders of magnitude faster than networks that do not. Indeed, we
show that properly-initialized deep sigmoidal networks consistently outperform
deep ReLU networks. Overall, our analysis reveals that controlling the entire
distribution of Jacobian singular values is an important design consideration
in deep learning.
",1,0,0,1,0,0
2896,2897,On the Performance of a Canonical Labeling for Matching Correlated Erdős-Rényi Graphs,"  Graph matching in two correlated random graphs refers to the task of
identifying the correspondence between vertex sets of the graphs. Recent
results have characterized the exact information-theoretic threshold for graph
matching in correlated Erdős-Rényi graphs. However, very little is known
about the existence of efficient algorithms to achieve graph matching without
seeds. In this work we identify a region in which a straightforward $O(n^2\log
n)$-time canonical labeling algorithm, initially introduced in the context of
graph isomorphism, succeeds in matching correlated Erdős-Rényi graphs.
The algorithm has two steps. In the first step, all vertices are labeled by
their degrees and a trivial minimum distance matching (i.e., simply sorting
vertices according to their degrees) matches a fixed number of highest degree
vertices in the two graphs. Having identified this subset of vertices, the
remaining vertices are matched using a matching algorithm for bipartite graphs.
",0,0,0,1,0,0
7997,7998,Coalescing particle systems and applications to nonlinear Fokker-Planck equations,"  We study a stochastic particle system with a logarithmically-singular
inter-particle interaction potential which allows for inelastic particle
collisions. We relate the squared Bessel process to the evolution of localized
clusters of particles, and develop a numerical method capable of detecting
collisions of many point particles without the use of pairwise computations, or
very refined adaptive timestepping. We show that when the system is in an
appropriate parameter regime, the hydrodynamic limit of the empirical mass
density of the system is a solution to a nonlinear Fokker-Planck equation, such
as the Patlak-Keller-Segel (PKS) model, or its multispecies variant. We then
show that the presented numerical method is well-suited for the simulation of
the formation of finite-time singularities in the PKS, as well as PKS pre- and
post-blow-up dynamics. Additionally, we present numerical evidence that blow-up
with an increasing total second moment in the two species Keller-Segel system
occurs with a linearly increasing second moment in one component, and a
linearly decreasing second moment in the other component.
",0,0,1,0,0,0
13544,13545,Generalized Expectation Consistent Signal Recovery for Nonlinear Measurements,"  In this paper, we propose a generalized expectation consistent signal
recovery algorithm to estimate the signal $\mathbf{x}$ from the nonlinear
measurements of a linear transform output $\mathbf{z}=\mathbf{A}\mathbf{x}$.
This estimation problem has been encountered in many applications, such as
communications with front-end impairments, compressed sensing, and phase
retrieval. The proposed algorithm extends the prior art called generalized
turbo signal recovery from a partial discrete Fourier transform matrix
$\mathbf{A}$ to a class of general matrices. Numerical results show the
excellent agreement of the proposed algorithm with the theoretical
Bayesian-optimal estimator derived using the replica method.
",1,0,1,0,0,0
4716,4717,Dual-Primal Graph Convolutional Networks,"  In recent years, there has been a surge of interest in developing deep
learning methods for non-Euclidean structured data such as graphs. In this
paper, we propose Dual-Primal Graph CNN, a graph convolutional architecture
that alternates convolution-like operations on the graph and its dual. Our
approach allows to learn both vertex- and edge features and generalizes the
previous graph attention (GAT) model. We provide extensive experimental
validation showing state-of-the-art results on a variety of tasks tested on
established graph benchmarks, including CORA and Citeseer citation networks as
well as MovieLens, Flixter, Douban and Yahoo Music graph-guided recommender
systems.
",0,0,0,1,0,0
10747,10748,Mixed Precision Training,"  Deep neural networks have enabled progress in a wide variety of applications.
Growing the size of the neural network typically results in improved accuracy.
As model sizes grow, the memory and compute requirements for training these
models also increases. We introduce a technique to train deep neural networks
using half precision floating point numbers. In our technique, weights,
activations and gradients are stored in IEEE half-precision format.
Half-precision floating numbers have limited numerical range compared to
single-precision numbers. We propose two techniques to handle this loss of
information. Firstly, we recommend maintaining a single-precision copy of the
weights that accumulates the gradients after each optimizer step. This
single-precision copy is rounded to half-precision format during training.
Secondly, we propose scaling the loss appropriately to handle the loss of
information with half-precision gradients. We demonstrate that this approach
works for a wide variety of models including convolution neural networks,
recurrent neural networks and generative adversarial networks. This technique
works for large scale models with more than 100 million parameters trained on
large datasets. Using this approach, we can reduce the memory consumption of
deep learning models by nearly 2x. In future processors, we can also expect a
significant computation speedup using half-precision hardware units.
",1,0,0,1,0,0
19372,19373,Variational Bayesian Inference For A Scale Mixture Of Normal Distributions Handling Missing Data,"  In this paper, a scale mixture of Normal distributions model is developed for
classification and clustering of data having outliers and missing values. The
classification method, based on a mixture model, focuses on the introduction of
latent variables that gives us the possibility to handle sensitivity of model
to outliers and to allow a less restrictive modelling of missing data.
Inference is processed through a Variational Bayesian Approximation and a
Bayesian treatment is adopted for model learning, supervised classification and
clustering.
",0,0,0,1,0,0
1309,1310,Siamese Network of Deep Fisher-Vector Descriptors for Image Retrieval,"  This paper addresses the problem of large scale image retrieval, with the aim
of accurately ranking the similarity of a large number of images to a given
query image. To achieve this, we propose a novel Siamese network. This network
consists of two computational strands, each comprising of a CNN component
followed by a Fisher vector component. The CNN component produces dense, deep
convolutional descriptors that are then aggregated by the Fisher Vector method.
Crucially, we propose to simultaneously learn both the CNN filter weights and
Fisher Vector model parameters. This allows us to account for the evolving
distribution of deep descriptors over the course of the learning process. We
show that the proposed approach gives significant improvements over the
state-of-the-art methods on the Oxford and Paris image retrieval datasets.
Additionally, we provide a baseline performance measure for both these datasets
with the inclusion of 1 million distractors.
",1,0,0,0,0,0
18826,18827,"Alignment, Orientation, and Coulomb Explosion of Difluoroiodobenzene Studied with the Pixel Imaging Mass Spectrometry (PImMS) Camera","  Laser-induced adiabatic alignment and mixed-field orientation of
2,6-difluoroiodobenzene (C6H3F2I) molecules are probed by Coulomb explosion
imaging following either near-infrared strong-field ionization or
extreme-ultraviolet multi-photon inner-shell ionization using free-electron
laser pulses. The resulting photoelectrons and fragment ions are captured by a
double-sided velocity map imaging spectrometer and projected onto two
position-sensitive detectors. The ion side of the spectrometer is equipped with
the Pixel Imaging Mass Spectrometry (PImMS) camera, a time-stamping pixelated
detector that can record the hit positions and arrival times of up to four ions
per pixel per acquisition cycle. Thus, the time-of-flight trace and ion
momentum distributions for all fragments can be recorded simultaneously. We
show that we can obtain a high degree of one- and three-dimensional alignment
and mixed- field orientation, and compare the Coulomb explosion process induced
at both wavelengths.
",0,1,0,0,0,0
20114,20115,Towards Robust Neural Networks via Random Self-ensemble,"  Recent studies have revealed the vulnerability of deep neural networks: A
small adversarial perturbation that is imperceptible to human can easily make a
well-trained deep neural network misclassify. This makes it unsafe to apply
neural networks in security-critical applications. In this paper, we propose a
new defense algorithm called Random Self-Ensemble (RSE) by combining two
important concepts: {\bf randomness} and {\bf ensemble}. To protect a targeted
model, RSE adds random noise layers to the neural network to prevent the strong
gradient-based attacks, and ensembles the prediction over random noises to
stabilize the performance. We show that our algorithm is equivalent to ensemble
an infinite number of noisy models $f_\epsilon$ without any additional memory
overhead, and the proposed training procedure based on noisy stochastic
gradient descent can ensure the ensemble model has a good predictive
capability. Our algorithm significantly outperforms previous defense techniques
on real data sets. For instance, on CIFAR-10 with VGG network (which has 92\%
accuracy without any attack), under the strong C\&W attack within a certain
distortion tolerance, the accuracy of unprotected model drops to less than
10\%, the best previous defense technique has $48\%$ accuracy, while our method
still has $86\%$ prediction accuracy under the same level of attack. Finally,
our method is simple and easy to integrate into any neural network.
",1,0,0,1,0,0
14026,14027,Planar graphs as L-intersection or L-contact graphs,"  The L-intersection graphs are the graphs that have a representation as
intersection graphs of axis parallel shapes in the plane. A subfamily of these
graphs are {L, |, --}-contact graphs which are the contact graphs of axis
parallel L, |, and -- shapes in the plane. We prove here two results that were
conjectured by Chaplick and Ueckerdt in 2013. We show that planar graphs are
L-intersection graphs, and that triangle-free planar graphs are {L, |,
--}-contact graphs. These results are obtained by a new and simple
decomposition technique for 4-connected triangulations. Our results also
provide a much simpler proof of the known fact that planar graphs are segment
intersection graphs.
",1,0,0,0,0,0
16863,16864,Dimension theory and components of algebraic stacks,"  We prove some basic results on the dimension theory of algebraic stacks, and
on the multiplicities of their irreducible components, for which we do not know
a reference.
",0,0,1,0,0,0
16570,16571,On the Log Partition Function of Ising Model on Stochastic Block Model,"  A sparse stochastic block model (SBM) with two communities is defined by the
community probability $\pi_0,\pi_1$, and the connection probability between
communities $a,b\in\{0,1\}$, namely $q_{ab} = \frac{\alpha_{ab}}{n}$. When
$q_{ab}$ is constant in $a,b$, the random graph is simply the
Erdős-Rény random graph. We evaluate the log partition function of the
Ising model on sparse SBM with two communities.
As an application, we give consistent parameter estimation of the sparse SBM
with two communities in a special case. More specifically, let $d_0,d_1$ be the
average degree of the two communities, i.e.,
$d_0\overset{def}{=}\pi_0\alpha_{00}+\pi_1\alpha_{01},d_1\overset{def}{=}\pi_0\alpha_{10}+\pi_1\alpha_{11}$.
We focus on the regime $d_0=d_1$ (the regime $d_0\ne d_1$ is trivial). In this
regime, there exists $d,\lambda$ and $r\geq 0$ with $\pi_0=\frac{1}{1+r},
\pi_1=\frac{r}{1+r}$, $\alpha_{00}=d(1+r\lambda), \alpha_{01}=\alpha_{10} =
d(1-\lambda), \alpha_{11} = d(1+\frac{\lambda}{r})$. We give a consistent
estimator of $r$ when $\lambda<0$. The estimator of $\lambda$ given by
\citep{mossel2015reconstruction} is valid in the general situation. We also
provide a random clustering algorithm which does not require knowledge of
parameters and which is positively correlated with the true community label
when $\lambda<0$.
",0,0,0,1,0,0
19305,19306,From the Icosahedron to E8,"  The regular icosahedron is connected to many exceptional objects in
mathematics. Here we describe two constructions of the $\mathrm{E}_8$ lattice
from the icosahedron. One uses a subring of the quaternions called the
""icosians"", while the other uses du Val's work on the resolution of Kleinian
singularities. Together they link the golden ratio, the quaternions, the
quintic equation, the 600-cell, and the Poincare homology 3-sphere. We leave it
as a challenge to the reader to find the connection between these two
constructions.
",0,0,1,0,0,0
67,68,$S^1$-equivariant Index theorems and Morse inequalities on complex manifolds with boundary,"  Let $M$ be a complex manifold of dimension $n$ with smooth connected boundary
$X$. Assume that $\overline M$ admits a holomorphic $S^1$-action preserving the
boundary $X$ and the $S^1$-action is transversal and CR on $X$. We show that
the $\overline\partial$-Neumann Laplacian on $M$ is transversally elliptic and
as a consequence, the $m$-th Fourier component of the $q$-th Dolbeault
cohomology group $H^q_m(\overline M)$ is finite dimensional, for every
$m\in\mathbb Z$ and every $q=0,1,\ldots,n$. This enables us to define
$\sum^{n}_{j=0}(-1)^j{\rm dim\,}H^q_m(\overline M)$ the $m$-th Fourier
component of the Euler characteristic on $M$ and to study large $m$-behavior of
$H^q_m(\overline M)$. In this paper, we establish an index formula for
$\sum^{n}_{j=0}(-1)^j{\rm dim\,}H^q_m(\overline M)$ and Morse inequalities for
$H^q_m(\overline M)$.
",0,0,1,0,0,0
5846,5847,On Bousfield's problem for solvable groups of finite Prüfer rank,"  For a group $G$ and $R=\mathbb Z,\mathbb Z/p,\mathbb Q$ we denote by $\hat
G_R$ the $R$-completion of $G.$ We study the map $H_n(G,K)\to H_n(\hat G_R,K),$
where $(R,K)=(\mathbb Z,\mathbb Z/p),(\mathbb Z/p,\mathbb Z/p),(\mathbb
Q,\mathbb Q).$ We prove that $H_2(G,K)\to H_2(\hat G_R,K)$ is an epimorphism
for a finitely generated solvable group $G$ of finite Prüfer rank. In
particular, Bousfield's $HK$-localisation of such groups coincides with the
$K$-completion for $K=\mathbb Z/p,\mathbb Q.$ Moreover, we prove that
$H_n(G,K)\to H_n(\hat G_R,K)$ is an epimorphism for any $n$ if $G$ is a
finitely presented group of the form $G=M\rtimes C,$ where $C$ is the infinite
cyclic group and $M$ is a $C$-module.
",0,0,1,0,0,0
11437,11438,Controlling of blow-up responses by a nonlinear $\cal{PT}$ symmetric coupling,"  We investigate the dynamics of a coupled waveguide system with competing
linear and nonlinear loss-gain profiles which can facilitate power saturation.
We show the usefulness of the model in achieving unidirectional beam
propagation. In this regard, the considered type of coupled waveguide system
has two drawbacks, (i) difficulty in achieving perfect isolation of light in a
waveguide and (ii) existence of blow-up type behavior for certain input power
situations. We here show a nonlinear $\cal{PT}$ symmetric coupling that helps
to overcome these two drawbacks. Such a nonlinear coupling has close connection
with the phenomenon of stimulated Raman scattering. In particular, we have
elucidated the role of this nonlinear coupling using an integrable $\cal{PT}$
symmetric situation. In particular, using the integrals of motion, we have
reduced this coupled waveguide problem to the problem of dynamics of a particle
in a potential. With the latter picture, we have clearly illustrated the role
of the considered nonlinear coupling. The above $\cal{PT}$ symmetric case
corresponds to a limiting form of a general equation describing the phenomenon
of stimulated Raman scattering. We also point out the ability to transport
light unidirectionally even in this general case.
",0,1,0,0,0,0
14354,14355,A generalization of crossing families,"  For a set of points in the plane, a \emph{crossing family} is a set of line
segments, each joining two of the points, such that any two line segments
cross. We investigate the following generalization of crossing families: a
\emph{spoke set} is a set of lines drawn through a point set such that each
unbounded region of the induced line arrangement contains at least one point of
the point set. We show that every point set has a spoke set of size
$\sqrt{\frac{n}{8}}$. We also characterize the matchings obtained by selecting
exactly one point in each unbounded region and connecting every such point to
the point in the antipodal unbounded region.
",1,0,0,0,0,0
2129,2130,Central limit theorem for the variable bandwidth kernel density estimators,"  In this paper we study the ideal variable bandwidth kernel density estimator
introduced by McKay (1993) and Jones, McKay and Hu (1994) and the plug-in
practical version of the variable bandwidth kernel estimator with two sequences
of bandwidths as in Giné and Sang (2013). Based on the bias and variance
analysis of the ideal and true variable bandwidth kernel density estimators, we
study the central limit theorems for each of them.
",0,0,1,1,0,0
7013,7014,"Online to Offline Conversions, Universality and Adaptive Minibatch Sizes","  We present an approach towards convex optimization that relies on a novel
scheme which converts online adaptive algorithms into offline methods. In the
offline optimization setting, our derived methods are shown to obtain
favourable adaptive guarantees which depend on the harmonic sum of the queried
gradients. We further show that our methods implicitly adapt to the objective's
structure: in the smooth case fast convergence rates are ensured without any
prior knowledge of the smoothness parameter, while still maintaining guarantees
in the non-smooth setting. Our approach has a natural extension to the
stochastic setting, resulting in a lazy version of SGD (stochastic GD), where
minibathces are chosen \emph{adaptively} depending on the magnitude of the
gradients. Thus providing a principled approach towards choosing minibatch
sizes.
",1,0,1,1,0,0
6030,6031,Semiparametric panel data models using neural networks,"  This paper presents an estimator for semiparametric models that uses a
feed-forward neural network to fit the nonparametric component. Unlike many
methodologies from the machine learning literature, this approach is suitable
for longitudinal/panel data. It provides unbiased estimation of the parametric
component of the model, with associated confidence intervals that have
near-nominal coverage rates. Simulations demonstrate (1) efficiency, (2) that
parametric estimates are unbiased, and (3) coverage properties of estimated
intervals. An application section demonstrates the method by predicting
county-level corn yield using daily weather data from the period 1981-2015,
along with parametric time trends representing technological change. The method
is shown to out-perform linear methods such as OLS and ridge/lasso, as well as
random forest. The procedures described in this paper are implemented in the R
package panelNNET.
",0,0,0,1,0,0
16959,16960,Spectral parameter power series for arbitrary order linear differential equations,"  Let $L$ be the $n$-th order linear differential operator $Ly = \phi_0y^{(n)}
+ \phi_1y^{(n-1)} + \cdots + \phi_ny$ with variable coefficients. A
representation is given for $n$ linearly independent solutions of $Ly=\lambda r
y$ as power series in $\lambda$, generalizing the SPPS (spectral parameter
power series) solution which has been previously developed for $n=2$. The
coefficient functions in these series are obtained by recursively iterating a
simple integration process, begining with a solution system for $\lambda=0$. It
is shown how to obtain such an initializing system working upwards from
equations of lower order. The values of the successive derivatives of the power
series solutions at the basepoint of integration are given, which provides a
technique for numerical solution of $n$-th order initial value problems and
spectral problems.
",0,0,1,0,0,0
10712,10713,The Impact of Small-Cell Bandwidth Requirements on Strategic Operators,"  Small-cell deployment in licensed and unlicensed spectrum is considered to be
one of the key approaches to cope with the ongoing wireless data demand
explosion. Compared to traditional cellular base stations with large
transmission power, small-cells typically have relatively low transmission
power, which makes them attractive for some spectrum bands that have strict
power regulations, for example, the 3.5GHz band [1]. In this paper we consider
a heterogeneous wireless network consisting of one or more service providers
(SPs). Each SP operates in both macro-cells and small-cells, and provides
service to two types of users: mobile and fixed. Mobile users can only
associate with macro-cells whereas fixed users can connect to either macro- or
small-cells. The SP charges a price per unit rate for each type of service.
Each SP is given a fixed amount of bandwidth and splits it between macro- and
small-cells. Motivated by bandwidth regulations, such as those for the 3.5Gz
band, we assume a minimum amount of bandwidth has to be set aside for
small-cells. We study the optimal pricing and bandwidth allocation strategies
in both monopoly and competitive scenarios. In the monopoly scenario the
strategy is unique. In the competitive scenario there exists a unique Nash
equilibrium, which depends on the regulatory constraints. We also analyze the
social welfare achieved, and compare it to that without the small-cell
bandwidth constraints. Finally, we discuss implications of our results on the
effectiveness of the minimum bandwidth constraint on influencing small-cell
deployments.
",1,0,1,0,0,0
20127,20128,MITHRIL: Mining Sporadic Associations for Cache Prefetching,"  The growing pressure on cloud application scalability has accentuated storage
performance as a critical bottle- neck. Although cache replacement algorithms
have been extensively studied, cache prefetching - reducing latency by
retrieving items before they are actually requested remains an underexplored
area. Existing approaches to history-based prefetching, in particular, provide
too few benefits for real systems for the resources they cost. We propose
MITHRIL, a prefetching layer that efficiently exploits historical patterns in
cache request associations. MITHRIL is inspired by sporadic association rule
mining and only relies on the timestamps of requests. Through evaluation of 135
block-storage traces, we show that MITHRIL is effective, giving an average of a
55% hit ratio increase over LRU and PROBABILITY GRAPH, a 36% hit ratio gain
over AMP at reasonable cost. We further show that MITHRIL can supplement any
cache replacement algorithm and be readily integrated into existing systems.
Furthermore, we demonstrate the improvement comes from MITHRIL being able to
capture mid-frequency blocks.
",1,0,0,0,0,0
4378,4379,Graphical Sequent Calculi for Modal Logics,"  The syntax of modal graphs is defined in terms of the continuous cut and
broken cut following Charles Peirce's notation in the gamma part of his
graphical logic of existential graphs. Graphical calculi for normal modal
logics are developed based on a reformulation of the graphical calculus for
classical propositional logic. These graphical calculi are of the nature of
deep inference. The relationship between graphical calculi and sequent calculi
for modal logics is shown by translations between graphs and modal formulas.
",1,0,0,0,0,0
18461,18462,Histograms of Gaussian normal distribution for feature matching in clutter scenes,"  3D feature descriptor provide information between corresponding models and
scenes. 3D objection recognition in cluttered scenes, however, remains a
largely unsolved problem. Practical applications impose several challenges
which are not fully addressed by existing methods. Especially in cluttered
scenes there are many feature mismatches between scenes and models. We
therefore propose Histograms of Gaussian Normal Distribution (HGND) for
extracting salient features on a local reference frame (LRF) that enables us to
solve this problem. We propose a LRF on each local surface patches using the
scatter matrix's eigenvectors. Then the HGND information of each salient point
is calculated on the LRF, for which we use both the mesh and point data of the
depth image. Experiments on 45 cluttered scenes of the Bologna Dataset and 50
cluttered scenes of the UWA Dataset are made to evaluate the robustness and
descriptiveness of our HGND. Experiments carried out by us demonstrate that
HGND obtains a more reliable matching rate than state-of-the-art approaches in
cluttered situations.
",1,0,0,0,0,0
11051,11052,Angle-dependent electron spin resonance of YbRh$_2$Si$_2$ measured with planar microwave resonators and in-situ rotation,"  We present a new experimental approach to investigate the magnetic properties
of the anisotropic heavy-fermion system YbRh$_2$Si$_2$ as a function of
crystallographic orientation. Angle-dependent electron spin resonance (ESR)
measurements are performed at a low temperature of 1.6 K and at an ESR
frequency of 4.4 GHz utilizing a superconducting planar microwave resonator in
a $^4$He-cryostat in combination with in-situ sample rotation. The obtained ESR
g-factor of YbRh$_2$Si$_2$ as a function of the crystallographic angle is
consistent with results of previous measurements using conventional ESR
spectrometers at higher frequencies and fields. Perspectives to implement this
experimental approach into a dilution refrigerator and to reach the
magnetically ordered phase of YbRh$_2$Si$_2$ are discussed.
",0,1,0,0,0,0
20696,20697,Analysis of Approximate Stochastic Gradient Using Quadratic Constraints and Sequential Semidefinite Programs,"  We present convergence rate analysis for the approximate stochastic gradient
method, where individual gradient updates are corrupted by computation errors.
We develop stochastic quadratic constraints to formulate a small linear matrix
inequality (LMI) whose feasible set characterizes convergence properties of the
approximate stochastic gradient. Based on this LMI condition, we develop a
sequential minimization approach to analyze the intricate trade-offs that
couple stepsize selection, convergence rate, optimization accuracy, and
robustness to gradient inaccuracy. We also analytically solve this LMI
condition and obtain theoretical formulas that quantify the convergence
properties of the approximate stochastic gradient under various assumptions on
the loss functions.
",0,0,0,1,0,0
9956,9957,Emergent Phases of Fractonic Matter,"  Fractons are emergent particles which are immobile in isolation, but which
can move together in dipolar pairs or other small clusters. These exotic
excitations naturally occur in certain quantum phases of matter described by
tensor gauge theories. Previous research has focused on the properties of small
numbers of fractons and their interactions, effectively mapping out the
""Standard Model"" of fractons. In the present work, however, we consider systems
with a finite density of either fractons or their dipolar bound states, with a
focus on the $U(1)$ fracton models. We study some of the phases in which
emergent fractonic matter can exist, thereby initiating the study of the
""condensed matter"" of fractons. We begin by considering a system with a finite
density of fractons, which we show can exhibit microemulsion physics, in which
fractons form small-scale clusters emulsed in a phase dominated by long-range
repulsion. We then move on to study systems with a finite density of mobile
dipoles, which have phases analogous to many conventional condensed matter
phases. We focus on two major examples: Fermi liquids and quantum Hall phases.
A finite density of fermionic dipoles will form a Fermi surface and enter a
Fermi liquid phase. Interestingly, this dipolar Fermi liquid exhibits a
finite-temperature phase transition, corresponding to an unbinding transition
of fractons. Finally, we study chiral two-dimensional phases corresponding to
dipoles in ""quantum Hall"" states of their emergent magnetic field. We study
numerous aspects of these generalized quantum Hall systems, such as their edge
theories and ground state degeneracies.
",0,1,0,0,0,0
19587,19588,Probabilistic Combination of Noisy Points and Planes for RGB-D Odometry,"  This work proposes a visual odometry method that combines points and plane
primitives, extracted from a noisy depth camera. Depth measurement uncertainty
is modelled and propagated through the extraction of geometric primitives to
the frame-to-frame motion estimation, where pose is optimized by weighting the
residuals of 3D point and planes matches, according to their uncertainties.
Results on an RGB-D dataset show that the combination of points and planes,
through the proposed method, is able to perform well in poorly textured
environments, where point-based odometry is bound to fail.
",1,0,0,0,0,0
8923,8924,A generalization of the injectivity condition for Projected Entangled Pair States,"  We introduce a family of tensor network states that we term semi-injective
Projected Entangled-Pair States (PEPS). They extend the class of injective PEPS
and include other states, like the ground states of the AKLT and the CZX models
in square lattices. We construct parent Hamiltonians for which semi-injective
PEPS are unique ground states. We also determine the necessary and sufficient
conditions for two tensors to generate the same family of such states in two
spatial dimensions. Using this result, we show that the third cohomology
labeling of Symmetry Protected Topological phases extends to semi-injective
PEPS.
",0,1,0,0,0,0
12735,12736,Exploring Heritability of Functional Brain Networks with Inexact Graph Matching,"  Data-driven brain parcellations aim to provide a more accurate representation
of an individual's functional connectivity, since they are able to capture
individual variability that arises due to development or disease. This renders
comparisons between the emerging brain connectivity networks more challenging,
since correspondences between their elements are not preserved. Unveiling these
correspondences is of major importance to keep track of local functional
connectivity changes. We propose a novel method based on graph edit distance
for the comparison of brain graphs directly in their domain, that can
accurately reflect similarities between individual networks while providing the
network element correspondences. This method is validated on a dataset of 116
twin subjects provided by the Human Connectome Project.
",1,0,0,0,0,0
2434,2435,Ensembles of Multiple Models and Architectures for Robust Brain Tumour Segmentation,"  Deep learning approaches such as convolutional neural nets have consistently
outperformed previous methods on challenging tasks such as dense, semantic
segmentation. However, the various proposed networks perform differently, with
behaviour largely influenced by architectural choices and training settings.
This paper explores Ensembles of Multiple Models and Architectures (EMMA) for
robust performance through aggregation of predictions from a wide range of
methods. The approach reduces the influence of the meta-parameters of
individual models and the risk of overfitting the configuration to a particular
database. EMMA can be seen as an unbiased, generic deep learning model which is
shown to yield excellent performance, winning the first position in the BRATS
2017 competition among 50+ participating teams.
",1,0,0,0,0,0
9177,9178,Giant room-temperature barocaloric effects in PDMS rubber at low pressures,"  The barocaloric effect is still an incipient scientific topic, but it has
been attracting an increasing attention in the last years due to the promising
perspectives for its application in alternative cooling devices. Here, we
present giant values of barocaloric entropy change and temperature change
induced by low pressures in PDMS elastomer around room temperature. Adiabatic
temperature changes of 12.0 K and 28.5 K were directly measured for pressure
changes of 173 MPa and 390 MPa, respectively, associated with large normalized
temperature changes (~70 K GPa-1). From adiabatic temperature change data, we
obtained entropy change values larger than 140 J kg-1 K-1. We found barocaloric
effect values that exceed those previously reported for any promising
barocaloric materials from direct measurements of temperature change around
room temperature. Our results stimulate the study of the barocaloric effect in
elastomeric polymers and broaden the pathway to use this effect in solid-state
cooling technologies.
",0,1,0,0,0,0
3048,3049,"Primitivity, Uniform Minimality and State Complexity of Boolean Operations","  A minimal deterministic finite automaton (DFA) is uniformly minimal if it
always remains minimal when the final state set is replaced by a non-empty
proper subset of the state set. We prove that a permutation DFA is uniformly
minimal if and only if its transition monoid is a primitive group. We use this
to study boolean operations on group languages, which are recognized by direct
products of permutation DFAs. A direct product cannot be uniformly minimal,
except in the trivial case where one of the DFAs in the product is a one-state
DFA. However, non-trivial direct products can satisfy a weaker condition we
call uniform boolean minimality, where only final state sets used to recognize
boolean operations are considered. We give sufficient conditions for a direct
product of two DFAs to be uniformly boolean minimal, which in turn gives
sufficient conditions for pairs of group languages to have maximal state
complexity under all binary boolean operations (""maximal boolean complexity"").
In the case of permutation DFAs with one final state, we give necessary and
sufficient conditions for pairs of group languages to have maximal boolean
complexity. Our results demonstrate a connection between primitive groups and
automata with strong minimality properties.
",1,0,1,0,0,0
13739,13740,Enhancing the Regularization Effect of Weight Pruning in Artificial Neural Networks,"  Artificial neural networks (ANNs) may not be worth their computational/memory
costs when used in mobile phones or embedded devices. Parameter-pruning
algorithms combat these costs, with some algorithms capable of removing over
90% of an ANN's weights without harming the ANN's performance. Removing weights
from an ANN is a form of regularization, but existing pruning algorithms do not
significantly improve generalization error. We show that pruning ANNs can
improve generalization if pruning targets large weights instead of small
weights. Applying our pruning algorithm to an ANN leads to a higher image
classification accuracy on CIFAR-10 data than applying the popular regularizer
dropout. The pruning couples this higher accuracy with an 85% reduction of the
ANN's parameter count.
",0,0,0,1,0,0
18715,18716,High Luminosity Large Hadron Collider HL-LHC,"  HL-LHC federates the efforts and R&D of a large international community
towards the ambitious HL- LHC objectives and contributes to establishing the
European Research Area (ERA) as a focal point of global research cooperation
and a leader in frontier knowledge and technologies. HL-LHC relies on strong
participation from various partners, in particular from leading US and Japanese
laboratories. This participation will be required for the execution of the
construction phase as a global project. In particular, the US LHC Accelerator
R&D Program (LARP) has developed some of the key technologies for the HL-LHC,
such as the large-aperture niobium-tin ($Nb_{3}Sn) quadrupoles and the crab
cavities. The proposed governance model is tailored accordingly and should pave
the way for the organization of the construction phase.
",0,1,0,0,0,0
17284,17285,Finite model reasoning over existential rules,"  Ontology-based query answering (OBQA) asks whether a Boolean conjunctive
query is satisfied by all models of a logical theory consisting of a relational
database paired with an ontology. The introduction of existential rules (i.e.,
Datalog rules extended with existential quantifiers in rule-heads) as a means
to specify the ontology gave birth to Datalog+/-, a framework that has received
increasing attention in the last decade, with focus also on decidability and
finite controllability to support effective reasoning. Five basic decidable
fragments have been singled out: linear, weakly-acyclic, guarded, sticky, and
shy. Moreover, for all these fragments, except shy, the important property of
finite controllability has been proved, ensuring that a query is satisfied by
all models of the theory iff it is satisfied by all its finite models. In this
paper we complete the picture by demonstrating that finite controllability of
OBQA holds also for shy ontologies, and it therefore applies to all basic
decidable Datalog+/- classes. To make the demonstration, we devise a general
technique to facilitate the process of (dis)proving finite controllability of
an arbitrary ontological fragment. This paper is under consideration for
acceptance in TPLP.
",1,0,0,0,0,0
2603,2604,Modern-day Universities and Regional Development,"  Nowadays it is quite evident that knowledge-based society necessarily
involves the revaluation of human and intangible assets, as the advancement of
local economies significantly depend on the qualitative and quantitative
characteristics of human capital[Lundvall, 2004]. As we can instantaneously
link the universities as main actors in the creation of highly-qualified labour
force, the role of universities increases parallel to the previously mentioned
progresses. Universities are the general institutions of education, however i
nthe need of adaptation to present local needs, their activities have broadened
in the past decades [Wright et al, 2008; Etzkowitz, 2002]. Most universities
experienced a transition period in which next to their classic activities,
namely education and research, so called third mission activities also started
to count, thus serving many purposes of economy and society.
",1,0,0,0,0,0
11015,11016,Resource Sharing Among mmWave Cellular Service Providers in a Vertically Differentiated Duopoly,"  With the increasing interest in the use of millimeter wave bands for 5G
cellular systems comes renewed interest in resource sharing. Properties of
millimeter wave bands such as massive bandwidth, highly directional antennas,
high penetration loss, and susceptibility to shadowing, suggest technical
advantages to spectrum and infrastructure sharing in millimeter wave cellular
networks. However, technical advantages do not necessarily translate to
increased profit for service providers, or increased consumer surplus. In this
paper, detailed network simulations are used to better understand the economic
implications of resource sharing in a vertically differentiated duopoly market
for cellular service. The results suggest that resource sharing is less often
profitable for millimeter wave service providers compared to microwave cellular
service providers, and does not necessarily increase consumer surplus.
",1,0,0,0,0,0
10454,10455,Non-compact subsets of the Zariski space of an integral domain,"  Let $V$ be a minimal valuation overring of an integral domain $D$ and let
$\mathrm{Zar}(D)$ be the Zariski space of the valuation overrings of $D$.
Starting from a result in the theory of semistar operations, we prove a
criterion under which the set $\mathrm{Zar}(D)\setminus\{V\}$ is not compact.
We then use it to prove that, in many cases, $\mathrm{Zar}(D)$ is not a
Noetherian space, and apply it to the study of the spaces of Kronecker function
rings and of Noetherian overrings.
",0,0,1,0,0,0
2521,2522,DiVM: Model Checking with LLVM and Graph Memory,"  In this paper, we introduce the concept of a virtual machine with
graph-organised memory as a versatile backend for both explicit-state and
abstraction-driven verification of software. Our virtual machine uses the LLVM
IR as its instruction set, enriched with a small set of hypercalls. We show
that the provided hypercalls are sufficient to implement a small operating
system, which can then be linked with applications to provide a
POSIX-compatible verification environment. Finally, we demonstrate the
viability of the approach through a comparison with a more
traditionally-designed LLVM model checker.
",1,0,0,0,0,0
6480,6481,"Stability conditions, $τ$-tilting Theory and Maximal Green Sequences","  Extending the notion of maximal green sequences to an abelian category, we
characterize the stability functions, as defined by Rudakov, that induce a
maximal green sequence in an abelian length category. Furthermore, we use
$\tau$-tilting theory to give a description of the wall and chamber structure
of any finite dimensional algebra. Finally we introduce the notion of green
paths in the wall and chamber structure of an algebra and show that green paths
serve as geometrical generalization of maximal green sequences in this context.
",0,0,1,0,0,0
9096,9097,Some Repeated-Root Constacyclic Codes over Galois Rings,"  Codes over Galois rings have been studied extensively during the last three
decades. Negacyclic codes over $GR(2^a,m)$ of length $2^s$ have been
characterized: the ring $\mathcal{R}_2(a,m,-1)= \frac{GR(2^a,m)[x]}{\langle
x^{2^s}+1\rangle}$ is a chain ring. Furthermore, these results have been
generalized to $\lambda$-constacyclic codes for any unit $\lambda$ of the form
$4z-1$, $z\in GR(2^a, m)$. In this paper, we study more general cases and
investigate all cases where $\mathcal{R}_p(a,m,\gamma)=
\frac{GR(p^a,m)[x]}{\langle x^{p^s}-\gamma \rangle}$ is a chain ring. In
particular, necessary and sufficient conditions for the ring
$\mathcal{R}_p(a,m,\gamma)$ to be a chain ring are obtained. In addition, by
using this structure we investigate all $\gamma$-constacyclic codes over
$GR(p^a,m)$ when $\mathcal{R}_p(a,m,\gamma)$ is a chain ring. Necessary and
sufficient conditions for the existence of self-orthogonal and self-dual
$\gamma$-constacyclic codes are also provided. Among others, for any prime $p$,
the structure of $\mathcal{R}_p(a,m,\gamma)=\frac{GR(p^a,m)[x]}{\langle
x^{p^s}-\gamma\rangle}$ is used to establish the Hamming and homogeneous
distances of $\gamma$-constacyclic codes.
",1,0,1,0,0,0
3212,3213,Computer Algebra for Microhydrodynamics,"  I describe a method for computer algebra that helps with laborious
calculations typically encountered in theoretical microhydrodynamics. The
program mimics how humans calculate by matching patterns and making
replacements according to the rules of algebra and calculus. This note gives an
overview and walks through an example, while the accompanying code repository
contains the implementation details, a tutorial, and more examples. The code
repository is attached as supplementary material to this note, and maintained
at this https URL
",1,1,0,0,0,0
385,386,Topology of Large-Scale Structures of Galaxies in Two Dimensions - Systematic Effects,"  We study the two-dimensional topology of the galactic distribution when
projected onto two-dimensional spherical shells. Using the latest Horizon Run 4
simulation data, we construct the genus of the two-dimensional field and
consider how this statistic is affected by late-time nonlinear effects --
principally gravitational collapse and redshift space distortion (RSD). We also
consider systematic and numerical artifacts such as shot noise, galaxy bias,
and finite pixel effects. We model the systematics using a Hermite polynomial
expansion and perform a comprehensive analysis of known effects on the
two-dimensional genus, with a view toward using the statistic for cosmological
parameter estimation. We find that the finite pixel effect is dominated by an
amplitude drop and can be made less than $1\%$ by adopting pixels smaller than
$1/3$ of the angular smoothing length. Nonlinear gravitational evolution
introduces time-dependent coefficients of the zeroth, first, and second Hermite
polynomials, but the genus amplitude changes by less than $1\%$ between $z=1$
and $z=0$ for smoothing scales $R_{\rm G} > 9 {\rm Mpc/h}$. Non-zero terms are
measured up to third order in the Hermite polynomial expansion when studying
RSD. Differences in shapes of the genus curves in real and redshift space are
small when we adopt thick redshift shells, but the amplitude change remains a
significant $\sim {\cal O}(10\%)$ effect. The combined effects of galaxy
biasing and shot noise produce systematic effects up to the second Hermite
polynomial. It is shown that, when sampling, the use of galaxy mass cuts
significantly reduces the effect of shot noise relative to random sampling.
",0,1,0,0,0,0
16012,16013,Transverse Magnetic Susceptibility of a Frustrated Spin-$\frac{1}{2}$ $J_{1}$--$J_{2}$--$J_{1}^{\perp}$ Heisenberg Antiferromagnet on a Bilayer Honeycomb Lattice,"  We use the coupled cluster method (CCM) to study a frustrated
spin-$\frac{1}{2}$ $J_{1}$--$J_{2}$--$J_{1}^{\perp}$ Heisenberg antiferromagnet
on a bilayer honeycomb lattice with $AA$ stacking. Both nearest-neighbor (NN)
and frustrating next-nearest-neighbor antiferromagnetic (AFM) exchange
interactions are present in each layer, with respective exchange coupling
constants $J_{1}>0$ and $J_{2} \equiv \kappa J_{1} > 0$. The two layers are
coupled with NN AFM exchanges with coupling strength $J_{1}^{\perp}\equiv
\delta J_{1}>0$. We calculate to high orders of approximation within the CCM
the zero-field transverse magnetic susceptibility $\chi$ in the Néel phase.
We thus obtain an accurate estimate of the full boundary of the Néel phase in
the $\kappa\delta$ plane for the zero-temperature quantum phase diagram. We
demonstrate explicitly that the phase boundary derived from $\chi$ is fully
consistent with that obtained from the vanishing of the Néel magnetic order
parameter. We thus conclude that at all points along the Néel phase boundary
quasiclassical magnetic order gives way to a nonclassical paramagnetic phase
with a nonzero energy gap. The Néel phase boundary exhibits a marked
reentrant behavior, which we discuss in detail.
",0,1,0,0,0,0
8042,8043,Realization of an atomically thin mirror using monolayer MoSe2,"  Advent of new materials such as van der Waals heterostructures, propels new
research directions in condensed matter physics and enables development of
novel devices with unique functionalities. Here, we show experimentally that a
monolayer of MoSe2 embedded in a charge controlled heterostructure can be used
to realize an electrically tunable atomically-thin mirror, that effects 90%
extinction of an incident field that is resonant with its exciton transition.
The corresponding maximum reflection coefficient of 45% is only limited by the
ratio of the radiative decay rate to the linewidth of exciton transition and is
independent of incident light intensity up to 400 Watts/cm2. We demonstrate
that the reflectivity of the mirror can be drastically modified by applying a
gate voltage that modifies the monolayer charge density. Our findings could
find applications ranging from fast programmable spatial light modulators to
suspended ultra-light mirrors for optomechanical devices.
",0,1,0,0,0,0
5386,5387,Towards Optimal Strategy for Adaptive Probing in Incomplete Networks,"  We investigate a graph probing problem in which an agent has only an
incomplete view $G' \subsetneq G$ of the network and wishes to explore the
network with least effort. In each step, the agent selects a node $u$ in $G'$
to probe. After probing $u$, the agent gains the information about $u$ and its
neighbors. All the neighbors of $u$ become \emph{observed} and are
\emph{probable} in the subsequent steps (if they have not been probed). What is
the best probing strategy to maximize the number of nodes explored in $k$
probes? This problem serves as a fundamental component for other
decision-making problems in incomplete networks such as information harvesting
in social networks, network crawling, network security, and viral marketing
with incomplete information.
While there are a few methods proposed for the problem, none can perform
consistently well across different network types. In this paper, we establish a
strong (in)approximability for the problem, proving that no algorithm can
guarantees finite approximation ratio unless P=NP. On the bright side, we
design learning frameworks to capture the best probing strategies for
individual network. Our extensive experiments suggest that our framework can
learn efficient probing strategies that \emph{consistently} outperform previous
heuristics and metric-based approaches.
",1,1,0,0,0,0
19339,19340,Understanding a Version of Multivariate Symmetric Uncertainty to assist in Feature Selection,"  In this paper, we analyze the behavior of the multivariate symmetric
uncertainty (MSU) measure through the use of statistical simulation techniques
under various mixes of informative and non-informative randomly generated
features. Experiments show how the number of attributes, their cardinalities,
and the sample size affect the MSU. We discovered a condition that preserves
good quality in the MSU under different combinations of these three factors,
providing a new useful criterion to help drive the process of dimension
reduction.
",1,0,0,1,0,0
11215,11216,Learning K-way D-dimensional Discrete Code For Compact Embedding Representations,"  Embedding methods such as word embedding have become pillars for many
applications containing discrete structures. Conventional embedding methods
directly associate each symbol with a continuous embedding vector, which is
equivalent to applying linear transformation based on ""one-hot"" encoding of the
discrete symbols. Despite its simplicity, such approach yields number of
parameters that grows linearly with the vocabulary size and can lead to
overfitting. In this work we propose a much more compact K-way D-dimensional
discrete encoding scheme to replace the ""one-hot"" encoding. In ""KD encoding"",
each symbol is represented by a $D$-dimensional code, and each of its dimension
has a cardinality of $K$. The final symbol embedding vector can be generated by
composing the code embedding vectors. To learn the semantically meaningful
code, we derive a relaxed discrete optimization technique based on stochastic
gradient descent. By adopting the new coding system, the efficiency of
parameterization can be significantly improved (from linear to logarithmic),
and this can also mitigate the over-fitting problem. In our experiments with
language modeling, the number of embedding parameters can be reduced by 97\%
while achieving similar or better performance.
",1,0,0,1,0,0
3057,3058,A review of Dan's reduction method for multiple polylogarithms,"  In this paper we will give an account of Dan's reduction method for reducing
the weight $ n $ multiple logarithm $ I_{1,1,\ldots,1}(x_1, x_2, \ldots, x_n) $
to an explicit sum of lower depth multiple polylogarithms in $ \leq n - 2 $
variables.
We provide a detailed explanation of the method Dan outlines, and we fill in
the missing proofs for Dan's claims. This establishes the validity of the
method itself, and allows us to produce a corrected version of Dan's reduction
of $ I_{1,1,1,1} $ to $ I_{3,1} $'s and $ I_4 $'s. We then use the symbol of
multiple polylogarithms to answer Dan's question about how this reduction
compares with his earlier reduction of $ I_{1,1,1,1} $, and his question about
the nature of the resulting functional equation of $ I_{3,1} $.
Finally, we apply the method to $ I_{1,1,1,1,1} $ at weight 5 to first
produce a reduction to depth $ \leq 3 $ integrals. Using some functional
equations from our thesis, we further reduce this to $ I_{3,1,1} $, $ I_{3,2} $
and $ I_5 $, modulo products. We also see how to reduce $ I_{3,1,1} $ to $
I_{3,2} $, modulo $ \delta $ (modulo products and depth 1 terms), and indicate
how this allows us to reduce $ I_{1,1,1,1,1} $ to $ I_{3,2} $'s only, modulo $
\delta $.
",0,0,1,0,0,0
10824,10825,Unveiling Eilenberg-type Correspondences: Birkhoff's Theorem for (finite) Algebras + Duality,"  The purpose of the present paper is to show that: Eilenberg-type
correspondences = Birkhoff's theorem for (finite) algebras + duality. We
consider algebras for a monad T on a category D and we study (pseudo)varieties
of T-algebras. Pseudovarieties of algebras are also known in the literature as
varieties of finite algebras. Two well-known theorems that characterize
varieties and pseudovarieties of algebras play an important role here:
Birkhoff's theorem and Birkhoff's theorem for finite algebras, the latter also
known as Reiterman's theorem. We prove, under mild assumptions, a categorical
version of Birkhoff's theorem for (finite) algebras to establish a one-to-one
correspondence between (pseudo)varieties of T-algebras and (pseudo)equational
T-theories. Now, if C is a category that is dual to D and B is the comonad on C
that is the dual of T, we get a one-to-one correspondence between
(pseudo)equational T-theories and their dual, (pseudo)coequational B-theories.
Particular instances of (pseudo)coequational B-theories have been already
studied in language theory under the name of ""varieties of languages"" to
establish Eilenberg-type correspondences. All in all, we get a one-to-one
correspondence between (pseudo)varieties of T-algebras and (pseudo)coequational
B-theories, which will be shown to be exactly the nature of Eilenberg-type
correspondences.
",1,0,1,0,0,0
12005,12006,Higher-rank graph algebras are iterated Cuntz-Pimsner algebras,"  Given a finitely aligned $k$-graph $\Lambda$, we let $\Lambda^i$ denote the
$(k-1)$-graph formed by removing all edges of degree $e_i$ from $\Lambda$. We
show that the Toeplitz-Cuntz-Krieger algebra of $\Lambda$, denoted by
$\mathcal{T}C^*(\Lambda)$, may be realised as the Toeplitz algebra of a Hilbert
$\mathcal{T}C^*(\Lambda^i)$-bimodule. When $\Lambda$ is locally-convex, we show
that the Cuntz-Krieger algebra of $\Lambda$, which we denote by $C^*(\Lambda)$,
may be realised as the Cuntz-Pimsner algebra of a Hilbert
$C^*(\Lambda^i)$-bimodule. Consequently, $\mathcal{T}C^*(\Lambda)$ and
$C^*(\Lambda)$ may be viewed as iterated Toeplitz and iterated Cuntz-Pimsner
algebras over $c_0(\Lambda^0)$ respectively.
",0,0,1,0,0,0
8613,8614,Integrated Fabry-Perot cavities as a mechanism for enhancing micro-ring resonator performance,"  We propose and experimentally demonstrate the enhancement in the filtering
quality (Q) factor of an integrated micro-ring resonator (MRR) by embedding it
in an integrated Fabry-Perot (FP) cavity formed by cascaded Sagnac loop
reflectors (SLRs). By utilizing coherent interference within the FP cavity to
reshape the transmission spectrum of the MRR, both the Q factor and the
extinction ratio (ER) can be significantly improved. The device is
theoretically analyzed, and practically fabricated on a silicon-on-insulator
(SOI) wafer. Experimental results show that up to 11-times improvement in Q
factor, together with an 8-dB increase in ER, can be achieved via our proposed
method. The impact of varying structural parameters on the device performance
is also investigated and verified by the measured spectra of the fabricated
devices with different structural parameters.
",0,1,0,0,0,0
13951,13952,Cayley deformations of compact complex surfaces,"  In this article, we consider Cayley deformations of a compact complex surface
in a Calabi--Yau four-fold. We will study complex deformations of compact
complex submanifolds of Calabi--Yau manifolds with a view to explaining why
complex and Cayley deformations of a compact complex surface are the same. We
in fact prove that the moduli space of complex deformations of any compact
complex embedded submanifold of a Calabi--Yau manifold is a smooth manifold.
",0,0,1,0,0,0
12225,12226,Efficient anchor loss suppression in coupled near-field optomechanical resonators,"  Elastic dissipation through radiation towards the substrate is a major loss
channel in micro- and nanomechanical resonators. Engineering the coupling of
these resonators with optical cavities further complicates and constrains the
design of low-loss optomechanical devices. In this work we rely on the coherent
cancellation of mechanical radiation to demonstrate material and surface
absorption limited silicon near-field optomechanical resonators oscillating at
tens of MHz. The effectiveness of our dissipation suppression scheme is
investigated at room and cryogenic temperatures. While at room temperature we
can reach a maximum quality factor of 7.61k ($fQ$-product of the order of
$10^{11}$~Hz), at 22~K the quality factor increases to 37k, resulting in a
$fQ$-product of $2\times10^{12}$~Hz.
",0,1,0,0,0,0
14853,14854,Active Learning amidst Logical Knowledge,"  Structured prediction is ubiquitous in applications of machine learning such
as knowledge extraction and natural language processing. Structure often can be
formulated in terms of logical constraints. We consider the question of how to
perform efficient active learning in the presence of logical constraints among
variables inferred by different classifiers. We propose several methods and
provide theoretical results that demonstrate the inappropriateness of employing
uncertainty guided sampling, a commonly used active learning method.
Furthermore, experiments on ten different datasets demonstrate that the methods
significantly outperform alternatives in practice. The results are of practical
significance in situations where labeled data is scarce.
",1,0,0,0,0,0
487,488,BiHom-Lie colour algebras structures,"  BiHom-Lie Colour algebra is a generalized Hom-Lie Colour algebra endowed with
two commuting multiplicative linear maps. The main purpose of this paper is to
define representations and a cohomology of BiHom-Lie colour algebras and to
study some key constructions and properties.
Moreover, we discuss $\alpha^{k}\beta^l$-generalized derivations,
$\alpha^{k}\beta^l$-quasi-derivations and $\alpha^{k}\beta^l$-quasi-centroid.
We provide some properties and their relationships with BiHom-Jordan colour
algebra.
",0,0,1,0,0,0
20922,20923,Existence of solutions for a semirelativistic Hartree equation with unbounded potentials,"  We prove the existence of a solution to the semirelativistic Hartree equation
$$\sqrt{-\Delta+m^2}u+ V(x) u = A(x)\left( W * |u|^p \right) |u|^{p-2}u $$
under suitable growth assumption on the potential functions $V$ and $A$. In
particular, both can be unbounded from above.
",0,0,1,0,0,0
2311,2312,The Stochastic Matching Problem: Beating Half with a Non-Adaptive Algorithm,"  In the stochastic matching problem, we are given a general (not necessarily
bipartite) graph $G(V,E)$, where each edge in $E$ is realized with some
constant probability $p > 0$ and the goal is to compute a bounded-degree
(bounded by a function depending only on $p$) subgraph $H$ of $G$ such that the
expected maximum matching size in $H$ is close to the expected maximum matching
size in $G$. The algorithms in this setting are considered non-adaptive as they
have to choose the subgraph $H$ without knowing any information about the set
of realized edges in $G$. Originally motivated by an application to kidney
exchange, the stochastic matching problem and its variants have received
significant attention in recent years.
The state-of-the-art non-adaptive algorithms for stochastic matching achieve
an approximation ratio of $\frac{1}{2}-\epsilon$ for any $\epsilon > 0$,
naturally raising the question that if $1/2$ is the limit of what can be
achieved with a non-adaptive algorithm. In this work, we resolve this question
by presenting the first algorithm for stochastic matching with an approximation
guarantee that is strictly better than $1/2$: the algorithm computes a subgraph
$H$ of $G$ with the maximum degree $O(\frac{\log{(1/ p)}}{p})$ such that the
ratio of expected size of a maximum matching in realizations of $H$ and $G$ is
at least $1/2+\delta_0$ for some absolute constant $\delta_0 > 0$. The degree
bound on $H$ achieved by our algorithm is essentially the best possible (up to
an $O(\log{(1/p)})$ factor) for any constant factor approximation algorithm,
since an $\Omega(\frac{1}{p})$ degree in $H$ is necessary for a vertex to
acquire at least one incident edge in a realization.
",1,0,0,0,0,0
19125,19126,Delone dynamical systems and spectral convergence,"  In the realm of Delone sets in locally compact, second countable, Hausdorff
groups, we develop a dynamical systems approach in order to study the
continuity behavior of measured quantities arising from point sets. A special
focus is both on the autocorrelation, as well as on the density of states for
random bounded operators. It is shown that for uniquely ergodic limit systems,
the latter measures behave continuously with respect to the Chabauty-Fell
convergence of hulls. In the special situation of Euclidean spaces, our results
complement recent developments in describing spectra as topological limits: we
show that the measured quantities under consideration can be approximated via
periodic analogs.
",0,0,1,0,0,0
11654,11655,"Reply to Hicks et al 2017, Reply to Morrison et al 2016 Refining the relevant population in forensic voice comparison, Reply to Hicks et al 2015 The importance of distinguishing info from evidence/observations when formulating propositions","  The present letter to the editor is one in a series of publications
discussing the formulation of hypotheses (propositions) for the evaluation of
strength of forensic evidence. In particular, the discussion focusses on the
issue of what information may be used to define the relevant population
specified as part of the different-speaker hypothesis in forensic voice
comparison. The previous publications in the series are: Hicks et al. 2015
<this http URL>; Morrison et al. (2016)
<this http URL>; Hicks et al. (2017)
<this http URL>. The latter letter to the
editor mostly resolves the apparent disagreement between the two groups of
authors. We briefly discuss one outstanding point of apparent disagreement, and
attempt to correct a misinterpretation of our earlier remarks. We believe that
at this point there is no actual disagreement, and that both groups of authors
are calling for greater collaboration in order to reduce the likelihood of
future misunderstandings.
",0,0,0,1,0,0
12417,12418,Electric field modulation of the non-linear areal magnetic anisotropy energy,"  We study the ferromagnetic layer thickness dependence of the
voltage-controlled magnetic anisotropy (VCMA) in gated CoFeB/MgO
heterostructures with heavy metal underlayers. When the effective CoFeB
thickness is below ~1 nm, the VCMA efficiency of Ta/CoFeB/MgO heterostructures
considerably decreases with decreasing CoFeB thickness. We find that a high
order phenomenological term used to describe the thickness dependence of the
areal magnetic anisotropy energy can also account for the change in the areal
VCMA efficiency. In this structure, the higher order term competes against the
common interfacial VCMA, thereby reducing the efficiency at lower CoFeB
thickness. The areal VCMA efficiency does not saturate even when the effective
CoFeB thickness exceeds ~1 nm. We consider the higher order term is related to
the strain that develops at the CoFeB/MgO interface: as the average strain of
the CoFeB layer changes with its thickness, the electronic structure of the
CoFeB/MgO interface varies leading to changes in areal magnetic anisotropy
energy and VCMA efficiency.
",0,1,0,0,0,0
9731,9732,Sample and Computationally Efficient Learning Algorithms under S-Concave Distributions,"  We provide new results for noise-tolerant and sample-efficient learning
algorithms under $s$-concave distributions. The new class of $s$-concave
distributions is a broad and natural generalization of log-concavity, and
includes many important additional distributions, e.g., the Pareto distribution
and $t$-distribution. This class has been studied in the context of efficient
sampling, integration, and optimization, but much remains unknown about the
geometry of this class of distributions and their applications in the context
of learning. The challenge is that unlike the commonly used distributions in
learning (uniform or more generally log-concave distributions), this broader
class is not closed under the marginalization operator and many such
distributions are fat-tailed. In this work, we introduce new convex geometry
tools to study the properties of $s$-concave distributions and use these
properties to provide bounds on quantities of interest to learning including
the probability of disagreement between two halfspaces, disagreement outside a
band, and the disagreement coefficient. We use these results to significantly
generalize prior results for margin-based active learning, disagreement-based
active learning, and passive learning of intersections of halfspaces. Our
analysis of geometric properties of $s$-concave distributions might be of
independent interest to optimization more broadly.
",1,0,0,1,0,0
17809,17810,Stability and optimality of distributed secondary frequency control schemes in power networks,"  We present a systematic method for designing distributed generation and
demand control schemes for secondary frequency regulation in power networks
such that stability and an economically optimal power allocation can be
guaranteed. A dissipativity condition is imposed on net power supply variables
to provide stability guarantees. Furthermore, economic optimality is achieved
by explicit decentralized steady state conditions on the generation and
controllable demand. We discuss how various classes of dynamics used in recent
studies fit within our framework and give examples of higher order generation
and controllable demand dynamics that can be included within our analysis. In
case of linear dynamics, we discuss how the proposed dissipativity condition
can be efficiently verified using an appropriate linear matrix inequality.
Moreover, it is shown how the addition of a suitable observer layer can relax
the requirement for demand measurements in the employed controller. The
efficiency and practicality of the proposed results are demonstrated with a
simulation on the Northeast Power Coordinating Council (NPCC) 140-bus system.
",1,0,1,0,0,0
16059,16060,Automatic Skin Lesion Analysis using Large-scale Dermoscopy Images and Deep Residual Networks,"  Malignant melanoma has one of the most rapidly increasing incidences in the
world and has a considerable mortality rate. Early diagnosis is particularly
important since melanoma can be cured with prompt excision. Dermoscopy images
play an important role in the non-invasive early detection of melanoma [1].
However, melanoma detection using human vision alone can be subjective,
inaccurate and poorly reproducible even among experienced dermatologists. This
is attributed to the challenges in interpreting images with diverse
characteristics including lesions of varying sizes and shapes, lesions that may
have fuzzy boundaries, different skin colors and the presence of hair [2].
Therefore, the automatic analysis of dermoscopy images is a valuable aid for
clinical decision making and for image-based diagnosis to identify diseases
such as melanoma [1-4]. Deep residual networks (ResNets) has achieved
state-of-the-art results in image classification and detection related problems
[5-8]. In this ISIC 2017 skin lesion analysis challenge [9], we propose to
exploit the deep ResNets for robust visual features learning and
representations.
",1,0,0,0,0,0
14645,14646,Sketch Layer Separation in Multi-Spectral Historical Document Images,"  High-resolution imaging has delivered new prospects for detecting the
material composition and structure of cultural treasures. Despite the various
techniques for analysis, a significant diagnostic gap remained in the range of
available research capabilities for works on paper. Old master drawings were
mostly composed in a multi-step manner with various materials. This resulted in
the overlapping of different layers which made the subjacent strata difficult
to differentiate. The separation of stratified layers using imaging methods
could provide insights into the artistic work processes and help answer
questions about the object, its attribution, or in identifying forgeries. The
pattern recognition procedure was tested with mock replicas to achieve the
separation and the capability of displaying concealed red chalk under ink. In
contrast to RGB-sensor based imaging, the multi- or hyperspectral technology
allows accurate layer separation by recording the characteristic signatures of
the material's reflectance. The risk of damage to the artworks as a result of
the examination can be reduced by using combinations of defined spectra for
lightning and image capturing. By guaranteeing the maximum level of
readability, our results suggest that the technique can be applied to a broader
range of objects and assist in diagnostic research into cultural treasures in
the future.
",1,0,0,0,0,0
19385,19386,Cobordism maps on PFH induced by Lefschetz fibration over higher genus base,"  In this note, we discuss the cobordism maps on periodic Floer homology(PFH)
induced by Lefschetz fibration. In the first part of the note, we define the
cobordism maps on PFH induced by Lefschetz fibration via Seiberg Witten theory
and the isomorphism between PFH and Seiberg Witten cohomology. The second part
is to define the cobordism maps induced by Lefschetz fibration provided that
the cobordism satisfies certain conditions. Under certain monotone assumptions,
we show that these two definitions in fact are equivalent.
",0,0,1,0,0,0
11374,11375,Configurable 3D Scene Synthesis and 2D Image Rendering with Per-Pixel Ground Truth using Stochastic Grammars,"  We propose a systematic learning-based approach to the generation of massive
quantities of synthetic 3D scenes and arbitrary numbers of photorealistic 2D
images thereof, with associated ground truth information, for the purposes of
training, benchmarking, and diagnosing learning-based computer vision and
robotics algorithms. In particular, we devise a learning-based pipeline of
algorithms capable of automatically generating and rendering a potentially
infinite variety of indoor scenes by using a stochastic grammar, represented as
an attributed Spatial And-Or Graph, in conjunction with state-of-the-art
physics-based rendering. Our pipeline is capable of synthesizing scene layouts
with high diversity, and it is configurable inasmuch as it enables the precise
customization and control of important attributes of the generated scenes. It
renders photorealistic RGB images of the generated scenes while automatically
synthesizing detailed, per-pixel ground truth data, including visible surface
depth and normal, object identity, and material information (detailed to object
parts), as well as environments (e.g., illuminations and camera viewpoints). We
demonstrate the value of our synthesized dataset, by improving performance in
certain machine-learning-based scene understanding tasks--depth and surface
normal prediction, semantic segmentation, reconstruction, etc.--and by
providing benchmarks for and diagnostics of trained models by modifying object
attributes and scene properties in a controllable manner.
",1,0,0,1,0,0
1587,1588,Winning on the Merits: The Joint Effects of Content and Style on Debate Outcomes,"  Debate and deliberation play essential roles in politics and government, but
most models presume that debates are won mainly via superior style or agenda
control. Ideally, however, debates would be won on the merits, as a function of
which side has the stronger arguments. We propose a predictive model of debate
that estimates the effects of linguistic features and the latent persuasive
strengths of different topics, as well as the interactions between the two.
Using a dataset of 118 Oxford-style debates, our model's combination of content
(as latent topics) and style (as linguistic features) allows us to predict
audience-adjudicated winners with 74% accuracy, significantly outperforming
linguistic features alone (66%). Our model finds that winning sides employ
stronger arguments, and allows us to identify the linguistic features
associated with strong or weak arguments.
",1,0,0,0,0,0
13367,13368,Static vs Adaptive Strategies for Optimal Execution with Signals,"  We consider an optimal execution problem in which a trader is looking at a
short-term price predictive signal while trading. In the case where the trader
is creating an instantaneous market impact, we show that transactions costs
resulting from the optimal adaptive strategy are substantially lower than the
corresponding costs of the optimal static strategy. Later, we investigate the
case where the trader is creating transient market impact. We show that
strategies in which the trader is observing the signal a number of times during
the trading period, can dramatically reduce the transaction costs and improve
the performance of the optimal static strategy. These results answer a question
which was raised by Brigo and Piat [6], by analyzing two cases where adaptive
strategies can improve the performance of the execution.
",0,0,0,0,0,1
14184,14185,Parameters of Three Selected Model Galactic Potentials Based on the Velocities of Objects at Distances up to 200 kpc,"  This paper is a continuation of our recent paper devoted to refining the
parameters of three component (bulge, disk, halo) axisymmetric model Galactic
gravitational potentials differing by the expression for the dark matter halo
using the velocities of distant objects. In all models the bulge and disk
potentials are described by the Miyamoto-Nagai expressions. In our previous
paper we used the Allen-Santill'an (I), Wilkinson--Evans (II), and
Navarro-Frenk-White (III) models to describe the halo. In this paper we use a
spherical logarithmic Binney potential (model IV), a Plummer sphere (model V),
and a Hernquist potential (model VI) to describe the halo. A set of present-day
observational data in the range of Galactocentric distances R from 0 to 200 kpc
is used to refine the parameters of the listed models, which are employed most
commonly at present. The model rotation curves are fitted to the observed
velocities by taking into account the constraints on the local matter density
and the vertical force . Model VI looks best among the three models considered
here from the viewpoint of the achieved accuracy of fitting the model rotation
curves to the measurements. This model is close to the Navarro-Frenk-White
model III refined and considered best in our previous paper, which is shown
using the integration of the orbits of two globular clusters, Lynga 7 and NGC
5053, as an example.
",0,1,0,0,0,0
17212,17213,Isotonic regression in general dimensions,"  We study the least squares regression function estimator over the class of
real-valued functions on $[0,1]^d$ that are increasing in each coordinate. For
uniformly bounded signals and with a fixed, cubic lattice design, we establish
that the estimator achieves the minimax rate of order
$n^{-\min\{2/(d+2),1/d\}}$ in the empirical $L_2$ loss, up to poly-logarithmic
factors. Further, we prove a sharp oracle inequality, which reveals in
particular that when the true regression function is piecewise constant on $k$
hyperrectangles, the least squares estimator enjoys a faster, adaptive rate of
convergence of $(k/n)^{\min(1,2/d)}$, again up to poly-logarithmic factors.
Previous results are confined to the case $d \leq 2$. Finally, we establish
corresponding bounds (which are new even in the case $d=2$) in the more
challenging random design setting. There are two surprising features of these
results: first, they demonstrate that it is possible for a global empirical
risk minimisation procedure to be rate optimal up to poly-logarithmic factors
even when the corresponding entropy integral for the function class diverges
rapidly; second, they indicate that the adaptation rate for shape-constrained
estimators can be strictly worse than the parametric rate.
",0,0,1,1,0,0
913,914,Strong isomorphism in Marinatto-Weber type quantum games,"  Our purpose is to focus attention on a new criterion for quantum schemes by
bringing together the notions of quantum game and game isomorphism. A quantum
game scheme is required to generate the classical game as a special case. Now,
given a quantum game scheme and two isomorphic classical games, we additionally
require the resulting quantum games to be isomorphic as well. We show how this
isomorphism condition influences the players' strategy sets. We are concerned
with the Marinatto-Weber type quantum game scheme and the strong isomorphism
between games in strategic form.
",1,0,0,0,0,0
14629,14630,Large sets avoiding linear patterns,"  We prove that for any dimension function $h$ with $h \prec x^d$ and for any
countable set of linear patterns, there exists a compact set $E$ with
$\mathcal{H}^h(E)>0$ avoiding all the given patterns. We also give several
applications and recover results of Keleti, Maga, and Máthé.
",0,0,1,0,0,0
7819,7820,The Effect of Phasor Measurement Units on the Accuracy of the Network Estimated Variables,"  The most commonly used weighted least square state estimator in power
industry is nonlinear and formulated by using conventional measurements such as
line flow and injection measurements. PMUs (Phasor Measurement Units) are
gradually adding them to improve the state estimation process. In this paper
the way of corporation the PMU data to the conventional measurements and a
linear formulation of the state estimation using only PMU measured data are
investigated. Six cases are tested while gradually increasing the number of
PMUs which are added to the measurement set and the effect of PMUs on the
accuracy of variables are illustrated and compared by applying them on IEEE 14,
30 test systems.
",1,0,1,0,0,0
7826,7827,Risk-Sensitive Cooperative Games for Human-Machine Systems,"  Autonomous systems can substantially enhance a human's efficiency and
effectiveness in complex environments. Machines, however, are often unable to
observe the preferences of the humans that they serve. Despite the fact that
the human's and machine's objectives are aligned, asymmetric information, along
with heterogeneous sensitivities to risk by the human and machine, make their
joint optimization process a game with strategic interactions. We propose a
framework based on risk-sensitive dynamic games; the human seeks to optimize
her risk-sensitive criterion according to her true preferences, while the
machine seeks to adaptively learn the human's preferences and at the same time
provide a good service to the human. We develop a class of performance measures
for the proposed framework based on the concept of regret. We then evaluate
their dependence on the risk-sensitivity and the degree of uncertainty. We
present applications of our framework to self-driving taxis, and robo-financial
advising.
",1,0,0,1,0,0
17352,17353,A Kronecker-type identity and the representations of a number as a sum of three squares,"  By considering a limiting case of a Kronecker-type identity, we obtain an
identity found by both Andrews and Crandall. We then use the Andrews-Crandall
identity to give a new proof of a formula of Gauss for the representations of a
number as a sum of three squares. From the Kronecker-type identity, we also
deduce Gauss's theorem that every positive integer is representable as a sum of
three triangular numbers.
",0,0,1,0,0,0
11484,11485,Universal and generalizable restoration strategies for degraded ecological networks,"  Humans are increasingly stressing ecosystems via habitat destruction, climate
change and global population movements leading to the widespread loss of
biodiversity and the disruption of key ecological services. Ecosystems
characterized primarily by mutualistic relationships between species such as
plant-pollinator interactions may be particularly vulnerable to such
perturbations because the loss of biodiversity can cause extinction cascades
that can compromise the entire network. Here, we develop a general restoration
strategy based on network-science for degraded ecosystems. Specifically, we
show that network topology can be used to identify the optimal sequence of
species reintroductions needed to maximize biodiversity gains following partial
and full ecosystem collapse. This restoration strategy generalizes across
topologically-disparate and geographically-distributed ecosystems.
Additionally, we find that although higher connectance and diversity promote
persistence in pristine ecosystems, these attributes reduce the effectiveness
of restoration efforts in degraded networks. Hence, focusing on restoring the
factors that promote persistence in pristine ecosystems may yield suboptimal
recovery strategies for degraded ecosystems. Overall, our results have
important insights for designing effective ecosystem restoration strategies to
preserve biodiversity and ensure the delivery of critical natural services that
fuel economic development, food security and human health around the globe
",0,0,0,0,1,0
384,385,The list chromatic number of graphs with small clique number,"  We prove that every triangle-free graph with maximum degree $\Delta$ has list
chromatic number at most $(1+o(1))\frac{\Delta}{\ln \Delta}$. This matches the
best-known bound for graphs of girth at least 5. We also provide a new proof
that for any $r\geq 4$ every $K_r$-free graph has list-chromatic number at most
$200r\frac{\Delta\ln\ln\Delta}{\ln\Delta}$.
",0,0,1,0,0,0
2785,2786,Learning Multimodal Transition Dynamics for Model-Based Reinforcement Learning,"  In this paper we study how to learn stochastic, multimodal transition
dynamics in reinforcement learning (RL) tasks. We focus on evaluating
transition function estimation, while we defer planning over this model to
future work. Stochasticity is a fundamental property of many task environments.
However, discriminative function approximators have difficulty estimating
multimodal stochasticity. In contrast, deep generative models do capture
complex high-dimensional outcome distributions. First we discuss why, amongst
such models, conditional variational inference (VI) is theoretically most
appealing for model-based RL. Subsequently, we compare different VI models on
their ability to learn complex stochasticity on simulated functions, as well as
on a typical RL gridworld with multimodal dynamics. Results show VI
successfully predicts multimodal outcomes, but also robustly ignores these for
deterministic parts of the transition dynamics. In summary, we show a robust
method to learn multimodal transitions using function approximation, which is a
key preliminary for model-based RL in stochastic domains.
",1,0,0,1,0,0
3735,3736,Prediction of Stable Ground-State Lithium Polyhydrides under High Pressures,"  Hydrogen-rich compounds are important for understanding the dissociation of
dense molecular hydrogen, as well as searching for room temperature
Bardeen-Cooper-Schrieffer (BCS) superconductors. A recent high pressure
experiment reported the successful synthesis of novel insulating lithium
polyhydrides when above 130 GPa. However, the results are in sharp contrast to
previous theoretical prediction by PBE functional that around this pressure
range all lithium polyhydrides (LiHn (n = 2-8)) should be metallic. In order to
address this discrepancy, we perform unbiased structure search with first
principles calculation by including the van der Waals interaction that was
ignored in previous prediction to predict the high pressure stable structures
of LiHn (n = 2-11, 13) up to 200 GPa. We reproduce the previously predicted
structures, and further find novel compositions that adopt more stable
structures. The van der Waals functional (vdW-DF) significantly alters the
relative stability of lithium polyhydrides, and predicts that the stable
stoichiometries for the ground-state should be LiH2 and LiH9 at 130-170 GPa,
and LiH2, LiH8 and LiH10 at 180-200 GPa. Accurate electronic structure
calculation with GW approximation indicates that LiH, LiH2, LiH7, and LiH9 are
insulative up to at least 208 GPa, and all other lithium polyhydrides are
metallic. The calculated vibron frequencies of these insulating phases are also
in accordance with the experimental infrared (IR) data. This reconciliation
with the experimental observation suggests that LiH2, LiH7, and LiH9 are the
possible candidates for lithium polyhydrides synthesized in that experiment.
Our results reinstate the credibility of density functional theory in
description H-rich compounds, and demonstrate the importance of considering van
der Waals interaction in this class of materials.
",0,1,0,0,0,0
14417,14418,Life efficiency does not always increase with the dissipation rate,"  There does not exist a general positive correlation between important
life-supporting properties and the entropy production rate. The simple reason
is that nondissipative and time-symmetric kinetic aspects are also relevant for
establishing optimal functioning. In fact those aspects are even crucial in the
nonlinear regimes around equilibrium where we find biological processing on
mesoscopic scales. We make these claims specific via examples of molecular
motors, of circadian cycles and of sensory adaptation, whose performance in
some regimes is indeed spoiled by increasing the dissipated power. We use the
relation between dissipation and the amount of time-reversal breaking to keep
the discussion quantitative also in effective models where the physical entropy
production is not clearly identifiable.
",0,1,0,0,0,0
12895,12896,Multi-Agent Coverage Control with Energy Depletion and Repletion,"  We develop a hybrid system model to describe the behavior of multiple agents
cooperatively solving an optimal coverage problem under energy depletion and
repletion constraints. The model captures the controlled switching of agents
between coverage (when energy is depleted) and battery charging (when energy is
replenished) modes. It guarantees the feasibility of the coverage problem by
defining a guard function on each agent's battery level to prevent it from
dying on its way to a charging station. The charging station plays the role of
a centralized scheduler to solve the contention problem of agents competing for
the only charging resource in the mission space. The optimal coverage problem
is transformed into a parametric optimization problem to determine an optimal
recharging policy. This problem is solved through the use of Infinitesimal
Perturbation Analysis (IPA), with simulation results showing that a full
recharging policy is optimal.
",1,0,0,0,0,0
11855,11856,Towards Modeling the Interaction of Spatial-Associative Neural Network Representations for Multisensory Perception,"  Our daily perceptual experience is driven by different neural mechanisms that
yield multisensory interaction as the interplay between exogenous stimuli and
endogenous expectations. While the interaction of multisensory cues according
to their spatiotemporal properties and the formation of multisensory
feature-based representations have been widely studied, the interaction of
spatial-associative neural representations has received considerably less
attention. In this paper, we propose a neural network architecture that models
the interaction of spatial-associative representations to perform causal
inference of audiovisual stimuli. We investigate the spatial alignment of
exogenous audiovisual stimuli modulated by associative congruence. In the
spatial layer, topographically arranged networks account for the interaction of
audiovisual input in terms of population codes. In the associative layer,
congruent audiovisual representations are obtained via the experience-driven
development of feature-based associations. Levels of congruency are obtained as
a by-product of the neurodynamics of self-organizing networks, where the amount
of neural activation triggered by the input can be expressed via a nonlinear
distance function. Our novel proposal is that activity-driven levels of
congruency can be used as top-down modulatory projections to spatially
distributed representations of sensory input, e.g. semantically related
audiovisual pairs will yield a higher level of integration than unrelated
pairs. Furthermore, levels of neural response in unimodal layers may be seen as
sensory reliability for the dynamic weighting of crossmodal cues. We describe a
series of planned experiments to validate our model in the tasks of
multisensory interaction on the basis of semantic congruence and unimodal cue
reliability.
",0,0,0,0,1,0
5190,5191,Domain Adaptation by Using Causal Inference to Predict Invariant Conditional Distributions,"  An important goal common to domain adaptation and causal inference is to make
accurate predictions when the distributions for the source (or training)
domain(s) and target (or test) domain(s) differ. In many cases, these different
distributions can be modeled as different contexts of a single underlying
system, in which each distribution corresponds to a different perturbation of
the system, or in causal terms, an intervention. We focus on a class of such
causal domain adaptation problems, where data for one or more source domains
are given, and the task is to predict the distribution of a certain target
variable from measurements of other variables in one or more target domains. We
propose an approach for solving these problems that exploits causal inference
and does not rely on prior knowledge of the causal graph, the type of
interventions or the intervention targets. We demonstrate our approach by
evaluating a possible implementation on simulated and real world data.
",1,0,0,1,0,0
9143,9144,Intelligent User Interfaces - A Tutorial,"  IUIs aim to incorporate intelligent automated capabilities in human computer
interaction, where the net impact is a human-computer interaction that improves
performance or usability in critical ways. It also involves designing and
implementing an artificial intelligence (AI) component that effectively
leverages human skills and capabilities, so that human performance with an
application excels. IUIs embody capabilities that have traditionally been
associated more strongly with humans than with computers: how to perceive,
interpret, learn, use language, reason, plan, and decide.
",1,0,0,0,0,0
1760,1761,Historic Emergence of Diversity in Painting: Heterogeneity in Chromatic Distance in Images and Characterization of Massive Painting Data Set,"  Painting is an art form that has long functioned as a major channel for the
creative expression and communication of humans, its evolution taking place
under an interplay with the science, technology, and social environments of the
times. Therefore, understanding the process based on comprehensive data could
shed light on how humans acted and manifested creatively under changing
conditions. Yet, there exist few systematic frameworks that characterize the
process for painting, which would require robust statistical methods for
defining painting characteristics and identifying human's creative
developments, and data of high quality and sufficient quantity. Here we propose
that the color contrast of a painting image signifying the heterogeneity in
inter-pixel chromatic distance can be a useful representation of its style,
integrating both the color and geometry. From the color contrasts of paintings
from a large-scale, comprehensive archive of 179,853 high-quality images
spanning several centuries we characterize the temporal evolutionary patterns
of paintings, and present a deep study of an extraordinary expansion in
creative diversity and individuality that came to define the modern era.
",1,1,0,0,0,0
6699,6700,Motions about a fixed point by hypergeometric functions: new non-complex analytical solutions and integration of the herpolhode,"  We study four problems in the dynamics of a body moving about a fixed point,
providing a non-complex, analytical solution for all of them. For the first
two, we will work on the motion first integrals. For the symmetrical heavy
body, that is the Lagrange-Poisson case, we compute the second and third Euler
angles in explicit and real forms by means of multiple hypergeometric functions
(Lauricella, functions). Releasing the weight load but adding the complication
of the asymmetry, by means of elliptic integrals of third kind, we provide the
precession angle completing some previous treatments of the Euler-Poinsot case.
Integrating then the relevant differential equation, we reach the finite polar
equation of a special trajectory named the {\it herpolhode}. In the last
problem we keep the symmetry of the first problem, but without the weight, and
take into account a viscous dissipation. The approach of first integrals is no
longer practicable in this situation and the Euler equations are faced directly
leading to dumped goniometric functions obtained as particular occurrences of
Bessel functions of order $-1/2$.
",0,0,1,0,0,0
3005,3006,Transport of Intensity Equation Microscopy for Dynamic Microtubules,"  Microtubules (MTs) are filamentous protein polymers roughly 25 nm in
diameter. Ubiquitous in eukaryotes, MTs are well known for their structural
role but also act as actuators, sensors, and, in association with other
proteins, checkpoint regulators. The thin diameter and transparency of
microtubules classifies them as sub-resolution phase objects, with concomitant
imaging challenges. Label-free methods for imaging microtubules are preferred
when long exposure times would lead to phototoxicity in fluorescence, or for
retaining more native structure and activity.
This method approaches quantitative phase imaging of MTs as an inverse
problem based on the Transport of Intensity Equation. In a co-registered
comparison of MT signal-to-background-noise ratio, TIE Microscopy of MTs shows
an improvement of more than three times that of video-enhanced bright field
imaging.
This method avoids the anisotropy caused by prisms used in differential
interference contrast and takes only two defocused images as input. Unlike
other label-free techniques for imaging microtubules, in TIE microscopy
background removal is a natural consequence of taking the difference of two
defocused images, so the need to frequently update a background image is
eliminated.
",0,1,0,0,0,0
3812,3813,Ubenwa: Cry-based Diagnosis of Birth Asphyxia,"  Every year, 3 million newborns die within the first month of life. Birth
asphyxia and other breathing-related conditions are a leading cause of
mortality during the neonatal phase. Current diagnostic methods are too
sophisticated in terms of equipment, required expertise, and general logistics.
Consequently, early detection of asphyxia in newborns is very difficult in many
parts of the world, especially in resource-poor settings. We are developing a
machine learning system, dubbed Ubenwa, which enables diagnosis of asphyxia
through automated analysis of the infant cry. Deployed via smartphone and
wearable technology, Ubenwa will drastically reduce the time, cost and skill
required to make accurate and potentially life-saving diagnoses.
",1,0,0,1,0,0
12673,12674,A Theoretical Analysis of First Heuristics of Crowdsourced Entity Resolution,"  Entity resolution (ER) is the task of identifying all records in a database
that refer to the same underlying entity, and are therefore duplicates of each
other. Due to inherent ambiguity of data representation and poor data quality,
ER is a challenging task for any automated process. As a remedy, human-powered
ER via crowdsourcing has become popular in recent years. Using crowd to answer
queries is costly and time consuming. Furthermore, crowd-answers can often be
faulty. Therefore, crowd-based ER methods aim to minimize human participation
without sacrificing the quality and use a computer generated similarity matrix
actively. While, some of these methods perform well in practice, no theoretical
analysis exists for them, and further their worst case performances do not
reflect the experimental findings. This creates a disparity in the
understanding of the popular heuristics for this problem. In this paper, we
make the first attempt to close this gap. We provide a thorough analysis of the
prominent heuristic algorithms for crowd-based ER. We justify experimental
observations with our analysis and information theoretic lower bounds.
",1,0,0,0,0,0
5269,5270,Spectral and Energy Efficiency of Uplink D2D Underlaid Massive MIMO Cellular Networks,"  One of key 5G scenarios is that device-to-device (D2D) and massive
multiple-input multiple-output (MIMO) will be co-existed. However, interference
in the uplink D2D underlaid massive MIMO cellular networks needs to be
coordinated, due to the vast cellular and D2D transmissions. To this end, this
paper introduces a spatially dynamic power control solution for mitigating the
cellular-to-D2D and D2D-to-cellular interference. In particular, the proposed
D2D power control policy is rather flexible including the special cases of no
D2D links or using maximum transmit power. Under the considered power control,
an analytical approach is developed to evaluate the spectral efficiency (SE)
and energy efficiency (EE) in such networks. Thus, the exact expressions of SE
for a cellular user or D2D transmitter are derived, which quantify the impacts
of key system parameters such as massive MIMO antennas and D2D density.
Moreover, the D2D scale properties are obtained, which provide the sufficient
conditions for achieving the anticipated SE. Numerical results corroborate our
analysis and show that the proposed power control solution can efficiently
mitigate interference between the cellular and D2D tier. The results
demonstrate that there exists the optimal D2D density for maximizing the area
SE of D2D tier. In addition, the achievable EE of a cellular user can be
comparable to that of a D2D user.
",1,0,1,0,0,0
15293,15294,Linguistic Relativity and Programming Languages,"  The use of programming languages can wax and wane across the decades. We
examine the split-apply- combine pattern that is common in statistical
computing, and consider how its invocation or implementation in languages like
MATLAB and APL differ from R/dplyr. The differences in spelling illustrate how
the concept of linguistic relativity applies to programming languages in ways
that are analogous to human languages. Finally, we discuss how Julia, by being
a high performance yet general purpose dynamic language, allows its users to
express different abstractions to suit individual preferences.
",1,0,0,1,0,0
6953,6954,ZOOpt: Toolbox for Derivative-Free Optimization,"  Recent advances of derivative-free optimization allow efficient approximating
the global optimal solutions of sophisticated functions, such as functions with
many local optima, non-differentiable and non-continuous functions. This
article describes the ZOOpt (this https URL) toolbox that
provides efficient derivative-free solvers and are designed easy to use. ZOOpt
provides a Python package for single-thread optimization, and a light-weighted
distributed version with the help of the Julia language for Python described
functions. ZOOpt toolbox particularly focuses on optimization problems in
machine learning, addressing high-dimensional, noisy, and large-scale problems.
The toolbox is being maintained toward ready-to-use tool in real-world machine
learning tasks.
",0,0,0,1,0,0
641,642,Large Magellanic Cloud Near-Infrared Synoptic Survey. V. Period-Luminosity Relations of Miras,"  We study the near-infrared properties of 690 Mira candidates in the central
region of the Large Magellanic Cloud, based on time-series observations at
JHKs. We use densely-sampled I-band observations from the OGLE project to
generate template light curves in the near infrared and derive robust mean
magnitudes at those wavelengths. We obtain near-infrared Period-Luminosity
relations for Oxygen-rich Miras with a scatter as low as 0.12 mag at Ks. We
study the Period-Luminosity-Color relations and the color excesses of
Carbon-rich Miras, which show evidence for a substantially different reddening
law.
",0,0,0,1,0,0
6470,6471,Ultra Reliable Short Message Relaying with Wireless Power Transfer,"  We consider a dual-hop wireless network where an energy constrained relay
node first harvests energy through the received radio-frequency signal from the
source, and then uses the harvested energy to forward the source's information
to the destination node. The throughput and delay metrics are investigated for
a decode-and-forward relaying mechanism at finite blocklength regime and
delay-limited transmission mode. We consider ultra-reliable communication
scenarios under discussion for the next fifth-generation of wireless systems,
with error and latency constraints. The impact on these metrics of the
blocklength, information bits, and relay position is investigated.
",1,0,0,1,0,0
5250,5251,Spectral Norm Regularization for Improving the Generalizability of Deep Learning,"  We investigate the generalizability of deep learning based on the sensitivity
to input perturbation. We hypothesize that the high sensitivity to the
perturbation of data degrades the performance on it. To reduce the sensitivity
to perturbation, we propose a simple and effective regularization method,
referred to as spectral norm regularization, which penalizes the high spectral
norm of weight matrices in neural networks. We provide supportive evidence for
the abovementioned hypothesis by experimentally confirming that the models
trained using spectral norm regularization exhibit better generalizability than
other baseline methods.
",1,0,0,1,0,0
8447,8448,Distance Covariance in Metric Spaces: Non-Parametric Independence Testing in Metric Spaces (Master's thesis),"  The aim of this thesis is to find a solution to the non-parametric
independence problem in separable metric spaces. Suppose we are given finite
collection of samples from an i.i.d. sequence of paired random elements, where
each marginal has values in some separable metric space. The non-parametric
independence problem raises the question on how one can use these samples to
reasonably draw inference on whether the marginal random elements are
independent or not. We will try to answer this question by utilizing the
so-called distance covariance functional in metric spaces developed by Russell
Lyons. We show that, if the marginal spaces are so-called metric spaces of
strong negative type (e.g. seperable Hilbert spaces), then the distance
covariance functional becomes a direct indicator of independence. That is, one
can directly determine whether the marginals are independent or not based
solely on the value of this functional. As the functional formally takes the
simultaneous distribution as argument, its value is not known in the posed
non-parametric independence problem. Hence, we construct estimators of the
distance covariance functional, and show that they exhibit asymptotic
properties which can be used to construct asymptotically consistent statistical
tests of independence. Finally, as the rejection thresholds of these
statistical tests are non-traceable we argue that they can be reasonably
bootstrapped.
",0,0,1,1,0,0
8523,8524,Random Walk in a N-cube Without Hamiltonian Cycle to Chaotic Pseudorandom Number Generation: Theoretical and Practical Considerations,"  Designing a pseudorandom number generator (PRNG) is a difficult and complex
task. Many recent works have considered chaotic functions as the basis of built
PRNGs: the quality of the output would indeed be an obvious consequence of some
chaos properties. However, there is no direct reasoning that goes from chaotic
functions to uniform distribution of the output. Moreover, embedding such kind
of functions into a PRNG does not necessarily allow to get a chaotic output,
which could be required for simulating some chaotic behaviors.
In a previous work, some of the authors have proposed the idea of walking
into a $\mathsf{N}$-cube where a balanced Hamiltonian cycle has been removed as
the basis of a chaotic PRNG. In this article, all the difficult issues observed
in the previous work have been tackled. The chaotic behavior of the whole PRNG
is proven. The construction of the balanced Hamiltonian cycle is theoretically
and practically solved. An upper bound of the expected length of the walk to
obtain a uniform distribution is calculated. Finally practical experiments show
that the generators successfully pass the classical statistical tests.
",1,1,0,0,0,0
8627,8628,MSE estimates for multitaper spectral estimation and off-grid compressive sensing,"  We obtain estimates for the Mean Squared Error (MSE) for the multitaper
spectral estimator and certain compressive acquisition methods for multi-band
signals. We confirm a fact discovered by Thomson [Spectrum estimation and
harmonic analysis, Proc. IEEE, 1982]: assuming bandwidth $W$ and $N$ time
domain observations, the average of the square of the first $K=2NW$ Slepian
functions approaches, as $K$ grows, an ideal band-pass kernel for the interval
$[-W,W]$. We provide an analytic proof of this fact and measure the
corresponding rate of convergence in the $L^{1}$ norm. This validates a
heuristic approximation used to control the MSE of the multitaper estimator.
The estimates have also consequences for the method of compressive acquisition
of multi-band signals introduced by Davenport and Wakin, giving MSE
approximation bounds for the dictionary formed by modulation of the critical
number of prolates.
",1,0,1,1,0,0
4488,4489,Stateless Puzzles for Real Time Online Fraud Preemption,"  The profitability of fraud in online systems such as app markets and social
networks marks the failure of existing defense mechanisms. In this paper, we
propose FraudSys, a real-time fraud preemption approach that imposes
Bitcoin-inspired computational puzzles on the devices that post online system
activities, such as reviews and likes. We introduce and leverage several novel
concepts that include (i) stateless, verifiable computational puzzles, that
impose minimal performance overhead, but enable the efficient verification of
their authenticity, (ii) a real-time, graph-based solution to assign fraud
scores to user activities, and (iii) mechanisms to dynamically adjust puzzle
difficulty levels based on fraud scores and the computational capabilities of
devices. FraudSys does not alter the experience of users in online systems, but
delays fraudulent actions and consumes significant computational resources of
the fraudsters. Using real datasets from Google Play and Facebook, we
demonstrate the feasibility of FraudSys by showing that the devices of honest
users are minimally impacted, while fraudster controlled devices receive daily
computational penalties of up to 3,079 hours. In addition, we show that with
FraudSys, fraud does not pay off, as a user equipped with mining hardware
(e.g., AntMiner S7) will earn less than half through fraud than from honest
Bitcoin mining.
",1,0,0,0,0,0
15001,15002,Corral Framework: Trustworthy and Fully Functional Data Intensive Parallel Astronomical Pipelines,"  Data processing pipelines represent an important slice of the astronomical
software library that include chains of processes that transform raw data into
valuable information via data reduction and analysis. In this work we present
Corral, a Python framework for astronomical pipeline generation. Corral
features a Model-View-Controller design pattern on top of an SQL Relational
Database capable of handling: custom data models; processing stages; and
communication alerts, and also provides automatic quality and structural
metrics based on unit testing. The Model-View-Controller provides concept
separation between the user logic and the data models, delivering at the same
time multi-processing and distributed computing capabilities. Corral represents
an improvement over commonly found data processing pipelines in Astronomy since
the design pattern eases the programmer from dealing with processing flow and
parallelization issues, allowing them to focus on the specific algorithms
needed for the successive data transformations and at the same time provides a
broad measure of quality over the created pipeline. Corral and working examples
of pipelines that use it are available to the community at
this https URL.
",1,1,0,0,0,0
15423,15424,Riesz sequences and generalized arithmetic progressions,"  The purpose of this note is to verify that the results attained in [6] admit
an extension to the multidimensional setting. Namely, for subsets of the two
dimensional torus we find the sharp growth rate of the step(s) of a generalized
arithmetic progression in terms of its size which may be found in an
exponential systems satisfying the Riesz sequence property.
",0,0,1,0,0,0
17320,17321,Uniqueness of planar vortex patch in incompressible steady flow,"  We investigate a steady planar flow of an ideal fluid in a bounded simple
connected domain and focus on the vortex patch problem with prescribed
vorticity strength. There are two methods to deal with the existence of
solutions for this problem: the vorticity method and the stream function
method. A long standing open problem is whether these two entirely different
methods result in the same solution. In this paper, we will give a positive
answer to this problem by studying the local uniqueness of the solutions.
Another result obtained in this paper is that if the domain is convex, then the
vortex patch problem has a unique solution.
",0,0,1,0,0,0
635,636,Eigendecompositions of Transfer Operators in Reproducing Kernel Hilbert Spaces,"  Transfer operators such as the Perron--Frobenius or Koopman operator play an
important role in the global analysis of complex dynamical systems. The
eigenfunctions of these operators can be used to detect metastable sets, to
project the dynamics onto the dominant slow processes, or to separate
superimposed signals. We extend transfer operator theory to reproducing kernel
Hilbert spaces and show that these operators are related to Hilbert space
representations of conditional distributions, known as conditional mean
embeddings in the machine learning community. Moreover, numerical methods to
compute empirical estimates of these embeddings are akin to data-driven methods
for the approximation of transfer operators such as extended dynamic mode
decomposition and its variants. One main benefit of the presented kernel-based
approaches is that these methods can be applied to any domain where a
similarity measure given by a kernel is available. We illustrate the results
with the aid of guiding examples and highlight potential applications in
molecular dynamics as well as video and text data analysis.
",1,0,0,1,0,0
12544,12545,Global smoothing of a subanalytic set,"  We give rather simple answers to two long-standing questions in real-analytic
geometry, on global smoothing of a subanalytic set, and on transformation of a
proper real-analytic mapping to a mapping with equidimensional fibres by global
blowings-up of the target. These questions are related: a positive answer to
the second can be used to reduce the first to the simpler semianalytic case. We
show that the second question has a negative answer, in general, and that the
first problem nevertheless has a positive solution.
",0,0,1,0,0,0
18036,18037,Terminal-Pairability in Complete Bipartite Graphs,"  We investigate the terminal-pairibility problem in the case when the base
graph is a complete bipartite graph, and the demand graph is also bipartite
with the same color classes. We improve the lower bound on maximum value of
$\Delta(D)$ which still guarantees that the demand graph $D$ is
terminal-pairable in this setting. We also prove a sharp theorem on the maximum
number of edges such a demand graph can have.
",0,0,1,0,0,0
16950,16951,A CutFEM method for two-phase flow problems,"  In this article, we present a cut finite element method for two-phase
Navier-Stokes flows. The main feature of the method is the formulation of a
unified continuous interior penalty stabilisation approach for, on the one
hand, stabilising advection and the pressure-velocity coupling and, on the
other hand, stabilising the cut region. The accuracy of the algorithm is
enhanced by the development of extended fictitious domains to guarantee a well
defined velocity from previous time steps in the current geometry. Finally, the
robustness of the moving-interface algorithm is further improved by the
introduction of a curvature smoothing technique that reduces spurious
velocities. The algorithm is shown to perform remarkably well for low capillary
number flows, and is a first step towards flexible and robust CutFEM algorithms
for the simulation of microfluidic devices.
",1,0,0,0,0,0
3143,3144,A Unifying Framework for Convergence Analysis of Approximate Newton Methods,"  Many machine learning models are reformulated as optimization problems. Thus,
it is important to solve a large-scale optimization problem in big data
applications. Recently, subsampled Newton methods have emerged to attract much
attention for optimization due to their efficiency at each iteration, rectified
a weakness in the ordinary Newton method of suffering a high cost in each
iteration while commanding a high convergence rate. Other efficient stochastic
second order methods are also proposed. However, the convergence properties of
these methods are still not well understood. There are also several important
gaps between the current convergence theory and the performance in real
applications. In this paper, we aim to fill these gaps. We propose a unifying
framework to analyze local convergence properties of second order methods.
Based on this framework, our theoretical analysis matches the performance in
real applications.
",1,0,0,0,0,0
5139,5140,Non Relativistic Limit of Integrable QFT with fermionic excitations,"  The aim of this paper is to investigate the non-relativistic limit of
integrable quantum field theories with fermionic fields, such as the O(N)
Gross-Neveu model, the supersymmetric Sinh-Gordon and non-linear sigma models.
The non-relativistic limit of these theories is implemented by a double scaling
limit which consists of sending the speed of light c to infinity and rescaling
at the same time the relevant coupling constant of the model in such a way to
have finite energy excitations. For the general purpose of mapping the space of
continuous non-relativistic integrable models, this paper completes and
integrates the analysis done in Ref.[1] on the non-relativistic limit of purely
bosonic theories.
",0,1,0,0,0,0
12120,12121,Inference for Multiple Change-points in Linear and Non-linear Time Series Models,"  In this paper we develop a generalized likelihood ratio scan method (GLRSM)
for multiple change-points inference in piecewise stationary time series, which
estimates the number and positions of change-points and provides a confidence
interval for each change-point. The computational complexity of using GLRSM for
multiple change-points detection is as low as $O(n(\log n)^3)$ for a series of
length $n$. Consistency of the estimated numbers and positions of the
change-points is established. Extensive simulation studies are provided to
demonstrate the effectiveness of the proposed methodology under different
scenarios.
",0,0,1,1,0,0
15400,15401,The Urban Last Mile Problem: Autonomous Drone Delivery to Your Balcony,"  Drone delivery has been a hot topic in the industry in the past few years.
However, existing approaches either focus on rural areas or rely on centralized
drop-off locations from where the last mile delivery is performed. In this
paper we tackle the problem of autonomous last mile delivery in urban
environments using an off-the-shelf drone. We build a prototype system that is
able to fly to the approximate delivery location using GPS and then find the
exact drop-off location using visual navigation. The drop-off location could,
e.g., be on a balcony or porch, and simply needs to be indicated by a visual
marker on the wall or window. We test our system components in simulated
environments, including the visual navigation and collision avoidance. Finally,
we deploy our drone in a real-world environment and show how it can find the
drop-off point on a balcony. To stimulate future research in this topic we open
source our code.
",1,0,0,0,0,0
10498,10499,Robust Motion Planning employing Signal Temporal Logic,"  Motion planning classically concerns the problem of accomplishing a goal
configuration while avoiding obstacles. However, the need for more
sophisticated motion planning methodologies, taking temporal aspects into
account, has emerged. To address this issue, temporal logics have recently been
used to formulate such advanced specifications. This paper will consider Signal
Temporal Logic in combination with Model Predictive Control. A robustness
metric, called Discrete Average Space Robustness, is introduced and used to
maximize the satisfaction of specifications which results in a natural
robustness against noise. The comprised optimization problem is convex and
formulated as a Linear Program.
",1,0,0,0,0,0
13938,13939,The Bright and Dark Sides of High-Redshift starburst galaxies from {\it Herschel} and {\it Subaru} observations,"  We present rest-frame optical spectra from the FMOS-COSMOS survey of twelve
$z \sim 1.6$ \textit{Herschel} starburst galaxies, with Star Formation Rate
(SFR) elevated by $\times$8, on average, above the star-forming Main Sequence
(MS). Comparing the H$\alpha$ to IR luminosity ratio and the Balmer Decrement
we find that the optically-thin regions of the sources contain on average only
$\sim 10$ percent of the total SFR whereas $\sim90$ percent comes from an
extremely obscured component which is revealed only by far-IR observations and
is optically-thick even in H$\alpha$. We measure the [NII]$_{6583}$/H$\alpha$
ratio, suggesting that the less obscured regions have a metal content similar
to that of the MS population at the same stellar masses and redshifts. However,
our objects appear to be metal-rich outliers from the metallicity-SFR
anticorrelation observed at fixed stellar mass for the MS population. The
[SII]$_{6732}$/[SII]$_{6717}$ ratio from the average spectrum indicates an
electron density $n_{\rm e} \sim 1,100\ \mathrm{cm}^{-3}$, larger than what
estimated for MS galaxies but only at the 1.5$\sigma$ level. Our results
provide supporting evidence that high-$z$ MS outliers are the analogous of
local ULIRGs, and are consistent with a major merger origin for the starburst
event.
",0,1,0,0,0,0
11008,11009,Generative adversarial network-based approach to signal reconstruction from magnitude spectrograms,"  In this paper, we address the problem of reconstructing a time-domain signal
(or a phase spectrogram) solely from a magnitude spectrogram. Since magnitude
spectrograms do not contain phase information, we must restore or infer phase
information to reconstruct a time-domain signal. One widely used approach for
dealing with the signal reconstruction problem was proposed by Griffin and Lim.
This method usually requires many iterations for the signal reconstruction
process and depending on the inputs, it does not always produce high-quality
audio signals. To overcome these shortcomings, we apply a learning-based
approach to the signal reconstruction problem by modeling the signal
reconstruction process using a deep neural network and training it using the
idea of a generative adversarial network. Experimental evaluations revealed
that our method was able to reconstruct signals faster with higher quality than
the Griffin-Lim method.
",0,0,0,1,0,0
8843,8844,Model-Based Value Estimation for Efficient Model-Free Reinforcement Learning,"  Recent model-free reinforcement learning algorithms have proposed
incorporating learned dynamics models as a source of additional data with the
intention of reducing sample complexity. Such methods hold the promise of
incorporating imagined data coupled with a notion of model uncertainty to
accelerate the learning of continuous control tasks. Unfortunately, they rely
on heuristics that limit usage of the dynamics model. We present model-based
value expansion, which controls for uncertainty in the model by only allowing
imagination to fixed depth. By enabling wider use of learned dynamics models
within a model-free reinforcement learning algorithm, we improve value
estimation, which, in turn, reduces the sample complexity of learning.
",0,0,0,1,0,0
17369,17370,Learning with Confident Examples: Rank Pruning for Robust Classification with Noisy Labels,"  Noisy PN learning is the problem of binary classification when training
examples may be mislabeled (flipped) uniformly with noise rate rho1 for
positive examples and rho0 for negative examples. We propose Rank Pruning (RP)
to solve noisy PN learning and the open problem of estimating the noise rates,
i.e. the fraction of wrong positive and negative labels. Unlike prior
solutions, RP is time-efficient and general, requiring O(T) for any
unrestricted choice of probabilistic classifier with T fitting time. We prove
RP has consistent noise estimation and equivalent expected risk as learning
with uncorrupted labels in ideal conditions, and derive closed-form solutions
when conditions are non-ideal. RP achieves state-of-the-art noise estimation
and F1, error, and AUC-PR for both MNIST and CIFAR datasets, regardless of the
amount of noise and performs similarly impressively when a large portion of
training examples are noise drawn from a third distribution. To highlight, RP
with a CNN classifier can predict if an MNIST digit is a ""one""or ""not"" with
only 0.25% error, and 0.46 error across all digits, even when 50% of positive
examples are mislabeled and 50% of observed positive labels are mislabeled
negative examples.
",1,0,0,1,0,0
1472,1473,A powerful approach to the study of moderate effect modification in observational studies,"  Effect modification means the magnitude or stability of a treatment effect
varies as a function of an observed covariate. Generally, larger and more
stable treatment effects are insensitive to larger biases from unmeasured
covariates, so a causal conclusion may be considerably firmer if this pattern
is noted if it occurs. We propose a new strategy, called the submax-method,
that combines exploratory and confirmatory efforts to determine whether there
is stronger evidence of causality - that is, greater insensitivity to
unmeasured confounding - in some subgroups of individuals. It uses the joint
distribution of test statistics that split the data in various ways based on
certain observed covariates. For $L$ binary covariates, the method splits the
population $L$ times into two subpopulations, perhaps first men and women,
perhaps then smokers and nonsmokers, computing a test statistic from each
subpopulation, and appends the test statistic for the whole population, making
$2L+1$ test statistics in total. Although $L$ binary covariates define $2^{L}$
interaction groups, only $2L+1$ tests are performed, and at least $L+1$ of
these tests use at least half of the data. The submax-method achieves the
highest design sensitivity and the highest Bahadur efficiency of its component
tests. Moreover, the form of the test is sufficiently tractable that its large
sample power may be studied analytically. The simulation suggests that the
submax method exhibits superior performance, in comparison with an approach
using CART, when there is effect modification of moderate size. Using data from
the NHANES I Epidemiologic Follow-Up Survey, an observational study of the
effects of physical activity on survival is used to illustrate the method. The
method is implemented in the $\texttt{R}$ package $\texttt{submax}$ which
contains the NHANES example.
",0,0,0,1,0,0
20795,20796,Provenance and Pseudo-Provenance for Seeded Learning-Based Automated Test Generation,"  Many methods for automated software test generation, including some that
explicitly use machine learning (and some that use ML more broadly conceived)
derive new tests from existing tests (often referred to as seeds). Often, the
seed tests from which new tests are derived are manually constructed, or at
least simpler than the tests that are produced as the final outputs of such
test generators. We propose annotation of generated tests with a provenance
(trail) showing how individual generated tests of interest (especially failing
tests) derive from seed tests, and how the population of generated tests
relates to the original seed tests. In some cases, post-processing of generated
tests can invalidate provenance information, in which case we also propose a
method for attempting to construct ""pseudo-provenance"" describing how the tests
could have been (partly) generated from seeds.
",1,0,0,1,0,0
9053,9054,GAMBIT: The Global and Modular Beyond-the-Standard-Model Inference Tool,"  We describe the open-source global fitting package GAMBIT: the Global And
Modular Beyond-the-Standard-Model Inference Tool. GAMBIT combines extensive
calculations of observables and likelihoods in particle and astroparticle
physics with a hierarchical model database, advanced tools for automatically
building analyses of essentially any model, a flexible and powerful system for
interfacing to external codes, a suite of different statistical methods and
parameter scanning algorithms, and a host of other utilities designed to make
scans faster, safer and more easily-extendible than in the past. Here we give a
detailed description of the framework, its design and motivation, and the
current models and other specific components presently implemented in GAMBIT.
Accompanying papers deal with individual modules and present first GAMBIT
results. GAMBIT can be downloaded from gambit.hepforge.org.
",0,1,0,0,0,0
2479,2480,Automated Refactoring: Can They Pass The Turing Test?,"  Refactoring is a maintenance activity that aims to improve design quality
while preserving the behavior of a system. Several (semi)automated approaches
have been proposed to support developers in this maintenance activity, based on
the correction of anti-patterns, which are ""poor solutions"" to recurring design
problems. However, little quantitative evidence exists about the impact of
automatically refactored code on program comprehension, and in which context
automated refactoring can be as effective as manual refactoring. We performed
an empirical study to investigate whether the use of automated refactoring
approaches affects the understandability of systems during comprehension tasks.
(1) We surveyed 80 developers, asking them to identify from a set of 20
refactoring changes if they were generated by developers or by machine, and to
rate the refactorings according to their design quality; (2) we asked 30
developers to complete code comprehension tasks on 10 systems that were
refactored by either a freelancer or an automated refactoring tool. We measured
developers' performance using the NASA task load index for their effort; the
time that they spent performing the tasks; and their percentages of correct
answers. Results show that for 3 out the 5 types of studied anti-patterns,
developers cannot recognize the origin of the refactoring (i.e., whether it was
performed by a human or an automatic tool). We also observe that developers do
not prefer human refactorings over automated refactorings, except when
refactoring Blob classes; and that there is no statistically significant
difference between the impact on code understandability of human refactorings
and automated refactorings. We conclude that automated refactorings can be as
effective as manual refactorings. However, for complex anti-patterns types like
the Blob, the perceived quality of human refactorings is slightly higher.
",1,0,0,0,0,0
12943,12944,Optimization over Degree Sequences,"  We introduce and study the problem of optimizing arbitrary functions over
degree sequences of hypergraphs and multihypergraphs. We show that over
multihypergraphs the problem can be solved in polynomial time. For hypergraphs,
we show that deciding if a given sequence is the degree sequence of a
3-hypergraph is NP-complete, thereby solving a 30 year long open problem. This
implies that optimization over hypergraphs is hard already for simple concave
functions. In contrast, we show that for graphs, if the functions at vertices
are the same, then the problem is polynomial time solvable. We also provide
positive results for convex optimization over multihypergraphs and graphs and
exploit connections to degree sequence polytopes and threshold graphs. We then
elaborate on connections to the emerging theory of shifted combinatorial
optimization.
",1,0,1,0,0,0
2313,2314,Attaining Capacity with Algebraic Geometry Codes through the $(U|U+V)$ Construction and Koetter-Vardy Soft Decoding,"  In this paper we show how to attain the capacity of discrete symmetric
channels with polynomial time decoding complexity by considering iterated
$(U|U+V)$ constructions with Reed-Solomon code or algebraic geometry code
components. These codes are decoded with a recursive computation of the {\em a
posteriori} probabilities of the code symbols together with the Koetter-Vardy
soft decoder used for decoding the code components in polynomial time. We show
that when the number of levels of the iterated $(U|U+V)$ construction tends to
infinity, we attain the capacity of any discrete symmetric channel in this way.
This result follows from the polarization theorem together with a simple lemma
explaining how the Koetter-Vardy decoder behaves for Reed-Solomon codes of rate
close to $1$. However, even if this way of attaining the capacity of a
symmetric channel is essentially the Ar{\i}kan polarization theorem, there are
some differences with standard polar codes.
Indeed, with this strategy we can operate succesfully close to channel
capacity even with a small number of levels of the iterated $(U|U+V)$
construction and the probability of error decays quasi-exponentially with the
codelength in such a case (i.e. exponentially if we forget about the
logarithmic terms in the exponent). We can even improve on this result by
considering the algebraic geometry codes constructed in \cite{TVZ82}. In such a
case, the probability of error decays exponentially in the codelength for any
rate below the capacity of the channel. Moreover, when comparing this strategy
to Reed-Solomon codes (or more generally algebraic geometry codes) decoded with
the Koetter-Vardy decoding algorithm, it does not only improve the noise level
that the code can tolerate, it also results in a significant complexity gain.
",1,0,0,0,0,0
11873,11874,Curriculum-Based Neighborhood Sampling For Sequence Prediction,"  The task of multi-step ahead prediction in language models is challenging
considering the discrepancy between training and testing. At test time, a
language model is required to make predictions given past predictions as input,
instead of the past targets that are provided during training. This difference,
known as exposure bias, can lead to the compounding of errors along a generated
sequence at test time.
In order to improve generalization in neural language models and address
compounding errors, we propose a curriculum learning based method that
gradually changes an initially deterministic teacher policy to a gradually more
stochastic policy, which we refer to as \textit{Nearest-Neighbor Replacement
Sampling}. A chosen input at a given timestep is replaced with a sampled
nearest neighbor of the past target with a truncated probability proportional
to the cosine similarity between the original word and its top $k$ most similar
words. This allows the teacher to explore alternatives when the teacher
provides a sub-optimal policy or when the initial policy is difficult for the
learner to model. The proposed strategy is straightforward, online and requires
little additional memory requirements. We report our main findings on two
language modelling benchmarks and find that the proposed approach performs
particularly well when used in conjunction with scheduled sampling, that too
attempts to mitigate compounding errors in language models.
",0,0,0,1,0,0
9320,9321,Time Series Prediction for Graphs in Kernel and Dissimilarity Spaces,"  Graph models are relevant in many fields, such as distributed computing,
intelligent tutoring systems or social network analysis. In many cases, such
models need to take changes in the graph structure into account, i.e. a varying
number of nodes or edges. Predicting such changes within graphs can be expected
to yield important insight with respect to the underlying dynamics, e.g. with
respect to user behaviour. However, predictive techniques in the past have
almost exclusively focused on single edges or nodes. In this contribution, we
attempt to predict the future state of a graph as a whole. We propose to phrase
time series prediction as a regression problem and apply dissimilarity- or
kernel-based regression techniques, such as 1-nearest neighbor, kernel
regression and Gaussian process regression, which can be applied to graphs via
graph kernels. The output of the regression is a point embedded in a
pseudo-Euclidean space, which can be analyzed using subsequent dissimilarity-
or kernel-based processing methods. We discuss strategies to speed up Gaussian
Processes regression from cubic to linear time and evaluate our approach on two
well-established theoretical models of graph evolution as well as two real data
sets from the domain of intelligent tutoring systems. We find that simple
regression methods, such as kernel regression, are sufficient to capture the
dynamics in the theoretical models, but that Gaussian process regression
significantly improves the prediction error for real-world data.
",1,0,0,0,0,0
13301,13302,A Novel Partitioning Method for Accelerating the Block Cimmino Algorithm,"  We propose a novel block-row partitioning method in order to improve the
convergence rate of the block Cimmino algorithm for solving general sparse
linear systems of equations. The convergence rate of the block Cimmino
algorithm depends on the orthogonality among the block rows obtained by the
partitioning method. The proposed method takes numerical orthogonality among
block rows into account by proposing a row inner-product graph model of the
coefficient matrix. In the graph partitioning formulation defined on this graph
model, the partitioning objective of minimizing the cutsize directly
corresponds to minimizing the sum of inter-block inner products between block
rows thus leading to an improvement in the eigenvalue spectrum of the iteration
matrix. This in turn leads to a significant reduction in the number of
iterations required for convergence. Extensive experiments conducted on a large
set of matrices confirm the validity of the proposed method against a
state-of-the-art method.
",1,0,0,0,0,0
18908,18909,Ordering dynamics of self-propelled particles in an inhomogeneous medium,"  Ordering dynamics of self-propelled particles in an inhomogeneous medium in
two-dimensions is studied. We write coarse-grained hydrodynamic equations of
motion for coarse-grained density and velocity fields in the presence of an
external random disorder field, which is quenched in time. The strength of
inhomogeneity is tuned from zero disorder (clean system) to large disorder. In
the clean system, the velocity field grows algebraically as $L_{\rm V} \sim
t^{0.5}$. The density field does not show clean power-law growth; however, it
follows $L_{\rm \rho} \sim t^{0.8}$ approximately. In the inhomogeneous system,
we find a disorder dependent growth. For both the density and the velocity,
growth slow down with increasing strength of disorder. The velocity shows a
disorder dependent power-law growth $L_{\rm V}(t,\Delta) \sim t^{1/\bar z_{\rm
V}(\Delta)}$ for intermediate times. At late times, there is a crossover to
logarithmic growth $L_{\rm V}(t,\Delta) \sim (\ln t)^{1/\varphi}$, where
$\varphi$ is a disorder independent exponent. Two-point correlation functions
for the velocity shows dynamical scaling, but the density does not.
",0,1,0,0,0,0
17135,17136,A cautionary tale: limitations of a brightness-based spectroscopic approach to chromatic exoplanet radii,"  Determining wavelength-dependent exoplanet radii measurements is an excellent
way to probe the composition of exoplanet atmospheres. In light of this, Borsa
et al. (2016) sought to develop a technique to obtain such measurements by
comparing ground-based transmission spectra to the expected brightness
variations during an exoplanet transit. However, we demonstrate herein that
this is not possible due to the transit light curve normalisation necessary to
remove the effects of the Earth's atmosphere on the ground-based observations.
This is because the recoverable exoplanet radius is set by the planet-to-star
radius ratio within the transit light curve; we demonstrate this both
analytically and with simulated planet transits, as well as through a
reanalysis of the HD 189733b data.
",0,1,0,0,0,0
3161,3162,Real time observation of granular rock analogue material deformation and failure using nonlinear laser interferometry,"  A better understanding and anticipation of natural processes such as
landsliding or seismic fault activity requires detailed theoretical and
experimental analysis of rock mechanics and geomaterial dynamics. These last
decades, considerable progress has been made towards understanding deformation
and fracture process in laboratory experiment on granular rock materials, as
the well-known shear banding experiment. One of the reasons for this progress
is the continuous improvement in the instrumental techniques of observation.
But the lack of real time methods does not allow the detection of indicators of
the upcoming fracture process and thus to anticipate the phenomenon. Here, we
have performed uniaxial compression experiments to analyse the response of a
granular rock material sample to different shocks. We use a novel
interferometric laser sensor based on the nonlinear self-mixing interferometry
technique to observe in real time the deformations of the sample and assess its
usefulness as a diagnostic tool for the analysis of geomaterial dynamics. Due
to the high spatial and temporal resolution of this approach, we observe both
vibrations processes in response to a dynamic loading and the onset of failure.
The latter is preceded by a continuous variation of vibration period of the
material. After several shocks, the material response is no longer reversible
and we detect a progressive accumulation of irreversible deformation leading to
the fracture process. We demonstrate that material failure is anticipated by
the critical slowing down of the surface vibrational motion, which may
therefore be envisioned as an early warning signal or predictor to the
macroscopic failure of the sample. The nonlinear self-mixing interferometry
technique is readily extensible to fault propagation measurements. As such, it
opens a new window of observation for the study of geomaterial deformation and
failure.
",0,1,0,0,0,0
3327,3328,Optimal top dag compression,"  It is shown that for a given ordered node-labelled tree of size $n$ and with
$s$ many different node labels, one can construct in linear time a top dag of
height $O(\log n)$ and size $O(n / \log_\sigma n) \cap O(d \cdot \log n)$,
where $\sigma = \max\{ 2, s\}$ and $d$ is the size of the minimal dag. The size
bound $O(n / \log_\sigma n)$ is optimal and improves on previous bounds.
",1,0,0,0,0,0
2477,2478,Increasing Papers' Discoverability with Precise Semantic Labeling: the sci.AI Platform,"  The number of published findings in biomedicine increases continually. At the
same time, specifics of the domain's terminology complicates the task of
relevant publications retrieval. In the current research, we investigate
influence of terms' variability and ambiguity on a paper's likelihood of being
retrieved. We obtained statistics that demonstrate significance of the issue
and its challenges, followed by presenting the sci.AI platform, which allows
precise terms labeling as a resolution.
",1,0,0,0,0,0
12522,12523,Local isometric immersions of pseudo-spherical surfaces and k-th order evolution equations,"  We consider the class of evolution equations that describe pseudo-spherical
surfaces of the form u\_t = F (u, $\partial$u/$\partial$x, ..., $\partial$^k
u/$\partial$x^k), k $\ge$ 2 classified by Chern-Tenenblat. This class of
equations is characterized by the property that to each solution of a
differential equation within this class, there corresponds a 2-dimensional
Riemannian metric of curvature-1. We investigate the following problem: given
such a metric, is there a local isometric immersion in R 3 such that the
coefficients of the second fundamental form of the surface depend on a jet of
finite order of u? By extending our previous result for second order evolution
equation to k-th order equations, we prove that there is only one type of
equations that admit such an isometric immersion. We prove that the
coefficients of the second fundamental forms of the local isometric immersion
determined by the solutions u are universal, i.e., they are independent of u.
Moreover, we show that there exists a foliation of the domain of the parameters
of the surface by straight lines with the property that the mean curvature of
the surface is constant along the images of these straight lines under the
isometric immersion.
",0,0,1,0,0,0
8909,8910,An overview of the marine food web in Icelandic waters using Ecopath with Ecosim,"  Fishing activities have broad impacts that affect, although not exclusively,
the targeted stocks. These impacts affect predators and prey of the harvested
species, as well as the whole ecosystem it inhabits. Ecosystem models can be
used to study the interactions that occur within a system, including those
between different organisms and those between fisheries and targeted species.
Trophic web models like Ecopath with Ecosim (EwE) can handle fishing fleets as
a top predator, with top-down impact on harvested organisms. The aim of this
study was to better understand the Icelandic marine ecosystem and the
interactions within. This was done by constructing an EwE model of Icelandic
waters. The model was run from 1984 to 2013 and was fitted to time series of
biomass estimates, landings data and mean annual temperature. The final model
was chosen by selecting the model with the lowest Akaike information criterion.
A skill assessment was performed using the Pearson's correlation coefficient,
the coefficient of determination, the modelling efficiency and the reliability
index to evaluate the model performance. The model performed satisfactorily
when simulating previously estimated biomass and known landings. Most of the
groups with time series were estimated to have top-down control over their
prey. These are harvested species with direct and/or indirect links to lower
trophic levels and future fishing policies should take this into account. This
model could be used as a tool to investigate how such policies could impact the
marine ecosystem in Icelandic waters.
",0,0,0,0,1,0
1046,1047,Temporal processing and context dependency in C. elegans mechanosensation,"  A quantitative understanding of how sensory signals are transformed into
motor outputs places useful constraints on brain function and helps reveal the
brain's underlying computations. We investigate how the nematode C. elegans
responds to time-varying mechanosensory signals using a high-throughput
optogenetic assay and automated behavior quantification. In the prevailing
picture of the touch circuit, the animal's behavior is determined by which
neurons are stimulated and by the stimulus amplitude. In contrast, we find that
the behavioral response is tuned to temporal properties of mechanosensory
signals, like its integral and derivative, that extend over many seconds.
Mechanosensory signals, even in the same neurons, can be tailored to elicit
different behavioral responses. Moreover, we find that the animal's response
also depends on its behavioral context. Most dramatically, the animal ignores
all tested mechanosensory stimuli during turns. Finally, we present a
linear-nonlinear model that predicts the animal's behavioral response to
stimulus.
",0,0,0,0,1,0
16013,16014,Minimum polyhedron with $n$ vertices,"  We study a polyhedron with $n$ vertices of fixed volume having minimum
surface area. Completing the proof of Toth, we show that all faces of a minimum
polyhedron are triangles, and further prove that a minimum polyhedron does not
allow deformation of a single vertex. We also present possible minimum shapes
for $n\le 12$, some of them are quite unexpected, in particular $n=8$.
",0,0,1,0,0,0
19722,19723,"Circularizing Planet Nine through dynamical friction with an extended, cold planetesimal belt","  Unexpected clustering in the orbital elements of minor bodies beyond the
Kuiper belt has led to speculations that our solar system actually hosts nine
planets, the eight established plus a hypothetical ""Planet Nine"". Several
recent studies have shown that a planet with a mass of about 10 Earth masses on
a distant eccentric orbit with perihelion far beyond the Kuiper belt could
create and maintain this clustering. The evolutionary path resulting in an
orbit such as the one suggested for Planet Nine is nevertheless not easily
explained. Here we investigate whether a planet scattered away from the
giant-planet region could be lifted to an orbit similar to the one suggested
for Planet Nine through dynamical friction with a cold, distant planetesimal
belt. Recent simulations of planetesimal formation via the streaming
instability suggest that planetesimals can readily form beyond 100au. We
explore this circularisation by dynamical friction with a set of numerical
simulations. We find that a planet that is scattered from the region close to
Neptune onto an eccentric orbit has a 20-30% chance of obtaining an orbit
similar to that of Planet Nine after 4.6Gyr. Our simulations also result in
strong or partial clustering of the planetesimals; however, whether or not this
clustering is observable depends on the location of the inner edge of the
planetesimal belt. If the inner edge is located at 200au the degree of
clustering amongst observable objects is significant.
",0,1,0,0,0,0
11559,11560,Estimation of the multifractional function and the stability index of linear multifractional stable processes,"  In this paper we are interested in multifractional stable processes where the
self-similarity index $H$ is a function of time, in other words $H$ becomes
time changing, and the stability index $\alpha$ is a constant. Using $\beta$-
negative power variations ($-1/2<\beta<0$), we propose estimators for the value
of the multifractional function $H$ at a fixed time $t_0$ and for $\alpha$ for
two cases: multifractional Brownian motion ($\alpha=2$) and linear
multifractional stable motion ($0<\alpha<2$). We get the consistency of our
estimates for the underlying processes with the rate of convergence.
",0,0,1,1,0,0
813,814,New type integral inequalities for convex functions with applications II,"  We have recently established some integral inequalities for convex functions
via the Hermite-Hadamard's inequalities. In continuation here, we also
establish some interesting new integral inequalities for convex functions via
the Hermite--Hadamard's inequalities and Jensen's integral inequality. Useful
applications involving special means are also included.
",0,0,1,0,0,0
9258,9259,"Imaginary time, shredded propagator method for large-scale GW calculations","  The GW method is a many-body approach capable of providing quasiparticle
bands for realistic systems spanning physics, chemistry, and materials science.
Despite its power, GW is not routinely applied to large complex materials due
to its computational expense. We perform an exact recasting of the GW
polarizability and the self-energy as Laplace integrals over imaginary time
propagators. We then ""shred"" the propagators (via energy windowing) and
approximate them in a controlled manner by using Gauss-Laguerre quadrature and
discrete variable methods to treat the imaginary time propagators in real
space. The resulting cubic scaling GW method has a sufficiently small prefactor
to outperform standard quartic scaling methods on small systems (>=10 atoms)
and also represents a substantial improvement over several other cubic methods
tested. This approach is useful for evaluating quantum mechanical response
function involving large sums containing energy (difference) denominators.
",0,1,0,0,0,0
15656,15657,Supercurrent as a Probe for Topological Superconductivity in Magnetic Adatom Chains,"  A magnetic adatom chain, proximity coupled to a conventional superconductor
with spin-orbit coupling, exhibits locally an odd-parity, spin-triplet pairing
amplitude. We show that the singlet-triplet junction, thus formed, leads to a
net spin accumulation in the near vicinity of the chain. The accumulated spins
are polarized along the direction of the local $\mathbf{d}$-vector for triplet
pairing and generate an enhanced persistent current flowing around the chain.
The spin polarization and the ""supercurrent"" reverse their directions beyond a
critical exchange coupling strength at which the singlet superconducting order
changes its sign on the chain. The current is strongly enhanced in the
topological superconducting regime where Majorana bound states appear at the
chain ends. The current and the spin profile offer alternative routes to
characterize the topological superconducting state in adatom chains and
islands.
",0,1,0,0,0,0
7105,7106,Inequalities for the fundamental Robin eigenvalue of the Laplacian for box-shaped domains,"  This document consists of two papers, both submitted, and supplementary
material. The submitted papers are here given as Parts I and II.
Part I establishes results, used in Part II, 'on functions and inverses, both
positive, decreasing and convex'.
Part II uses results from Part I to extablish 'inequalities for the
fundamental Robin eigenvalue for the Laplacian on N-dimensional boxes'
",0,0,1,0,0,0
19756,19757,Extending the modeling of the anisotropic galaxy power spectrum to $k = 0.4 \ h\mathrm{Mpc}^{-1}$,"  We present a new model for the redshift-space power spectrum of galaxies and
demonstrate its accuracy in modeling the monopole, quadrupole, and hexadecapole
of the galaxy density field down to scales of $k = 0.4 \ h\mathrm{Mpc}^{-1}$.
The model describes the clustering of galaxies in the context of a halo model
and the clustering of the underlying halos in redshift space using a
combination of Eulerian perturbation theory and $N$-body simulations. The
modeling of redshift-space distortions is done using the so-called distribution
function approach. The final model has 13 free parameters, and each parameter
is physically motivated rather than a nuisance parameter, which allows the use
of well-motivated priors. We account for the Finger-of-God effect from centrals
and both isolated and non-isolated satellites rather than using a single
velocity dispersion to describe the combined effect. We test and validate the
accuracy of the model on several sets of high-fidelity $N$-body simulations, as
well as realistic mock catalogs designed to simulate the BOSS DR12 CMASS data
set. The suite of simulations covers a range of cosmologies and galaxy bias
models, providing a rigorous test of the level of theoretical systematics
present in the model. The level of bias in the recovered values of $f \sigma_8$
is found to be small. When including scales to $k = 0.4 \ h\mathrm{Mpc}^{-1}$,
we find 15-30\% gains in the statistical precision of $f \sigma_8$ relative to
$k = 0.2 \ h\mathrm{Mpc}^{-1}$ and a roughly 10-15\% improvement for the
perpendicular Alcock-Paczynski parameter $\alpha_\perp$. Using the BOSS DR12
CMASS mocks as a benchmark for comparison, we estimate an uncertainty on $f
\sigma_8$ that is $\sim$10-20\% larger than other similar Fourier-space RSD
models in the literature that use $k \leq 0.2 \ h\mathrm{Mpc}^{-1}$, suggesting
that these models likely have a too-limited parametrization.
",0,1,0,0,0,0
8115,8116,Identities involving Bernoulli and Euler polynomials,"  We present various identities involving the classical Bernoulli and Euler
polynomials. Among others, we prove that $$ \sum_{k=0}^{[n/4]}(-1)^k {n\choose
4k}\frac{B_{n-4k}(z) }{2^{6k}} =\frac{1}{2^{n+1}}\sum_{k=0}^{n} (-1)^k
\frac{1+i^k}{(1+i)^k} {n\choose k}{B_{n-k}(2z)} $$ and $$ \sum_{k=1}^{n}
2^{2k-1} {2n\choose 2k-1} B_{2k-1}(z) = \sum_{k=1}^n k \, 2^{2k} {2n\choose 2k}
E_{2k-1}(z). $$ Applications of our results lead to formulas for Bernoulli and
Euler numbers, like, for instance, $$ n E_{n-1} =\sum_{k=1}^{[n/2]}
\frac{2^{2k}-1}{k} (2^{2k}-2^n){n\choose 2k-1} B_{2k}B_{n-2k}. $$
",0,0,1,0,0,0
20944,20945,Implicit Entity Linking in Tweets,"  Over the years, Twitter has become one of the largest communication platforms
providing key data to various applications such as brand monitoring, trend
detection, among others. Entity linking is one of the major tasks in natural
language understanding from tweets and it associates entity mentions in text to
corresponding entries in knowledge bases in order to provide unambiguous
interpretation and additional con- text. State-of-the-art techniques have
focused on linking explicitly mentioned entities in tweets with reasonable
success. However, we argue that in addition to explicit mentions i.e. The movie
Gravity was more ex- pensive than the mars orbiter mission entities (movie
Gravity) can also be mentioned implicitly i.e. This new space movie is crazy.
you must watch it!. This paper introduces the problem of implicit entity
linking in tweets. We propose an approach that models the entities by
exploiting their factual and contextual knowledge. We demonstrate how to use
these models to perform implicit entity linking on a ground truth dataset with
397 tweets from two domains, namely, Movie and Book. Specifically, we show: 1)
the importance of linking implicit entities and its value addition to the
standard entity linking task, and 2) the importance of exploiting contextual
knowledge associated with an entity for linking their implicit mentions. We
also make the ground truth dataset publicly available to foster the research in
this new research area.
",1,0,0,0,0,0
1564,1565,Characterizing The Influence of Continuous Integration. Empirical Results from 250+ Open Source and Proprietary Projects,"  Continuous integration (CI) tools integrate code changes by automatically
compiling, building, and executing test cases upon submission of code changes.
Use of CI tools is getting increasingly popular, yet how proprietary projects
reap the benefits of CI remains unknown. To investigate the influence of CI on
software development, we analyze 150 open source software (OSS) projects, and
123 proprietary projects. For OSS projects, we observe the expected benefits
after CI adoption, e.g., improvements in bug and issue resolution. However, for
the proprietary projects, we cannot make similar observations. Our findings
indicate that only adoption of CI might not be enough to the improve software
development process. CI can be effective for software development if
practitioners use CI's feedback mechanism efficiently, by applying the practice
of making frequent commits. For our set of proprietary projects we observe
practitioners commit less frequently, and hence not use CI effectively for
obtaining feedback on the submitted code changes. Based on our findings we
recommend industry practitioners to adopt the best practices of CI to reap the
benefits of CI tools for example, making frequent commits.
",1,0,0,0,0,0
18210,18211,Shock-darkening in ordinary chondrites: determination of the pressure-temperature conditions by shock physics mesoscale modeling,"  We determined the shock-darkening pressure range in ordinary chondrites using
the iSALE shock physics code. We simulated planar shock waves on a mesoscale in
a sample layer at different nominal pressures. Iron and troilite grains were
resolved in a porous olivine matrix in the sample layer. We used equations of
state (Tillotson EoS and ANEOS) and basic strength and thermal properties to
describe the material phases. We used Lagrangian tracers to record peak shock
pressures in each material unit. The post-shock temperatures (and the fractions
of tracers experiencing temperatures above the melting point) for each material
were estimated after the passage of the shock wave and after reflections of the
shock at grain boundaries in the heterogeneous materials. The results showed
that shock-darkening, associated with troilite melt and the onset of olivine
melt, happened between 40 and 50 GPa - with 52 GPa being the pressure at which
all tracers in the troilite material reach the melting point. We demonstrate
the difficulties of shock heating in iron and also the importance of porosity.
Material impedances, grain shapes and the porosity models available in the
iSALE code are discussed. We also discussed possible not-shock-related triggers
for iron melt.
",0,1,0,0,0,0
10931,10932,High moments of the Estermann function,"  For $a/q\in\mathbb{Q}$ the Estermann function is defined as
$D(s,a/q):=\sum_{n\geq1}d(n)n^{-s}\operatorname{e}(n\frac aq)$ if $\Re(s)>1$
and by meromorphic continuation otherwise. For $q$ prime, we compute the
moments of $D(s,a/q)$ at the central point $s=1/2$, when averaging over $1\leq
a<q$.
As a consequence we deduce the asymptotic for the iterated moment of
Dirichlet $L$-functions $\sum_{\chi_1,\dots,\chi_k\mod
q}|L(\frac12,\chi_1)|^2\cdots |L(\frac12,\chi_k)|^2|L(\frac12,\chi_1\cdots
\chi_k)|^2$, obtaining a power saving error term.
Also, we compute the moments of certain functions defined in terms of
continued fractions. For example, writing $f_{\pm}(a/q):=\sum_{j=0}^r
(\pm1)^jb_j$ where $[0;b_0,\dots,b_r]$ is the continued fraction expansion of
$a/q$ we prove that for $k\geq2$ and $q$ primes one has
$\sum_{a=1}^{q-1}f_{\pm}(a/q)^k\sim2 \frac{\zeta(k)^2}{\zeta(2k)} q^k$ as
$q\to\infty$.
",0,0,1,0,0,0
7598,7599,Verification Studies for the Noh Problem using Non-ideal Equations of State and Finite Strength Shocks,"  The Noh verification test problem is extended beyond the commonly studied
ideal gamma-law gas to more realistic equations of state (EOSs) including the
stiff gas, the Noble-Abel gas, and the Carnahan-Starling EOS for hard-sphere
fluids. Self-similarity methods are used to solve the Euler compressible flow
equations, which in combination with the Rankine-Hugoniot jump conditions
provide a tractable general solution. This solution can be applied to fluids
with EOSs that meet criterion such as it being a convex function and having a
corresponding bulk modulus. For the planar case the solution can be applied to
shocks of arbitrary strength, but for cylindrical and spherical geometries it
is required that the analysis be restricted to strong shocks. The exact
solutions are used to perform a variety of quantitative code verification
studies of the Los Alamos National Laboratory Lagrangian hydrocode FLAG.
",1,1,0,0,0,0
14597,14598,Existence and a priori estimates of solutions for quasilinear singular elliptic systems with variable exponents,"  This article sets forth results on the existence, a priori estimates and
boundedness of positive solutions of a singular quasilinear systems of elliptic
equations involving variable exponents. The approach is based on Schauder's
fixed point Theorem. A Moser iteration procedure is also obtained for singular
cooperative systems involving variable exponents establishing a priori
estimates and boundedness of solutions.
",0,0,1,0,0,0
4657,4658,Topological Perspectives on Statistical Quantities I,"  In statistics cumulants are defined to be functions that measure the linear
independence of random variables. In the non-communicative case the Boolean
cumulants can be described as functions that measure deviation of a map between
algebras from being an algebra morphism. In Algebraic topology maps that are
homotopic to being algebra morphisms are studied using the theory of $A_\infty$
algebras. In this paper we will explore the link between these two points of
views on maps between algebras that are not algebra maps.
",0,0,1,0,0,0
5251,5252,Estimating the chromospheric magnetic field from a revised NLTE modeling: the case of HR7428,"  In this work we use the semi-empirical atmospheric modeling method to obtain
the chro-mospheric temperature, pressure, density and magnetic field
distribution versus height in the K2 primary component of the RS CVn binary
system HR 7428. While temperature, pressure, density are the standard output of
the semi-empirical modeling technique, the chromospheric magnetic field
estimation versus height comes from considering the possibility of not
im-posing hydrostatic equilibrium in the semi-empirical computation. The
stability of the best non-hydrostatic equilibrium model, implies the presence
of an additive (toward the center of the star) pressure, that decreases in
strength from the base of the chromosphere toward the outer layers.
Interpreting the additive pressure as magnetic pressure we estimated a magnetic
field intensity of about 500 gauss at the base of the chromosphere.
",0,1,0,0,0,0
3136,3137,Robust Navigation In GNSS Degraded Environment Using Graph Optimization,"  Robust navigation in urban environments has received a considerable amount of
both academic and commercial interest over recent years. This is primarily due
to large commercial organizations such as Google and Uber stepping into the
autonomous navigation market. Most of this research has shied away from Global
Navigation Satellite System (GNSS) based navigation. The aversion to utilizing
GNSS data is due to the degraded nature of the data in urban environment (e.g.,
multipath, poor satellite visibility). The degradation of the GNSS data in
urban environments makes it such that traditional (GNSS) positioning methods
(e.g., extended Kalman filter, particle filters) perform poorly. However,
recent advances in robust graph theoretic based sensor fusion methods,
primarily applied to Simultaneous Localization and Mapping (SLAM) based robotic
applications, can also be applied to GNSS data processing. This paper will
utilize one such method known as the factor graph in conjunction several robust
optimization techniques to evaluate their applicability to robust GNSS data
processing. The goals of this study are two-fold. First, for GNSS applications,
we will experimentally evaluate the effectiveness of robust optimization
techniques within a graph-theoretic estimation framework. Second, by releasing
the software developed and data sets used for this study, we will introduce a
new open-source front-end to the Georgia Tech Smoothing and Mapping (GTSAM)
library for the purpose of integrating GNSS pseudorange observations.
",1,0,0,0,0,0
3898,3899,Singular Riemannian flows and characteristic numbers,"  Let $M$ be an even-dimensional, oriented closed manifold. We show that the
restriction of a singular Riemannian flow on $M$ to a small tubular
neighborhood of each connected component of its singular stratum is
foliated-diffeomorphic to an isometric flow on the same neighborhood. We then
prove a formula that computes characteristic numbers of $M$ as the sum of
residues associated to the infinitesimal foliation at the components of the
singular stratum of the flow.
",0,0,1,0,0,0
9921,9922,Elements of $C^*$-algebras Attaining Their Norm in a Finite-Dimensional Representation,"  We characterize the class of RFD $C^*$-algebras as those containing a dense
subset of elements that attain their norm under a finite-dimensional
representation. We show further that this subset is the whole space precisely
when every irreducible representation of the $C^*$-algebra is
finite-dimensional, which is equivalent to the $C^*$-algebra having no simple
infinite-dimensional AF subquotient. We apply techniques from this proof to
show the existence of elements in more general classes of $C^*$-algebras whose
norms in finite-dimensional representations fit certain prescribed properties.
",0,0,1,0,0,0
17399,17400,Dimension Spectra of Lines,"  This paper investigates the algorithmic dimension spectra of lines in the
Euclidean plane. Given any line L with slope a and vertical intercept b, the
dimension spectrum sp(L) is the set of all effective Hausdorff dimensions of
individual points on L. We draw on Kolmogorov complexity and geometrical
arguments to show that if the effective Hausdorff dimension dim(a, b) is equal
to the effective packing dimension Dim(a, b), then sp(L) contains a unit
interval. We also show that, if the dimension dim(a, b) is at least one, then
sp(L) is infinite. Together with previous work, this implies that the dimension
spectrum of any line is infinite.
",1,0,0,0,0,0
2373,2374,Asymptotics of maximum likelihood estimation for stable law with $(M)$ parameterization,"  Asymptotics of maximum likelihood estimation for $\alpha$-stable law are
analytically investigated with $(M)$ parameterization. The consistency and
asymptotic normality are shown on the interior of the whole parameter space.
Although these asymptotics have been proved with $(B)$ parameterization, there
are several gaps between. Especially in the latter, the density, so that scores
and their derivatives are discontinuous at $\alpha=1$ for $\beta\neq 0$ and
usual asymptotics are impossible, whereas in $(M)$ form these quantities are
shown to be continuous on the interior of the parameter space. We fill these
gaps and provide a convenient theory for applied people. We numerically
approximate the Fisher information matrix around the Cauchy law
$(\alpha,\beta)=(1,0)$. The results exhibit continuity at $\alpha=1,\,\beta\neq
0$ and this secures the accuracy of our calculations.
",0,0,1,1,0,0
11684,11685,Search for Common Minima in Joint Optimization of Multiple Cost Functions,"  We present a novel optimization method, named the Combined Optimization
Method (COM), for the joint optimization of two or more cost functions. Unlike
the conventional joint optimization schemes, which try to find minima in a
weighted sum of cost functions, the COM explores search space for common minima
shared by all the cost functions. Given a set of multiple cost functions that
have qualitatively different distributions of local minima with each other, the
proposed method finds the common minima with a high success rate without the
help of any metaheuristics. As a demonstration, we apply the COM to the crystal
structure prediction in materials science. By introducing the concept of data
assimilation, i.e., adopting the theoretical potential energy of the crystal
and the crystallinity, which characterizes the agreement with the theoretical
and experimental X-ray diffraction patterns, as cost functions, we show that
the correct crystal structures of Si diamond, low quartz, and low cristobalite
can be predicted with significantly higher success rates than the previous
methods.
",0,0,0,1,0,0
3038,3039,On Fairness and Calibration,"  The machine learning community has become increasingly concerned with the
potential for bias and discrimination in predictive models. This has motivated
a growing line of work on what it means for a classification procedure to be
""fair."" In this paper, we investigate the tension between minimizing error
disparity across different population groups while maintaining calibrated
probability estimates. We show that calibration is compatible only with a
single error constraint (i.e. equal false-negatives rates across groups), and
show that any algorithm that satisfies this relaxation is no better than
randomizing a percentage of predictions for an existing classifier. These
unsettling findings, which extend and generalize existing results, are
empirically confirmed on several datasets.
",1,0,0,1,0,0
11255,11256,"Combinatorial metrics: MacWilliams-type identities, isometries and extension property","  In this work we characterize the combinatorial metrics admitting a
MacWilliams-type identity and describe the group of linear isometries of such
metrics. Considering coverings that are not connected, we classify the metrics
satisfying the MacWilliams extension property.
",1,0,0,0,0,0
7436,7437,On Optimal Spectrum Access of Cognitive Relay With Finite Packet Buffer,"  We investigate a cognitive radio system where secondary user (SU) relays
primary user (PU) packets using two-phase relaying. SU transmits its own
packets with some access probability in relaying phase using time sharing. PU
and SU have queues of finite capacity which results in packet loss when the
queues are full. Utilizing knowledge of relay queue state, SU aims to maximize
its packet throughput while keeping packet loss probability of PU below a
threshold. By exploiting structure of the problem, we formulate it as a linear
program and find optimal access policy of SU. We also propose low complexity
sub-optimal access policies, namely constant probability transmission and step
transmission. Numerical results are presented to compare performance of
proposed methods and study effect of queue sizes on packet throughput.
",1,0,0,0,0,0
1194,1195,On a Neumann-type series for modified Bessel functions of the first kind,"  In this paper, we are interested in a Neumann-type series for modified Bessel
functions of the first kind which arises in the study of Dunkl operators
associated with dihedral groups and as an instance of the Laguerre semigroup
constructed by Ben Said-Kobayashi-Orsted. We first revisit the particular case
corresponding to the group of square-preserving symmetries for which we give
two new and different proofs other than the existing ones. The first proof uses
the expansion of powers in a Neumann series of Bessel functions while the
second one is based on a quadratic transformation for the Gauss hypergeometric
function and opens the way to derive further expressions when the orders of the
underlying dihedral groups are powers of two. More generally, we give another
proof of De Bie \& al formula expressing this series as a $\Phi_2$-Horn
confluent hypergeometric function. In the course of proving, we shed the light
on the occurrence of multiple angles in their formula through elementary
symmetric functions, and get a new representation of Gegenbauer polynomials.
",0,0,1,0,0,0
1718,1719,Spin Transport and Accumulation in 2D Weyl Fermion System,"  In this work, we study the spin Hall effect and Rashba-Edelstein effect of a
2D Weyl fermion system in the clean limit using the Kubo formalism. Spin
transport is solely due to the spin-torque current in this strongly spin-orbit
coupled (SOC) system, and chiral spin-flip scattering off non-SOC scalar
impurities, with potential strength $V$ and size $a$, gives rise to a
skew-scattering mechanism for the spin Hall effect. The key result is that the
resultant spin-Hall angle has a fixed sign, with $\theta^{SH} \sim O
\left(\tfrac{V^2}{v_F^2/a^2} (k_F a)^4 \right)$ being a strongly-dependent
function of $k_F a$, with $k_F$ and $v_F$ being the Fermi wave-vector and Fermi
velocity respectively. This, therefore, allows for the possibility of tuning
the SHE by adjusting the Fermi energy or impurity size.
",0,1,0,0,0,0
98,99,Exponential Sums and Riesz energies,"  We bound an exponential sum that appears in the study of irregularities of
distribution (the low-frequency Fourier energy of the sum of several Dirac
measures) by geometric quantities: a special case is that for all $\left\{ x_1,
\dots, x_N\right\} \subset \mathbb{T}^2$, $X \geq 1$ and a universal $c>0$ $$
\sum_{i,j=1}^{N}{ \frac{X^2}{1 + X^4 \|x_i -x_j\|^4}} \lesssim \sum_{k \in
\mathbb{Z}^2 \atop \|k\| \leq X}{ \left| \sum_{n=1}^{N}{ e^{2 \pi i
\left\langle k, x_n \right\rangle}}\right|^2} \lesssim \sum_{i,j=1}^{N}{ X^2
e^{-c X^2\|x_i -x_j\|^2}}.$$ Since this exponential sum is intimately tied to
rather subtle distribution properties of the points, we obtain nonlocal
structural statements for near-minimizers of the Riesz-type energy. In the
regime $X \gtrsim N^{1/2}$ both upper and lower bound match for
maximally-separated point sets satisfying $\|x_i -x_j\| \gtrsim N^{-1/2}$.
",0,0,1,0,0,0
10624,10625,Mean Birds: Detecting Aggression and Bullying on Twitter,"  In recent years, bullying and aggression against users on social media have
grown significantly, causing serious consequences to victims of all
demographics. In particular, cyberbullying affects more than half of young
social media users worldwide, and has also led to teenage suicides, prompted by
prolonged and/or coordinated digital harassment. Nonetheless, tools and
technologies for understanding and mitigating it are scarce and mostly
ineffective. In this paper, we present a principled and scalable approach to
detect bullying and aggressive behavior on Twitter. We propose a robust
methodology for extracting text, user, and network-based attributes, studying
the properties of cyberbullies and aggressors, and what features distinguish
them from regular users. We find that bully users post less, participate in
fewer online communities, and are less popular than normal users, while
aggressors are quite popular and tend to include more negativity in their
posts. We evaluate our methodology using a corpus of 1.6M tweets posted over 3
months, and show that machine learning classification algorithms can accurately
detect users exhibiting bullying and aggressive behavior, achieving over 90%
AUC.
",1,0,0,0,0,0
16583,16584,Rapid Design of Wide-Area Heterogeneous Electromagnetic Metasurfaces beyond the Unit-Cell Approximation,"  We propose a novel numerical approach for the optimal design of wide-area
heterogeneous electromagnetic metasurfaces beyond the conventionally used
unit-cell approximation. The proposed method exploits the combination of
Rigorous Coupled Wave Analysis (RCWA) and global optimization techniques (two
evolutionary algorithms namely the Genetic Algorithm (GA) and a modified form
of the Artificial Bee Colony (ABC with memetic search phase method) are
considered). As a specific example, we consider the design of beam deflectors
using all-dielectric nanoantennae for operation in the visible wavelength
region; beam deflectors can serve as building blocks for other more complicated
devices like metalenses. Compared to previous reports using local optimization
approaches our approach improves device efficiency; transmission efficiency is
especially improved for wide deflection angle beam deflectors. The ABC method
with memetic search phase is also an improvement over the more commonly used GA
as it reaches similar efficiency levels with upto 35% reduction in computation
time. The method described here is of interest for the rapid design of a wide
variety of electromagnetic metasurfaces irrespective of their operational
wavelength.
",0,1,0,0,0,0
3839,3840,Highly accurate acoustic scattering: Isogeometric Analysis coupled with local high order Farfield Expansion ABC,"  This work is concerned with a unique combination of high order local
absorbing boundary conditions (ABC) with a general curvilinear Finite Element
Method (FEM) and its implementation in Isogeometric Analysis (IGA) for
time-harmonic acoustic waves. The ABC employed were recently devised by
Villamizar, Acosta and Dastrup [J. Comput. Phys. 333 (2017) 331] . They are
derived from exact Farfield Expansions representations of the outgoing waves in
the exterior of the regions enclosed by the artificial boundary. As a
consequence, the error due to the ABC on the artificial boundary can be reduced
conveniently such that the dominant error comes from the volume discretization
method used in the interior of the computational domain. Reciprocally, the
error in the interior can be made as small as the error at the artificial
boundary by appropriate implementation of {\it p-} and {\it h}- refinement. We
apply this novel method to cylindrical, spherical and arbitrary shape
scatterers including a prototype submarine. Our numerical results exhibits
spectral-like approximation and high order convergence rate. Additionally, they
show that the proposed method can reduce both the pollution and artificial
boundary errors to negligible levels even in very low- and high- frequency
regimes with rather coarse discretization densities in the IGA. As a result, we
have developed a highly accurate computational platform to numerically solve
time-harmonic acoustic wave scattering in two- and three-dimensions.
",1,0,0,0,0,0
12765,12766,Spectral Clustering Methods for Multiplex Networks,"  Multiplex networks offer an important tool for the study of complex systems
and extending techniques originally designed for single--layer networks is an
important area of study. One of the most important methods for analyzing
networks is clustering the nodes into communities that represent common
connectivity patterns. In this paper we extend spectral clustering to multiplex
structures and discuss some of the difficulties that arise in attempting to
define a natural generalization. In order to analyze our approach, we describe
three simple, synthetic multiplex networks and compare the performance of
different multiplex models. Our results suggest that a dynamically motivated
model is more successful than a structurally motivated model in discovering the
appropriate communities.
",1,1,0,0,0,0
707,708,Computing representation matrices for the action of Frobenius to cohomology groups,"  This paper is concerned with the computation of representation matrices for
the action of Frobenius to the cohomology groups of algebraic varieties.
Specifically we shall give an algorithm to compute the matrices for arbitrary
algebraic varieties with defining equations over perfect fields of positive
characteristic, and estimate its complexity. Moreover, we propose a specific
efficient method, which works for complete intersections.
",1,0,1,0,0,0
9354,9355,Searching for chemical signatures of brown dwarf formation,"  Recent studies have shown that close-in brown dwarfs in the mass range 35-55
M$_{\rm Jup}$ are almost depleted as companions to stars, suggesting that
objects with masses above and below this gap might have different formation
mechanisms. We determine the fundamental stellar parameters, as well as
individual abundances for a large sample of stars known to have a substellar
companion in the brown dwarf regime. The sample is divided into stars hosting
""massive"" and ""low-mass"" brown dwarfs. Following previous works a threshold of
42.5 M$_{\rm Jup}$ was considered. Our results confirm that stars with brown
dwarf companions do not follow the well-established gas-giant planet
metallicity correlation seen in main-sequence planet hosts. Stars harbouring
""massive"" brown dwarfs show similar metallicity and abundance distribution as
stars without known planets or with low-mass planets. We find a tendency of
stars harbouring ""less-massive"" brown dwarfs of having slightly larger
metallicity, [X$_{\rm Fe}$/Fe] values, and abundances of Sc II, Mn I, and Ni I
in comparison with the stars having the massive brown dwarfs. The data suggest,
as previously reported, that massive and low-mass brown dwarfs might present
differences in period and eccentricity. We find evidence of a non-metallicity
dependent mechanism for the formation of massive brown dwarfs. Our results
agree with a scenario in which massive brown dwarfs are formed as stars. At
high-metallicities, the core-accretion mechanism might become efficient in the
formation of low-mass brown dwarfs while at lower metallicities low-mass brown
dwarfs could form by gravitational instability in turbulent protostellar discs.
",0,1,0,0,0,0
18212,18213,Carnot Efficiency of Publication,"  This paper analyzes publication efficiency in terms of Hirsch-index or
h-index and total citations, with an analogy to the Carnot efficiency used in
thermodynamics. Such publication efficiency, with typical value of 30%, can be
utilized to normalize the research output judgment, favoring quality outputs in
reduced quantity, which is currently lacking in many discipline.
",1,0,0,0,0,0
1244,1245,The Australian PCEHR system: Ensuring Privacy and Security through an Improved Access Control Mechanism,"  An Electronic Health Record (EHR) is designed to store diverse data
accurately from a range of health care providers and to capture the status of a
patient by a range of health care providers across time. Realising the numerous
benefits of the system, EHR adoption is growing globally and many countries
invest heavily in electronic health systems. In Australia, the Government
invested $467 million to build key components of the Personally Controlled
Electronic Health Record (PCEHR) system in July 2012. However, in the last
three years, the uptake from individuals and health care providers has not been
satisfactory. Unauthorised access of the PCEHR was one of the major barriers.
We propose an improved access control model for the PCEHR system to resolve the
unauthorised access issue. We discuss the unauthorised access issue with real
examples and present a potential solution to overcome the issue to make the
PCEHR system a success in Australia.
",1,0,0,0,0,0
878,879,Landau Collision Integral Solver with Adaptive Mesh Refinement on Emerging Architectures,"  The Landau collision integral is an accurate model for the small-angle
dominated Coulomb collisions in fusion plasmas. We investigate a high order
accurate, fully conservative, finite element discretization of the nonlinear
multi-species Landau integral with adaptive mesh refinement using the PETSc
library (www.mcs.anl.gov/petsc). We develop algorithms and techniques to
efficiently utilize emerging architectures with an approach that minimizes
memory usage and movement and is suitable for vector processing. The Landau
collision integral is vectorized with Intel AVX-512 intrinsics and the solver
sustains as much as 22% of the theoretical peak flop rate of the Second
Generation Intel Xeon Phi, Knights Landing, processor.
",1,0,0,0,0,0
6952,6953,Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery,"  Obtaining models that capture imaging markers relevant for disease
progression and treatment monitoring is challenging. Models are typically based
on large amounts of data with annotated examples of known markers aiming at
automating detection. High annotation effort and the limitation to a vocabulary
of known markers limit the power of such approaches. Here, we perform
unsupervised learning to identify anomalies in imaging data as candidates for
markers. We propose AnoGAN, a deep convolutional generative adversarial network
to learn a manifold of normal anatomical variability, accompanying a novel
anomaly scoring scheme based on the mapping from image space to a latent space.
Applied to new data, the model labels anomalies, and scores image patches
indicating their fit into the learned distribution. Results on optical
coherence tomography images of the retina demonstrate that the approach
correctly identifies anomalous images, such as images containing retinal fluid
or hyperreflective foci.
",1,0,0,0,0,0
11311,11312,Kernel Feature Selection via Conditional Covariance Minimization,"  We propose a method for feature selection that employs kernel-based measures
of independence to find a subset of covariates that is maximally predictive of
the response. Building on past work in kernel dimension reduction, we show how
to perform feature selection via a constrained optimization problem involving
the trace of the conditional covariance operator. We prove various consistency
results for this procedure, and also demonstrate that our method compares
favorably with other state-of-the-art algorithms on a variety of synthetic and
real data sets.
",1,0,0,1,0,0
11934,11935,Voice Conversion Based on Cross-Domain Features Using Variational Auto Encoders,"  An effective approach to non-parallel voice conversion (VC) is to utilize
deep neural networks (DNNs), specifically variational auto encoders (VAEs), to
model the latent structure of speech in an unsupervised manner. A previous
study has confirmed the ef- fectiveness of VAE using the STRAIGHT spectra for
VC. How- ever, VAE using other types of spectral features such as mel- cepstral
coefficients (MCCs), which are related to human per- ception and have been
widely used in VC, have not been prop- erly investigated. Instead of using one
specific type of spectral feature, it is expected that VAE may benefit from
using multi- ple types of spectral features simultaneously, thereby improving
the capability of VAE for VC. To this end, we propose a novel VAE framework
(called cross-domain VAE, CDVAE) for VC. Specifically, the proposed framework
utilizes both STRAIGHT spectra and MCCs by explicitly regularizing multiple
objectives in order to constrain the behavior of the learned encoder and de-
coder. Experimental results demonstrate that the proposed CD- VAE framework
outperforms the conventional VAE framework in terms of subjective tests.
",1,0,0,0,0,0
17684,17685,On locally compact semitopological $0$-bisimple inverse $ω$-semigroups,"  We describe the structure of Hausdorff locally compact semitopological
$0$-bisimple inverse $\omega$-semigroups with compact maximal subgroups. In
particular, we show that a Hausdorff locally compact semitopological
$0$-bisimple inverse $\omega$-semigroup with a compact maximal subgroup is
either compact or topologically isomorphic to the topological sum of its
$\mathscr{H}$-classes. We describe the structure of Hausdorff locally compact
semitopological $0$-bisimple inverse $\omega$-semigroups with a monothetic
maximal subgroups. In particular we prove the dichotomy for $T_1$ locally
compact semitopological Reilly semigroup
$\left(\textbf{B}(\mathbb{Z}_{+},\theta)^0,\tau\right)$ with adjoined zero and
with a non-annihilating homomorphism $\theta\colon \mathbb{Z}_{+}\to
\mathbb{Z}_{+}$: $\left(\textbf{B}(\mathbb{Z}_{+},\theta)^0,\tau\right)$ is
either compact or discrete. At the end we discuss on the remainder under the
closure of the discrete Reilly semigroup $\textbf{B}(\mathbb{Z}_{+},\theta)^0$
in a semitopological semigroup.
",0,0,1,0,0,0
8767,8768,Bielliptic intermediate modular curves,"  We determine which of the modular curves $X_\Delta(N)$, that is, curves lying
between $X_0(N)$ and $X_1(N)$, are bielliptic. Somewhat surprisingly, we find
that one of these curves has exceptional automorphisms. Finally we find all
$X_\Delta(N)$ that have infinitely many quadratic points over $\mathbb{Q}$.
",0,0,1,0,0,0
18720,18721,Rigidity of branching microstructures in shape memory alloys,"  We analyze generic sequences for which the geometrically linear energy
\[E_\eta(u,\chi):= \eta^{-\frac{2}{3}}\int_{B_{0}(1)} \left| e(u)-
\sum_{i=1}^3 \chi_ie_i\right|^2 d x+\eta^\frac{1}{3} \sum_{i=1}^3
|D\chi_i|(B_{0}(1))\] remains bounded in the limit $\eta \to 0$. Here $ e(u)
:=1/2(Du + Du^T)$ is the (linearized) strain of the displacement $u$, the
strains $e_i$ correspond to the martensite strains of a shape memory alloy
undergoing cubic-to-tetragonal transformations and $\chi_i:B_{0}(1) \to
\{0,1\}$ is the partition into phases. In this regime it is known that in
addition to simple laminates also branched structures are possible, which if
austenite was present would enable the alloy to form habit planes.
In an ansatz-free manner we prove that the alignment of macroscopic
interfaces between martensite twins is as predicted by well-known rank-one
conditions. Our proof proceeds via the non-convex, non-discrete-valued
differential inclusion \[e(u) \in \bigcup_{1\leq i\neq j\leq 3}
\operatorname{conv} \{e_i,e_j\}\] satisfied by the weak limits of bounded
energy sequences and of which we classify all solutions. In particular, there
exist no convex integration solutions of the inclusion with complicated
geometric structures.
",0,0,1,0,0,0
5655,5656,Interior transmission eigenvalue problems on compact manifolds with boundary conductivity parameters,"  In this paper, we consider an interior transmission eigenvalue (ITE) problem
on some compact $C^{\infty }$-Riemannian manifolds with a common smooth
boundary. In particular, these manifolds may have different topologies, but we
impose some conditions of Riemannian metrics, indices of refraction and
boundary conductivity parameters on the boundary. Then we prove the
discreteness of the set of ITEs, the existence of infinitely many ITEs, and its
Weyl type lower bound. For our settings, we can adopt the argument by
Lakshtanov and Vainberg, considering the Dirichlet-to-Neumann map. As an
application, we derive the existence of non-scattering energies for
time-harmonic acoustic equations. For the sake of simplicity, we consider the
scattering theory on the Euclidean space. However, the argument is applicable
for certain kinds of non-compact manifolds with ends on which we can define the
scattering matrix.
",0,0,1,0,0,0
18137,18138,Bayesian uncertainty quantification for epidemic spread on networks,"  While there exist a number of mathematical approaches to modeling the spread
of disease on a network, analyzing such systems in the presence of uncertainty
introduces significant complexity. In scenarios where system parameters must be
inferred from limited observations, general approaches to uncertainty
quantification can generate approximate distributions of the unknown
parameters, but these methods often become computationally expensive if the
underlying disease model is complex. In this paper, we apply the recent
massively parallelizable Bayesian uncertainty quantification framework $\Pi4U$
to a model of a disease spreading on a network of communities, showing that the
method can accurately and tractably recover system parameters and select
optimal models in this setting.
",0,0,0,1,0,0
6569,6570,Frictional Effects on RNA Folding: Speed Limit and Kramers Turnover,"  We investigated frictional effects on the folding rates of a human telomerase
hairpin (hTR HP) and H-type pseudoknot from the Beet Western Yellow Virus (BWYV
PK) using simulations of the Three Interaction Site (TIS) model for RNA. The
heat capacity from TIS model simulations, calculated using temperature replica
exchange simulations, reproduces nearly quantitatively the available
experimental data for the hTR HP. The corresponding results for BWYV PK serve
as predictions. We calculated the folding rates ($k_\mathrm{F}$) from more than
100 folding trajectories for each value of the solvent viscosity ($\eta$) at a
fixed salt concentration of 200 mM. By using the theoretical estimate
($\propto$$\sqrt{N}$ where $N$ is the number of nucleotides) for folding free
energy barrier, $k_\mathrm{F}$ data for both the RNAs are quantitatively fit
using one-dimensional Kramers' theory with two parameters specifying the
curvatures in the unfolded basin and the barrier top. In the high-friction
regime ($\eta\gtrsim10^{-5}\,\textrm{Pa\ensuremath{\cdot}s}$), for both HP and
PK, $k_\mathrm{F}$s decrease as $1/\eta$ whereas in the low friction regime,
$k_\mathrm{F}$ values increase as $\eta$ increases, leading to a maximum
folding rate at a moderate viscosity
($\sim10^{-6}\,\textrm{Pa\ensuremath{\cdot}s}$), which is the Kramers turnover.
From the fits, we find that the speed limit to RNA folding at water viscosity
is between 1 and 4 $\mathrm{\mu s}$, which is in accord with our previous
theoretical prediction as well as results from several single molecule
experiments. Both the RNA constructs fold by parallel pathways. Surprisingly,
we find that the flux through the pathways could be altered by changing solvent
viscosity, a prediction that is more easily testable in RNA than in proteins.
",0,0,0,0,1,0
19054,19055,Exponentially convergent data assimilation algorithm for Navier-Stokes equations,"  The paper presents a new state estimation algorithm for a bilinear equation
representing the Fourier- Galerkin (FG) approximation of the Navier-Stokes (NS)
equations on a torus in R2. This state equation is subject to uncertain but
bounded noise in the input (Kolmogorov forcing) and initial conditions, and its
output is incomplete and contains bounded noise. The algorithm designs a
time-dependent gain such that the estimation error converges to zero
exponentially. The sufficient condition for the existence of the gain are
formulated in the form of algebraic Riccati equations. To demonstrate the
results we apply the proposed algorithm to the reconstruction a chaotic fluid
flow from incomplete and noisy data.
",0,1,0,0,0,0
8482,8483,Experimental demonstration of a Josephson magnetic memory cell with a programmable π-junction,"  We experimentally demonstrate the operation of a Josephson magnetic random
access memory unit cell, built with a Ni_80Fe_20/Cu/Ni pseudo spin-valve
Josephson junction with Nb electrodes and an integrated readout SQUID in a
fully planarized Nb fabrication process. We show that the parallel and
anti-parallel memory states of the spin-valve can be mapped onto a junction
equilibrium phase of either zero or pi by appropriate choice of the ferromagnet
thicknesses, and that the magnetic Josephson junction can be written to either
a zero-junction or pi-junction state by application of write fields of
approximately 5 mT. This work represents a first step towards a scalable,
dense, and power-efficient cryogenic memory for superconducting
high-performance digital computing.
",0,1,0,0,0,0
3072,3073,Data Augmentation for Robust Keyword Spotting under Playback Interference,"  Accurate on-device keyword spotting (KWS) with low false accept and false
reject rate is crucial to customer experience for far-field voice control of
conversational agents. It is particularly challenging to maintain low false
reject rate in real world conditions where there is (a) ambient noise from
external sources such as TV, household appliances, or other speech that is not
directed at the device (b) imperfect cancellation of the audio playback from
the device, resulting in residual echo, after being processed by the Acoustic
Echo Cancellation (AEC) system. In this paper, we propose a data augmentation
strategy to improve keyword spotting performance under these challenging
conditions. The training set audio is artificially corrupted by mixing in music
and TV/movie audio, at different signal to interference ratios. Our results
show that we get around 30-45% relative reduction in false reject rates, at a
range of false alarm rates, under audio playback from such devices.
",0,0,0,1,0,0
9874,9875,Support Spinor Machine,"  We generalize a support vector machine to a support spinor machine by using
the mathematical structure of wedge product over vector machine in order to
extend field from vector field to spinor field. The separated hyperplane is
extended to Kolmogorov space in time series data which allow us to extend a
structure of support vector machine to a support tensor machine and a support
tensor machine moduli space. Our performance test on support spinor machine is
done over one class classification of end point in physiology state of time
series data after empirical mode analysis and compared with support vector
machine test. We implement algorithm of support spinor machine by using
Holo-Hilbert amplitude modulation for fully nonlinear and nonstationary time
series data analysis.
",1,0,0,1,0,0
20262,20263,Optimal Non-uniform Deployments in Ultra-Dense Finite-Area Cellular Networks,"  Network densification and heterogenisation through the deployment of small
cellular access points (picocells and femtocells) are seen as key mechanisms in
handling the exponential increase in cellular data traffic. Modelling such
networks by leveraging tools from Stochastic Geometry has proven particularly
useful in understanding the fundamental limits imposed on network coverage and
capacity by co-channel interference. Most of these works however assume
infinite sized and uniformly distributed networks on the Euclidean plane. In
contrast, we study finite sized non-uniformly distributed networks, and find
the optimal non-uniform distribution of access points which maximises network
coverage for a given non-uniform distribution of mobile users, and vice versa.
",1,0,0,0,0,0
4660,4661,Gee-Haw Whammy Diddle,"  Gee-Haw Whammy Diddle is a seemingly simple mechanical toy consisting of a
wooden stick and a second stick that is made up of a series of notches with a
propeller at its end. When the wooden stick is pulled over the notches, the
propeller starts to rotate. In spite of its simplicity, physical principles
governing the motion of the stick and the propeller are rather complicated and
interesting. Here we provide a thorough analysis of the system and parameters
influencing the motion. We show that contrary to the results published on this
topic so far, neither elliptic motion of the stick nor frequency
synchronization is needed for starting the motion of the propeller.
",0,1,0,0,0,0
17851,17852,Water flow in Carbon and Silicon Carbide nanotubes,"  In this work the conduction of ion-water solution through two discrete
bundles of armchair carbon and silicon carbide nanotubes, as useful membranes
for water desalination, is studied. In order that studies on different types of
nanotubes be comparable, the chiral vectors of C and Si-C nanotubes are
selected as (7,7) and (5,5), respectively, so that a similar volume of fluid is
investigated flowing through two similar dimension membranes. Different
hydrostatic pressures are applied and the flow rates of water and ions are
calculated through molecular dynamics simulations. Consequently, according to
conductance of water per each nanotube, per nanosecond, it is perceived that at
lower pressures (below 150 MPa) the Si-C nanotubes seem to be more applicable,
while higher hydrostatic pressures make carbon nanotube membranes more suitable
for water desalination.
",0,1,0,0,0,0
11145,11146,Stochastic comparisons of the largest claim amounts from two sets of interdependent heterogeneous portfolios,"  Let $ X_{\lambda_1},\ldots,X_{\lambda_n}$ be dependent non-negative random
variables and $Y_i=I_{p_i} X_{\lambda_i}$, $i=1,\ldots,n$, where
$I_{p_1},\ldots,I_{p_n}$ are independent Bernoulli random variables independent
of $X_{\lambda_i}$'s, with ${\rm E}[I_{p_i}]=p_i$, $i=1,\ldots,n$. In actuarial
sciences, $Y_i$ corresponds to the claim amount in a portfolio of risks. In
this paper, we compare the largest claim amounts of two sets of interdependent
portfolios, in the sense of usual stochastic order, when the variables in one
set have the parameters $\lambda_1,\ldots,\lambda_n$ and $p_1,\ldots,p_n$ and
the variables in the other set have the parameters
$\lambda^{*}_1,\ldots,\lambda^{*}_n$ and $p^*_1,\ldots,p^*_n$. For
illustration, we apply the results to some important models in actuary.
",0,0,0,0,0,1
2140,2141,Human-Level Intelligence or Animal-Like Abilities?,"  The vision systems of the eagle and the snake outperform everything that we
can make in the laboratory, but snakes and eagles cannot build an eyeglass or a
telescope or a microscope. (Judea Pearl)
",1,0,0,1,0,0
18478,18479,Oxidation of clofibric acid in aqueous solution using a non-thermal plasma discharge or gamma radiation,"  In this work, we study degradation of clofibric acid (CFA) in aqueous
solution using either ionizing radiation from a $^{60}$Co source or a
non-thermal plasma produced by discharges in the air above the solution. The
results obtained with the two technologies are compared in terms of
effectiveness of CFA degradation and its by-products. In both cases the CFA
degradation follows a quasi-exponential decay in time well modelled by a
kinetic scheme which considers the competition between CFA and all reaction
intermediates for the reactive species generated in solution as well as the
amount of the end product formed. A new degradation law is deduced to explain
the results. Although the end-product CO$_2$ was detected and the CFA
conversion found to be very high under the studied conditions, HPLC analysis
reveals several degradation intermediates still bearing the aromatic ring with
the chlorine substituent. The extent of mineralization is rather limited. The
energy yield is found to be higher in the gamma radiation experiments.
",0,1,0,0,0,0
17867,17868,Never Forget: Balancing Exploration and Exploitation via Learning Optical Flow,"  Exploration bonus derived from the novelty of the states in an environment
has become a popular approach to motivate exploration for deep reinforcement
learning agents in the past few years. Recent methods such as curiosity-driven
exploration usually estimate the novelty of new observations by the prediction
errors of their system dynamics models. Due to the capacity limitation of the
models and difficulty of performing next-frame prediction, however, these
methods typically fail to balance between exploration and exploitation in
high-dimensional observation tasks, resulting in the agents forgetting the
visited paths and exploring those states repeatedly. Such inefficient
exploration behavior causes significant performance drops, especially in large
environments with sparse reward signals. In this paper, we propose to introduce
the concept of optical flow estimation from the field of computer vision to
deal with the above issue. We propose to employ optical flow estimation errors
to examine the novelty of new observations, such that agents are able to
memorize and understand the visited states in a more comprehensive fashion. We
compare our method against the previous approaches in a number of experimental
experiments. Our results indicate that the proposed method appears to deliver
superior and long-lasting performance than the previous methods. We further
provide a set of comprehensive ablative analysis of the proposed method, and
investigate the impact of optical flow estimation on the learning curves of the
DRL agents.
",1,0,0,0,0,0
81,82,Exploring RNN-Transducer for Chinese Speech Recognition,"  End-to-end approaches have drawn much attention recently for significantly
simplifying the construction of an automatic speech recognition (ASR) system.
RNN transducer (RNN-T) is one of the popular end-to-end methods. Previous
studies have shown that RNN-T is difficult to train and a very complex training
process is needed for a reasonable performance. In this paper, we explore RNN-T
for a Chinese large vocabulary continuous speech recognition (LVCSR) task and
aim to simplify the training process while maintaining performance. First, a
new strategy of learning rate decay is proposed to accelerate the model
convergence. Second, we find that adding convolutional layers at the beginning
of the network and using ordered data can discard the pre-training process of
the encoder without loss of performance. Besides, we design experiments to find
a balance among the usage of GPU memory, training circle and model performance.
Finally, we achieve 16.9% character error rate (CER) on our test set which is
2% absolute improvement from a strong BLSTM CE system with language model
trained on the same text corpus.
",1,0,0,0,0,0
10484,10485,Early stopping for statistical inverse problems via truncated SVD estimation,"  We consider truncated SVD (or spectral cut-off, projection) estimators for a
prototypical statistical inverse problem in dimension $D$. Since calculating
the singular value decomposition (SVD) only for the largest singular values is
much less costly than the full SVD, our aim is to select a data-driven
truncation level $\widehat m\in\{1,\ldots,D\}$ only based on the knowledge of
the first $\widehat m$ singular values and vectors. We analyse in detail
whether sequential {\it early stopping} rules of this type can preserve
statistical optimality. Information-constrained lower bounds and matching upper
bounds for a residual based stopping rule are provided, which give a clear
picture in which situation optimal sequential adaptation is feasible. Finally,
a hybrid two-step approach is proposed which allows for classical oracle
inequalities while considerably reducing numerical complexity.
",0,0,1,1,0,0
1161,1162,Equivalent electric circuit of magnetosphere-ionosphere-atmosphere interaction,"  The aim of this study is to investigate the magnetospheric disturbances
effects on complicated nonlinear system of atmospheric processes. During
substorms and storms, the ionosphere was subjected to rather a significant
Joule heating, and the power of precipitating energetic particles was also
great. Nevertheless, there were no abnormal variations of meteoparameters in
the lower atmosphere. If there is a mechanism for the powerful magnetospheric
disturbance effect on meteorological processes in the atmosphere, it supposes a
more complicated series of many intermediates, and is not associated directly
with the energy that arrives into the ionosphere during storms. I discuss the
problem of the effect of the solar wind electric field sharp increase via the
global electric circuit during magnetospheric disturbances on the cloud layer
formation.
",0,1,0,0,0,0
16821,16822,Fraternal Dropout,"  Recurrent neural networks (RNNs) are important class of architectures among
neural networks useful for language modeling and sequential prediction.
However, optimizing RNNs is known to be harder compared to feed-forward neural
networks. A number of techniques have been proposed in literature to address
this problem. In this paper we propose a simple technique called fraternal
dropout that takes advantage of dropout to achieve this goal. Specifically, we
propose to train two identical copies of an RNN (that share parameters) with
different dropout masks while minimizing the difference between their
(pre-softmax) predictions. In this way our regularization encourages the
representations of RNNs to be invariant to dropout mask, thus being robust. We
show that our regularization term is upper bounded by the expectation-linear
dropout objective which has been shown to address the gap due to the difference
between the train and inference phases of dropout. We evaluate our model and
achieve state-of-the-art results in sequence modeling tasks on two benchmark
datasets - Penn Treebank and Wikitext-2. We also show that our approach leads
to performance improvement by a significant margin in image captioning
(Microsoft COCO) and semi-supervised (CIFAR-10) tasks.
",1,0,0,1,0,0
130,131,On the trade-off between labels and weights in quantitative bisimulation,"  Reductions for transition systems have been recently introduced as a uniform
and principled method for comparing the expressiveness of system models with
respect to a range of properties, especially bisimulations. In this paper we
study the expressiveness (w.r.t. bisimulations) of models for quantitative
computations such as weighted labelled transition systems (WLTSs), uniform
labelled transition systems (ULTraSs), and state-to-function transition systems
(FuTSs). We prove that there is a trade-off between labels and weights: at one
extreme lays the class of (unlabelled) weighted transition systems where
information is presented using weights only; at the other lays the class of
labelled transition systems (LTSs) where information is shifted on labels.
These categories of systems cannot be further reduced in any significant way
and subsume all the aforementioned models.
",1,0,0,0,0,0
15067,15068,The distribution of symmetry of a naturally reductive nilpotent Lie group,"  We show that the distribution of symmetry of a naturally reductive nilpotent
Lie group coincides with the invariant distribution induced by the set of fixed
vectors of the isotropy. This extends a known result on compact naturally
reductive spaces. We also address the study of the quotient by the foliation of
symmetry.
",0,0,1,0,0,0
19055,19056,A Multi-Modal Approach to Infer Image Affect,"  The group affect or emotion in an image of people can be inferred by
extracting features about both the people in the picture and the overall makeup
of the scene. The state-of-the-art on this problem investigates a combination
of facial features, scene extraction and even audio tonality. This paper
combines three additional modalities, namely, human pose, text-based tagging
and CNN extracted features / predictions. To the best of our knowledge, this is
the first time all of the modalities were extracted using deep neural networks.
We evaluate the performance of our approach against baselines and identify
insights throughout this paper.
",0,0,0,1,0,0
15550,15551,Combinatorial distance geometry in normed spaces,"  We survey problems and results from combinatorial geometry in normed spaces,
concentrating on problems that involve distances. These include various
properties of unit-distance graphs, minimum-distance graphs, diameter graphs,
as well as minimum spanning trees and Steiner minimum trees. In particular, we
discuss translative kissing (or Hadwiger) numbers, equilateral sets, and the
Borsuk problem in normed spaces. We show how to use the angular measure of
Peter Brass to prove various statements about Hadwiger and blocking numbers of
convex bodies in the plane, including some new results. We also include some
new results on thin cones and their application to distinct distances and other
combinatorial problems for normed spaces.
",0,0,1,0,0,0
11991,11992,Magnetic Correlations in the Two-dimensional Repulsive Fermi Hubbard Model,"  The repulsive Fermi Hubbard model on the square lattice has a rich phase
diagram near half-filling (corresponding to the particle density per lattice
site $n=1$): for $n=1$ the ground state is an antiferromagnetic insulator, at
$0.6 < n \lesssim 0.8$, it is a $d_{x^2-y^2}$-wave superfluid (at least for
moderately strong interactions $U \lesssim 4t$ in terms of the hopping $t$),
and the region $1-n \ll 1$ is most likely subject to phase separation. Much of
this physics is preempted at finite temperatures and to an extent driven by
strong magnetic fluctuations, their quantitative characteristics and how they
change with the doping level being much less understood. Experiments on
ultra-cold atoms have recently gained access to this interesting fluctuation
regime, which is now under extensive investigation. In this work we employ a
self-consistent skeleton diagrammatic approach to quantify the characteristic
temperature scale $T_{M}(n)$ for the onset of magnetic fluctuations with a
large correlation length and identify their nature. Our results suggest that
the strongest fluctuations---and hence highest $T_{M}$ and easiest experimental
access to this regime---are observed at $U/t \approx 4-6$.
",0,1,0,0,0,0
1485,1486,Exciting Nucleons in Compton Scattering and Hydrogen-Like Atoms,"  This PhD thesis is devoted to the low-energy structure of the nucleon (proton
and neutron) as seen through electromagnetic probes, e.g., electron and Compton
scattering. The research presented here is based primarily on dispersion theory
and chiral effective-field theory. The main motivation is the recent proton
radius puzzle, which is the discrepancy between the classic proton charge
radius determinations (based on electron-proton scattering and normal hydrogen
spectroscopy) and the highly precise extraction based on first muonic-hydrogen
experiments by the CREMA Collaboration. The precision of muonic-hydrogen
experiments is presently limited by the knowledge of proton structure effects
beyond the charge radius. A major part of this thesis is devoted to calculating
these effects using everything we know about the nucleon electromagnetic
structure from both theory and experiment.
The thesis consists of eight chapters. The first and last are, respectively,
the introduction and conclusion. The remainder of this thesis can roughly be
divided into the following three topics: finite-size effects in hydrogen-like
atoms, real and virtual Compton scattering, and two-photon-exchange effects.
",0,1,0,0,0,0
3082,3083,Adaptive Noise Cancellation Using Deep Cerebellar Model Articulation Controller,"  This paper proposes a deep cerebellar model articulation controller (DCMAC)
for adaptive noise cancellation (ANC). We expand upon the conventional CMAC by
stacking sin-gle-layer CMAC models into multiple layers to form a DCMAC model
and derive a modified backpropagation training algorithm to learn the DCMAC
parameters. Com-pared with conventional CMAC, the DCMAC can characterize
nonlinear transformations more effectively because of its deep structure.
Experimental results confirm that the pro-posed DCMAC model outperforms the
CMAC in terms of residual noise in an ANC task, showing that DCMAC provides
enhanced modeling capability based on channel characteristics.
",1,0,0,0,0,0
20627,20628,Lectures on the mean values of functionals -- An elementary introduction to infinite-dimensional probability,"  This is an elementary introduction to infinite-dimensional probability. In
the lectures, we compute the exact mean values of some functionals on C[0,1]
and L[0,1] by considering these functionals as infinite-dimensional random
variables. The results show that there exist the complete concentration of
measure phenomenon for these mean values since the variances are all zeroes.
",0,1,1,1,0,0
10227,10228,Condensates in double-well potential with synthetic gauge potentials and vortex seeding,"  We demonstrate an enhancement in the vortex generation when artificial gauge
potential is introduced to condensates confined in a double well potential.
This is due to the lower energy required to create a vortex in the low
condensate density region within the barrier. Furthermore, we study the
transport of vortices between the two wells, and show that the traverse time
for vortices is longer for the lower height of the well. We also show that the
critical value of synthetic magnetic field to inject vortices into the bulk of
the condensate is lower in the double-well potential compared to the harmonic
confining potential.
",0,1,0,0,0,0
9577,9578,The maximum number of cycles in a graph with fixed number of edges,"  The main topic considered is maximizing the number of cycles in a graph with
given number of edges. In 2009, Király conjectured that there is constant $c$
such that any graph with $m$ edges has at most $(1.4)^m$ cycles. In this paper,
it is shown that for sufficiently large $m$, a graph with $m$ edges has at most
$(1.443)^m$ cycles. For sufficiently large $m$, examples of a graph with $m$
edges and $(1.37)^m$ cycles are presented. For a graph with given number of
vertices and edges an upper bound on the maximal number of cycles is given.
Also, exponentially tight bounds are proved for the maximum number of cycles in
a multigraph with given number of edges, as well as in a multigraph with given
number of vertices and edges.
",0,0,1,0,0,0
20349,20350,Sketching Linear Classifiers over Data Streams,"  We introduce a new sub-linear space sketch---the Weight-Median Sketch---for
learning compressed linear classifiers over data streams while supporting the
efficient recovery of large-magnitude weights in the model. This enables
memory-limited execution of several statistical analyses over streams,
including online feature selection, streaming data explanation, relative
deltoid detection, and streaming estimation of pointwise mutual information.
Unlike related sketches that capture the most frequently-occurring features (or
items) in a data stream, the Weight-Median Sketch captures the features that
are most discriminative of one stream (or class) compared to another. The
Weight-Median Sketch adopts the core data structure used in the Count-Sketch,
but, instead of sketching counts, it captures sketched gradient updates to the
model parameters. We provide a theoretical analysis that establishes recovery
guarantees for batch and online learning, and demonstrate empirical
improvements in memory-accuracy trade-offs over alternative memory-budgeted
methods, including count-based sketches and feature hashing.
",1,0,0,1,0,0
18315,18316,Memristor equations: incomplete physics and undefined passivity/activity,"  In his seminal paper, Chua presented a fundamental physical claim by
introducing the memristor, ""The missing circuit element"". The memristor
equations were originally supposed to represent a passive circuit element
because, with active circuitry, arbitrary elements can be realized without
limitations. Therefore, if the memristor equations do not guarantee that the
circuit element can be realized by a passive system, the fundamental physics
claim about the memristor as ""missing circuit element"" loses all its weight.
The question of passivity/activity belongs to physics thus we incorporate
thermodynamics into the study of this problem. We show that the memristor
equations are physically incomplete regarding the problem of
passivity/activity. As a consequence, the claim that the present memristor
functions describe a passive device lead to unphysical results, such as
violating the Second Law of thermodynamics, in infinitely large number of
cases. The seminal memristor equations cannot introduce a new physical circuit
element without making the model more physical such as providing the
Fluctuation Dissipation Theory of memristors.
",1,0,0,0,0,0
15885,15886,Bridging Static and Dynamic Program Analysis using Fuzzy Logic,"  Static program analysis is used to summarize properties over all dynamic
executions. In a unifying approach based on 3-valued logic properties are
either assigned a definite value or unknown. But in summarizing a set of
executions, a property is more accurately represented as being biased towards
true, or towards false. Compilers use program analysis to determine benefit of
an optimization. Since benefit (e.g., performance) is justified based on the
common case understanding bias is essential in guiding the compiler.
Furthermore, successful optimization also relies on understanding the quality
of the information, i.e. the plausibility of the bias. If the quality of the
static information is too low to form a decision we would like a mechanism that
improves dynamically.
We consider the problem of building such a reasoning framework and present
the fuzzy data-flow analysis. Our approach generalize previous work that use
3-valued logic. We derive fuzzy extensions of data-flow analyses used by the
lazy code motion optimization and unveil opportunities previous work would not
detect due to limited expressiveness. Furthermore we show how the results of
our analysis can be used in an adaptive classifier that improve as the
application executes.
",1,0,0,0,0,0
628,629,"Contributed Discussion to Uncertainty Quantification for the Horseshoe by Stéphanie van der Pas, Botond Szabó and Aad van der Vaart","  We begin by introducing the main ideas of the paper under discussion. We
discuss some interesting issues regarding adaptive component-wise credible
intervals. We then briefly touch upon the concepts of self-similarity and
excessive bias restriction. This is then followed by some comments on the
extensive simulation study carried out in the paper.
",0,0,1,1,0,0
585,586,Triplet Network with Attention for Speaker Diarization,"  In automatic speech processing systems, speaker diarization is a crucial
front-end component to separate segments from different speakers. Inspired by
the recent success of deep neural networks (DNNs) in semantic inferencing,
triplet loss-based architectures have been successfully used for this problem.
However, existing work utilizes conventional i-vectors as the input
representation and builds simple fully connected networks for metric learning,
thus not fully leveraging the modeling power of DNN architectures. This paper
investigates the importance of learning effective representations from the
sequences directly in metric learning pipelines for speaker diarization. More
specifically, we propose to employ attention models to learn embeddings and the
metric jointly in an end-to-end fashion. Experiments are conducted on the
CALLHOME conversational speech corpus. The diarization results demonstrate
that, besides providing a unified model, the proposed approach achieves
improved performance when compared against existing approaches.
",0,0,0,1,0,0
9602,9603,Proceedings 14th International Workshop on the ACL2 Theorem Prover and its Applications,"  This volume contains the proceedings of the Fourteenth International Workshop
on the ACL2 Theorem Prover and Its Applications, ACL2 2017, a two-day workshop
held in Austin, Texas, USA, on May 22-23, 2017. ACL2 workshops occur at
approximately 18-month intervals, and they provide a technical forum for
researchers to present and discuss improvements and extensions to the theorem
prover, comparisons of ACL2 with other systems, and applications of ACL2 in
formal verification.
ACL2 is a state-of-the-art automated reasoning system that has been
successfully applied in academia, government, and industry for specification
and verification of computing systems and in teaching computer science courses.
Boyer, Kaufmann, and Moore were awarded the 2005 ACM Software System Award for
their work on ACL2 and the other theorem provers in the Boyer-Moore
theorem-prover family.
The proceedings of ACL2 2017 include the seven technical papers and two
extended abstracts that were presented at the workshop. Each submission
received two or three reviews. The workshop also included three invited talks:
""Using Mechanized Mathematics in an Organization with a Simulation-Based
Mentality"", by Glenn Henry of Centaur Technology, Inc.; ""Formal Verification of
Financial Algorithms, Progress and Prospects"", by Grant Passmore of Aesthetic
Integration; and ""Verifying Oracle's SPARC Processors with ACL2"" by Greg
Grohoski of Oracle. The workshop also included several rump sessions discussing
ongoing research and the use of ACL2 within industry.
",1,0,0,0,0,0
8029,8030,Information sensitivity functions to assess parameter information gain and identifiability of dynamical systems,"  A new class of functions, called the `Information sensitivity functions'
(ISFs), which quantify the information gain about the parameters through the
measurements/observables of a dynamical system are presented. These functions
can be easily computed through classical sensitivity functions alone and are
based on Bayesian and information-theoretic approaches. While marginal
information gain is quantified by decrease in differential entropy,
correlations between arbitrary sets of parameters are assessed through mutual
information. For individual parameters these information gains are also
presented as marginal posterior variances, and, to assess the effect of
correlations, as conditional variances when other parameters are given. The
easy to interpret ISFs can be used to a) identify time-intervals or regions in
dynamical system behaviour where information about the parameters is
concentrated; b) assess the effect of measurement noise on the information gain
for the parameters; c) assess whether sufficient information in an experimental
protocol (input, measurements, and their frequency) is available to identify
the parameters; d) assess correlation in the posterior distribution of the
parameters to identify the sets of parameters that are likely to be
indistinguishable; and e) assess identifiability problems for particular sets
of parameters.
",0,0,0,1,0,0
20023,20024,An alternative approach for compatibility of two discrete conditional distributions,"  Conditional specification of distributions is a developing area with
increasing applications. In the finite discrete case, a variety of compatible
conditions can be derived. In this paper, we propose an alternative approach to
study the compatibility of two conditional probability distributions under the
finite discrete setup. A technique based on rank-based criterion is shown to be
particularly convenient for identifying compatible distributions corresponding
to complete conditional specification including the case with zeros.The
proposed methods are illustrated with several examples.
",0,0,1,1,0,0
15643,15644,Fiber plucking by molecular motors yields large emergent contractility in stiff biopolymer networks,"  The mechanical properties of the cell depend crucially on the tension of its
cytoskeleton, a biopolymer network that is put under stress by active motor
proteins. While the fibrous nature of the network is known to strongly affect
the transmission of these forces to the cellular scale, our understanding of
this process remains incomplete. Here we investigate the transmission of forces
through the network at the individual filament level, and show that active
forces can be geometrically amplified as a transverse motor-generated force
force ""plucks"" the fiber and induces a nonlinear tension. In stiff and densely
connnected networks, this tension results in large network-wide tensile
stresses that far exceed the expectation drawn from a linear elastic theory.
This amplification mechanism competes with a recently characterized
network-level amplification due to fiber buckling, suggesting that that fiber
networks provide several distinct pathways for living systems to amplify their
molecular forces.
",0,0,0,0,1,0
10133,10134,Cellular automata connections,"  It is shown that any two cellular automata (CA) in rule space can be
connected by a continuous path parameterized by a real number $\kappa \in (0,
\infty)$, each point in the path corresponding to a coupled map lattice (CML).
In the limits $\kappa \to 0$ and $\kappa \to \infty$ the CML becomes each of
the two CA entering in the connection. A mean-field, reduced model is obtained
from the connection and allows to gain insight in those parameter regimes at
intermediate $\kappa$ where the dynamics is approximately homogeneous within
each neighborhood.
",0,1,1,0,0,0
20515,20516,Blind source separation of tensor-valued time series,"  The blind source separation model for multivariate time series generally
assumes that the observed series is a linear transformation of an unobserved
series with temporally uncorrelated or independent components. Given the
observations, the objective is to find a linear transformation that recovers
the latent series. Several methods for accomplishing this exist and three
particular ones are the classic SOBI and the recently proposed generalized FOBI
(gFOBI) and generalized JADE (gJADE), each based on the use of joint lagged
moments. In this paper we generalize the methodologies behind these algorithms
for tensor-valued time series. We assume that our data consists of a tensor
observed at each time point and that the observations are linear
transformations of latent tensors we wish to estimate. The tensorial
generalizations are shown to have particularly elegant forms and we show that
each of them is Fisher consistent and orthogonal equivariant. Comparing the new
methods with the original ones in various settings shows that the tensorial
extensions are superior to both their vector-valued counterparts and to two
existing tensorial dimension reduction methods for i.i.d. data. Finally,
applications to fMRI-data and video processing show that the methods are
capable of extracting relevant information from noisy high-dimensional data.
",0,0,1,1,0,0
11495,11496,An Operational Framework for Specifying Memory Models using Instantaneous Instruction Execution,"  There has been great progress recently in formally specifying the memory
model of microprocessors like ARM and POWER. These specifications are, however,
too complicated for reasoning about program behaviors, verifying compilers
etc., because they involve microarchitectural details like the reorder buffer
(ROB), partial and speculative execution, instruction replay on speculation
failure, etc. In this paper we present a new Instantaneous Instruction
Execution (I2E) framework which allows us to specify weak memory models in the
same style as SC and TSO. Each instruction in I2E is executed instantaneously
and in-order such that the state of the processor is always correct. The effect
of instruction reordering is captured by the way data is moved between the
processors and the memory non-deterministically, using three conceptual
devices: invalidation buffers, timestamps and dynamic store buffers. We prove
that I2E models capture the behaviors of modern microarchitectures and
cache-coherent memory systems accurately, thus eliminating the need to think
about microarchitectural details.
",1,0,0,0,0,0
18599,18600,General purpose graphics-processing-unit implementation of cosmological domain wall network evolution,"  Topological defects unavoidably form at symmetry breaking phase transitions
in the early Universe. To probe the parameter space of theoretical models and
set tighter experimental constraints (exploiting the recent advances in
astrophysical observations), one requires more and more demanding simulations,
and therefore more hardware resources and computation time. Improving the speed
and efficiency of existing codes is essential. Here we present a General
Purpose Graphics Processing Unit implementation of the canonical
Press-Ryden-Spergel algorithm for the evolution of cosmological domain wall
networks. This is ported to the Open Computing Language standard, and as a
consequence significant speed-ups are achieved both in 2D and 3D simulations.
",0,1,0,0,0,0
3590,3591,Virtual retraction and Howson's theorem in pro-$p$ groups,"  We show that for every finitely generated closed subgroup $K$ of a
non-solvable Demushkin group $G$, there exists an open subgroup $U$ of $G$
containing $K$, and a continuous homomorphism $\tau \colon U \to K$ satisfying
$\tau(k) = k$ for every $k \in K$. We prove that the intersection of a pair of
finitely generated closed subgroups of a Demushkin group is finitely generated
(giving an explicit bound on the number of generators). Furthermore, we show
that these properties of Demushkin groups are preserved under free pro-$p$
products, and deduce that Howson's theorem holds for the Sylow subgroups of the
absolute Galois group of a number field. Finally, we confirm two conjectures of
Ribes, thus classifying the finitely generated pro-$p$ M. Hall groups.
",0,0,1,0,0,0
11697,11698,Exemplar or Matching: Modeling DCJ Problems with Unequal Content Genome Data,"  The edit distance under the DCJ model can be computed in linear time for
genomes with equal content or with Indels. But it becomes NP-Hard in the
presence of duplications, a problem largely unsolved especially when Indels are
considered. In this paper, we compare two mainstream methods to deal with
duplications and associate them with Indels: one by deletion, namely
DCJ-Indel-Exemplar distance; versus the other by gene matching, namely
DCJ-Indel-Matching distance. We design branch-and-bound algorithms with set of
optimization methods to compute exact distances for both. Furthermore, median
problems are discussed in alignment with both of these distance methods, which
are to find a median genome that minimizes distances between itself and three
given genomes. Lin-Kernighan (LK) heuristic is leveraged and powered up by
sub-graph decomposition and search space reduction technologies to handle
median computation. A wide range of experiments are conducted on synthetic data
sets and real data sets to show pros and cons of these two distance metrics per
se, as well as putting them in the median computation scenario.
",1,0,0,0,0,0
12436,12437,Causal Holography in Application to the Inverse Scattering Problems,"  For a given smooth compact manifold $M$, we introduce an open class $\mathcal
G(M)$ of Riemannian metrics, which we call \emph{metrics of the gradient type}.
For such metrics $g$, the geodesic flow $v^g$ on the spherical tangent bundle
$SM \to M$ admits a Lyapunov function (so the $v^g$-flow is traversing). It
turns out, that metrics of the gradient type are exactly the non-trapping
metrics.
For every $g \in \mathcal G(M)$, the geodesic scattering along the boundary
$\partial M$ can be expressed in terms of the \emph{scattering map} $C_{v^g}:
\partial_1^+(SM) \to \partial_1^-(SM)$. It acts from a domain
$\partial_1^+(SM)$ in the boundary $\partial(SM)$ to the complementary domain
$\partial_1^-(SM)$, both domains being diffeomorphic. We prove that, for a
\emph{boundary generic} metric $g \in \mathcal G(M)$ the map $C_{v^g}$ allows
for a reconstruction of $SM$ and of the geodesic foliation $\mathcal F(v^g)$ on
it, up to a homeomorphism (often a diffeomorphism).
Also, for such $g$, the knowledge of the scattering map $C_{v^g}$ makes it
possible to recover the homology of $M$, the Gromov simplicial semi-norm on it,
and the fundamental group of $M$. Additionally, $C_{v^g}$ allows to reconstruct
the naturally stratified topological type of the space of geodesics on $M$.
",0,0,1,0,0,0
17817,17818,Making 360$^{\circ}$ Video Watchable in 2D: Learning Videography for Click Free Viewing,"  360$^{\circ}$ video requires human viewers to actively control ""where"" to
look while watching the video. Although it provides a more immersive experience
of the visual content, it also introduces additional burden for viewers;
awkward interfaces to navigate the video lead to suboptimal viewing
experiences. Virtual cinematography is an appealing direction to remedy these
problems, but conventional methods are limited to virtual environments or rely
on hand-crafted heuristics. We propose a new algorithm for virtual
cinematography that automatically controls a virtual camera within a
360$^{\circ}$ video. Compared to the state of the art, our algorithm allows
more general camera control, avoids redundant outputs, and extracts its output
videos substantially more efficiently. Experimental results on over 7 hours of
real ""in the wild"" video show that our generalized camera control is crucial
for viewing 360$^{\circ}$ video, while the proposed efficient algorithm is
essential for making the generalized control computationally tractable.
",1,0,0,0,0,0
7849,7850,Virtual Astronaut for Scientific Visualization - A Prototype for Santa Maria Crater on Mars,"  To support scientific visualization of multiple-mission data from Mars, the
Virtual Astronaut (VA) creates an interactive virtual 3D environment built on
the Unity3D Game Engine. A prototype study was conducted based on orbital and
Opportunity Rover data covering Santa Maria Crater in Meridiani Planum on Mars.
The VA at Santa Maria provides dynamic visual representations of the imaging,
compositional, and mineralogical information. The VA lets one navigate through
the scene and provides geomorphic and geologic contexts for the rover
operations. User interactions include in-situ observations visualization,
feature measurement, and an animation control of rover drives. This paper
covers our approach and implementation of the VA system. A brief summary of the
prototype system functions and user feedback is also covered. Based on external
review and comments by the science community, the prototype at Santa Maria has
proven the VA to be an effective tool for virtual geovisual analysis.
",1,1,0,0,0,0
10177,10178,TwiInsight: Discovering Topics and Sentiments from Social Media Datasets,"  Social media platforms contain a great wealth of information which provides
opportunities for us to explore hidden patterns or unknown correlations, and
understand people's satisfaction with what they are discussing. As one
showcase, in this paper, we present a system, TwiInsight which explores the
insight of Twitter data. Different from other Twitter analysis systems,
TwiInsight automatically extracts the popular topics under different categories
(e.g., healthcare, food, technology, sports and transport) discussed in Twitter
via topic modeling and also identifies the correlated topics across different
categories. Additionally, it also discovers the people's opinions on the tweets
and topics via the sentiment analysis. The system also employs an intuitive and
informative visualization to show the uncovered insight. Furthermore, we also
develop and compare six most popular algorithms - three for sentiment analysis
and three for topic modeling.
",1,0,0,0,0,0
8173,8174,Existence of global weak solutions to the kinetic Peterlin model,"  We consider a class of kinetic models for polymeric fluids motivated by the
Peterlin dumbbell theories for dilute polymer solutions with a nonlinear spring
law for an infinitely extensible spring. The polymer molecules are suspended in
an incompressible viscous Newtonian fluid confined to a bounded domain in two
or three space dimensions. The unsteady motion of the solvent is described by
the incompressible Navier-Stokes equations with the elastic extra stress tensor
appearing as a forcing term in the momentum equation. The elastic stress tensor
is defined by the Kramers expression through the probability density function
that satisfies the corresponding Fokker-Planck equation. In this case, a
coefficient depending on the average length of polymer molecules appears in the
latter equation. Following the recent work of Barrett and Süli we prove the
existence of global-in-time weak solutions to the kinetic Peterlin model in two
space dimensions.
",0,0,1,0,0,0
5401,5402,Option Pricing Models Driven by the Space-Time Fractional Diffusion: Series Representation and Applications,"  In this paper, we focus on option pricing models based on space-time
fractional diffusion. We briefly revise recent results which show that the
option price can be represented in the terms of rapidly converging
double-series and apply these results to the data from real markets. We focus
on estimation of model parameters from the market data and estimation of
implied volatility within the space-time fractional option pricing models.
",0,0,0,0,0,1
10220,10221,Convergence and Consistency Analysis for A 3D Invariant-EKF SLAM,"  In this paper, we investigate the convergence and consistency properties of
an Invariant-Extended Kalman Filter (RI-EKF) based Simultaneous Localization
and Mapping (SLAM) algorithm. Basic convergence properties of this algorithm
are proven. These proofs do not require the restrictive assumption that the
Jacobians of the motion and observation models need to be evaluated at the
ground truth. It is also shown that the output of RI-EKF is invariant under any
stochastic rigid body transformation in contrast to $\mathbb{SO}(3)$ based EKF
SLAM algorithm ($\mathbb{SO}(3)$-EKF) that is only invariant under
deterministic rigid body transformation. Implications of these invariance
properties on the consistency of the estimator are also discussed. Monte Carlo
simulation results demonstrate that RI-EKF outperforms $\mathbb{SO}(3)$-EKF,
Robocentric-EKF and the ""First Estimates Jacobian"" EKF, for 3D point feature
based SLAM.
",1,0,0,0,0,0
6402,6403,Efficient Decomposition of High-Rank Tensors,"  Tensors are a natural way to express correlations among many physical
variables, but storing tensors in a computer naively requires memory which
scales exponentially in the rank of the tensor. This is not optimal, as the
required memory is actually set not by the rank but by the mutual information
amongst the variables in question. Representations such as the tensor tree
perform near-optimally when the tree decomposition is chosen to reflect the
correlation structure in question, but making such a choice is non-trivial and
good heuristics remain highly context-specific. In this work I present two new
algorithms for choosing efficient tree decompositions, independent of the
physical context of the tensor. The first is a brute-force algorithm which
produces optimal decompositions up to truncation error but is generally
impractical for high-rank tensors, as the number of possible choices grows
exponentially in rank. The second is a greedy algorithm, and while it is not
optimal it performs extremely well in numerical experiments while having
runtime which makes it practical even for tensors of very high rank.
",1,1,0,0,0,0
6850,6851,Commissioning and performance results of the WFIRST/PISCES integral field spectrograph,"  The Prototype Imaging Spectrograph for Coronagraphic Exoplanet Studies
(PISCES) is a high contrast integral field spectrograph (IFS) whose design was
driven by WFIRST coronagraph instrument requirements. We present commissioning
and operational results using PISCES as a camera on the High Contrast Imaging
Testbed at JPL. PISCES has demonstrated ability to achieve high contrast
spectral retrieval with flight-like data reduction and analysis techniques.
",0,1,0,0,0,0
19776,19777,Depth creates no more spurious local minima,"  We show that for any convex differentiable loss function, a deep linear
network has no spurious local minima as long as it is true for the two layer
case. When applied to the quadratic loss, our result immediately implies the
powerful result in [Kawaguchi 2016] that there is no spurious local minima in
deep linear networks. Further, with the recent work [Zhou and Liang 2018], we
can remove all the assumptions in [Kawaguchi 2016]. Our proof is short and
elementary. It builds on the recent work of [Laurent and von Brecht 2018] and
uses a new rank one perturbation argument.
",1,0,0,1,0,0
19304,19305,Tackling Diversity and Heterogeneity by Vertical Memory Management,"  Existing memory management mechanisms used in commodity computing machines
typically adopt hardware based address interleaving and OS directed random
memory allocation to service generic application requests. These conventional
memory management mechanisms are challenged by contention at multiple memory
levels, a daunting variety of workload behaviors, and an increasingly
complicated memory hierarchy. Our ISCA-41 paper proposes vertical partitioning
to eliminate shared resource contention at multiple levels in the memory
hierarchy. Combined with horizontal memory management policies, our framework
supports a flexible policy space for tackling diverse application needs in
production environment and is suitable for future heterogeneous memory systems.
",1,0,0,0,0,0
7756,7757,Ivanov-Regularised Least-Squares Estimators over Large RKHSs and Their Interpolation Spaces,"  We study kernel least-squares estimation under a norm constraint. This form
of regularisation is known as Ivanov regularisation and it provides better
control of the norm of the estimator than the well-established Tikhonov
regularisation. This choice of regularisation allows us to dispose of the
standard assumption that the reproducing kernel Hilbert space (RKHS) has a
Mercer kernel, which is restrictive as it usually requires compactness of the
covariate set. Instead, we assume only that the RKHS is separable with a
bounded and measurable kernel. We provide rates of convergence for the expected
squared $L^2$ error of our estimator under the weak assumption that the
variance of the response variables is bounded and the unknown regression
function lies in an interpolation space between $L^2$ and the RKHS. We then
obtain faster rates of convergence when the regression function is bounded by
clipping the estimator. In fact, we attain the optimal rate of convergence.
Furthermore, we provide a high-probability bound under the stronger assumption
that the response variables have subgaussian errors and that the regression
function lies in an interpolation space between $L^\infty$ and the RKHS.
Finally, we derive adaptive results for the settings in which the regression
function is bounded.
",0,0,1,1,0,0
11999,12000,Fairly Allocating Contiguous Blocks of Indivisible Items,"  In this paper, we study the classic problem of fairly allocating indivisible
items with the extra feature that the items lie on a line. Our goal is to find
a fair allocation that is contiguous, meaning that the bundle of each agent
forms a contiguous block on the line. While allocations satisfying the
classical fairness notions of proportionality, envy-freeness, and equitability
are not guaranteed to exist even without the contiguity requirement, we show
the existence of contiguous allocations satisfying approximate versions of
these notions that do not degrade as the number of agents or items increases.
We also study the efficiency loss of contiguous allocations due to fairness
constraints.
",1,0,0,0,0,0
15074,15075,CP-decomposition with Tensor Power Method for Convolutional Neural Networks Compression,"  Convolutional Neural Networks (CNNs) has shown a great success in many areas
including complex image classification tasks. However, they need a lot of
memory and computational cost, which hinders them from running in relatively
low-end smart devices such as smart phones. We propose a CNN compression method
based on CP-decomposition and Tensor Power Method. We also propose an iterative
fine tuning, with which we fine-tune the whole network after decomposing each
layer, but before decomposing the next layer. Significant reduction in memory
and computation cost is achieved compared to state-of-the-art previous work
with no more accuracy loss.
",1,0,0,0,0,0
11746,11747,Hysteretic behaviour of metal connectors for hybrid (high- and low-grade mixed species) cross laminated timber,"  Cross-laminated timber (CLT) is a prefabricated solid engineered wood product
made of at least three orthogonally bonded layers of solid-sawn lumber that are
laminated by gluing longitudinal and transverse layers with structural
adhesives to form a solid panel. Previous studies have shown that the CLT
buildings can perform well in seismic loading and are recognized as the
essential role of connector performance in structural design, modelling, and
analysis of CLT buildings. When CLT is composed of high-grade/high-density
layers for the outer lamellas and low-grade/low-density for the core of the
panels, the CLT panels are herein designated as hybrid CLT panels as opposed to
conventional CLT panels that are built using one lumber type for both outer and
core lamellas. This paper presents results of a testing program developed to
estimate the cyclic performance of CLT connectors applied on hybrid CLT layups.
Two connectors are selected, which can be used in wall-to-floor connections.
These are readily available in the North American market. Characterization of
the performance of connectors is done in two perpendicular directions under a
modified CUREE cyclic loading protocol. Depending on the mode of failure, in
some cases, testing results indicate that when the nails or screws penetrate
the low-grade/low-density core lumber, a statistically significant difference
is obtained between hybrid and conventional layups. However, in other cases,
due to damage in the face layer or in the connection, force-displacement
results for conventional and hybrid CLT layups were not statistically
significant.
",0,1,0,0,0,0
15577,15578,Human Perception of Performance,"  Humans are routinely asked to evaluate the performance of other individuals,
separating success from failure and affecting outcomes from science to
education and sports. Yet, in many contexts, the metrics driving the human
evaluation process remain unclear. Here we analyse a massive dataset capturing
players' evaluations by human judges to explore human perception of performance
in soccer, the world's most popular sport. We use machine learning to design an
artificial judge which accurately reproduces human evaluation, allowing us to
demonstrate how human observers are biased towards diverse contextual features.
By investigating the structure of the artificial judge, we uncover the aspects
of the players' behavior which attract the attention of human judges,
demonstrating that human evaluation is based on a noticeability heuristic where
only feature values far from the norm are considered to rate an individual's
performance.
",1,0,0,1,0,0
1651,1652,Switching Isotropic and Directional Exploration with Parameter Space Noise in Deep Reinforcement Learning,"  This paper proposes an exploration method for deep reinforcement learning
based on parameter space noise. Recent studies have experimentally shown that
parameter space noise results in better exploration than the commonly used
action space noise. Previous methods devised a way to update the diagonal
covariance matrix of a noise distribution and did not consider the direction of
the noise vector and its correlation. In addition, fast updates of the noise
distribution are required to facilitate policy learning. We propose a method
that deforms the noise distribution according to the accumulated returns and
the noises that have led to the returns. Moreover, this method switches
isotropic exploration and directional exploration in parameter space with
regard to obtained rewards. We validate our exploration strategy in the OpenAI
Gym continuous environments and modified environments with sparse rewards. The
proposed method achieves results that are competitive with a previous method at
baseline tasks. Moreover, our approach exhibits better performance in sparse
reward environments by exploration with the switching strategy.
",0,0,0,1,0,0
15222,15223,Bit-Vector Model Counting using Statistical Estimation,"  Approximate model counting for bit-vector SMT formulas (generalizing \#SAT)
has many applications such as probabilistic inference and quantitative
information-flow security, but it is computationally difficult. Adding random
parity constraints (XOR streamlining) and then checking satisfiability is an
effective approximation technique, but it requires a prior hypothesis about the
model count to produce useful results. We propose an approach inspired by
statistical estimation to continually refine a probabilistic estimate of the
model count for a formula, so that each XOR-streamlined query yields as much
information as possible. We implement this approach, with an approximate
probability model, as a wrapper around an off-the-shelf SMT solver or SAT
solver. Experimental results show that the implementation is faster than the
most similar previous approaches which used simpler refinement strategies. The
technique also lets us model count formulas over floating-point constraints,
which we demonstrate with an application to a vulnerability in differential
privacy mechanisms.
",1,0,0,0,0,0
387,388,Belyi map for the sporadic group J1,"  We compute the genus 0 Belyi map for the sporadic Janko group J1 of degree
266 and describe the applied method. This yields explicit polynomials having J1
as a Galois group over K(t), [K:Q] = 7.
",0,0,1,0,0,0
14288,14289,End-to-end DNN Based Speaker Recognition Inspired by i-vector and PLDA,"  Recently several end-to-end speaker verification systems based on deep neural
networks (DNNs) have been proposed. These systems have been proven to be
competitive for text-dependent tasks as well as for text-independent tasks with
short utterances. However, for text-independent tasks with longer utterances,
end-to-end systems are still outperformed by standard i-vector + PLDA systems.
In this work, we develop an end-to-end speaker verification system that is
initialized to mimic an i-vector + PLDA baseline. The system is then further
trained in an end-to-end manner but regularized so that it does not deviate too
far from the initial system. In this way we mitigate overfitting which normally
limits the performance of end-to-end systems. The proposed system outperforms
the i-vector + PLDA baseline on both long and short duration utterances.
",1,0,0,0,0,0
10036,10037,Rank modulation codes for DNA storage,"  Synthesis of DNA molecules offers unprecedented advances in storage
technology. Yet, the microscopic world in which these molecules reside induces
error patterns that are fundamentally different from their digital
counterparts. Hence, to maintain reliability in reading and writing, new coding
schemes must be developed. In a reading technique called shotgun sequencing, a
long DNA string is read in a sliding window fashion, and a profile vector is
produced. It was recently suggested by Kiah et al. that such a vector can
represent the permutation which is induced by its entries, and hence a
rank-modulation scheme arises. Although this interpretation suggests high error
tolerance, it is unclear which permutations are feasible, and how to produce a
DNA string whose profile vector induces a given permutation. In this paper, by
observing some necessary conditions, an upper bound for the number of feasible
permutations is given. Further, a technique for deciding the feasibility of a
permutation is devised. By using insights from this technique, an algorithm for
producing a considerable number of feasible permutations is given, which
applies to any alphabet size and any window length.
",1,0,0,0,0,0
6379,6380,Parabolic equations with divergence-free drift in space $L_{t}^{l}L_{x}^{q}$,"  In this paper we study the fundamental solution $\varGamma(t,x;\tau,\xi)$ of
the parabolic operator $L_{t}=\partial_{t}-\Delta+b(t,x)\cdot\nabla$, where for
every $t$, $b(t,\cdot)$ is a divergence-free vector field, and we consider the
case that $b$ belongs to the Lebesgue space
$L^{l}\left(0,T;L^{q}\left(\mathbb{R}^{n}\right)\right)$. The regularity of
weak solutions to the parabolic equation $L_{t}u=0$ depends critically on the
value of the parabolic exponent $\gamma=\frac{2}{l}+\frac{n}{q}$. Without the
divergence-free condition on $b$, the regularity of weak solutions has been
established when $\gamma\leq1$, and the heat kernel estimate has been obtained
as well, except for the case that $l=\infty,q=n$. The regularity of weak
solutions was deemed not true for the critical case
$L^{\infty}\left(0,T;L^{n}\left(\mathbb{R}^{n}\right)\right)$ for a general
$b$, while it is true for the divergence-free case, and a written proof can be
deduced from the results in [Semenov, 2006]. One of the results obtained in the
present paper establishes the Aronson type estimate for critical and
supercritical cases and for vector fields $b$ which are divergence-free. We
will prove the best possible lower and upper bounds for the fundamental
solution one can derive under the current approach. The significance of the
divergence-free condition enters the study of parabolic equations rather
recently, mainly due to the discovery of the compensated compactness. The
interest for the study of such parabolic equations comes from its connections
with Leray's weak solutions of the Navier-Stokes equations and the Taylor
diffusion associated with a vector field where the heat operator $L_{t}$
appears naturally.
",0,0,1,0,0,0
8737,8738,Concept of multiple-cell cavity for axion dark matter search,"  In cavity-based axion dark matter search experiments exploring high mass
regions, multiple-cavity design is considered to increase the detection volume
within a given magnet bore. We introduce a new idea, referred to as
multiple-cell cavity, which provides various benefits including a larger
detection volume, simpler experimental setup, and easier phase-matching
mechanism. We present the characteristics of this concept and demonstrate the
experimental feasibility with an example of a double-cell cavity.
",0,1,0,0,0,0
448,449,Experimental Evidence on a Refined Conjecture of the BSD type,"  Let $E/\mathbb{Q}$ be an elliptic curve of level $N$ and rank equal to $1$.
Let $p$ be a prime of ordinary reduction. We experimentally study conjecture
$4$ of B. Mazur and J. Tate in his article ""Refined Conjectures of the Birch
and Swinnerton-Dyer Type"". We report the computational evidence.
",0,0,1,0,0,0
16545,16546,Considering Multiple Uncertainties in Stochastic Security-Constrained Unit Commitment Using Point Estimation Method,"  Security-Constrained Unit Commitment (SCUC) is one of the most significant
problems in secure and optimal operation of modern electricity markets. New
sources of uncertainties such as wind speed volatility and price-sensitive
loads impose additional challenges to this large-scale problem. This paper
proposes a new Stochastic SCUC using point estimation method to model the power
system uncertainties more efficiently. Conventional scenario-based Stochastic
SCUC approaches consider the Mont Carlo method; which presents additional
computational burdens to this large-scale problem. In this paper we use point
estimation instead of scenario generating to detract computational burdens of
the problem. The proposed approach is implemented on a six-bus system and on a
modified IEEE 118-bus system with 94 uncertain variables. The efficacy of
proposed algorithm is confirmed, especially in the last case with notable
reduction in computational burden without considerable loss of precision.
",0,1,1,0,0,0
6399,6400,Self-consistent calculation of the flux-flow conductivity in diffusive superconductors,"  In the framework of Keldysh-Usadel kinetic theory, we study the temperature
dependence of flux-flow conductivity (FFC) in diffusive superconductors. By
using self-consistent vortex solutions we find the exact values of
dimensionless parameters that determine the diffusion-controlled FFC both in
the limit of the low temperatures and close to the critical one. Taking into
account the electron-phonon scattering we study the transition between
flux-flow regimes controlled either by the diffusion or the inelastic
relaxation of non-equilibrium quasiparticles. We demonstrate that the inelastic
electron-phonon relaxation leads to the strong suppression of FFC as compared
to the previous estimates making it possible to obtain the numerical agreement
with experimental results.
",0,1,0,0,0,0
12998,12999,Magnetic states of MnP: muon-spin rotation studies,"  Muon-spin rotation data collected at ambient pressure ($p$) and at $p=2.42$
GPa in MnP were analyzed to check their consistency with various low- and
high-pressure magnetic structures reported in the literature. Our analysis
confirms that in MnP the low-temperature and low-pressure helimagnetic phase is
characterised by an increased value of the average magnetic moment compared to
the high-temperature ferromagnetic phase. An elliptical double-helical
structure with a propagation vector ${\bf Q}=(0,0,0.117)$, an $a-$axis moment
elongated by approximately 18% and an additional tilt of the rotation plane
towards $c-$direction by $\simeq 4-8^{\rm o}$ leads to a good agreement between
the theory and the experiment. The analysis of the high-pressure $\mu$SR data
reveals that the new magnetic order appearing for pressures exceeding $1.5$ GPa
can not be described by keeping the propagation vector ${\bf Q} \parallel c$.
Even the extreme case -- decoupling the double-helical structure into four
individual helices -- remains inconsistent with the experiment. It is shown
that the high-pressure magnetic phase which is a precursor of superconductivity
is an incommensurate helical state with ${\bf Q} \parallel b$.
",0,1,0,0,0,0
6774,6775,Reactive User Behavior and Mobility Models,"  In this paper, we present a set of simulation models to more realistically
mimic the behaviour of users reading messages. We propose a User Behaviour
Model, where a simulated user reacts to a message by a flexible set of possible
reactions (e.g. ignore, read, like, save, etc.) and a mobility-based reaction
(visit a place, run away from danger, etc.). We describe our models and their
implementation in OMNeT++. We strongly believe that these models will
significantly contribute to the state of the art of simulating realistically
opportunistic networks.
",1,0,0,0,0,0
3137,3138,On reproduction of On the regularization of Wasserstein GANs,"  This report has several purposes. First, our report is written to investigate
the reproducibility of the submitted paper On the regularization of Wasserstein
GANs (2018). Second, among the experiments performed in the submitted paper,
five aspects were emphasized and reproduced: learning speed, stability,
robustness against hyperparameter, estimating the Wasserstein distance, and
various sampling method. Finally, we identify which parts of the contribution
can be reproduced, and at what cost in terms of resources. All source code for
reproduction is open to the public.
",1,0,0,1,0,0
9824,9825,DeepDiff: Deep-learning for predicting Differential gene expression from histone modifications,"  Computational methods that predict differential gene expression from histone
modification signals are highly desirable for understanding how histone
modifications control the functional heterogeneity of cells through influencing
differential gene regulation. Recent studies either failed to capture
combinatorial effects on differential prediction or primarily only focused on
cell type-specific analysis. In this paper, we develop a novel attention-based
deep learning architecture, DeepDiff, that provides a unified and end-to-end
solution to model and to interpret how dependencies among histone modifications
control the differential patterns of gene regulation. DeepDiff uses a hierarchy
of multiple Long short-term memory (LSTM) modules to encode the spatial
structure of input signals and to model how various histone modifications
cooperate automatically. We introduce and train two levels of attention jointly
with the target prediction, enabling DeepDiff to attend differentially to
relevant modifications and to locate important genome positions for each
modification. Additionally, DeepDiff introduces a novel deep-learning based
multi-task formulation to use the cell-type-specific gene expression
predictions as auxiliary tasks, encouraging richer feature embeddings in our
primary task of differential expression prediction. Using data from Roadmap
Epigenomics Project (REMC) for ten different pairs of cell types, we show that
DeepDiff significantly outperforms the state-of-the-art baselines for
differential gene expression prediction. The learned attention weights are
validated by observations from previous studies about how epigenetic mechanisms
connect to differential gene expression. Codes and results are available at
\url{deepchrome.org}
",0,0,0,1,0,0
12905,12906,Algorithms and Architecture for Real-time Recommendations at News UK,"  Recommendation systems are recognised as being hugely important in industry,
and the area is now well understood. At News UK, there is a requirement to be
able to quickly generate recommendations for users on news items as they are
published. However, little has been published about systems that can generate
recommendations in response to changes in recommendable items and user
behaviour in a very short space of time. In this paper we describe a new
algorithm for updating collaborative filtering models incrementally, and
demonstrate its effectiveness on clickstream data from The Times. We also
describe the architecture that allows recommendations to be generated on the
fly, and how we have made each component scalable. The system is currently
being used in production at News UK.
",1,0,0,0,0,0
18721,18722,"Study and Observation of the Variation of Accuracies of KNN, SVM, LMNN, ENN Algorithms on Eleven Different Datasets from UCI Machine Learning Repository","  Machine learning qualifies computers to assimilate with data, without being
solely programmed [1, 2]. Machine learning can be classified as supervised and
unsupervised learning. In supervised learning, computers learn an objective
that portrays an input to an output hinged on training input-output pairs [3].
Most efficient and widely used supervised learning algorithms are K-Nearest
Neighbors (KNN), Support Vector Machine (SVM), Large Margin Nearest Neighbor
(LMNN), and Extended Nearest Neighbor (ENN). The main contribution of this
paper is to implement these elegant learning algorithms on eleven different
datasets from the UCI machine learning repository to observe the variation of
accuracies for each of the algorithms on all datasets. Analyzing the accuracy
of the algorithms will give us a brief idea about the relationship of the
machine learning algorithms and the data dimensionality. All the algorithms are
developed in Matlab. Upon such accuracy observation, the comparison can be
built among KNN, SVM, LMNN, and ENN regarding their performances on each
dataset.
",0,0,0,1,0,0
5664,5665,Revisiting Simple Neural Networks for Learning Representations of Knowledge Graphs,"  We address the problem of learning vector representations for entities and
relations in Knowledge Graphs (KGs) for Knowledge Base Completion (KBC). This
problem has received significant attention in the past few years and multiple
methods have been proposed. Most of the existing methods in the literature use
a predefined characteristic scoring function for evaluating the correctness of
KG triples. These scoring functions distinguish correct triples (high score)
from incorrect ones (low score). However, their performance vary across
different datasets. In this work, we demonstrate that a simple neural network
based score function can consistently achieve near start-of-the-art performance
on multiple datasets. We also quantitatively demonstrate biases in standard
benchmark datasets, and highlight the need to perform evaluation spanning
various datasets.
",1,0,0,1,0,0
18250,18251,$c$-vectors of 2-Calabi--Yau categories and Borel subalgebras of ${\mathfrak{sl}}_{\infty}$,"  We develop a general framework for $c$-vectors of 2-Calabi--Yau categories,
which deals with cluster tilting subcategories that are not reachable from each
other and contain infinitely many indecomposable objects. It does not rely on
iterative sequences of mutations.
We prove a categorical (infinite-rank) generalization of the
Nakanishi--Zelevinsky duality for $c$-vectors and establish two formulae for
the effective computation of $c$-vectors -- one in terms of indices and the
other in terms of dimension vectors for cluster tilted algebras.
In this framework, we construct a correspondence between the $c$-vectors of
the cluster categories ${\mathscr{C}}(A_{\infty})$ of type $A_{\infty}$ due to
Igusa--Todorov and the roots of the Borel subalgebras of
${\mathfrak{sl}}_{\infty}$. Contrary to the finite dimensional case, the Borel
subalgebras of ${\mathfrak{sl}}_{\infty}$ are not conjugate to each other. On
the categorical side, the cluster tilting subcategories of
${\mathscr{C}}(A_{\infty})$ exhibit different homological properties. The
correspondence builds a bridge between the two classes of objects.
",0,0,1,0,0,0
3594,3595,Boolean dimension and tree-width,"  The dimension is a key measure of complexity of partially ordered sets. Small
dimension allows succinct encoding. Indeed if $P$ has dimension $d$, then to
know whether $x \leq y$ in $P$ it is enough to check whether $x\leq y$ in each
of the $d$ linear extensions of a witnessing realizer. Focusing on the encoding
aspect Nešetřil and Pudlák defined a more expressive version of
dimension. A poset $P$ has boolean dimension at most $d$ if it is possible to
decide whether $x \leq y$ in $P$ by looking at the relative position of $x$ and
$y$ in only $d$ permutations of the elements of $P$. We prove that posets with
cover graphs of bounded tree-width have bounded boolean dimension. This stays
in contrast with the fact that there are posets with cover graphs of tree-width
three and arbitrarily large dimension. This result might be a step towards a
resolution of the long-standing open problem: Do planar posets have bounded
boolean dimension?
",1,0,0,0,0,0
15080,15081,"Inverse scattering transform for the nonlocal reverse space-time Sine-Gordon, Sinh-Gordon and nonlinear Schrödinger equations with nonzero boundary conditions","  The reverse space-time (RST) Sine-Gordon, Sinh-Gordon and nonlinear
Schrödinger equations were recently introduced and shown to be integrable
infinite-dimensional dynamical systems. The inverse scattering transform (IST)
for rapidly decaying data was also constructed. In this paper, IST for these
equations with nonzero boundary conditions (NZBCs) at infinity is presented.
The NZBC problem is more complicated due to the associated branching structure
of the associated linear eigenfunctions. With constant amplitude at infinity,
four cases are analyzed; they correspond to two different signs of nonlinearity
and two different values of the phase at infinity. Special soliton solutions
are discussed and explicit 1-soliton and 2-soliton solutions are found. In
terms of IST, the difference between the RST Sine-Gordon/Sinh-Gordon equations
and the RST NLS equation is the time dependence of the scattering data.
Spatially dependent boundary conditions are also briefly considered.
",0,1,0,0,0,0
6478,6479,Entropy Formula for Random $\mathbb{Z}^k$-actions,"  In this paper, entropies, including measure-theoretic entropy and topological
entropy, are considered for random $\mathbb{Z}^k$-actions which are generated
by random compositions of the generators of $\mathbb{Z}^k$-actions. Applying
Pesin's theory for commutative diffeomorphisms we obtain a measure-theoretic
entropy formula of $C^{2}$ random $\mathbb{Z}^k$-actions via the Lyapunov
spectra of the generators. Some formulas and bounds of topological entropy for
certain random $\mathbb{Z}^k$(or $\mathbb{Z}_+^k$ )-actions generated by more
general maps, such as Lipschitz maps, continuous maps on finite graphs and
$C^{1}$ expanding maps, are also obtained. Moreover, as an application, we give
a formula of Friedland's entropy for certain $C^{2}$ $\mathbb{Z}^k$-actions.
",0,0,1,0,0,0
17983,17984,Introducing SPAIN (SParse Audion INpainter),"  A novel sparsity-based algorithm for audio inpainting is proposed by
translating the SPADE algorithm by Kitić et. al.---the state-of-the-art for
audio declipping---into the task of audio inpainting. SPAIN (SParse Audio
INpainter) comes in synthesis and analysis variants. Experiments show that both
A-SPAIN and S-SPAIN outperform other sparsity-based inpainting algorithms and
that A-SPAIN performs on a par with the state-of-the-art method based on linear
prediction.
",1,0,0,0,0,0
2751,2752,Impact of surface functionalisation on the quantum coherence of nitrogen vacancy centres in nanodiamond,"  Nanoscale quantum probes such as the nitrogen-vacancy centre in diamond have
demonstrated remarkable sensing capabilities over the past decade as control
over the fabrication and manipulation of these systems has evolved. However, as
the size of these nanoscale quantum probes is reduced, the surface termination
of the host material begins to play a prominent role as a source of magnetic
and electric field noise. In this work, we show that borane-reduced nanodiamond
surfaces can on average double the spin relaxation time of individual
nitrogen-vacancy centres in nanodiamonds when compared to the thermally
oxidised surfaces. Using a combination of infra-red and x-ray absorption
spectroscopy techniques, we correlate the changes in quantum relaxation rates
with the conversion of sp2 carbon to C-O and C-H bonds on the diamond surface.
These findings implicate double-bonded carbon species as a dominant source of
spin noise for near surface NV centres and show that through tailored
engineering of the surface, we can improve the quantum properties and magnetic
sensitivity of these nanoscale probes.
",0,1,0,0,0,0
13143,13144,The unpolarized Shafarevich Conjecture for K3 Surfaces,"  We prove the unpolarized Shafarevich conjecture for K3 surfaces: the set of
isomorphism classes of K3 surfaces over a fixed number field with good
reduction away from a fixed and finite set of places is finite. Our proof is
based on the theorems of Faltings and André, as well as the Kuga-Satake
construction.
",0,0,1,0,0,0
19702,19703,On One Property of Tikhonov Regularization Algorithm,"  For linear inverse problem with Gaussian random noise we show that Tikhonov
regularization algorithm is minimax in the class of linear estimators and is
asymptotically minimax in the sense of sharp asymptotic in the class of all
estimators. The results are valid if some a priori information on a Fourier
coefficients of solution is provided. For trigonometric basis this a priori
information implies that the solution belongs to a ball in Besov space
$B^r_{2\infty}$.
",0,0,1,1,0,0
16357,16358,The search for neutron-antineutron oscillations at the Sudbury Neutrino Observatory,"  Tests on $B-L$ symmetry breaking models are important probes to search for
new physics. One proposed model with $\Delta(B-L)=2$ involves the oscillations
of a neutron to an antineutron. In this paper a new limit on this process is
derived for the data acquired from all three operational phases of the Sudbury
Neutrino Observatory experiment. The search was concentrated in oscillations
occurring within the deuteron, and 23 events are observed against a background
expectation of 30.5 events. These translate to a lower limit on the nuclear
lifetime of $1.48\times 10^{31}$ years at 90% confidence level (CL) when no
restriction is placed on the signal likelihood space (unbounded).
Alternatively, a lower limit on the nuclear lifetime was found to be
$1.18\times 10^{31}$ years at 90% CL when the signal was forced into a positive
likelihood space (bounded). Values for the free oscillation time derived from
various models are also provided in this article. This is the first search for
neutron-antineutron oscillation with the deuteron as a target.
",0,1,0,0,0,0
10102,10103,Constraints from Dust Mass and Mass Accretion Rate Measurements on Angular Momentum Transport in Protoplanetary Disks,"  We investigate the relation between disk mass and mass accretion rate to
constrain the mechanism of angular momentum transport in protoplanetary disks.
Dust mass and mass accretion rate in Chamaeleon I are correlated with a slope
close to linear, similar to the one recently identified in Lupus. We
investigate the effect of stellar mass and find that the intrinsic scatter
around the best-fit Mdust-Mstar and Macc-Mstar relations is uncorrelated. Disks
with a constant alpha viscosity can fit the observed relations between dust
mass, mass accretion rate, and stellar mass, but over-predict the strength of
the correlation between disk mass and mass accretion rate when using standard
initial conditions. We find two possible solutions. 1) The observed scatter in
Mdust and Macc is not primoridal, but arises from additional physical processes
or uncertainties in estimating the disk gas mass. Most likely grain growth and
radial drift affect the observable dust mass, while variability on large time
scales affects the mass accretion rates. 2) The observed scatter is primordial,
but disks have not evolved substantially at the age of Lupus and Chamaeleon I
due to a low viscosity or a large initial disk radius. More accurate estimates
of the disk mass and gas disk sizes in a large sample of protoplanetary disks,
either through direct observations of the gas or spatially resolved
multi-wavelength observations of the dust with ALMA, are needed to discriminate
between both scenarios or to constrain alternative angular momentum transport
mechanisms such as MHD disk winds.
",0,1,0,0,0,0
12126,12127,The ALF (Algorithms for Lattice Fermions) project release 1.0. Documentation for the auxiliary field quantum Monte Carlo code,"  The Algorithms for Lattice Fermions package provides a general code for the
finite temperature auxiliary field quantum Monte Carlo algorithm. The code is
engineered to be able to simulate any model that can be written in terms of
sums of single-body operators, of squares of single-body operators and
single-body operators coupled to an Ising field with given dynamics. We provide
predefined types that allow the user to specify the model, the Bravais lattice
as well as equal time and time displaced observables. The code supports an MPI
implementation. Examples such as the Hubbard model on the honeycomb lattice and
the Hubbard model on the square lattice coupled to a transverse Ising field are
provided and discussed in the documentation. We furthermore discuss how to use
the package to implement the Kondo lattice model and the
$SU(N)$-Hubbard-Heisenberg model. One can download the code from our Git
instance at this https URL and sign in to file issues.
",0,1,0,0,0,0
4179,4180,Searching for Biophysically Realistic Parameters for Dynamic Neuron Models by Genetic Algorithms from Calcium Imaging Recording,"  Individual Neurons in the nervous systems exploit various dynamics. To
capture these dynamics for single neurons, we tune the parameters of an
electrophysiological model of nerve cells, to fit experimental data obtained by
calcium imaging. A search for the biophysical parameters of this model is
performed by means of a genetic algorithm, where the model neuron is exposed to
a predefined input current representing overall inputs from other parts of the
nervous system. The algorithm is then constrained for keeping the ion-channel
currents within reasonable ranges, while producing the best fit to a calcium
imaging time series of the AVA interneuron, from the brain of the soil-worm, C.
elegans. Our settings enable us to project a set of biophysical parameters to
the the neuron kinetics observed in neuronal imaging.
",1,0,0,0,0,0
16029,16030,Very metal-poor stars observed by the RAVE survey,"  We present a novel analysis of the metal-poor star sample in the complete
Radial Velocity Experiment (RAVE) Data Release 5 catalog with the goal of
identifying and characterizing all very metal-poor stars observed by the
survey. Using a three-stage method, we first identified the candidate stars
using only their spectra as input information. We employed an algorithm called
t-SNE to construct a low-dimensional projection of the spectrum space and
isolate the region containing metal-poor stars. Following this step, we
measured the equivalent widths of the near-infrared CaII triplet lines with a
method based on flexible Gaussian processes to model the correlated noise
present in the spectra. In the last step, we constructed a calibration relation
that converts the measured equivalent widths and the color information coming
from the 2MASS and WISE surveys into metallicity and temperature estimates. We
identified 877 stars with at least a 50% probability of being very metal-poor
$(\rm [Fe/H] < -2\,\rm dex)$, out of which 43 are likely extremely metal-poor
$(\rm [Fe/H] < -3\,\rm dex )$. The comparison of the derived values to a small
subsample of stars with literature metallicity values shows that our method
works reliably and correctly estimates the uncertainties, which typically have
values $\sigma_{\rm [Fe/H]} \approx 0.2\,\mathrm{dex}$. In addition, when
compared to the metallicity results derived using the RAVE DR5 pipeline, it is
evident that we achieve better accuracy than the pipeline and therefore more
reliably evaluate the very metal-poor subsample. Based on the repeated
observations of the same stars, our method gives very consistent results. The
method used in this work can also easily be extended to other large-scale data
sets, including to the data from the Gaia mission and the upcoming 4MOST
survey.
",0,1,0,0,0,0
17601,17602,Interesting Paths in the Mapper,"  The Mapper produces a compact summary of high dimensional data as a
simplicial complex. We study the problem of quantifying the interestingness of
subpopulations in a Mapper, which appear as long paths, flares, or loops.
First, we create a weighted directed graph G using the 1-skeleton of the
Mapper. We use the average values at the vertices of a target function to
direct edges (from low to high). The difference between the average values at
vertices (high-low) is set as the edge's weight. Covariation of the remaining h
functions (independent variables) is captured by a h-bit binary signature
assigned to the edge. An interesting path in G is a directed path whose edges
all have the same signature. We define the interestingness score of such a path
as a sum of its edge weights multiplied by a nonlinear function of their ranks
in the path.
Second, we study three optimization problems on this graph G. In the problem
Max-IP, we seek an interesting path in G with the maximum interestingness
score. We show that Max-IP is NP-complete. For the special case when G is a
directed acyclic graph (DAG), we show that Max-IP can be solved in polynomial
time - in O(mnd_i) where d_i is the maximum indegree of a vertex in G.
In the more general problem IP, the goal is to find a collection of
edge-disjoint interesting paths such that the overall sum of their
interestingness scores is maximized. We also study a variant of IP termed k-IP,
where the goal is to identify a collection of edge-disjoint interesting paths
each with k edges, and their total interestingness score is maximized. While
k-IP can be solved in polynomial time for k <= 2, we show k-IP is NP-complete
for k >= 3 even when G is a DAG. We develop polynomial time heuristics for IP
and k-IP on DAGs.
",1,0,1,0,0,0
19085,19086,Fast Matrix Inversion and Determinant Computation for Polarimetric Synthetic Aperture Radar,"  This paper introduces a fast algorithm for simultaneous inversion and
determinant computation of small sized matrices in the context of fully
Polarimetric Synthetic Aperture Radar (PolSAR) image processing and analysis.
The proposed fast algorithm is based on the computation of the adjoint matrix
and the symmetry of the input matrix. The algorithm is implemented in a general
purpose graphical processing unit (GPGPU) and compared to the usual approach
based on Cholesky factorization. The assessment with simulated observations and
data from an actual PolSAR sensor show a speedup factor of about two when
compared to the usual Cholesky factorization. Moreover, the expressions
provided here can be implemented in any platform.
",1,0,0,1,0,0
10562,10563,Applying the Delta method in metric analytics: A practical guide with novel ideas,"  During the last decade, the information technology industry has adopted a
data-driven culture, relying on online metrics to measure and monitor business
performance. Under the setting of big data, the majority of such metrics
approximately follow normal distributions, opening up potential opportunities
to model them directly without extra model assumptions and solve big data
problems via closed-form formulas using distributed algorithms at a fraction of
the cost of simulation-based procedures like bootstrap. However, certain
attributes of the metrics, such as their corresponding data generating
processes and aggregation levels, pose numerous challenges for constructing
trustworthy estimation and inference procedures. Motivated by four real-life
examples in metric development and analytics for large-scale A/B testing, we
provide a practical guide to applying the Delta method, one of the most
important tools from the classic statistics literature, to address the
aforementioned challenges. We emphasize the central role of the Delta method in
metric analytics by highlighting both its classic and novel applications.
",0,0,0,1,0,0
5948,5949,"The use of Charts, Pivot Tables, and Array Formulas in two Popular Spreadsheet Corpora","  The use of spreadsheets in industry is widespread. Companies base decisions
on information coming from spreadsheets. Unfortunately, spreadsheets are
error-prone and this increases the risk that companies base their decisions on
inaccurate information, which can lead to incorrect decisions and loss of
money. In general, spreadsheet research is aimed to reduce the error-proneness
of spreadsheets. Most research is concentrated on the use of formulas. However,
there are other constructions in spreadsheets, like charts, pivot tables, and
array formulas, that are also used to present decision support information to
the user. There is almost no research about how these constructions are used.
To improve spreadsheet quality it is important to understand how spreadsheets
are used and to obtain a complete understanding, the use of charts, pivot
tables, and array formulas should be included in research. In this paper, we
analyze two popular spreadsheet corpora: Enron and EUSES on the use of the
aforementioned constructions.
",1,0,0,0,0,0
19812,19813,A Further Analysis of The Role of Heterogeneity in Coevolutionary Spatial Games,"  Heterogeneity has been studied as one of the most common explanations of the
puzzle of cooperation in social dilemmas. A large number of papers have been
published discussing the effects of increasing heterogeneity in structured
populations of agents, where it has been established that heterogeneity may
favour cooperative behaviour if it supports agents to locally coordinate their
strategies. In this paper, assuming an existing model of a heterogeneous
weighted network, we aim to further this analysis by exploring the relationship
(if any) between heterogeneity and cooperation. We adopt a weighted network
which is fully populated by agents playing both the Prisoner's Dilemma or the
Optional Prisoner's Dilemma games with coevolutionary rules, i.e., not only the
strategies but also the link weights evolve over time. Surprisingly, results
show that the heterogeneity of link weights (states) on their own does not
always promote cooperation; rather cooperation is actually favoured by the
increase in the number of overlapping states and not by the heterogeneity
itself. We believe that these results can guide further research towards a more
accurate analysis of the role of heterogeneity in social dilemmas.
",1,0,0,0,0,0
7797,7798,Strong-coupling superconductivity induced by calcium intercalation in bilayer transition-metal dichalcogenides,"  We theoretically investigate the possibility of achieving a superconducting
state in transition-metal dichalcogenide bilayers through intercalation, a
process previously and widely used to achieve metallization and superconducting
states in novel superconductors. For the Ca-intercalated bilayers MoS$_2$ and
WS$_2$, we find that the superconducting state is characterized by an
electron-phonon coupling constant larger than $1.0$ and a superconducting
critical temperature of $13.3$ and $9.3$ K, respectively. These results are
superior to other predicted or experimentally observed two-dimensional
conventional superconductors and suggest that the investigated materials may be
good candidates for nanoscale superconductors. More interestingly, we proved
that the obtained thermodynamic properties go beyond the predictions of the
mean-field Bardeen--Cooper--Schrieffer approximation and that the calculations
conducted within the framework of the strong-coupling Eliashberg theory should
be treated as those that yield quantitative results.
",0,1,0,0,0,0
5305,5306,Semantic Code Repair using Neuro-Symbolic Transformation Networks,"  We study the problem of semantic code repair, which can be broadly defined as
automatically fixing non-syntactic bugs in source code. The majority of past
work in semantic code repair assumed access to unit tests against which
candidate repairs could be validated. In contrast, the goal here is to develop
a strong statistical model to accurately predict both bug locations and exact
fixes without access to information about the intended correct behavior of the
program. Achieving such a goal requires a robust contextual repair model, which
we train on a large corpus of real-world source code that has been augmented
with synthetically injected bugs. Our framework adopts a two-stage approach
where first a large set of repair candidates are generated by rule-based
processors, and then these candidates are scored by a statistical model using a
novel neural network architecture which we refer to as Share, Specialize, and
Compete. Specifically, the architecture (1) generates a shared encoding of the
source code using an RNN over the abstract syntax tree, (2) scores each
candidate repair using specialized network modules, and (3) then normalizes
these scores together so they can compete against one another in comparable
probability space. We evaluate our model on a real-world test set gathered from
GitHub containing four common categories of bugs. Our model is able to predict
the exact correct repair 41\% of the time with a single guess, compared to 13\%
accuracy for an attentional sequence-to-sequence model.
",1,0,0,0,0,0
2969,2970,Predictions of planet detections with near infrared radial velocities in the up-coming SPIRou Legacy Survey-Planet Search,"  The SPIRou near infrared spectro-polarimeter is destined to begin science
operations at the Canada-France-Hawaii Telescope in mid-2018. One of the
instrument's primary science goals is to discover the closest exoplanets to the
Solar System by conducting a 3-5 year long radial velocity survey of nearby M
dwarfs at an expected precision of $\sim 1$ m s$^{-1}$; the SPIRou Legacy
Survey-Planet Search (SLS-PS). In this study we conduct a detailed Monte-Carlo
simulation of the SLS-PS using our current understanding of the occurrence rate
of M dwarf planetary systems and physical models of stellar activity. From
simultaneous modelling of planetary signals and activity, we predict the
population of planets detected in the SLS-PS. With our fiducial survey strategy
and expected instrument performance over a nominal survey length of $\sim 3$
years, we expect SPIRou to detect $85.3^{+29.3}_{-12.4}$ planets including
$20.0^{+16.8}_{-7.2}$ habitable zone planets and $8.1^{+7.6}_{-3.2}$ Earth-like
planets from a sample of 100 M1-M8.5 dwarfs out to 11 pc. By studying
mid-to-late M dwarfs previously inaccessible to existing optical velocimeters,
SPIRou will put meaningful constraints on the occurrence rate of planets around
those stars including the value of $\eta_{\oplus}$ at an expected level of
precision of $\lesssim 45$%. We also predict a subset of $46.7^{+16.0}_{-6.0}$
planets may be accessible with dedicated high-contrast imagers on the next
generation of ELTs including $4.9^{+4.7}_{-2.0}$ potentially imagable
Earth-like planets. Lastly, we compare the results of our fiducial survey
strategy to other foreseeable survey versions to quantify which strategy is
optimized to reach the SLS-PS science goals. The results of our simulations are
made available to the community on github.
",0,1,0,0,0,0
713,714,Construction of constant mean curvature n-noids using the DPW method,"  We construct constant mean curvature surfaces in euclidean space with genus
zero and n ends asymptotic to Delaunay surfaces using the DPW method.
",0,0,1,0,0,0
19273,19274,Graph heat mixture model learning,"  Graph inference methods have recently attracted a great interest from the
scientific community, due to the large value they bring in data interpretation
and analysis. However, most of the available state-of-the-art methods focus on
scenarios where all available data can be explained through the same graph, or
groups corresponding to each graph are known a priori. In this paper, we argue
that this is not always realistic and we introduce a generative model for mixed
signals following a heat diffusion process on multiple graphs. We propose an
expectation-maximisation algorithm that can successfully separate signals into
corresponding groups, and infer multiple graphs that govern their behaviour. We
demonstrate the benefits of our method on both synthetic and real data.
",1,0,0,1,0,0
16461,16462,This Looks Like That: Deep Learning for Interpretable Image Recognition,"  When we are faced with challenging image classification tasks, we often
explain our reasoning by dissecting the image, and pointing out prototypical
aspects of one class or another. The mounting evidence for each of the classes
helps us make our final decision. In this work, we introduce a deep network
architecture that reasons in a similar way: the network dissects the image by
finding prototypical parts, and combines evidence from the prototypes to make a
final classification. The model thus reasons in a way that is qualitatively
similar to the way ornithologists, physicians, geologists, architects, and
others would explain to people on how to solve challenging image classification
tasks. The network uses only image-level labels for training, meaning that
there are no labels for parts of images. We demonstrate our method on the
CUB-200-2011 dataset and the CBIS-DDSM dataset. Our experiments show that our
interpretable network can achieve comparable accuracy with its analogous
standard non-interpretable counterpart as well as other interpretable deep
models.
",0,0,0,1,0,0
17295,17296,Synthesis and electronic properties of Ruddlesden-Popper strontium iridate epitaxial thin films stabilized by control of growth kinetics,"  We report on the selective fabrication of high-quality Sr$_2$IrO$_4$ and
SrIrO$_3$ epitaxial thin films from a single polycrystalline Sr$_2$IrO$_4$
target by pulsed laser deposition. Using a combination of X-ray diffraction and
photoemission spectroscopy characterizations, we discover that within a
relatively narrow range of substrate temperature, the oxygen partial pressure
plays a critical role in the cation stoichiometric ratio of the films, and
triggers the stabilization of different Ruddlesden-Popper (RP) phases. Resonant
X-ray absorption spectroscopy measurements taken at the Ir $L$-edge and the O
$K$-edge demonstrate the presence of strong spin-orbit coupling, and reveal the
electronic and orbital structures of both compounds. These results suggest that
in addition to the conventional thermodynamics consideration, higher members of
the Sr$_{n+1}$Ir$_n$O$_{3n+1}$ series can possibly be achieved by kinetic
control away from the thermodynamic limit. These findings offer a new approach
to the synthesis of ultra-thin films of the RP series of iridates and can be
extended to other complex oxides with layered structure.
",0,1,0,0,0,0
1719,1720,Model-Based Control Using Koopman Operators,"  This paper explores the application of Koopman operator theory to the control
of robotic systems. The operator is introduced as a method to generate
data-driven models that have utility for model-based control methods. We then
motivate the use of the Koopman operator towards augmenting model-based
control. Specifically, we illustrate how the operator can be used to obtain a
linearizable data-driven model for an unknown dynamical process that is useful
for model-based control synthesis. Simulated results show that with increasing
complexity in the choice of the basis functions, a closed-loop controller is
able to invert and stabilize a cart- and VTOL-pendulum systems. Furthermore,
the specification of the basis function are shown to be of importance when
generating a Koopman operator for specific robotic systems. Experimental
results with the Sphero SPRK robot explore the utility of the Koopman operator
in a reduced state representation setting where increased complexity in the
basis function improve open- and closed-loop controller performance in various
terrains, including sand.
",1,0,0,0,0,0
6060,6061,Low-level Active Visual Navigation: Increasing robustness of vision-based localization using potential fields,"  This paper proposes a low-level visual navigation algorithm to improve visual
localization of a mobile robot. The algorithm, based on artificial potential
fields, associates each feature in the current image frame with an attractive
or neutral potential energy, with the objective of generating a control action
that drives the vehicle towards the goal, while still favoring feature rich
areas within a local scope, thus improving the localization performance. One
key property of the proposed method is that it does not rely on mapping, and
therefore it is a lightweight solution that can be deployed on miniaturized
aerial robots, in which memory and computational power are major constraints.
Simulations and real experimental results using a mini quadrotor equipped with
a downward looking camera demonstrate that the proposed method can effectively
drive the vehicle to a designated goal through a path that prevents
localization failure.
",1,0,0,0,0,0
19028,19029,Stochastic Block Models with Multiple Continuous Attributes,"  The stochastic block model (SBM) is a probabilistic model for community
structure in networks. Typically, only the adjacency matrix is used to perform
SBM parameter inference. In this paper, we consider circumstances in which
nodes have an associated vector of continuous attributes that are also used to
learn the node-to-community assignments and corresponding SBM parameters. While
this assumption is not realistic for every application, our model assumes that
the attributes associated with the nodes in a network's community can be
described by a common multivariate Gaussian model. In this augmented,
attributed SBM, the objective is to simultaneously learn the SBM connectivity
probabilities with the multivariate Gaussian parameters describing each
community. While there are recent examples in the literature that combine
connectivity and attribute information to inform community detection, our model
is the first augmented stochastic block model to handle multiple continuous
attributes. This provides the flexibility in biological data to, for example,
augment connectivity information with continuous measurements from multiple
experimental modalities. Because the lack of labeled network data often makes
community detection results difficult to validate, we highlight the usefulness
of our model for two network prediction tasks: link prediction and
collaborative filtering. As a result of fitting this attributed stochastic
block model, one can predict the attribute vector or connectivity patterns for
a new node in the event of the complementary source of information
(connectivity or attributes, respectively). We also highlight two biological
examples where the attributed stochastic block model provides satisfactory
performance in the link prediction and collaborative filtering tasks.
",1,0,0,1,0,0
7788,7789,AspEm: Embedding Learning by Aspects in Heterogeneous Information Networks,"  Heterogeneous information networks (HINs) are ubiquitous in real-world
applications. Due to the heterogeneity in HINs, the typed edges may not fully
align with each other. In order to capture the semantic subtlety, we propose
the concept of aspects with each aspect being a unit representing one
underlying semantic facet. Meanwhile, network embedding has emerged as a
powerful method for learning network representation, where the learned
embedding can be used as features in various downstream applications.
Therefore, we are motivated to propose a novel embedding learning
framework---AspEm---to preserve the semantic information in HINs based on
multiple aspects. Instead of preserving information of the network in one
semantic space, AspEm encapsulates information regarding each aspect
individually. In order to select aspects for embedding purpose, we further
devise a solution for AspEm based on dataset-wide statistics. To corroborate
the efficacy of AspEm, we conducted experiments on two real-words datasets with
two types of applications---classification and link prediction. Experiment
results demonstrate that AspEm can outperform baseline network embedding
learning methods by considering multiple aspects, where the aspects can be
selected from the given HIN in an unsupervised manner.
",1,0,0,0,0,0
4542,4543,Emergent transport in a many-body open system driven by interacting quantum baths,"  We analyze an open many-body system that is strongly coupled at its
boundaries to interacting quantum baths. We show that the two-body interactions
inside the baths induce emergent phenomena in the spin transport. The system
and baths are modeled as independent spin chains resulting in a global
non-homogeneous XXZ model. The evolution of the system-bath state is simulated
using matrix-product-states methods. We present two phase transitions induced
by bath interactions. For weak bath interactions we observe ballistic and
insulating phases. However, for strong bath interactions a diffusive phase
emerges with a distinct power-law decay of the time-dependent spin current
$Q\propto t^{-\alpha}$. Furthermore, we investigate long-lasting current
oscillations arising from the non-Markovian dynamics in the homogeneous case,
and find a sharp change in their frequency scaling coinciding with the triple
point of the phase diagram.
",0,1,0,0,0,0
8575,8576,"Discrete Integrable Systems, Supersymmetric Quantum Mechanics, and Framed BPS States - I","  It is possible to understand whether a given BPS spectrum is generated by a
relevant deformation of a 4D N=2 SCFT or of an asymptotically free theory from
the periodicity properties of the corresponding quantum monodromy. With the aim
of giving a better understanding of the above conjecture, in this paper we
revisit the description of framed BPS states of four-dimensional relativistic
quantum field theories with eight conserved supercharges in terms of
supersymmetric quantum mechanics. We unveil aspects of the deep
interrelationship in between the Seiberg-dualities of the latter, the discrete
symmetries of the theory in the bulk, and quantum discrete integrable systems.
",0,1,1,0,0,0
16375,16376,Pattern Generation for Walking on Slippery Terrains,"  In this paper, we extend state of the art Model Predictive Control (MPC)
approaches to generate safe bipedal walking on slippery surfaces. In this
setting, we formulate walking as a trade off between realizing a desired
walking velocity and preserving robust foot-ground contact. Exploiting this
formulation inside MPC, we show that safe walking on various flat terrains can
be achieved by compromising three main attributes, i. e. walking velocity
tracking, the Zero Moment Point (ZMP) modulation, and the Required Coefficient
of Friction (RCoF) regulation. Simulation results show that increasing the
walking velocity increases the possibility of slippage, while reducing the
slippage possibility conflicts with reducing the tip-over possibility of the
contact and vice versa.
",1,0,0,0,0,0
12818,12819,1-bit Massive MU-MIMO Precoding in VLSI,"  Massive multiuser (MU) multiple-input multiple-output (MIMO) will be a core
technology in fifth-generation (5G) wireless systems as it offers significant
improvements in spectral efficiency compared to existing multi-antenna
technologies. The presence of hundreds of antenna elements at the base station
(BS), however, results in excessively high hardware costs and power
consumption, and requires high interconnect throughput between the
baseband-processing unit and the radio unit. Massive MU-MIMO that uses
low-resolution analog-to-digital and digital-to-analog converters (DACs) has
the potential to address all these issues. In this paper, we focus on downlink
precoding for massive MU-MIMO systems with 1-bit DACs at the BS. The objective
is to design precoders that simultaneously mitigate multi-user interference
(MUI) and quantization artifacts. We propose two nonlinear 1-bit precoding
algorithms and corresponding very-large scale integration (VLSI) designs. Our
algorithms rely on biconvex relaxation, which enables the design of efficient
1-bit precoding algorithms that achieve superior error-rate performance
compared to that of linear precoding algorithms followed by quantization. To
showcase the efficacy of our algorithms, we design VLSI architectures that
enable efficient 1-bit precoding for massive MU-MIMO systems in which hundreds
of antennas serve tens of user equipments. We present corresponding
field-programmable gate array (FPGA) implementations to demonstrate that 1-bit
precoding enables reliable and high-rate downlink data transmission in
practical systems.
",1,0,0,0,0,0
11795,11796,Computation of Ground States of the Gross-Pitaevskii Functional via Riemannian Optimization,"  In this paper we combine concepts from Riemannian Optimization and the theory
of Sobolev gradients to derive a new conjugate gradient method for direct
minimization of the Gross-Pitaevskii energy functional with rotation. The
conservation of the number of particles constrains the minimizers to lie on a
manifold corresponding to the unit $L^2$ norm. The idea developed here is to
transform the original constrained optimization problem to an unconstrained
problem on this (spherical) Riemannian manifold, so that fast minimization
algorithms can be applied as alternatives to more standard constrained
formulations. First, we obtain Sobolev gradients using an equivalent definition
of an $H^1$ inner product which takes into account rotation. Then, the
Riemannian gradient (RG) steepest descent method is derived based on projected
gradients and retraction of an intermediate solution back to the constraint
manifold. Finally, we use the concept of the Riemannian vector transport to
propose a Riemannian conjugate gradient (RCG) method for this problem. It is
derived at the continuous level based on the ""optimize-then-discretize""
paradigm instead of the usual ""discretize-then-optimize"" approach, as this
ensures robustness of the method when adaptive mesh refinement is performed in
computations. We evaluate various design choices inherent in the formulation of
the method and conclude with recommendations concerning selection of the best
options. Numerical tests demonstrate that the proposed RCG method outperforms
the simple gradient descent (RG) method in terms of rate of convergence. While
on simple problems a Newton-type method implemented in the {\tt Ipopt} library
exhibits a faster convergence than the (RCG) approach, the two methods perform
similarly on more complex problems requiring the use of mesh adaptation. At the
same time the (RCG) approach has far fewer tunable parameters.
",0,1,1,0,0,0
3252,3253,End-to-End Multi-Task Denoising for joint SDR and PESQ Optimization,"  Supervised learning based on a deep neural network recently has achieved
substantial improvement on speech enhancement. Denoising networks learn mapping
from noisy speech to clean one directly, or to a spectra mask which is the
ratio between clean and noisy spectrum. In either case, the network is
optimized by minimizing mean square error (MSE) between predefined labels and
network output of spectra or time-domain signal. However, existing schemes have
either of two critical issues: spectra and metric mismatches. The spectra
mismatch is a well known issue that any spectra modification after short-time
Fourier transform (STFT), in general, cannot be fully recovered after inverse
STFT. The metric mismatch is that a conventional MSE metric is sub-optimal to
maximize our target metrics, signal-to-distortion ratio (SDR) and perceptual
evaluation of speech quality (PESQ). This paper presents a new end-to-end
denoising framework with the goal of joint SDR and PESQ optimization. First,
the network optimization is performed on the time-domain signals after ISTFT to
avoid spectra mismatch. Second, two loss functions which have improved
correlations with SDR and PESQ metrics are proposed to minimize metric
mismatch. The experimental result showed that the proposed denoising scheme
significantly improved both SDR and PESQ performance over the existing methods.
",1,0,0,1,0,0
5290,5291,Temporally Identity-Aware SSD with Attentional LSTM,"  Temporal object detection has attracted significant attention, but most
popular detection methods can not leverage the rich temporal information in
videos. Very recently, many different algorithms have been developed for video
detection task, but real-time online approaches are frequently deficient. In
this paper, based on attention mechanism and convolutional long short-term
memory (ConvLSTM), we propose a temporal signal-shot detector (TSSD) for
real-world detection. Distinct from previous methods, we take aim at temporally
integrating pyramidal feature hierarchy using ConvLSTM, and design a novel
structure including a low-level temporal unit as well as a high-level one
(HL-TU) for multi-scale feature maps. Moreover, we develop a creative temporal
analysis unit, namely, attentional ConvLSTM (AC-LSTM), in which a temporal
attention module is specially tailored for background suppression and scale
suppression while a ConvLSTM integrates attention-aware features through time.
An association loss is designed for temporal coherence. Besides, online tubelet
analysis (OTA) is exploited for identification. Finally, our method is
evaluated on ImageNet VID dataset and 2DMOT15 dataset. Extensive comparisons on
the detection and tracking capability validate the superiority of the proposed
approach. Consequently, the developed TSSD-OTA is fairly faster and achieves an
overall competitive performance in terms of detection and tracking. The source
code will be made available.
",1,0,0,0,0,0
10703,10704,Performance study of SKIROC2 and SKIROC2A with BGA testboard,"  SKIROC2 is an ASIC to readout the silicon pad detectors for the
electromagnetic calorimeter in the International Linear Collider.
Characteristics of SKIROC2 and the new version of SKIROC2A, packaged with BGA,
are measured with testboards and charge injection. The results on the
signal-to-noise ratio of both trigger and ADC output, threshold tuning
capability and timing resolution are presented.
",0,1,0,0,0,0
5546,5547,Quantifying Program Bias,"  With the range and sensitivity of algorithmic decisions expanding at a
break-neck speed, it is imperative that we aggressively investigate whether
programs are biased. We propose a novel probabilistic program analysis
technique and apply it to quantifying bias in decision-making programs.
Specifically, we (i) present a sound and complete automated verification
technique for proving quantitative properties of probabilistic programs; (ii)
show that certain notions of bias, recently proposed in the fairness
literature, can be phrased as quantitative correctness properties; and (iii)
present FairSquare, the first verification tool for quantifying program bias,
and evaluate it on a range of decision-making programs.
",1,0,0,0,0,0
14249,14250,Topological Kondo insulators in one dimension: Continuous Haldane-type ground-state evolution from the strongly-interacting to the non-interacting limit,"  We study, by means of the density-matrix renormalization group (DMRG)
technique, the evolution of the ground state in a one-dimensional topological
insulator, from the non-interacting to the strongly-interacting limit, where
the system can be mapped onto a topological Kondo-insulator model. We focus on
a toy model Hamiltonian (i.e., the interacting ""$sp$-ladder"" model), which
could be experimentally realized in optical lattices with higher orbitals
loaded with ultra-cold fermionic atoms. Our goal is to shed light on the
emergence of the strongly-interacting ground state and its topological
classification as the Hubbard-$U$ interaction parameter of the model is
increased. Our numerical results show that the ground state can be generically
classified as a symmetry-protected topological phase of the Haldane-type, even
in the non-interacting case $U=0$ where the system can be additionally
classified as a time-reversal $\mathbb{Z}_{2}$-topological insulator, and
evolves adiabatically between the non-interacting and strongly interacting
limits.
",0,1,0,0,0,0
15284,15285,Discrete Time Dynamic Programming with Recursive Preferences: Optimality and Applications,"  This paper provides an alternative approach to the theory of dynamic
programming, designed to accommodate the kinds of recursive preference
specifications that have become popular in economic and financial analysis,
while still supporting traditional additively separable rewards. The approach
exploits the theory of monotone convex operators, which turns out to be well
suited to dynamic maximization. The intuition is that convexity is preserved
under maximization, so convexity properties found in preferences extend
naturally to the Bellman operator.
",0,0,0,0,0,1
3964,3965,Passive Compliance Control of Aerial Manipulators,"  This paper presents a passive compliance control for aerial manipulators to
achieve stable environmental interactions. The main challenge is the absence of
actuation along body-planar directions of the aerial vehicle which might be
required during the interaction to preserve passivity. The controller proposed
in this paper guarantees passivity of the manipulator through a proper choice
of end-effector coordinates, and that of vehicle fuselage is guaranteed by
exploiting time domain passivity technique. Simulation studies validate the
proposed approach.
",1,0,0,0,0,0
14139,14140,Understanding Negations in Information Processing: Learning from Replicating Human Behavior,"  Information systems experience an ever-growing volume of unstructured data,
particularly in the form of textual materials. This represents a rich source of
information from which one can create value for people, organizations and
businesses. For instance, recommender systems can benefit from automatically
understanding preferences based on user reviews or social media. However, it is
difficult for computer programs to correctly infer meaning from narrative
content. One major challenge is negations that invert the interpretation of
words and sentences. As a remedy, this paper proposes a novel learning strategy
to detect negations: we apply reinforcement learning to find a policy that
replicates the human perception of negations based on an exogenous response,
such as a user rating for reviews. Our method yields several benefits, as it
eliminates the former need for expensive and subjective manual labeling in an
intermediate stage. Moreover, the inferred policy can be used to derive
statistical inferences and implications regarding how humans process and act on
negations.
",1,0,0,1,0,0
7092,7093,Re-DPoctor: Real-time health data releasing with w-day differential privacy,"  Wearable devices enable users to collect health data and share them with
healthcare providers for improved health service. Since health data contain
privacy-sensitive information, unprotected data release system may result in
privacy leakage problem. Most of the existing work use differential privacy for
private data release. However, they have limitations in healthcare scenarios
because they do not consider the unique features of health data being collected
from wearables, such as continuous real-time collection and pattern
preservation. In this paper, we propose Re-DPoctor, a real-time health data
releasing scheme with $w$-day differential privacy where the privacy of health
data collected from any consecutive $w$ days is preserved. We improve utility
by using a specially-designed partition algorithm to protect the health data
patterns. Meanwhile, we improve privacy preservation by applying newly proposed
adaptive sampling technique and budget allocation method. We prove that
Re-DPoctor satisfies $w$-day differential privacy. Experiments on real health
data demonstrate that our method achieves better utility with strong privacy
guarantee than existing state-of-the-art methods.
",1,0,0,0,0,0
13675,13676,The weak order on integer posets,"  We explore lattice structures on integer binary relations (i.e. binary
relations on the set $\{1, 2, \dots, n\}$ for a fixed integer $n$) and on
integer posets (i.e. partial orders on the set $\{1, 2, \dots, n\}$ for a fixed
integer $n$). We first observe that the weak order on the symmetric group
naturally extends to a lattice structure on all integer binary relations. We
then show that the subposet of this weak order induced by integer posets
defines as well a lattice. We finally study the subposets of this weak order
induced by specific families of integer posets corresponding to the elements,
the intervals, and the faces of the permutahedron, the associahedron, and some
recent generalizations of those.
",0,0,1,0,0,0
5309,5310,Deep Temporal-Recurrent-Replicated-Softmax for Topical Trends over Time,"  Dynamic topic modeling facilitates the identification of topical trends over
time in temporal collections of unstructured documents. We introduce a novel
unsupervised neural dynamic topic model named as Recurrent Neural
Network-Replicated Softmax Model (RNNRSM), where the discovered topics at each
time influence the topic discovery in the subsequent time steps. We account for
the temporal ordering of documents by explicitly modeling a joint distribution
of latent topical dependencies over time, using distributional estimators with
temporal recurrent connections. Applying RNN-RSM to 19 years of articles on NLP
research, we demonstrate that compared to state-of-the art topic models, RNNRSM
shows better generalization, topic interpretation, evolution and trends. We
also introduce a metric (named as SPAN) to quantify the capability of dynamic
topic model to capture word evolution in topics over time.
",1,0,0,0,0,0
9505,9506,Flow Fields: Dense Correspondence Fields for Highly Accurate Large Displacement Optical Flow Estimation,"  Modern large displacement optical flow algorithms usually use an
initialization by either sparse descriptor matching techniques or dense
approximate nearest neighbor fields. While the latter have the advantage of
being dense, they have the major disadvantage of being very outlier-prone as
they are not designed to find the optical flow, but the visually most similar
correspondence. In this article we present a dense correspondence field
approach that is much less outlier-prone and thus much better suited for
optical flow estimation than approximate nearest neighbor fields. Our approach
does not require explicit regularization, smoothing (like median filtering) or
a new data term. Instead we solely rely on patch matching techniques and a
novel multi-scale matching strategy. We also present enhancements for outlier
filtering. We show that our approach is better suited for large displacement
optical flow estimation than modern descriptor matching techniques. We do so by
initializing EpicFlow with our approach instead of their originally used
state-of-the-art descriptor matching technique. We significantly outperform the
original EpicFlow on MPI-Sintel, KITTI 2012, KITTI 2015 and Middlebury. In this
extended article of our former conference publication we further improve our
approach in matching accuracy as well as runtime and present more experiments
and insights.
",1,0,0,0,0,0
530,531,A Data-Driven Sparse-Learning Approach to Model Reduction in Chemical Reaction Networks,"  In this paper, we propose an optimization-based sparse learning approach to
identify the set of most influential reactions in a chemical reaction network.
This reduced set of reactions is then employed to construct a reduced chemical
reaction mechanism, which is relevant to chemical interaction network modeling.
The problem of identifying influential reactions is first formulated as a
mixed-integer quadratic program, and then a relaxation method is leveraged to
reduce the computational complexity of our approach. Qualitative and
quantitative validation of the sparse encoding approach demonstrates that the
model captures important network structural properties with moderate
computational load.
",1,0,0,0,0,0
16119,16120,Topology of polyhedral products over simplicial multiwedges,"  We prove that certain conditions on multigraded Betti numbers of a simplicial
complex $K$ imply existence of a higher Massey product in cohomology of a
moment-angle-complex $\mathcal Z_K$, which contains a unique element (a
strictly defined product). Using the simplicial multiwedge construction, we
find a family $\mathcal{F}$ of polyhedral products being smooth closed
manifolds such that for any $l,r\geq 2$ there exists an $l$-connected manifold
$M\in\mathcal F$ with a nontrivial strictly defined $r$-fold Massey product in
$H^{*}(M)$. As an application to homological algebra, we determine a wide class
of triangulated spheres $K$ such that a nontrivial higher Massey product of any
order may exist in Koszul homology of their Stanley--Reisner rings. As an
application to rational homotopy theory, we establish a combinatorial criterion
for a simple graph $\Gamma$ to provide a (rationally) formal generalized
moment-angle manifold $\mathcal Z_{P}^{J}=(D^{2j_{i}},S^{2j_{i}-1})^{\partial
P^*}$, $J=(j_{1},\ldots,j_m)$ over a graph-associahedron $P=P_{\Gamma}$ and
compute all the diffeomorphism types of formal moment-angle manifolds over
graph-associahedra.
",0,0,1,0,0,0
5120,5121,Mapping Images to Scene Graphs with Permutation-Invariant Structured Prediction,"  Machine understanding of complex images is a key goal of artificial
intelligence. One challenge underlying this task is that visual scenes contain
multiple inter-related objects, and that global context plays an important role
in interpreting the scene. A natural modeling framework for capturing such
effects is structured prediction, which optimizes over complex labels, while
modeling within-label interactions. However, it is unclear what principles
should guide the design of a structured prediction model that utilizes the
power of deep learning components. Here we propose a design principle for such
architectures that follows from a natural requirement of permutation
invariance. We prove a necessary and sufficient characterization for
architectures that follow this invariance, and discuss its implication on model
design. Finally, we show that the resulting model achieves new state of the art
results on the Visual Genome scene graph labeling benchmark, outperforming all
recent approaches.
",0,0,0,1,0,0
16319,16320,High-speed X-ray imaging spectroscopy system with Zynq SoC for solar observations,"  We have developed a system combining a back-illuminated
Complementary-Metal-Oxide-Semiconductor (CMOS) imaging sensor and Xilinx Zynq
System-on-Chip (SoC) device for a soft X-ray (0.5-10 keV) imaging spectroscopy
observation of the Sun to investigate the dynamics of the solar corona. Because
typical timescales of energy release phenomena in the corona span a few minutes
at most, we aim to obtain the corresponding energy spectra and derive the
physical parameters, i.e., temperature and emission measure, every few tens of
seconds or less for future solar X-ray observations. An X-ray photon-counting
technique, with a frame rate of a few hundred frames per second or more, can
achieve such results. We used the Zynq SoC device to achieve the requirements.
Zynq contains an ARM processor core, which is also known as the Processing
System (PS) part, and a Programmable Logic (PL) part in a single chip. We use
the PL and PS to control the sensor and seamless recording of data to a storage
system, respectively. We aim to use the system for the third flight of the
Focusing Optics Solar X-ray Imager (FOXSI-3) sounding rocket experiment for the
first photon-counting X-ray imaging and spectroscopy of the Sun.
",0,1,0,0,0,0
19389,19390,Riemannian almost product manifolds generated by a circulant structure,"  A 4-dimensional Riemannian manifold equipped with a circulant structure,
which is an isometry with respect to the metric and its fourth power is the
identity, is considered. The almost product manifold associated with the
considered manifold is studied. The relation between the covariant derivatives
of the almost product structure and the circulant structure is obtained. The
conditions for the covariant derivative of the circulant structure, which imply
that an almost product manifold belongs to each of the basic classes of the
Staikova-Gribachev classification, are given.
",0,0,1,0,0,0
13607,13608,Gas dynamics in strong centrifugal fields,"  Dynamics of waves generated by scopes in gas centrifuges (GC) for isotope
separation is considered. The centrifugal acceleration in the GC reaches values
of the order of $10^6$g. The centrifugal and Coriolis forces modify essentially
the conventional sound waves. Three families of the waves with different
polarisation and dispersion exist in these conditions. Dynamics of the flow in
the model GC Iguasu is investigated numerically. Comparison of the results of
the numerical modelling of the wave dynamics with the analytical predictions is
performed. New phenomena of the resonances in the GC is found. The resonances
occur for the waves polarized along the rotational axis having the smallest
dumping due to the viscosity.
",0,1,0,0,0,0
2284,2285,Power Control and Relay Selection in Full-Duplex Cognitive Relay Networks: Coherent versus Non-coherent Scenarios,"  This paper investigates power control and relay selection in Full Duplex
Cognitive Relay Networks (FDCRNs), where the secondary-user (SU) relays can
simultaneously receive and forward the signal from the SU source. We study both
non-coherent and coherent scenarios. In the non-coherent case, the SU relay
forwards the signal from the SU source without regulating the phase, while in
the coherent scenario, the SU relay regulates the phase when forwarding the
signal to minimize the interference at the primary-user (PU) receiver. We
consider the problem of maximizing the transmission rate from the SU source to
the SU destination subject to the interference constraint at the PU receiver
and power constraints at both the SU source and SU relay. We develop
low-complexity and high-performance joint power control and relay selection
algorithms. The superior performance of the proposed algorithms are confirmed
using extensive numerical evaluation. In particular, we demonstrate the
significant gain of phase regulation at the SU relay (i.e., the gain of the
coherent mechanism over the noncoherent mechanism).
",1,0,1,1,0,0
5,6,On maximizing the fundamental frequency of the complement of an obstacle,"  Let $\Omega \subset \mathbb{R}^n$ be a bounded domain satisfying a
Hayman-type asymmetry condition, and let $ D $ be an arbitrary bounded domain
referred to as ""obstacle"". We are interested in the behaviour of the first
Dirichlet eigenvalue $ \lambda_1(\Omega \setminus (x+D)) $. First, we prove an
upper bound on $ \lambda_1(\Omega \setminus (x+D)) $ in terms of the distance
of the set $ x+D $ to the set of maximum points $ x_0 $ of the first Dirichlet
ground state $ \phi_{\lambda_1} > 0 $ of $ \Omega $. In short, a direct
corollary is that if \begin{equation} \mu_\Omega := \max_{x}\lambda_1(\Omega
\setminus (x+D)) \end{equation} is large enough in terms of $ \lambda_1(\Omega)
$, then all maximizer sets $ x+D $ of $ \mu_\Omega $ are close to each maximum
point $ x_0 $ of $ \phi_{\lambda_1} $.
Second, we discuss the distribution of $ \phi_{\lambda_1(\Omega)} $ and the
possibility to inscribe wavelength balls at a given point in $ \Omega $.
Finally, we specify our observations to convex obstacles $ D $ and show that
if $ \mu_\Omega $ is sufficiently large with respect to $ \lambda_1(\Omega) $,
then all maximizers $ x+D $ of $ \mu_\Omega $ contain all maximum points $ x_0
$ of $ \phi_{\lambda_1(\Omega)} $.
",0,0,1,0,0,0
8432,8433,A Feature Embedding Strategy for High-level CNN representations from Multiple ConvNets,"  Following the rapidly growing digital image usage, automatic image
categorization has become preeminent research area. It has broaden and adopted
many algorithms from time to time, whereby multi-feature (generally,
hand-engineered features) based image characterization comes handy to improve
accuracy. Recently, in machine learning, pre-trained deep convolutional neural
networks (DCNNs or ConvNets) have been that the features extracted through such
DCNN can improve classification accuracy. Thence, in this paper, we further
investigate a feature embedding strategy to exploit cues from multiple DCNNs.
We derive a generalized feature space by embedding three different DCNN
bottleneck features with weights respect to their Softmax cross-entropy loss.
Test outcomes on six different object classification data-sets and an action
classification data-set show that regardless of variation in image statistics
and tasks the proposed multi-DCNN bottleneck feature fusion is well suited to
image classification tasks and an effective complement of DCNN. The comparisons
to existing fusion-based image classification approaches prove that the
proposed method surmounts the state-of-the-art methods and produces competitive
results with fully trained DCNNs as well.
",1,0,0,0,0,0
16729,16730,Linear Disentangled Representation Learning for Facial Actions,"  Limited annotated data available for the recognition of facial expression and
action units embarrasses the training of deep networks, which can learn
disentangled invariant features. However, a linear model with just several
parameters normally is not demanding in terms of training data. In this paper,
we propose an elegant linear model to untangle confounding factors in
challenging realistic multichannel signals such as 2D face videos. The simple
yet powerful model does not rely on huge training data and is natural for
recognizing facial actions without explicitly disentangling the identity. Base
on well-understood intuitive linear models such as Sparse Representation based
Classification (SRC), previous attempts require a prepossessing of explicit
decoupling which is practically inexact. Instead, we exploit the low-rank
property across frames to subtract the underlying neutral faces which are
modeled jointly with sparse representation on the action components with group
sparsity enforced. On the extended Cohn-Kanade dataset (CK+), our one-shot
automatic method on raw face videos performs as competitive as SRC applied on
manually prepared action components and performs even better than SRC in terms
of true positive rate. We apply the model to the even more challenging task of
facial action unit recognition, verified on the MPI Face Video Database
(MPI-VDB) achieving a decent performance. All the programs and data have been
made publicly available.
",1,0,0,1,0,0
7453,7454,Sample Complexity of Estimating the Policy Gradient for Nearly Deterministic Dynamical Systems,"  Reinforcement learning is a promising approach to learning robot controllers.
It has recently been shown that algorithms based on finite-difference estimates
of the policy gradient are competitive with algorithms based on the policy
gradient theorem. We propose a theoretical framework for understanding this
phenomenon. Our key insight is that many dynamical systems (especially those of
interest in robot control tasks) are \emph{nearly deterministic}---i.e., they
can be modeled as a deterministic system with a small stochastic perturbation.
We show that for such systems, finite-difference estimates of the policy
gradient can have substantially lower variance than estimates based on the
policy gradient theorem. We interpret these results in the context of
counterfactual estimation. Finally, we empirically evaluate our insights in an
experiment on the inverted pendulum.
",1,0,0,1,0,0
2891,2892,Parallel transport in shape analysis: a scalable numerical scheme,"  The analysis of manifold-valued data requires efficient tools from Riemannian
geometry to cope with the computational complexity at stake. This complexity
arises from the always-increasing dimension of the data, and the absence of
closed-form expressions to basic operations such as the Riemannian logarithm.
In this paper, we adapt a generic numerical scheme recently introduced for
computing parallel transport along geodesics in a Riemannian manifold to
finite-dimensional manifolds of diffeomorphisms. We provide a qualitative and
quantitative analysis of its behavior on high-dimensional manifolds, and
investigate an application with the prediction of brain structures progression.
",0,0,1,1,0,0
16247,16248,Estimating ground-level PM2.5 by fusing satellite and station observations: A geo-intelligent deep learning approach,"  Fusing satellite observations and station measurements to estimate
ground-level PM2.5 is promising for monitoring PM2.5 pollution. A
geo-intelligent approach, which incorporates geographical correlation into an
intelligent deep learning architecture, is developed to estimate PM2.5.
Specifically, it considers geographical distance and spatiotemporally
correlated PM2.5 in a deep belief network (denoted as Geoi-DBN). Geoi-DBN can
capture the essential features associated with PM2.5 from latent factors. It
was trained and tested with data from China in 2015. The results show that
Geoi-DBN performs significantly better than the traditional neural network. The
cross-validation R increases from 0.63 to 0.94, and RMSE decreases from 29.56
to 13.68${\mu}$g/m3. On the basis of the derived PM2.5 distribution, it is
predicted that over 80% of the Chinese population live in areas with an annual
mean PM2.5 of greater than 35${\mu}$g/m3. This study provides a new perspective
for air pollution monitoring in large geographic regions.
",0,1,0,0,0,0
14644,14645,Ultra-wide-band slow light in photonic crystal coupled-cavity waveguides,"  Slow light propagation in structured materials is a highly promising approach
for realizing on-chip integrated photonic devices based on enhanced optical
nonlinearities. One of the most successful research avenues consists in
engineering the band dispersion of light-guiding photonic crystal (PC)
structures. The primary goal of such devices is to achieve slow-light operation
over the largest possible bandwidth, with large group index, minimal index
dispersion, and constant transmission spectrum. Here, we report on the
experimental demonstration of to date record high GBP in silicon-based
coupled-cavity waveguides (CCWs) operating at telecom wavelengths. Our results
rely on novel CCW designs, optimized using a genetic algorithm, and refined
nanofabrication processes.
",0,1,0,0,0,0
13391,13392,Convolutional Sparse Representations with Gradient Penalties,"  While convolutional sparse representations enjoy a number of useful
properties, they have received limited attention for image reconstruction
problems. The present paper compares the performance of block-based and
convolutional sparse representations in the removal of Gaussian white noise.
While the usual formulation of the convolutional sparse coding problem is
slightly inferior to the block-based representations in this problem, the
performance of the convolutional form can be boosted beyond that of the
block-based form by the inclusion of suitable penalties on the gradients of the
coefficient maps.
",1,0,0,0,0,0
20895,20896,End-to-End Learning of Semantic Grasping,"  We consider the task of semantic robotic grasping, in which a robot picks up
an object of a user-specified class using only monocular images. Inspired by
the two-stream hypothesis of visual reasoning, we present a semantic grasping
framework that learns object detection, classification, and grasp planning in
an end-to-end fashion. A ""ventral stream"" recognizes object class while a
""dorsal stream"" simultaneously interprets the geometric relationships necessary
to execute successful grasps. We leverage the autonomous data collection
capabilities of robots to obtain a large self-supervised dataset for training
the dorsal stream, and use semi-supervised label propagation to train the
ventral stream with only a modest amount of human supervision. We
experimentally show that our approach improves upon grasping systems whose
components are not learned end-to-end, including a baseline method that uses
bounding box detection. Furthermore, we show that jointly training our model
with auxiliary data consisting of non-semantic grasping data, as well as
semantically labeled images without grasp actions, has the potential to
substantially improve semantic grasping performance.
",1,0,0,1,0,0
7303,7304,Mixed Graphical Models for Causal Analysis of Multi-modal Variables,"  Graphical causal models are an important tool for knowledge discovery because
they can represent both the causal relations between variables and the
multivariate probability distributions over the data. Once learned, causal
graphs can be used for classification, feature selection and hypothesis
generation, while revealing the underlying causal network structure and thus
allowing for arbitrary likelihood queries over the data. However, current
algorithms for learning sparse directed graphs are generally designed to handle
only one type of data (continuous-only or discrete-only), which limits their
applicability to a large class of multi-modal biological datasets that include
mixed type variables. To address this issue, we developed new methods that
modify and combine existing methods for finding undirected graphs with methods
for finding directed graphs. These hybrid methods are not only faster, but also
perform better than the directed graph estimation methods alone for a variety
of parameter settings and data set sizes. Here, we describe a new conditional
independence test for learning directed graphs over mixed data types and we
compare performances of different graph learning strategies on synthetic data.
",1,0,0,1,0,0
9187,9188,Maximum a Posteriori Policy Optimisation,"  We introduce a new algorithm for reinforcement learning called Maximum
aposteriori Policy Optimisation (MPO) based on coordinate ascent on a relative
entropy objective. We show that several existing methods can directly be
related to our derivation. We develop two off-policy algorithms and demonstrate
that they are competitive with the state-of-the-art in deep reinforcement
learning. In particular, for continuous control, our method outperforms
existing methods with respect to sample efficiency, premature convergence and
robustness to hyperparameter settings while achieving similar or better final
performance.
",1,0,0,1,0,0
19833,19834,On Generalized Gibbs Ensembles with an infinite set of conserved charges,"  We revisit the question of whether and how the steady states arising after
non-equilibrium time evolution in integrable models (and in particular in the
XXZ spin chain) can be described by the so-called Generalized Gibbs Ensemble
(GGE). It is known that the micro-canonical ensemble built on a complete set of
charges correctly describes the long-time limit of local observables, and
recently a canonical ensemble was built by Ilievski et. al. using particle
occupation number operators. Here we provide an alternative construction by
considering truncated GGE's (tGGE's) that only include a finite number of well
localized conserved operators. It is shown that the tGGE's can approximate the
steady states with arbitrary precision, i.e. all physical observables are
exactly reproduced in the infinite truncation limit. In addition, we show that
a complete canonical ensemble can in fact be built in terms of a new (discrete)
set of charges built as linear combinations of the standard ones.
Our general arguments are applied to concrete quench situations in the XXZ
chain, where the initial states are simple two-site or four-site product
states. Depending on the quench we find that numerical results for the local
correlators can be obtained with remarkable precision using truncated GGE's
with only 10-100 charges.
",0,1,0,0,0,0
19896,19897,Are You Tampering With My Data?,"  We propose a novel approach towards adversarial attacks on neural networks
(NN), focusing on tampering the data used for training instead of generating
attacks on trained models. Our network-agnostic method creates a backdoor
during training which can be exploited at test time to force a neural network
to exhibit abnormal behaviour. We demonstrate on two widely used datasets
(CIFAR-10 and SVHN) that a universal modification of just one pixel per image
for all the images of a class in the training set is enough to corrupt the
training procedure of several state-of-the-art deep neural networks causing the
networks to misclassify any images to which the modification is applied. Our
aim is to bring to the attention of the machine learning community, the
possibility that even learning-based methods that are personally trained on
public datasets can be subject to attacks by a skillful adversary.
",0,0,0,1,0,0
7498,7499,A Generalization of Convolutional Neural Networks to Graph-Structured Data,"  This paper introduces a generalization of Convolutional Neural Networks
(CNNs) from low-dimensional grid data, such as images, to graph-structured
data. We propose a novel spatial convolution utilizing a random walk to uncover
the relations within the input, analogous to the way the standard convolution
uses the spatial neighborhood of a pixel on the grid. The convolution has an
intuitive interpretation, is efficient and scalable and can also be used on
data with varying graph structure. Furthermore, this generalization can be
applied to many standard regression or classification problems, by learning the
the underlying graph. We empirically demonstrate the performance of the
proposed CNN on MNIST, and challenge the state-of-the-art on Merck molecular
activity data set.
",1,0,0,1,0,0
4525,4526,On a backward problem for multidimensional Ginzburg-Landau equation with random data,"  In this paper, we consider a backward in time problem for Ginzburg-Landau
equation in multidimensional domain associated with some random data. The
problem is ill-posed in the sense of Hadamard. To regularize the instable
solution, we develop a new regularized method combined with statistical
approach to solve this problem. We prove a upper bound, on the rate of
convergence of the mean integrated squared error in $L^2 $ norm and $H^1$ norm.
",0,0,1,0,0,0
20199,20200,How Should a Robot Assess Risk? Towards an Axiomatic Theory of Risk in Robotics,"  Endowing robots with the capability of assessing risk and making risk-aware
decisions is widely considered a key step toward ensuring safety for robots
operating under uncertainty. But, how should a robot quantify risk? A natural
and common approach is to consider the framework whereby costs are assigned to
stochastic outcomes - an assignment captured by a cost random variable.
Quantifying risk then corresponds to evaluating a risk metric, i.e., a mapping
from the cost random variable to a real number. Yet, the question of what
constitutes a ""good"" risk metric has received little attention within the
robotics community. The goal of this paper is to explore and partially address
this question by advocating axioms that risk metrics in robotics applications
should satisfy in order to be employed as rational assessments of risk. We
discuss general representation theorems that precisely characterize the class
of metrics that satisfy these axioms (referred to as distortion risk metrics),
and provide instantiations that can be used in applications. We further discuss
pitfalls of commonly used risk metrics in robotics, and discuss additional
properties that one must consider in sequential decision making tasks. Our hope
is that the ideas presented here will lead to a foundational framework for
quantifying risk (and hence safety) in robotics applications.
",1,0,0,0,0,0
880,881,Faster Tensor Canonicalization,"  The Butler-Portugal algorithm for obtaining the canonical form of a tensor
expression with respect to slot symmetries and dummy-index renaming suffers, in
certain cases with a high degree of symmetry, from $O(n!)$ explosion in both
computation time and memory. We present a modified algorithm which alleviates
this problem in the most common cases---tensor expressions with subsets of
indices which are totally symmetric or totally antisymmetric---in polynomial
time. We also present an implementation of the label-renaming mechanism which
improves upon that of the original Butler-Portugal algorithm, thus providing a
significant speed increase for the average case as well as the highly-symmetric
special case. The worst-case behavior remains $O(n!)$, although it occurs in
more limited situations unlikely to appear in actual computations. We comment
on possible strategies to take if the nature of a computation should make these
situations more likely.
",1,0,0,0,0,0
9522,9523,A study of existing Ontologies in the IoT-domain,"  Several domains have adopted the increasing use of IoT-based devices to
collect sensor data for generating abstractions and perceptions of the real
world. This sensor data is multi-modal and heterogeneous in nature. This
heterogeneity induces interoperability issues while developing cross-domain
applications, thereby restricting the possibility of reusing sensor data to
develop new applications. As a solution to this, semantic approaches have been
proposed in the literature to tackle problems related to interoperability of
sensor data. Several ontologies have been proposed to handle different aspects
of IoT-based sensor data collection, ranging from discovering the IoT sensors
for data collection to applying reasoning on the collected sensor data for
drawing inferences. In this paper, we survey these existing semantic ontologies
to provide an overview of the recent developments in this field. We highlight
the fundamental ontological concepts (e.g., sensor-capabilities and
context-awareness) required for an IoT-based application, and survey the
existing ontologies which include these concepts. Based on our study, we also
identify the shortcomings of currently available ontologies, which serves as a
stepping stone to state the need for a common unified ontology for the IoT
domain.
",1,0,0,0,0,0
16485,16486,Random Manifolds have no Totally Geodesic Submanifolds,"  For $n\geq 4$ we show that generic closed Riemannian $n$-manifolds have no
nontrivial totally geodesic submanifolds, answering a question of Spivak. An
immediate consequence is a severe restriction on the isometry group of a
generic Riemannian metric. Both results are widely believed to be true, but we
are not aware of any proofs in the literature.
",0,0,1,0,0,0
11864,11865,Local migration quantification method for scratch assays,"  Motivation: The scratch assay is a standard experimental protocol used to
characterize cell migration. It can be used to identify genes that regulate
migration and evaluate the efficacy of potential drugs that inhibit cancer
invasion. In these experiments, a scratch is made on a cell monolayer and
recolonisation of the scratched region is imaged to quantify cell migration
rates. A drawback of this methodology is the lack of its reproducibility
resulting in irregular cell-free areas with crooked leading edges. Existing
quantification methods deal poorly with such resulting irregularities present
in the data. Results: We introduce a new quantification method that can analyse
low quality experimental data. By considering in-silico and in-vitro data, we
show that the method provides a more accurate statistical classification of the
migration rates than two established quantification methods. The application of
this method will enable the quantification of migration rates of scratch assay
data previously unsuitable for analysis. Availability and Implementation: The
source code and the implementation of the algorithm as a GUI along with an
example dataset and user instructions, are available in
this https URL.
The datasets are available in
this https URL.
",0,0,0,0,1,0
6175,6176,A Simplified Approach to Analyze Complementary Sensitivity Trade-offs in Continuous-Time and Discrete-Time Systems,"  A simplified approach is proposed to investigate the continuous-time and
discrete-time complementary sensitivity Bode integrals (CSBIs) in this note.
For continuous-time feedback systems with unbounded frequency domain, the CSBI
weighted by $1/\omega^2$ is considered, where this simplified method reveals a
more explicit relationship between the value of CSBI and the structure of the
open-loop transfer function. With a minor modification of this method, the CSBI
of discrete-time system is derived, and illustrative examples are provided.
Compared with the existing results on CSBI, neither Cauchy integral theorem nor
Poisson integral formula are used throughout the analysis, and the analytic
constraint on the integrand is removed.
",1,0,0,0,0,0
12973,12974,Infinitely generated symbolic Rees algebras over finite fields,"  For the polynomial ring over an arbitrary field with twelve variables, there
exists a prime ideal whose symbolic Rees algebra is not finitely generated.
",0,0,1,0,0,0
11422,11423,"Ab initio study of magnetocrystalline anisotropy, magnetostriction, and Fermi surface of L10 FeNi (tetrataenite)","  The ordered L1$_0$ FeNi phase (tetrataenite) is recently considered as a
promising candidate for the rare-earth free permanent magnets applications. In
this work we calculate several characteristics of the L1$_0$ FeNi, where most
of the results come form the fully relativistic full potential FPLO method with
the generalized gradient approximation (GGA). A special attention deserves the
summary of the magnetocrystalline anisotropy energies (MAE's), the full
potential calculations of the anisotropy constant $K_3$, and the combined
analysis of the Fermi surface and three-dimensional $\mathbf{k}$-resolved MAE.
Other calculated parameters presented in this article are the magnetic moments
$m_{s}$ and $m_{l}$, magnetostrictive coefficient $\lambda_{001}$, bulk modulus
B$_0$, and lattice parameters. The MAE's summary shows rather big discrepancies
between the experimental MAE's from literature and also between the calculated
MAE's.
",0,1,0,0,0,0
19520,19521,Network Backboning with Noisy Data,"  Networks are powerful instruments to study complex phenomena, but they become
hard to analyze in data that contain noise. Network backbones provide a tool to
extract the latent structure from noisy networks by pruning non-salient edges.
We describe a new approach to extract such backbones. We assume that edge
weights are drawn from a binomial distribution, and estimate the error-variance
in edge weights using a Bayesian framework. Our approach uses a more realistic
null model for the edge weight creation process than prior work. In particular,
it simultaneously considers the propensity of nodes to send and receive
connections, whereas previous approaches only considered nodes as emitters of
edges. We test our model with real world networks of different types (flows,
stocks, co-occurrences, directed, undirected) and show that our Noise-Corrected
approach returns backbones that outperform other approaches on a number of
criteria. Our approach is scalable, able to deal with networks with millions of
edges.
",1,1,0,0,0,0
16272,16273,Progress on Experiments towards LWFA-driven Transverse Gradient Undulator-Based FELs,"  Free Electron Lasers (FEL) are commonly regarded as the potential key
application of laser wakefield accelerators (LWFA). It has been found that
electron bunches exiting from state-of-the-art LWFAs exhibit a normalized
6-dimensional beam brightness comparable to those in conventional linear
accelerators. Effectively exploiting this beneficial beam property for
LWFA-based FELs is challenging due to the extreme initial conditions
particularly in terms of beam divergence and energy spread. Several different
approaches for capturing, reshaping and matching LWFA beams to suited
undulators, such as bunch decompression or transverse-gradient undulator
schemes, are currently being explored. In this article the transverse gradient
undulator concept will be discussed with a focus on recent experimental
achievements.
",0,1,0,0,0,0
2801,2802,Multi-modal Feedback for Affordance-driven Interactive Reinforcement Learning,"  Interactive reinforcement learning (IRL) extends traditional reinforcement
learning (RL) by allowing an agent to interact with parent-like trainers during
a task. In this paper, we present an IRL approach using dynamic audio-visual
input in terms of vocal commands and hand gestures as feedback. Our
architecture integrates multi-modal information to provide robust commands from
multiple sensory cues along with a confidence value indicating the
trustworthiness of the feedback. The integration process also considers the
case in which the two modalities convey incongruent information. Additionally,
we modulate the influence of sensory-driven feedback in the IRL task using
goal-oriented knowledge in terms of contextual affordances. We implement a
neural network architecture to predict the effect of performed actions with
different objects to avoid failed-states, i.e., states from which it is not
possible to accomplish the task. In our experimental setup, we explore the
interplay of multimodal feedback and task-specific affordances in a robot
cleaning scenario. We compare the learning performance of the agent under four
different conditions: traditional RL, multi-modal IRL, and each of these two
setups with the use of contextual affordances. Our experiments show that the
best performance is obtained by using audio-visual feedback with
affordancemodulated IRL. The obtained results demonstrate the importance of
multi-modal sensory processing integrated with goal-oriented knowledge in IRL
tasks.
",1,0,0,0,0,0
7262,7263,Using Stock Prices as Ground Truth in Sentiment Analysis to Generate Profitable Trading Signals,"  The increasing availability of ""big"" (large volume) social media data has
motivated a great deal of research in applying sentiment analysis to predict
the movement of prices within financial markets. Previous work in this field
investigates how the true sentiment of text (i.e. positive or negative
opinions) can be used for financial predictions, based on the assumption that
sentiments expressed online are representative of the true market sentiment.
Here we consider the converse idea, that using the stock price as the
ground-truth in the system may be a better indication of sentiment. Tweets are
labelled as Buy or Sell dependent on whether the stock price discussed rose or
fell over the following hour, and from this, stock-specific dictionaries are
built for individual companies. A Bayesian classifier is used to generate stock
predictions, which are input to an automated trading algorithm. Placing 468
trades over a 1 month period yields a return rate of 5.18%, which annualises to
approximately 83% per annum. This approach performs significantly better than
random chance and outperforms two baseline sentiment analysis methods tested.
",0,0,0,0,0,1
10567,10568,Convex-constrained Sparse Additive Modeling and Its Extensions,"  Sparse additive modeling is a class of effective methods for performing
high-dimensional nonparametric regression. In this work we show how shape
constraints such as convexity/concavity and their extensions, can be integrated
into additive models. The proposed sparse difference of convex additive models
(SDCAM) can estimate most continuous functions without any a priori smoothness
assumption. Motivated by a characterization of difference of convex functions,
our method incorporates a natural regularization functional to avoid
overfitting and to reduce model complexity. Computationally, we develop an
efficient backfitting algorithm with linear per-iteration complexity.
Experiments on both synthetic and real data verify that our method is
competitive against state-of-the-art sparse additive models, with improved
performance in most scenarios.
",1,0,0,1,0,0
18430,18431,A Computational Model of a Single-Photon Avalanche Diode Sensor for Transient Imaging,"  Single-Photon Avalanche Diodes (SPAD) are affordable photodetectors, capable
to collect extremely fast low-energy events, due to their single-photon
sensibility. This makes them very suitable for time-of-flight-based range
imaging systems, allowing to reduce costs and power requirements, without
sacrifizing much temporal resolution. In this work we describe a computational
model to simulate the behaviour of SPAD sensors, aiming to provide a realistic
camera model for time-resolved light transport simulation, with applications on
prototyping new reconstructions techniques based on SPAD time-of-flight data.
Our model accounts for the major effects of the sensor on the incoming signal.
We compare our model against real-world measurements, and apply it to a variety
of scenarios, including complex multiply-scattered light transport.
",1,1,0,0,0,0
18562,18563,Tight Bounds for Online Coloring of Basic Graph Classes,"  We resolve a number of long-standing open problems in online graph coloring.
More specifically, we develop tight lower bounds on the performance of online
algorithms for fundamental graph classes. An important contribution is that our
bounds also hold for randomized online algorithms, for which hardly any results
were known. Technically, we construct lower bounds for chordal graphs. The
constructions then allow us to derive results on the performance of randomized
online algorithms for the following further graph classes: trees, planar,
bipartite, inductive, bounded-treewidth and disk graphs. It shows that the best
competitive ratio of both deterministic and randomized online algorithms is
$\Theta(\log n)$, where $n$ is the number of vertices of a graph. Furthermore,
we prove that this guarantee cannot be improved if an online algorithm has a
lookahead of size $O(n/\log n)$ or access to a reordering buffer of size
$n^{1-\epsilon}$, for any $0<\epsilon\leq 1$. A consequence of our results is
that, for all of the above mentioned graph classes except bipartite graphs, the
natural $\textit{First Fit}$ coloring algorithm achieves an optimal
performance, up to constant factors, among deterministic and randomized online
algorithms.
",1,0,0,0,0,0
5633,5634,Exchange striction driven magnetodielectric effect and potential photovoltaic effect in polar CaOFeS,"  CaOFeS is a semiconducting oxysulfide with polar layered triangular
structure. Here a comprehensive theoretical study has been performed to reveal
its physical properties, including magnetism, electronic structure, phase
transition, magnetodielectric effect, as well as optical absorption. Our
calculations confirm the Ising-like G-type antiferromagnetic ground state
driven by the next-nearest neighbor exchanges, which breaks the trigonal
symmetry and is responsible for the magnetodielectric effect driven by exchange
striction. In addition, a large coefficient of visible light absorption is
predicted, which leads to promising photovoltaic effect with the maximum
light-to-electricity energy conversion efficiency up to 24.2%.
",0,1,0,0,0,0
11440,11441,Using Convex Optimization of Autocorrelation with Constrained Support and Windowing for Improved Phase Retrieval Accuracy,"  In imaging modalities recording diffraction data, the original image can be
reconstructed assuming known phases. When phases are unknown, oversampling and
a constraint on the support region in the original object can be used to solve
a non-convex optimization problem.
Such schemes are ill-suited to find the optimum solution for sparse data,
since the recorded image does not correspond exactly to the original wave
function. We construct a convex optimization problem using a relaxed support
constraint and a maximum-likelihood treatment of the recorded data as a sample
from the underlying wave function. We also stress the need to use relevant
windowing techniques to account for the sampled pattern being finite.
On simulated data, we demonstrate the benefits of our approach in terms of
visual quality and an improvement in the crystallographic R-factor from .4 to
.1 for highly noisy data.
",1,0,0,0,0,0
13573,13574,Out of sight out of mind: Perceived physical distance between the observer and someone in pain shapes observer's neural empathic reactions,"  Social and affective relations may shape empathy to others' affective states.
Previous studies also revealed that people tend to form very different mental
representations of stimuli on the basis of their physical distance. In this
regard, embodied cognition proposes that different physical distances between
individuals activate different interpersonal processing modes, such that close
physical distance tends to activate the interpersonal processing mode typical
of socially and affectively close relationships. In Experiment 1, two groups of
participants were administered a pain decision task involving upright and
inverted face stimuli painfully or neutrally stimulated, and we monitored their
neural empathic reactions by means of event-related potentials (ERPs)
technique. Crucially, participants were presented with face stimuli of one of
two possible sizes in order to manipulate retinal size and perceived physical
distance, roughly corresponding to the close and far portions of social
distance. ERPs modulations compatible with an empathic reaction were observed
only for the group exposed to face stimuli appearing to be at a close social
distance from the participants. This reaction was absent in the group exposed
to smaller stimuli corresponding to face stimuli observed from a far social
distance. In Experiment 2, one different group of participants was engaged in a
match-to-sample task involving the two-size upright face stimuli of Experiment
1 to test whether the modulation of neural empathic reaction observed in
Experiment 1 could be ascribable to differences in the ability to identify
faces of the two different sizes. Results suggested that face stimuli of the
two sizes could be equally identifiable. In line with the Construal Level and
Embodied Simulation theoretical frameworks, we conclude that perceived physical
distance may shape empathy as well as social and affective distance.
",0,0,0,0,1,0
12344,12345,Machine Learning for the Geosciences: Challenges and Opportunities,"  Geosciences is a field of great societal relevance that requires solutions to
several urgent problems facing our humanity and the planet. As geosciences
enters the era of big data, machine learning (ML) -- that has been widely
successful in commercial domains -- offers immense potential to contribute to
problems in geosciences. However, problems in geosciences have several unique
challenges that are seldom found in traditional applications, requiring novel
problem formulations and methodologies in machine learning. This article
introduces researchers in the machine learning (ML) community to these
challenges offered by geoscience problems and the opportunities that exist for
advancing both machine learning and geosciences. We first highlight typical
sources of geoscience data and describe their properties that make it
challenging to use traditional machine learning techniques. We then describe
some of the common categories of geoscience problems where machine learning can
play a role, and discuss some of the existing efforts and promising directions
for methodological development in machine learning. We conclude by discussing
some of the emerging research themes in machine learning that are applicable
across all problems in the geosciences, and the importance of a deep
collaboration between machine learning and geosciences for synergistic
advancements in both disciplines.
",1,1,0,0,0,0
6052,6053,Parametrization and Generation of Geological Models with Generative Adversarial Networks,"  One of the main challenges in the parametrization of geological models is the
ability to capture complex geological structures often observed in subsurface
fields. In recent years, Generative Adversarial Networks (GAN) were proposed as
an efficient method for the generation and parametrization of complex data,
showing state-of-the-art performances in challenging computer vision tasks such
as reproducing natural images (handwritten digits, human faces, etc.). In this
work, we study the application of Wasserstein GAN for the parametrization of
geological models. The effectiveness of the method is assessed for uncertainty
propagation tasks using several test cases involving different permeability
patterns and subsurface flow problems. Results show that GANs are able to
generate samples that preserve the multipoint statistical features of the
geological models both visually and quantitatively. The generated samples
reproduce both the geological structures and the flow properties of the
reference data.
",0,1,0,1,0,0
7178,7179,A Data-Driven Analysis of the Influence of Care Coordination on Trauma Outcome,"  OBJECTIVE: To test the hypothesis that variation in care coordination is
related to LOS. DESIGN We applied a spectral co-clustering methodology to
simultaneously infer groups of patients and care coordination patterns, in the
form of interaction networks of health care professionals, from electronic
medical record (EMR) utilization data. The care coordination pattern for each
patient group was represented by standard social network characteristics and
its relationship with hospital LOS was assessed via a negative binomial
regression with a 95% confidence interval. SETTING AND PATIENTS This study
focuses on 5,588 adult patients hospitalized for trauma at the Vanderbilt
University Medical Center. The EMRs were accessed by healthcare professionals
from 179 operational areas during 158,467 operational actions. MAIN OUTCOME
MEASURES: Hospital LOS for trauma inpatients, as an indicator of care
coordination efficiency. RESULTS: Three general types of care coordination
patterns were discovered, each of which was affiliated with a specific patient
group. The first patient group exhibited the shortest hospital LOS and was
managed by a care coordination pattern that involved the smallest number of
operational areas (102 areas, as opposed to 125 and 138 for the other patient
groups), but exhibited the largest number of collaborations between operational
areas (e.g., an average of 27.1 connections per operational area compared to
22.5 and 23.3 for the other two groups). The hospital LOS for the second and
third patient groups was 14 hours (P = 0.024) and 10 hours (P = 0.042) longer
than the first patient group, respectively.
",1,0,0,0,0,0
12375,12376,Multigrid-based inversion for volumetric radar imaging with asteroid interior reconstruction as a potential application,"  This study concentrates on advancing mathematical and computational
methodology for radar tomography imaging in which the unknown volumetric
velocity distribution of a wave within a bounded domain is to be reconstructed.
Our goal is to enable effective simulation and inversion of a large amount of
full-wave data within a realistic 2D or 3D geometry. For propagating and
inverting the wave, we present a rigorous multigrid-based forward approach
which utilizes the finite-difference time-domain method and a nested finite
element grid structure. Based on the multigrid approach, we introduce and
validate a multiresolution algorithm which allows regularization of the unknown
distribution through a coarse-to-fine inversion scheme. In this approach,
sparse signals can be effectively inverted, as the coarse fluctuations are
reconstructed before the finer ones. Furthermore, the number of nonzero entries
in the system matrix can be compressed and thus the inversion procedure can be
speeded up. As a test scenario we investigate satellite-based asteroid interior
reconstruction. We use both full-wave and projected wave data and estimate the
accuracy of the inversion under different error sources: noise and positioning
inaccuracies. The results suggest that the present full-wave inversion approach
allows recovering the interior with a single satellite recording backscattering
data. It seems that robust results can be achieved, when the peak-to-peak
signal-to-noise ratio is above 10 dB. Furthermore, it seems that reconstructing
the deep interior can be enhanced if two satellites can be utilized in the
measurements.
",0,1,0,0,0,0
124,125,Stacking-based Deep Neural Network: Deep Analytic Network on Convolutional Spectral Histogram Features,"  Stacking-based deep neural network (S-DNN), in general, denotes a deep neural
network (DNN) resemblance in terms of its very deep, feedforward network
architecture. The typical S-DNN aggregates a variable number of individually
learnable modules in series to assemble a DNN-alike alternative to the targeted
object recognition tasks. This work likewise devises an S-DNN instantiation,
dubbed deep analytic network (DAN), on top of the spectral histogram (SH)
features. The DAN learning principle relies on ridge regression, and some key
DNN constituents, specifically, rectified linear unit, fine-tuning, and
normalization. The DAN aptitude is scrutinized on three repositories of varying
domains, including FERET (faces), MNIST (handwritten digits), and CIFAR10
(natural objects). The empirical results unveil that DAN escalates the SH
baseline performance over a sufficiently deep layer.
",1,0,0,0,0,0
14589,14590,"Dimension-free PAC-Bayesian bounds for matrices, vectors, and linear least squares regression","  This paper is focused on dimension-free PAC-Bayesian bounds, under weak
polynomial moment assumptions, allowing for heavy tailed sample distributions.
It covers the estimation of the mean of a vector or a matrix, with applications
to least squares linear regression. Special efforts are devoted to the
estimation of Gram matrices, due to their prominent role in high-dimension data
analysis.
",0,0,1,1,0,0
19892,19893,Theory of ground states for classical Heisenberg spin systems IV,"  We extend the theory of ground states of classical Heisenberg spin systems
previously published to the case where the interaction with an external
magnetic field is described by a Zeeman term. The ground state problem for the
Heisenberg-Zeeman Hamiltonian can be reduced first to the relative ground state
problem, and, in a second step, to the absolute ground state problem for pure
Heisenberg Hamiltonians depending on an additional Lagrange parameter. We
distinguish between continuous and discontinuous reduction. Moreover, there are
various general statements about Heisenberg-Zeeman systems that will be proven
under most general assumptions. One topic is the connection between the minimal
energy functions $E_{min}$ for the Heisenberg energy and $H_{min}$ for the
Heisenberg-Zeeman energy which turn out to be essentially mutual
Legendre-Fenchel transforms. This generalization of the traditional Legendre
transform is especially suited to cope with situations where the function
$E_{min}$ is not convex and consequently there is a magnetization jump at a
critical field. Another topic is magnetization and the occurrence of threshold
fields $B_{thr}$ and saturation fields $B_{sat}$, where we provide a general
formula for the latter. We suggest a distinction between ferromagnetic and
anti-ferromagnetic systems based on the vanishing of $B_{sat}$ for the former
ones. Parabolic systems are defined in such a way that $E_{min}$ and $H_{min}$
have a particularly simple form and studied in detail. For a large class of
parabolic systems the relative ground states can be constructed from the
absolute ground state by means of a so-called umbrella family. Finally we
provide a counter-example of a parabolic system where this construction is not
possible.
",0,1,0,0,0,0
19568,19569,On the Origin of Deep Learning,"  This paper is a review of the evolutionary history of deep learning models.
It covers from the genesis of neural networks when associationism modeling of
the brain is studied, to the models that dominate the last decade of research
in deep learning like convolutional neural networks, deep belief networks, and
recurrent neural networks. In addition to a review of these models, this paper
primarily focuses on the precedents of the models above, examining how the
initial ideas are assembled to construct the early models and how these
preliminary models are developed into their current forms. Many of these
evolutionary paths last more than half a century and have a diversity of
directions. For example, CNN is built on prior knowledge of biological vision
system; DBN is evolved from a trade-off of modeling power and computation
complexity of graphical models and many nowadays models are neural counterparts
of ancient linear models. This paper reviews these evolutionary paths and
offers a concise thought flow of how these models are developed, and aims to
provide a thorough background for deep learning. More importantly, along with
the path, this paper summarizes the gist behind these milestones and proposes
many directions to guide the future research of deep learning.
",1,0,0,1,0,0
20824,20825,"Deep Learning Scaling is Predictable, Empirically","  Deep learning (DL) creates impactful advances following a virtuous recipe:
model architecture search, creating large training data sets, and scaling
computation. It is widely believed that growing training sets and models should
improve accuracy and result in better products. As DL application domains grow,
we would like a deeper understanding of the relationships between training set
size, computational scale, and model accuracy improvements to advance the
state-of-the-art.
This paper presents a large scale empirical characterization of
generalization error and model size growth as training sets grow. We introduce
a methodology for this measurement and test four machine learning domains:
machine translation, language modeling, image processing, and speech
recognition. Our empirical results show power-law generalization error scaling
across a breadth of factors, resulting in power-law exponents---the ""steepness""
of the learning curve---yet to be explained by theoretical work. Further, model
improvements only shift the error but do not appear to affect the power-law
exponent. We also show that model size scales sublinearly with data size. These
scaling relationships have significant implications on deep learning research,
practice, and systems. They can assist model debugging, setting accuracy
targets, and decisions about data set growth. They can also guide computing
system design and underscore the importance of continued computational scaling.
",1,0,0,1,0,0
10140,10141,Nonzero positive solutions of a multi-parameter elliptic system with functional BCs,"  We prove, by topological methods, new results on the existence of nonzero
positive weak solutions for a class of multi-parameter second order elliptic
systems subject to functional boundary conditions. The setting is fairly
general and covers the case of multi-point, integral and nonlinear boundary
conditions. We also present a non-existence result. We provide some examples to
illustrate the applicability our theoretical results.
",0,0,1,0,0,0
9120,9121,Classification of finite W-groups,"  We determine the structure of the W-group $\mathcal{G}_F$, the small Galois
quotient of the absolute Galois group $G_F$ of the Pythagorean formally real
field $F$ when the space of orderings $X_F$ has finite order. Based on
Marshall's work (1979), we reduce the structure of $\mathcal{G}_F$ to that of
$\mathcal{G}_{\bar{F}}$, the W-group of the residue field $\bar{F}$ when $X_F$
is a connected space. In the disconnected case, the structure of
$\mathcal{G}_F$ is the free product of the W-groups $\mathcal{G}_{F_i}$
corresponding to the connected components $X_i$ of $X_F$. We also give a
completely Galois theoretic proof for Marshall's Basic Lemma.
",0,0,1,0,0,0
16187,16188,Individualized Risk Prognosis for Critical Care Patients: A Multi-task Gaussian Process Model,"  We report the development and validation of a data-driven real-time risk
score that provides timely assessments for the clinical acuity of ward patients
based on their temporal lab tests and vital signs, which allows for timely
intensive care unit (ICU) admissions. Unlike the existing risk scoring
technologies, the proposed score is individualized; it uses the electronic
health record (EHR) data to cluster the patients based on their static
covariates into subcohorts of similar patients, and then learns a separate
temporal, non-stationary multi-task Gaussian Process (GP) model that captures
the physiology of every subcohort. Experiments conducted on data from a
heterogeneous cohort of 6,094 patients admitted to the Ronald Reagan UCLA
medical center show that our risk score significantly outperforms the
state-of-the-art risk scoring technologies, such as the Rothman index and MEWS,
in terms of timeliness, true positive rate (TPR), and positive predictive value
(PPV). In particular, the proposed score increases the AUC with 20% and 38% as
compared to Rothman index and MEWS respectively, and can predict ICU admissions
8 hours before clinicians at a PPV of 35% and a TPR of 50%. Moreover, we show
that the proposed risk score allows for better decisions on when to discharge
clinically stable patients from the ward, thereby improving the efficiency of
hospital resource utilization.
",1,0,0,0,0,0
1165,1166,Spinor analysis,"  ""Let us call the novel quantities which, in addition to the vectors and
tensors, have appeared in the quantum mechanics of the spinning electron, and
which in the case of the Lorentz group are quite differently transformed from
tensors, as spinors for short. Is there no spinor analysis that every physicist
can learn, such as tensor analysis, and with the aid of which all the possible
spinors can be formed, and secondly, all the invariant equations in which
spinors occur?"" So Mr Ehrenfest asked me and the answer will be given below.
",0,1,0,0,0,0
1957,1958,Inference For High-Dimensional Split-Plot-Designs: A Unified Approach for Small to Large Numbers of Factor Levels,"  Statisticians increasingly face the problem to reconsider the adaptability of
classical inference techniques. In particular, divers types of high-dimensional
data structures are observed in various research areas; disclosing the
boundaries of conventional multivariate data analysis. Such situations occur,
e.g., frequently in life sciences whenever it is easier or cheaper to
repeatedly generate a large number $d$ of observations per subject than
recruiting many, say $N$, subjects. In this paper we discuss inference
procedures for such situations in general heteroscedastic split-plot designs
with $a$ independent groups of repeated measurements. These will, e.g., be able
to answer questions about the occurrence of certain time, group and
interactions effects or about particular profiles.
The test procedures are based on standardized quadratic forms involving
suitably symmetrized U-statistics-type estimators which are robust against an
increasing number of dimensions $d$ and/or groups $a$. We then discuss its
limit distributions in a general asymptotic framework and additionally propose
improved small sample approximations. Finally its small sample performance is
investigated in simulations and the applicability is illustrated by a real data
analysis.
",0,0,1,1,0,0
17363,17364,Line bundles defined by the Schwarz function,"  Cauchy and exponential transforms are characterized, and constructed, as
canonical holomorphic sections of certain line bundles on the Riemann sphere
defined in terms of the Schwarz function. A well known natural connection
between Schwarz reflection and line bundles defined on the Schottky double of a
planar domain is briefly discussed in the same context.
",0,0,1,0,0,0
2942,2943,On the optimal design of grid-based binary holograms for matter wave lithography,"  Grid based binary holography (GBH) is an attractive method for patterning
with light or matter waves. It is an approximate technique in which different
holographic masks can be used to produce similar patterns. Here we present an
optimal design method for GBH masks that allows for freely selecting the
fraction of open holes in the mask from below 10% to above 90%. Open-fraction
is an important design parameter when making masks for use in lithography
systems. The method also includes a rescaling feature that potentially enables
a better contrast of the generated patterns. Through simulations we investigate
the contrast and robustness of the patterns formed by masks generated by the
proposed optimal design method. It is demonstrated that high contrast patterns
are achievable for a wide range of open-fractions. We conclude that reaching a
desired open-fraction is a trade-off with the contrast of the pattern generated
by the mask.
",0,1,0,0,0,0
14041,14042,Configurational forces in electronic structure calculations using Kohn-Sham density functional theory,"  We derive the expressions for configurational forces in Kohn-Sham density
functional theory, which correspond to the generalized variational force
computed as the derivative of the Kohn-Sham energy functional with respect to
the position of a material point $\textbf{x}$. These configurational forces
that result from the inner variations of the Kohn-Sham energy functional
provide a unified framework to compute atomic forces as well as stress tensor
for geometry optimization. Importantly, owing to the variational nature of the
formulation, these configurational forces inherently account for the Pulay
corrections. The formulation presented in this work treats both pseudopotential
and all-electron calculations in single framework, and employs a local
variational real-space formulation of Kohn-Sham DFT expressed in terms of the
non-orthogonal wavefunctions that is amenable to reduced-order scaling
techniques. We demonstrate the accuracy and performance of the proposed
configurational force approach on benchmark all-electron and pseudopotential
calculations conducted using higher-order finite-element discretization. To
this end, we examine the rates of convergence of the finite-element
discretization in the computed forces and stresses for various materials
systems, and, further, verify the accuracy from finite-differencing the energy.
Wherever applicable, we also compare the forces and stresses with those
obtained from Kohn-Sham DFT calculations employing plane-wave basis
(pseudopotential calculations) and Gaussian basis (all-electron calculations).
Finally, we verify the accuracy of the forces on large materials systems
involving a metallic aluminum nanocluster containing 666 atoms and an alkane
chain containing 902 atoms, where the Kohn-Sham electronic ground state is
computed using a reduced-order scaling subspace projection technique (P.
Motamarri and V. Gavini, Phys. Rev. B 90, 115127).
",0,1,0,0,0,0
12918,12919,Spherical CNNs,"  Convolutional Neural Networks (CNNs) have become the method of choice for
learning problems involving 2D planar images. However, a number of problems of
recent interest have created a demand for models that can analyze spherical
images. Examples include omnidirectional vision for drones, robots, and
autonomous cars, molecular regression problems, and global weather and climate
modelling. A naive application of convolutional networks to a planar projection
of the spherical signal is destined to fail, because the space-varying
distortions introduced by such a projection will make translational weight
sharing ineffective.
In this paper we introduce the building blocks for constructing spherical
CNNs. We propose a definition for the spherical cross-correlation that is both
expressive and rotation-equivariant. The spherical correlation satisfies a
generalized Fourier theorem, which allows us to compute it efficiently using a
generalized (non-commutative) Fast Fourier Transform (FFT) algorithm. We
demonstrate the computational efficiency, numerical accuracy, and effectiveness
of spherical CNNs applied to 3D model recognition and atomization energy
regression.
",0,0,0,1,0,0
14547,14548,Generalization of the concepts of seniority number and ionicity,"  We present generalized versions of the concepts of seniority number and
ionicity. These generalized numbers count respectively the partially occupied
and fully occupied shells for any partition of the orbital space into shells.
The Hermitian operators whose eigenspaces correspond to wave functions of
definite generalized seniority or ionicity values are introduced. The
generalized seniority numbers (GSNs) afford to establish refined hierarchies of
configuration interaction (CI) spaces within those of fixed ordinary seniority.
Such a hierarchy is illustrated on the buckminsterfullerene molecule.
",0,1,0,0,0,0
12459,12460,An optimization method to simultaneously estimate electrophysiology and connectivity in a model central pattern generator,"  Central pattern generators (CPGs) appear to have evolved multiple times
throughout the animal kingdom, indicating that their design imparts a
significant evolutionary advantage. Insight into how this design is achieved is
hindered by the difficulty inherent in examining relationships among
electrophysiological properties of the constituent cells of a CPG and their
functional connectivity. That is: experimentally it is challenging to estimate
the values of more than two or three of these properties simultaneously. We
employ a method of statistical data assimilation (D.A.) to estimate the
synaptic weights, synaptic reversal potentials, and maximum conductances of ion
channels of the constituent neurons in a multi-modal network model. We then use
these estimates to predict the functional mode of activity that the network is
expressing. The measurements used are the membrane voltage time series of all
neurons in the circuit. We find that these measurements provide sufficient
information to yield accurate predictions of the network's associated
electrical activity. This experiment can apply directly in a real laboratory
using intracellular recordings from a known biological CPG whose structural
mapping is known, and which can be completely isolated from the animal. The
simulated results in this paper suggest that D.A. might provide a tool for
simultaneously estimating tens to hundreds of CPG properties, thereby offering
the opportunity to seek possible systematic relationships among these
properties and the emergent electrical activity.
",0,1,0,0,0,0
691,692,Recent Operation of the FNAL Magnetron $H^{-}$ Ion Source,"  This paper will detail changes in the operational paradigm of the Fermi
National Accelerator Laboratory (FNAL) magnetron $H^{-}$ ion source due to
upgrades in the accelerator system. Prior to November of 2012 the $H^{-}$ ions
for High Energy Physics (HEP) experiments were extracted at ~18 keV vertically
downward into a 90 degree bending magnet and accelerated through a
Cockcroft-Walton accelerating column to 750 keV. Following the upgrade in the
fall of 2012 the $H^{-}$ ions are now directly extracted from a magnetron at 35
keV and accelerated to 750 keV by a Radio Frequency Quadrupole (RFQ). This
change in extraction energy as well as the orientation of the ion source
required not only a redesign of the ion source, but an updated understanding of
its operation at these new values. Discussed in detail are the changes to the
ion source timing, arc discharge current, hydrogen gas pressure, and cesium
delivery system that were needed to maintain consistent operation at >99%
uptime for HEP, with an increased ion source lifetime of over 9 months.
",0,1,0,0,0,0
20206,20207,On a Possible Giant Impact Origin for the Colorado Plateau,"  It is proposed and substantiated that an extraterrestrial object of the
approximate size and mass of Planet Mars, impacting the Earth in an oblique
angle along an approximately NE-SW route (with respect to the current
orientation of the North America continent) around 750 million years ago (750
Ma), is likely to be the direct cause of a chain of events which led to the
rifting of the Rodinia supercontinent and the severing of the foundation of the
Colorado Plateau from its surrounding craton.
It is further argued that the impactor most likely originated as a rouge
exoplanet produced during one of the past crossings of our Solar System through
the Galactic spiral arms in its orbital motion around the center of the Milky
Way Galaxy. Recent work has shown that the sites of galactic spiral arms are
locations of density-wave collisionless shocks. The perturbations from such
shock are known lead to the formation of massive stars, which evolve quickly
and die as supernovae. The blastwaves from supernova explosions, in addition to
the collisionless shocks at the spiral arms, can perturb the orbits of the
streaming disk matter, occasionally producing rogue exoplanets that can reach
the inner confines of our Solar System. The similarity between the period of
spiral-arm crossings of our Solar System to the period of major extinction
events in the Phanerozoic Eon of the Earth's history, as well as to the period
of the supercontinent cycle (the so-called Wilson Cycle), indicates that the
global environment of the Milky Way Galaxy may have played a major role in
initiating Earth's past tectonic activities.
",0,1,0,0,0,0
3225,3226,Some exact Bradlow vortex solutions,"  We consider the Bradlow equation for vortices which was recently found by
Manton and find a two-parameter class of analytic solutions in closed form on
nontrivial geometries with non-constant curvature. The general solution to our
class of metrics is given by a hypergeometric function and the area of the
vortex domain by the Gaussian hypergeometric function.
",0,0,1,0,0,0
4754,4755,Understanding looping kinetics of a long polymer molecule in solution. Exact solution for delocalized sink model,"  The fundamental understanding of loop formation of long polymer chains in
solution has been an important thread of research for several theoretical and
experimental studies. Loop formations are important phenomenological parameters
in many important biological processes. Here we give a general method for
finding an exact analytical solution for the occurrence of looping of a long
polymer chains in solution modeled by using a Smoluchowski-like equation with a
delocalized sink. The average rate constant for the delocalized sink is
explicitly expressed in terms of the corresponding rate constants for localized
sinks with different initial conditions. Simple analytical expressions are
provided for average rate constant.
",0,1,0,0,0,0
6381,6382,Detecting Arbitrary Attacks Using Continuous Secured Side Information in Wireless Networks,"  This paper focuses on Byzantine attack detection for Gaussian two-hop one-way
relay network, where an amplify-and-forward relay may conduct Byzantine attacks
by forwarding altered symbols to the destination. For facilitating attack
detection, we utilize the openness of wireless medium to make the destination
observe some secured signals that are not attacked. Then, a detection scheme is
developed for the destination by using its secured observations to
statistically check other observations from the relay. On the other hand,
notice the Gaussian channel is continuous, which allows the possible Byzantine
attacks to be conducted within continuous alphabet(s). The existing work on
discrete channel is not applicable for investigating the performance of the
proposed scheme. The main contribution of this paper is to prove that if and
only if the wireless relay network satisfies a non-manipulable channel
condition, the proposed detection scheme achieves asymptotic errorless
performance against arbitrary attacks that allow the stochastic distributions
of altered symbols to vary arbitrarily and depend on each other. No pre-shared
secret or secret transmission is needed for the detection. Furthermore, we also
prove that the relay network is non-manipulable as long as all channel
coefficients are non-zero, which is not essential restrict for many practical
systems.
",1,0,0,0,0,0
4623,4624,Planning Hybrid Driving-Stepping Locomotion on Multiple Levels of Abstraction,"  Navigating in search and rescue environments is challenging, since a variety
of terrains has to be considered. Hybrid driving-stepping locomotion, as
provided by our robot Momaro, is a promising approach. Similar to other
locomotion methods, it incorporates many degrees of freedom---offering high
flexibility but making planning computationally expensive for larger
environments.
We propose a navigation planning method, which unifies different levels of
representation in a single planner. In the vicinity of the robot, it provides
plans with a fine resolution and a high robot state dimensionality. With
increasing distance from the robot, plans become coarser and the robot state
dimensionality decreases. We compensate this loss of information by enriching
coarser representations with additional semantics. Experiments show that the
proposed planner provides plans for large, challenging scenarios in feasible
time.
",1,0,0,0,0,0
18150,18151,Deep learning for extracting protein-protein interactions from biomedical literature,"  State-of-the-art methods for protein-protein interaction (PPI) extraction are
primarily feature-based or kernel-based by leveraging lexical and syntactic
information. But how to incorporate such knowledge in the recent deep learning
methods remains an open question. In this paper, we propose a multichannel
dependency-based convolutional neural network model (McDepCNN). It applies one
channel to the embedding vector of each word in the sentence, and another
channel to the embedding vector of the head of the corresponding word.
Therefore, the model can use richer information obtained from different
channels. Experiments on two public benchmarking datasets, AIMed and BioInfer,
demonstrate that McDepCNN compares favorably to the state-of-the-art
rich-feature and single-kernel based methods. In addition, McDepCNN achieves
24.4% relative improvement in F1-score over the state-of-the-art methods on
cross-corpus evaluation and 12% improvement in F1-score over kernel-based
methods on ""difficult"" instances. These results suggest that McDepCNN
generalizes more easily over different corpora, and is capable of capturing
long distance features in the sentences.
",1,0,0,0,0,0
16565,16566,Phase partitioning in a novel near equi-atomic AlCuFeMn alloy,"  A novel low cost, near equi-atomic alloy comprising of Al, Cu, Fe and Mn is
synthesized using arc-melting technique. The cast alloy possesses a dendritic
microstructure where the dendrites consist of disordered FCC and ordered FCC
phases. The inter-dendritic region is comprised of ordered FCC phase and
spinodally decomposed BCC phases. A Cu segregation is observed in the
inter-dendritic region while dendritic region is rich in Fe. The bulk hardness
of the alloy is ~ 380 HV, indicating significant yield strength.
",0,1,0,0,0,0
5127,5128,Converging Shock Flows for a Mie-Grüneisen Equation of State,"  Previous work has shown that the one-dimensional (1D) inviscid compressible
flow (Euler) equations admit a wide variety of scale-invariant solutions
(including the famous Noh, Sedov, and Guderley shock solutions) when the
included equation of state (EOS) closure model assumes a certain
scale-invariant form. However, this scale-invariant EOS class does not include
even simple models used for shock compression of crystalline solids, including
many broadly applicable representations of Mie-Grüneisen EOS. Intuitively,
this incompatibility naturally arises from the presence of multiple dimensional
scales in the Mie-Grüneisen EOS, which are otherwise absent from
scale-invariant models that feature only dimensionless parameters (such as the
adiabatic index in the ideal gas EOS). The current work extends previous
efforts intended to rectify this inconsistency, by using a scale-invariant EOS
model to approximate a Mie- Grüneisen EOS form. To this end, the adiabatic
bulk modulus for the Mie-Grüneisen EOS is constructed, and its key features
are used to motivate the selection of a scale-invariant approximation form. The
remaining surrogate model parameters are selected through enforcement of the
Rankine-Hugoniot jump conditions for an infinitely strong shock in a
Mie-Grüneisen material. Finally, the approximate EOS is used in conjunction
with the 1D inviscid Euler equations to calculate a semi-analytical,
Guderley-like imploding shock solution in a metal sphere, and to determine if
and when the solution may be valid for the underlying Mie-Grüneisen EOS.
",0,1,0,0,0,0
1366,1367,Learning to Acquire Information,"  We consider the problem of diagnosis where a set of simple observations are
used to infer a potentially complex hidden hypothesis. Finding the optimal
subset of observations is intractable in general, thus we focus on the problem
of active diagnosis, where the agent selects the next most-informative
observation based on the results of previous observations. We show that under
the assumption of uniform observation entropy, one can build an implication
model which directly predicts the outcome of the potential next observation
conditioned on the results of past observations, and selects the observation
with the maximum entropy. This approach enjoys reduced computation complexity
by bypassing the complicated hypothesis space, and can be trained on
observation data alone, learning how to query without knowledge of the hidden
hypothesis.
",1,0,0,1,0,0
4748,4749,In situ accretion of gaseous envelopes on to planetary cores embedded in evolving protoplanetary discs,"  The core accretion hypothesis posits that planets with significant gaseous
envelopes accreted them from their protoplanetary discs after the formation of
rocky/icy cores. Observations indicate that such exoplanets exist at a broad
range of orbital radii, but it is not known whether they accreted their
envelopes in situ, or originated elsewhere and migrated to their current
locations. We consider the evolution of solid cores embedded in evolving
viscous discs that undergo gaseous envelope accretion in situ with orbital
radii in the range $0.1-10\rm au$. Additionally, we determine the long-term
evolution of the planets that had no runaway gas accretion phase after disc
dispersal. We find: (i) Planets with $5 \rm M_{\oplus}$ cores never undergo
runaway accretion. The most massive envelope contained $2.8 \rm M_{\oplus}$
with the planet orbiting at $10 \rm au$. (ii) Accretion is more efficient onto
$10 \rm M_{\oplus}$ and $15 \rm M_{\oplus}$ cores. For orbital radii $a_{\rm p}
\ge 0.5 \rm au$, $15 \rm M_{\oplus}$ cores always experienced runaway gas
accretion. For $a_{\rm p} \ge 5 \rm au$, all but one of the $10 \rm M_{\oplus}$
cores experienced runaway gas accretion. No planets experienced runaway growth
at $a_{\rm p} = 0.1 \rm au$. (iii) We find that, after disc dispersal, planets
with significant gaseous envelopes cool and contract on Gyr time-scales, the
contraction time being sensitive to the opacity assumed. Our results indicate
that Hot Jupiters with core masses $\lesssim 15 \rm M_{\oplus}$ at $\lesssim
0.1 \rm au$ likely accreted their gaseous envelopes at larger distances and
migrated inwards. Consistently with the known exoplanet population,
Super-Earths and mini-Neptunes at small radii during the disc lifetime, accrete
only modest gaseous envelopes.
",0,1,0,0,0,0
19338,19339,Latent heterogeneous multilayer community detection,"  We propose a method for simultaneously detecting shared and unshared
communities in heterogeneous multilayer weighted and undirected networks. The
multilayer network is assumed to follow a generative probabilistic model that
takes into account the similarities and dissimilarities between the
communities. We make use of a variational Bayes approach for jointly inferring
the shared and unshared hidden communities from multilayer network
observations. We show the robustness of our approach compared to state-of-the
art algorithms in detecting disparate (shared and private) communities on
synthetic data as well as on real genome-wide fibroblast proliferation dataset.
",1,0,0,1,0,0
16978,16979,Spectroscopic study of the elusive globular cluster ESO452-SC11 and its surroundings,"  Globular clusters (GCs) are amongst the oldest objects in the Galaxy and play
a pivotal role in deciphering its early history. We present the first
spectroscopic study of the GC ESO452-SC11 using the AAOmega spectrograph at
medium resolution. Given the sparsity of this object and high degree of
foreground contamination due to its location toward the bulge, few details are
known for this cluster: there is no consensus of its age, metallicity, or its
association with the disk or bulge. We identify 5 members based on radial
velocity, metallicity, and position within the GC. Using spectral synthesis,
accurate abundances of Fe and several $\alpha$-, Fe-peak, neutron-capture
elements (Si,Ca,Ti,Cr,Co,Ni,Sr,Eu) were measured. Two of the 5 cluster
candidates are likely non-members, as they have deviant Fe abundances and
[$\alpha$/Fe] ratios. The mean radial velocity is 19$\pm$2 km s$^{-1}$ with a
low dispersion of 2.8$\pm$3.4 km s$^{-1}$, in line with its low mass. The mean
Fe-abundance from spectral fitting is $-0.88\pm0.03$, with a spread driven by
observational errors. The $\alpha$-elements of the GC candidates are marginally
lower than expected for the bulge at similar metallicities. As spectra of
hundreds of stars were collected in a 2 degree field around ESO452-SC11,
detailed abundances in the surrounding field were measured. Most non-members
have higher [$\alpha$/Fe] ratios, typical of the nearby bulge population. Stars
with measured Fe-peak abundances show a large scatter around Solar values,
though with large uncertainties. Our study provides the first systematic
measurement of Sr in a Galactic bulge GC. The Eu and Sr abundances of the GC
candidates are consistent with a disk or bulge association. Our calculations
place ESO452 on an elliptical orbit in the central 3 kpc of the bulge. We find
no evidence of extratidal stars in our data. (Abridged)
",0,1,0,0,0,0
14993,14994,Statistical study of auroral omega bands,"  The presence of very few statistical studies on auroral omega bands motivated
us to test-use a semi-automatic method for identifying large-scale undulations
of the diffuse aurora boundary and to investigate their occurrence. Five
identical all-sky cameras with overlapping fields of view provided data for 438
auroral omega-like structures over Fennoscandian Lapland from 1996 to 2007. The
results from this set of omega band events agree remarkably well with previous
observations of omega band occurrence in magnetic local time (MLT), lifetime,
location between the region 1 and 2 field-aligned currents, as well as current
density estimates. The average peak emission height of omega forms corresponds
to the estimated precipitation energies of a few keV, which experienced no
significant change during the events. Analysis of both local and global
magnetic indices demonstrates that omega bands are observed during substorm
expansion and recovery phases that are more intense than average substorm
expansion and recovery phases in the same region. The omega occurrence with
respect to the substorm expansion and recovery phases is in a very good
agreement with an earlier observed distribution of fast earthward flows in the
plasma sheet during expansion and recovery phases. These findings support the
theory that omegas are produced by fast earthward flows and auroral streamers,
despite the rarity of good conjugate observations.
",0,1,0,0,0,0
12077,12078,"Life in the ""Matrix"": Human Mobility Patterns in the Cyber Space","  With the wide adoption of the multi-community setting in many popular social
media platforms, the increasing user engagements across multiple online
communities warrant research attention. In this paper, we introduce a novel
analogy between the movements in the cyber space and the physical space. This
analogy implies a new way of studying human online activities by modelling the
activities across online communities in a similar fashion as the movements
among locations. First, we quantitatively validate the analogy by comparing
several important properties of human online activities and physical movements.
Our experiments reveal striking similarities between the cyber space and the
physical space. Next, inspired by the established methodology on human mobility
in the physical space, we propose a framework to study human ""mobility"" across
online platforms. We discover three interesting patterns of user engagements in
online communities. Furthermore, our experiments indicate that people with
different mobility patterns also exhibit divergent preferences to online
communities. This work not only attempts to achieve a better understanding of
human online activities, but also intends to open a promising research
direction with rich implications and applications.
",1,0,0,0,0,0
14594,14595,Hamiltonian Monte-Carlo for Orthogonal Matrices,"  We consider the problem of sampling from posterior distributions for Bayesian
models where some parameters are restricted to be orthogonal matrices. Such
matrices are sometimes used in neural networks models for reasons of
regularization and stabilization of training procedures, and also can
parameterize matrices of bounded rank, positive-definite matrices and others.
In \citet{byrne2013geodesic} authors have already considered sampling from
distributions over manifolds using exact geodesic flows in a scheme similar to
Hamiltonian Monte Carlo (HMC). We propose new sampling scheme for a set of
orthogonal matrices that is based on the same approach, uses ideas of
Riemannian optimization and does not require exact computation of geodesic
flows. The method is theoretically justified by proof of symplecticity for the
proposed iteration. In experiments we show that the new scheme is comparable or
faster in time per iteration and more sample-efficient comparing to
conventional HMC with explicit orthogonal parameterization and Geodesic
Monte-Carlo. We also provide promising results of Bayesian ensembling for
orthogonal neural networks and low-rank matrix factorization.
",1,0,0,1,0,0
10787,10788,Change-point inference on volatility in noisy Itô semimartingales,"  This work is concerned with tests on structural breaks in the spot volatility
process of a general Itô semimartingale based on discrete observations
contaminated with i.i.d. microstructure noise. We construct a consistent test
building up on infill asymptotic results for certain functionals of spectral
spot volatility estimates. A weak limit theorem is established under the null
hypothesis relying on extreme value theory. We prove consistency of the test
and of an associated estimator for the change point. A simulation study
illustrates the finite-sample performance of the method and efficiency gains
compared to a skip-sampling approach.
",0,0,1,1,0,0
15594,15595,Current-driven skyrmion dynamics in disordered films,"  A theoretical study of the current-driven dynamics of magnetic skyrmions in
disordered perpendicularly-magnetized ultrathin films is presented. The
disorder is simulated as a granular structure in which the local anisotropy
varies randomly from grain to grain. The skyrmion velocity is computed for
different disorder parameters and ensembles. Similar behavior is seen for
spin-torques due to in-plane currents and the spin Hall effect, where a pinning
regime can be identified at low currents with a transition towards the
disorder-free case at higher currents, similar to domain wall motion in
disordered films. Moreover, a current-dependent skyrmion Hall effect and
fluctuations in the core radius are found, which result from the interaction
with the pinning potential.
",0,1,0,0,0,0
899,900,Faster Clustering via Non-Backtracking Random Walks,"  This paper presents VEC-NBT, a variation on the unsupervised graph clustering
technique VEC, which improves upon the performance of the original algorithm
significantly for sparse graphs. VEC employs a novel application of the
state-of-the-art word2vec model to embed a graph in Euclidean space via random
walks on the nodes of the graph. In VEC-NBT, we modify the original algorithm
to use a non-backtracking random walk instead of the normal backtracking random
walk used in VEC. We introduce a modification to a non-backtracking random
walk, which we call a begrudgingly-backtracking random walk, and show
empirically that using this model of random walks for VEC-NBT requires shorter
walks on the graph to obtain results with comparable or greater accuracy than
VEC, especially for sparser graphs.
",1,0,0,1,0,0
11313,11314,Weight hierarchy of a class of linear codes relating to non-degenerate quadratic forms,"  In this paper, we discuss the generalized Hamming weights of a class of
linear codes associated with non-degenerate quadratic forms. In order to do so,
we study the quadratic forms over subspaces of finite field and obtain some
interesting results about subspaces and their dual spaces. On this basis, we
solve all the generalized Hamming weights of these linear codes.
",0,0,1,0,0,0
19803,19804,Substrate inhibition imposes fitness penalty at high protein stability,"  Proteins are only moderately stable. It has long been debated whether this
narrow range of stabilities is solely a result of neutral drift towards lower
stability or purifying selection against excess stability is also at work - for
which no experimental evidence was found so far. Here we show that mutations
outside the active site in the essential E. coli enzyme adenylate kinase result
in stability-dependent increase in substrate inhibition by AMP, thereby
impairing overall enzyme activity at high stability. Such inhibition caused
substantial fitness defects not only in the presence of excess substrate but
also under physiological conditions. In the latter case, substrate inhibition
caused differential accumulation of AMP in the stationary phase for the
inhibition prone mutants. Further, we show that changes in flux through Adk
could accurately describe the variation in fitness effects. Taken together,
these data suggest that selection against substrate inhibition and hence excess
stability may have resulted in a narrow range of optimal stability observed for
modern proteins.
",0,0,0,0,1,0
6722,6723,Artificial Intelligence Assisted Power Grid Hardening in Response to Extreme Weather Events,"  In this paper, an artificial intelligence based grid hardening model is
proposed with the objective of improving power grid resilience in response to
extreme weather events. At first, a machine learning model is proposed to
predict the component states (either operational or outage) in response to the
extreme event. Then, these predictions are fed into a hardening model, which
determines strategic locations for placement of distributed generation (DG)
units. In contrast to existing literature in hardening and resilience
enhancement, this paper co-optimizes grid economic and resilience objectives by
considering the intricate dependencies of the two. The numerical simulations on
the standard IEEE 118-bus test system illustrate the merits and applicability
of the proposed hardening model. The results indicate that the proposed
hardening model through decentralized and distributed local energy resources
can produce a more robust solution that can protect the system significantly
against multiple component outages due to an extreme event.
",1,0,0,0,0,0
1102,1103,Software-based Microarchitectural Attacks,"  Modern processors are highly optimized systems where every single cycle of
computation time matters. Many optimizations depend on the data that is being
processed. Software-based microarchitectural attacks exploit effects of these
optimizations. Microarchitectural side-channel attacks leak secrets from
cryptographic computations, from general purpose computations, or from the
kernel. This leakage even persists across all common isolation boundaries, such
as processes, containers, and virtual machines. Microarchitectural fault
attacks exploit the physical imperfections of modern computer systems.
Shrinking process technology introduces effects between isolated hardware
elements that can be exploited by attackers to take control of the entire
system. These attacks are especially interesting in scenarios where the
attacker is unprivileged or even sandboxed.
In this thesis, we focus on microarchitectural attacks and defenses on
commodity systems. We investigate known and new side channels and show that
microarchitectural attacks can be fully automated. Furthermore, we show that
these attacks can be mounted in highly restricted environments such as
sandboxed JavaScript code in websites. We show that microarchitectural attacks
exist on any modern computer system, including mobile devices (e.g.,
smartphones), personal computers, and commercial cloud systems. This thesis
consists of two parts. In the first part, we provide background on modern
processor architectures and discuss state-of-the-art attacks and defenses in
the area of microarchitectural side-channel attacks and microarchitectural
fault attacks. In the second part, a selection of our papers are provided
without modification from their original publications. I have co-authored these
papers, which have subsequently been anonymously peer-reviewed, accepted, and
presented at renowned international conferences.
",1,0,0,0,0,0
1514,1515,Total variation regularization with variable Lebesgue prior,"  This work proposes the variable exponent Lebesgue modular as a replacement
for the 1-norm in total variation (TV) regularization. It allows the exponent
to vary with spatial location and thus enables users to locally select whether
to preserve edges or smooth intensity variations. In contrast to earlier work
using TV-like methods with variable exponents, the exponent function is here
computed offline as a fixed parameter of the final optimization problem,
resulting in a convex goal functional. The obtained formulas for the convex
conjugate and the proximal operators are simple in structure and can be
evaluated very efficiently, an important property for practical usability.
Numerical results with variable $L^p$ TV prior in denoising and tomography
problems on synthetic data compare favorably to total generalized variation
(TGV) and TV.
",0,0,1,0,0,0
19274,19275,Technical Report: Reactive Navigation in Partially Known Non-Convex Environments,"  This paper presents a provably correct method for robot navigation in 2D
environments cluttered with familiar but unexpected non-convex, star-shaped
obstacles as well as completely unknown, convex obstacles. We presuppose a
limited range onboard sensor, capable of recognizing, localizing and
(leveraging ideas from constructive solid geometry) generating online from its
catalogue of the familiar, non-convex shapes an implicit representation of each
one. These representations underlie an online change of coordinates to a
completely convex model planning space wherein a previously developed online
construction yields a provably correct reactive controller that is pulled back
to the physically sensed representation to generate the actual robot commands.
We extend the construction to differential drive robots, and suggest the
empirical utility of the proposed control architecture using both formal proofs
and numerical simulations.
",1,0,0,0,0,0
5843,5844,Evaluation of matrix factorisation approaches for muscle synergy extraction,"  The muscle synergy concept provides a widely-accepted paradigm to break down
the complexity of motor control. In order to identify the synergies, different
matrix factorisation techniques have been used in a repertoire of fields such
as prosthesis control and biomechanical and clinical studies. However, the
relevance of these matrix factorisation techniques is still open for discussion
since there is no ground truth for the underlying synergies. Here, we evaluate
factorisation techniques and investigate the factors that affect the quality of
estimated synergies. We compared commonly used matrix factorisation methods:
Principal component analysis (PCA), Independent component analysis (ICA),
Non-negative matrix factorization (NMF) and second-order blind identification
(SOBI). Publicly available real data were used to assess the synergies
extracted by each factorisation method in the classification of wrist
movements. Synthetic datasets were utilised to explore the effect of muscle
synergy sparsity, level of noise and number of channels on the extracted
synergies. Results suggest that the sparse synergy model and a higher number of
channels would result in better-estimated synergies. Without dimensionality
reduction, SOBI showed better results than other factorisation methods. This
suggests that SOBI would be an alternative when a limited number of electrodes
is available but its performance was still poor in that case. Otherwise, NMF
had the best performance when the number of channels was higher than the number
of synergies. Therefore, NMF would be the best method for muscle synergy
extraction.
",0,0,0,0,1,0
14077,14078,Identifying Harm Events in Clinical Care through Medical Narratives,"  Preventable medical errors are estimated to be among the leading causes of
injury and death in the United States. To prevent such errors, healthcare
systems have implemented patient safety and incident reporting systems. These
systems enable clinicians to report unsafe conditions and cases where patients
have been harmed due to errors in medical care. These reports are narratives in
natural language and while they provide detailed information about the
situation, it is non-trivial to perform large scale analysis for identifying
common causes of errors and harm to the patients. In this work, we present a
method based on attentive convolutional and recurrent networks for identifying
harm events in patient care and categorize the harm based on its severity
level. We demonstrate that our methods can significantly improve the
performance over existing methods in identifying harm in clinical care.
",1,0,0,0,0,0
20486,20487,Effects of Network Structure on the Performance of a Modeled Traffic Network under Drivers' Bounded Rationality,"  We propose a minority route choice game to investigate the effect of the
network structure on traffic network performance under the assumption of
drivers' bounded rationality. We investigate ring-and-hub topologies to capture
the nature of traffic networks in cities, and employ a minority game-based
inductive learning process to model the characteristic behavior under the route
choice scenario. Through numerical experiments, we find that topological
changes in traffic networks induce a phase transition from an uncongested phase
to a congested phase. Understanding this phase transition is helpful in
planning new traffic networks.
",1,1,0,0,0,0
1968,1969,A Las Vegas algorithm to solve the elliptic curve discrete logarithm problem,"  In this paper, we describe a new Las Vegas algorithm to solve the elliptic
curve discrete logarithm problem. The algorithm depends on a property of the
group of rational points of an elliptic curve and is thus not a generic
algorithm. The algorithm that we describe has some similarities with the most
powerful index-calculus algorithm for the discrete logarithm problem over a
finite field.
",1,0,1,0,0,0
6962,6963,Testing Network Structure Using Relations Between Small Subgraph Probabilities,"  We study the problem of testing for structure in networks using relations
between the observed frequencies of small subgraphs. We consider the statistics
\begin{align*} T_3 & =(\text{edge frequency})^3 - \text{triangle frequency}\\
T_2 & =3(\text{edge frequency})^2(1-\text{edge frequency}) - \text{V-shape
frequency} \end{align*} and prove a central limit theorem for $(T_2, T_3)$
under an Erdős-Rényi null model. We then analyze the power of the
associated $\chi^2$ test statistic under a general class of alternative models.
In particular, when the alternative is a $k$-community stochastic block model,
with $k$ unknown, the power of the test approaches one. Moreover, the
signal-to-noise ratio required is strictly weaker than that required for
community detection. We also study the relation with other statistics over
three-node subgraphs, and analyze the error under two natural algorithms for
sampling small subgraphs. Together, our results show how global structural
characteristics of networks can be inferred from local subgraph frequencies,
without requiring the global community structure to be explicitly estimated.
",1,0,1,1,0,0
9962,9963,Intelligent Notification Systems: A Survey of the State of the Art and Research Challenges,"  Notifications provide a unique mechanism for increasing the effectiveness of
real-time information delivery systems. However, notifications that demand
users' attention at inopportune moments are more likely to have adverse effects
and might become a cause of potential disruption rather than proving beneficial
to users. In order to address these challenges a variety of intelligent
notification mechanisms based on monitoring and learning users' behavior have
been proposed. The goal of such mechanisms is maximizing users' receptivity to
the delivered information by automatically inferring the right time and the
right context for sending a certain type of information.
This article provides an overview of the current state of the art in the area
of intelligent notification mechanisms that relies on the awareness of users'
context and preferences. More specifically, we first present a survey of
studies focusing on understanding and modeling users' interruptibility and
receptivity to notifications from desktops and mobile devices. Then, we discuss
the existing challenges and opportunities in developing mechanisms for
intelligent notification systems in a variety of application scenarios.
",1,0,0,0,0,0
5629,5630,3D ab initio modeling in cryo-EM by autocorrelation analysis,"  Single-Particle Reconstruction (SPR) in Cryo-Electron Microscopy (cryo-EM) is
the task of estimating the 3D structure of a molecule from a set of noisy 2D
projections, taken from unknown viewing directions. Many algorithms for SPR
start from an initial reference molecule, and alternate between refining the
estimated viewing angles given the molecule, and refining the molecule given
the viewing angles. This scheme is called iterative refinement. Reliance on an
initial, user-chosen reference introduces model bias, and poor initialization
can lead to slow convergence. Furthermore, since no ground truth is available
for an unsolved molecule, it is difficult to validate the obtained results.
This creates the need for high quality ab initio models that can be quickly
obtained from experimental data with minimal priors, and which can also be used
for validation. We propose a procedure to obtain such an ab initio model
directly from raw data using Kam's autocorrelation method. Kam's method has
been known since 1980, but it leads to an underdetermined system, with missing
orthogonal matrices. Until now, this system has been solved only for special
cases, such as highly symmetric molecules or molecules for which a homologous
structure was already available. In this paper, we show that knowledge of just
two clean projections is sufficient to guarantee a unique solution to the
system. This system is solved by an optimization-based heuristic. For the first
time, we are then able to obtain a low-resolution ab initio model of an
asymmetric molecule directly from raw data, without 2D class averaging and
without tilting. Numerical results are presented on both synthetic and
experimental data.
",0,0,0,1,0,0
257,258,Learning to Succeed while Teaching to Fail: Privacy in Closed Machine Learning Systems,"  Security, privacy, and fairness have become critical in the era of data
science and machine learning. More and more we see that achieving universally
secure, private, and fair systems is practically impossible. We have seen for
example how generative adversarial networks can be used to learn about the
expected private training data; how the exploitation of additional data can
reveal private information in the original one; and how what looks like
unrelated features can teach us about each other. Confronted with this
challenge, in this paper we open a new line of research, where the security,
privacy, and fairness is learned and used in a closed environment. The goal is
to ensure that a given entity (e.g., the company or the government), trusted to
infer certain information with our data, is blocked from inferring protected
information from it. For example, a hospital might be allowed to produce
diagnosis on the patient (the positive task), without being able to infer the
gender of the subject (negative task). Similarly, a company can guarantee that
internally it is not using the provided data for any undesired task, an
important goal that is not contradicting the virtually impossible challenge of
blocking everybody from the undesired task. We design a system that learns to
succeed on the positive task while simultaneously fail at the negative one, and
illustrate this with challenging cases where the positive task is actually
harder than the negative one being blocked. Fairness, to the information in the
negative task, is often automatically obtained as a result of this proposed
approach. The particular framework and examples open the door to security,
privacy, and fairness in very important closed scenarios, ranging from private
data accumulation companies like social networks to law-enforcement and
hospitals.
",1,0,0,1,0,0
8427,8428,Representation theoretic realization of non-symmetric Macdonald polynomials at infinity,"  We study the nonsymmetric Macdonald polynomials specialized at infinity from
various points of view. First, we define a family of modules of the Iwahori
algebra whose characters are equal to the nonsymmetric Macdonald polynomials
specialized at infinity. Second, we show that these modules are isomorphic to
the dual spaces of sections of certain sheaves on the semi-infinite Schubert
varieties. Third, we prove that the global versions of these modules are
homologically dual to the level one affine Demazure modules.
",0,0,1,0,0,0
13295,13296,Magnetic droplet nucleation with homochiral Neel domain wall,"  We investigate the effect of the Dzyaloshinskii Moriya interaction (DMI) on
magnetic domain nucleation in a ferromagnetic thin film with perpendicular
magnetic anisotropy. We propose an extended droplet model to determine the
nucleation field as a function of the in-plane field. The model can explain the
experimentally observed nucleation in a CoNi microstrip with the interfacial
DMI. The results are also reproduced by micromagnetic simulation based on the
string model. The electrical measurement method proposed in this study can be
widely used to quantitatively determine the DMI energy density.
",0,1,0,0,0,0
19915,19916,Support Feature Machines,"  Support Vector Machines (SVMs) with various kernels have played dominant role
in machine learning for many years, finding numerous applications. Although
they have many attractive features interpretation of their solutions is quite
difficult, the use of a single kernel type may not be appropriate in all areas
of the input space, convergence problems for some kernels are not uncommon, the
standard quadratic programming solution has $O(m^3)$ time and $O(m^2)$ space
complexity for $m$ training patterns. Kernel methods work because they
implicitly provide new, useful features. Such features, derived from various
kernels and other vector transformations, may be used directly in any machine
learning algorithm, facilitating multiresolution, heterogeneous models of data.
Therefore Support Feature Machines (SFM) based on linear models in the extended
feature spaces, enabling control over selection of support features, give at
least as good results as any kernel-based SVMs, removing all problems related
to interpretation, scaling and convergence. This is demonstrated for a number
of benchmark datasets analyzed with linear discrimination, SVM, decision trees
and nearest neighbor methods.
",1,0,0,1,0,0
6324,6325,Partitioning the Outburst Energy of a Low Eddington Accretion Rate AGN at the Center of an Elliptical Galaxy: the Recent 12 Myr History of the Supermassive Black Hole in M87,"  M87, the active galaxy at the center of the Virgo cluster, is ideal for
studying the interaction of a supermassive black hole (SMBH) with a hot,
gas-rich environment. A deep Chandra observation of M87 exhibits an
approximately circular shock front (13 kpc radius, in projection) driven by the
expansion of the central cavity (filled by the SMBH with relativistic
radio-emitting plasma) with projected radius $\sim$1.9 kpc. We combine
constraints from X-ray and radio observations of M87 with a shock model to
derive the properties of the outburst that created the 13 kpc shock. Principal
constraints for the model are 1) the measured Mach number ($M$$\sim$1.2), 2)
the radius of the 13 kpc shock, and 3) the observed size of the central
cavity/bubble (the radio-bright cocoon) that serves as the piston to drive the
shock. We find an outburst of $\sim$5$\times$$10^{57}$ ergs that began about 12
Myr ago and lasted $\sim$2 Myr matches all the constraints. In this model,
$\sim$22% of the energy is carried by the shock as it expands. The remaining
$\sim$80% of the outburst energy is available to heat the core gas. More than
half the total outburst energy initially goes into the enthalpy of the central
bubble, the radio cocoon. As the buoyant bubble rises, much of its energy is
transferred to the ambient thermal gas. For an outburst repetition rate of
about 12 Myrs (the age of the outburst), 80% of the outburst energy is
sufficient to balance the radiative cooling.
",0,1,0,0,0,0
14439,14440,Uniform asymptotics as a stationary point approaches an endpoint,"  We obtain the rigorous uniform asymptotics of a particular integral where a
stationary point is close to an endpoint. There exists a general method
introduced by Bleistein for obtaining uniform asymptotics in this situation.
However, this method does not provide rigorous estimates for the error. Indeed,
the method of Bleistein starts with a change of variables, which implies that
the parameter governing how close the stationary point is to the endpoint
appears in several parts of the integrand, and this means that one cannot
obtain general error bounds. By adapting the above method to our particular
integral, we obtain rigorous uniform leading-order asymptotics. We also give a
rigorous derivation of the asymptotics to all orders of the same integral; the
novelty of this second approach is that it does not involve a global change of
variables.
",0,0,1,0,0,0
38,39,Suzaku Analysis of the Supernova Remnant G306.3-0.9 and the Gamma-ray View of Its Neighborhood,"  We present an investigation of the supernova remnant (SNR) G306.3$-$0.9 using
archival multi-wavelength data. The Suzaku spectra are well described by
two-component thermal plasma models: The soft component is in ionization
equilibrium and has a temperature $\sim$0.59 keV, while the hard component has
temperature $\sim$3.2 keV and ionization time-scale $\sim$$2.6\times10^{10}$
cm$^{-3}$ s. We clearly detected Fe K-shell line at energy of $\sim$6.5 keV
from this remnant. The overabundances of Si, S, Ar, Ca, and Fe confirm that the
X-ray emission has an ejecta origin. The centroid energy of the Fe-K line
supports that G306.3$-$0.9 is a remnant of a Type Ia supernova (SN) rather than
a core-collapse SN. The GeV gamma-ray emission from G306.3$-$0.9 and its
surrounding were analyzed using about 6 years of Fermi data. We report about
the non-detection of G306.3$-$0.9 and the detection of a new extended gamma-ray
source in the south-west of G306.3$-$0.9 with a significance of
$\sim$13$\sigma$. We discuss several scenarios for these results with the help
of data from other wavebands to understand the SNR and its neighborhood.
",0,1,0,0,0,0
19612,19613,Learning Latent Features with Pairwise Penalties in Matrix Completion,"  Low-rank matrix completion (MC) has achieved great success in many real-world
data applications. A latent feature model formulation is usually employed and,
to improve prediction performance, the similarities between latent variables
can be exploited by pairwise learning, e.g., the graph regularized matrix
factorization (GRMF) method. However, existing GRMF approaches often use a
squared L2 norm to measure the pairwise difference, which may be overly
influenced by dissimilar pairs and lead to inferior prediction. To fully
empower pairwise learning for matrix completion, we propose a general
optimization framework that allows a rich class of (non-)convex pairwise
penalty functions. A new and efficient algorithm is further developed to
uniformly solve the optimization problem, with a theoretical convergence
guarantee. In an important situation where the latent variables form a small
number of subgroups, its statistical guarantee is also fully characterized. In
particular, we theoretically characterize the complexity-regularized maximum
likelihood estimator, as a special case of our framework. It has a better error
bound when compared to the standard trace-norm regularized matrix completion.
We conduct extensive experiments on both synthetic and real datasets to
demonstrate the superior performance of this general framework.
",0,0,0,1,0,0
18056,18057,Asai cube L-functions and the local Langlands conjecture,"  Let $F$ be a non-archimedean locally compact field. We study a class of
Langlands-Shahidi pairs $({\bf H},{\bf L})$, consisting of a quasi-split
connected reductive group $\bf H$ over $F$ and a Levi subgroup $\bf L$ which is
closely related to a product of restriction of scalars of ${\rm GL}_1$'s or
${\rm GL}_2$'s. We prove the compatibility of the resulting local factors with
the Langlands correspondence. In particular, let $E$ be a cubic separable
extension of $F$. We consider a simply connected quasi-split semisimple group
$\bf H$ over $F$ of type $D_4$, with triality corresponding to $E$, and let
$\bf L$ be its Levi subgroup with derived group ${\rm Res}_{E/F} {\rm SL}_2$.
In this way we obtain Asai cube local factors attached to irreducible smooth
representations of ${\rm GL}_2(E)$; we prove that they are Weil-Deligne factors
obtained via the local Langlands correspondence for ${\rm GL}_2(E)$ and tensor
induction from $E$ to $F$. A consequence is that Asai cube $\gamma$- and
$\varepsilon$-factors become stable under twists by highly ramified characters.
",0,0,1,0,0,0
2582,2583,Cherlin's conjecture for almost simple groups of Lie rank 1,"  We prove Cherlin's conjecture, concerning binary primitive permutation
groups, for those groups with socle isomorphic to $\mathrm{PSL}_2(q)$,
${^2\mathrm{B}_2}(q)$, ${^2\mathrm{G}_2}(q)$ or $\mathrm{PSU}_3(q)$. Our method
uses the notion of a ""strongly non-binary action"".
",0,0,1,0,0,0
16541,16542,QAOA for Max-Cut requires hundreds of qubits for quantum speed-up,"  Computational quantum technologies are entering a new phase in which noisy
intermediate-scale quantum computers are available, but are still too small to
benefit from active error correction. Even with a finite coherence budget to
invest in quantum information processing, noisy devices with about 50 qubits
are expected to experimentally demonstrate quantum supremacy in the next few
years. Defined in terms of artificial tasks, current proposals for quantum
supremacy, even if successful, will not help to provide solutions to practical
problems. Instead, we believe that future users of quantum computers are
interested in actual applications and that noisy quantum devices may still
provide value by approximately solving hard combinatorial problems via hybrid
classical-quantum algorithms. To lower bound the size of quantum computers with
practical utility, we perform realistic simulations of the Quantum Approximate
Optimization Algorithm and conclude that quantum speedup will not be
attainable, at least for a representative combinatorial problem, until several
hundreds of qubits are available.
",1,0,0,0,0,0
16212,16213,Parametric Oscillatory Instability in a Fabry-Perot Cavity of the Einstein Telescope with different mirror's materials,"  We discuss the parametric oscillatory instability in a Fabry-Perot cavity of
the Einstein Telescope. Unstable combinations of elastic and optical modes for
two possible configurations of gravitational wave third-generation detector are
deduced. The results are compared with the results for gravita- tional wave
interferometers LIGO and LIGO Voyager.
",0,1,0,0,0,0
18296,18297,Mobility Transition at Grain Boundaries in Two-Step Sintered 8 mol% Yttria Stabilized Zirconia,"  Stagnation of grain growth is often attributed to impurity segregation.
Yttria-stabilized cubic zirconia does not evidence any segregation-induced
slowdown, as its grain growth obeys the parabolic law when the grain size
increases by more than one order of magnitude. However, lowering the
temperature below 1300 oC triggers an abrupt slowdown, constraining the average
grains to grow by less than 0.5 ${\mu}$m in 1000 h despite a relatively large
driving force imparted in the fine grains of ~0.5 ${\mu}$m. Yet isolated
pockets of abnormally large grains, along with pockets of abnormally small
grains, emerge in the same latter sample. Such microstructure bifurcation has
never been observed before, and can only be explained by an inhomogeneous
distribution of immobile four-grain junctions. The implications of these
findings for two-step sintering are discussed.
",0,1,0,0,0,0
17439,17440,Mitigating the Impact of Speech Recognition Errors on Chatbot using Sequence-to-Sequence Model,"  We apply sequence-to-sequence model to mitigate the impact of speech
recognition errors on open domain end-to-end dialog generation. We cast the
task as a domain adaptation problem where ASR transcriptions and original text
are in two different domains. In this paper, our proposed model includes two
individual encoders for each domain data and make their hidden states similar
to ensure the decoder predict the same dialog text. The method shows that the
sequence-to-sequence model can learn the ASR transcriptions and original text
pair having the same meaning and eliminate the speech recognition errors.
Experimental results on Cornell movie dialog dataset demonstrate that the
domain adaption system help the spoken dialog system generate more similar
responses with the original text answers.
",1,0,0,0,0,0
16799,16800,Variational Inference for Gaussian Process Models with Linear Complexity,"  Large-scale Gaussian process inference has long faced practical challenges
due to time and space complexity that is superlinear in dataset size. While
sparse variational Gaussian process models are capable of learning from
large-scale data, standard strategies for sparsifying the model can prevent the
approximation of complex functions. In this work, we propose a novel
variational Gaussian process model that decouples the representation of mean
and covariance functions in reproducing kernel Hilbert space. We show that this
new parametrization generalizes previous models. Furthermore, it yields a
variational inference problem that can be solved by stochastic gradient ascent
with time and space complexity that is only linear in the number of mean
function parameters, regardless of the choice of kernels, likelihoods, and
inducing points. This strategy makes the adoption of large-scale expressive
Gaussian process models possible. We run several experiments on regression
tasks and show that this decoupled approach greatly outperforms previous sparse
variational Gaussian process inference procedures.
",1,0,0,1,0,0
18740,18741,Beyond recursion operators,"  We briefly recall the history of the Nijenhuis torsion of (1,1)-tensors on
manifolds and of the lesser-known Haantjes torsion. We then show how the
Haantjes manifolds of Magri and the symplectic-Haantjes structures of Tempesta
and Tondo generalize the classical approach to integrable systems in the
bi-hamiltonian and symplectic-Nijenhuis formalisms, the sequence of powers of
the recursion operator being replaced by a family of commuting Haantjes
operators.
",0,0,1,0,0,0
1417,1418,"Second-generation p-values: improved rigor, reproducibility, & transparency in statistical analyses","  Verifying that a statistically significant result is scientifically
meaningful is not only good scientific practice, it is a natural way to control
the Type I error rate. Here we introduce a novel extension of the p-value - a
second-generation p-value - that formally accounts for scientific relevance and
leverages this natural Type I Error control. The approach relies on a
pre-specified interval null hypothesis that represents the collection of effect
sizes that are scientifically uninteresting or are practically null. The
second-generation p-value is the proportion of data-supported hypotheses that
are also null hypotheses. As such, second-generation p-values indicate when the
data are compatible with null hypotheses, or with alternative hypotheses, or
when the data are inconclusive. Moreover, second-generation p-values provide a
proper scientific adjustment for multiple comparisons and reduce false
discovery rates. This is an advance for environments rich in data, where
traditional p-value adjustments are needlessly punitive. Second-generation
p-values promote transparency, rigor and reproducibility of scientific results
by a priori specifying which candidate hypotheses are practically meaningful
and by providing a more reliable statistical summary of when the data are
compatible with alternative or null hypotheses.
",0,0,0,1,0,0
10936,10937,The Cramér-Rao inequality on singular statistical models I,"  We introduce the notion of the essential tangent bundle of a parametrized
measure model and the notion of reduced Fisher metric on a (possibly singular)
2-integrable measure model. Using these notions and a new characterization of
$k$-integrable parametrized measure models, we extend the Cramér-Rao
inequality to $2$-integrable (possibly singular) statistical models for general
$\varphi$-estimations, where $\varphi$ is a $V$-valued feature function and $V$
is a topological vector space. Thus we derive an intrinsic Cramér-Rao
inequality in the most general terms of parametric statistics.
",0,0,1,1,0,0
14936,14937,Graphene quantum dots prevent alpha-synucleinopathy in Parkinson's disease,"  While the emerging evidence indicates that the pathogenesis of Parkinson's
disease (PD) is strongly correlated to the accumulation of alpha-synuclein
({\alpha}-syn) aggregates, there has been no clinical success in
anti-aggregation agents for the disease to date. Here we show that graphene
quantum dots (GQDs) exhibit anti-amyloid activity via direct interaction with
{\alpha}-syn. Employing biophysical, biochemical, and cell-based assays as well
as molecular dynamics (MD) simulation, we find that GQDs have notable potency
in not only inhibiting fibrillization of {\alpha}-syn but also disaggregating
mature fibrils in a time-dependent manner. Remarkably, GQDs rescue neuronal
death and synaptic loss, reduce Lewy body (LB)/Lewy neurite (LN) formation,
ameliorate mitochondrial dysfunctions, and prevent neuron-to-neuron
transmission of {\alpha}-syn pathology induced by {\alpha}-syn preformed
fibrils (PFFs) in neurons. In addition, in vivo administration of GQDs protects
against {\alpha}-syn PFFs-induced loss of dopamine neurons, LB/LN pathology,
and behavioural deficits through the penetration of the blood-brain barrier
(BBB). The finding that GQDs function as an anti-aggregation agent provides a
promising novel therapeutic target for the treatment of PD and related
{\alpha}-synucleinopathies.
",0,1,0,0,0,0
20961,20962,Critical Percolation Without Fine Tuning on the Surface of a Topological Superconductor,"  We present numerical evidence that most two-dimensional surface states of a
bulk topological superconductor (TSC) sit at an integer quantum Hall plateau
transition. We study TSC surface states in class CI with quenched disorder.
Low-energy (finite-energy) surface states were expected to be critically
delocalized (Anderson localized). We confirm the low-energy picture, but find
instead that finite-energy states are also delocalized, with universal
statistics that are independent of the TSC winding number, and consistent with
the spin quantum Hall plateau transition (percolation).
",0,1,0,0,0,0
6282,6283,Existence and uniqueness of periodic solution of nth-order Equations with delay in Banach space having Fourier type,"  The aim of this work is to study the existence of a periodic solutions of
nth-order differential equations with delay d dt x(t) + d 2 dt 2 x(t) + d 3 dt
3 x(t) + ... + d n dt n x(t) = Ax(t) + L(xt) + f (t). Our approach is based on
the M-boundedness of linear operators, Fourier type, B s p,q-multipliers and
Besov spaces.
",0,0,1,0,0,0
13264,13265,Nonlocal Cauchy problems for wave equations and applications,"  In this paper, the existence, the uniqueness and estimates of solution to the
integral Cauchy problem for linear and nonlinear abstract wave equations are
proved. The equation includes a linear operator A defined in a Banach space E,
in which by choosing E and A we can obtain numerous classis of nonlocal initial
value problems for wave equations which occur in a wide variety of physical
systems.
",0,0,1,0,0,0
4248,4249,Reveal the Mantle and K-40 Components of Geoneutrinos with Liquid Scintillator Cherenkov Neutrino Detectors,"  In this article we present an idea of using liquid scintillator Cherenkov
neutrino detectors to detect the mantle and K-40 components of geoneutrinos.
Liquid scintillator Cherenkov detectors feature both energy and direction
measurement for charge particles. Geoneutrinos can be detected with the elastic
scattering process of neutrino and electron. With the directionality, the
dominant intrinsic background originated from solar neutrinos in common liquid
scintillator detectors can be suppressed. The mantle geoneutrinos can be
distinguished because they come mainly underneath. The K-40 geoneutrinos can
also be identified, if the detection threshold for direction measurement can be
lower than, for example, 0.8 MeV. According to our calculation, a moderate,
kilo-ton scale, detector can observe tens of candidates, and is a practical
start for an experiment.
",0,1,0,0,0,0
10132,10133,Feature Engineering for Predictive Modeling using Reinforcement Learning,"  Feature engineering is a crucial step in the process of predictive modeling.
It involves the transformation of given feature space, typically using
mathematical functions, with the objective of reducing the modeling error for a
given target. However, there is no well-defined basis for performing effective
feature engineering. It involves domain knowledge, intuition, and most of all,
a lengthy process of trial and error. The human attention involved in
overseeing this process significantly influences the cost of model generation.
We present a new framework to automate feature engineering. It is based on
performance driven exploration of a transformation graph, which systematically
and compactly enumerates the space of given options. A highly efficient
exploration strategy is derived through reinforcement learning on past
examples.
",1,0,0,1,0,0
2920,2921,Maximum Number of Modes of Gaussian Mixtures,"  Gaussian mixture models are widely used in Statistics. A fundamental aspect
of these distributions is the study of the local maxima of the density, or
modes. In particular, it is not known how many modes a mixture of $k$ Gaussians
in $d$ dimensions can have. We give a brief account of this problem's history.
Then, we give improved lower bounds and the first upper bound on the maximum
number of modes, provided it is finite.
",0,0,1,1,0,0
2102,2103,"Non-Euclidean geometry, nontrivial topology and quantum vacuum effects","  Space out of a topological defect of the Abrikosov-Nielsen-Olesen vortex type
is locally flat but non-Euclidean. If a spinor field is quantized in such a
space, then a variety of quantum effects is induced in the vacuum. Basing on
the continuum model for long-wavelength electronic excitations, originating in
the tight-binding approximation for the nearest neighbor interaction of atoms
in the crystal lattice, we consider quantum ground state effects in monolayer
structures warped into nanocones by a disclination; the nonzero size of the
disclination is taken into account, and a boundary condition at the edge of the
disclination is chosen to ensure self-adjointness of the Dirac-Weyl Hamiltonian
operator. In the case of carbon nanocones, we find circumstances when the
quantum ground state effects are independent of the boundary parameter and the
disclination size.
",0,1,0,0,0,0
3229,3230,Stability Enhanced Large-Margin Classifier Selection,"  Stability is an important aspect of a classification procedure because
unstable predictions can potentially reduce users' trust in a classification
system and also harm the reproducibility of scientific conclusions. The major
goal of our work is to introduce a novel concept of classification instability,
i.e., decision boundary instability (DBI), and incorporate it with the
generalization error (GE) as a standard for selecting the most accurate and
stable classifier. Specifically, we implement a two-stage algorithm: (i)
initially select a subset of classifiers whose estimated GEs are not
significantly different from the minimal estimated GE among all the candidate
classifiers; (ii) the optimal classifier is chosen as the one achieving the
minimal DBI among the subset selected in stage (i). This general selection
principle applies to both linear and nonlinear classifiers. Large-margin
classifiers are used as a prototypical example to illustrate the above idea.
Our selection method is shown to be consistent in the sense that the optimal
classifier simultaneously achieves the minimal GE and the minimal DBI. Various
simulations and real examples further demonstrate the advantage of our method
over several alternative approaches.
",0,0,0,1,0,0
6360,6361,Tuplemax Loss for Language Identification,"  In many scenarios of a language identification task, the user will specify a
small set of languages which he/she can speak instead of a large set of all
possible languages. We want to model such prior knowledge into the way we train
our neural networks, by replacing the commonly used softmax loss function with
a novel loss function named tuplemax loss. As a matter of fact, a typical
language identification system launched in North America has about 95% users
who could speak no more than two languages. Using the tuplemax loss, our system
achieved a 2.33% error rate, which is a relative 39.4% improvement over the
3.85% error rate of standard softmax loss method.
",1,0,0,0,0,0
10479,10480,The Vanishing viscosity limit for some symmetric flows,"  The focus of this paper is on the analysis of the boundary layer and the
associated vanishing viscosity limit for two classes of flows with symmetry,
namely, Plane-Parallel Channel Flows and Parallel Pipe Flows. We construct
explicit boundary layer correctors, which approximate the difference between
the Navier-Stokes and the Euler solutions. Using properties of these
correctors, we establish convergence of the Navier-Stokes solution to the Euler
solution as viscosity vanishes with optimal rates of convergence. In addition,
we investigate vorticity production on the boundary in the limit of vanishing
viscosity. Our work significantly extends prior work in the literature.
",0,0,1,0,0,0
6802,6803,Two-Stream 3D Convolutional Neural Network for Skeleton-Based Action Recognition,"  It remains a challenge to efficiently extract spatialtemporal information
from skeleton sequences for 3D human action recognition. Although most recent
action recognition methods are based on Recurrent Neural Networks which present
outstanding performance, one of the shortcomings of these methods is the
tendency to overemphasize the temporal information. Since 3D convolutional
neural network(3D CNN) is a powerful tool to simultaneously learn features from
both spatial and temporal dimensions through capturing the correlations between
three dimensional signals, this paper proposes a novel two-stream model using
3D CNN. To our best knowledge, this is the first application of 3D CNN in
skeleton-based action recognition. Our method consists of three stages. First,
skeleton joints are mapped into a 3D coordinate space and then encoding the
spatial and temporal information, respectively. Second, 3D CNN models are
seperately adopted to extract deep features from two streams. Third, to enhance
the ability of deep features to capture global relationships, we extend every
stream into multitemporal version. Extensive experiments on the SmartHome
dataset and the large-scale NTU RGB-D dataset demonstrate that our method
outperforms most of RNN-based methods, which verify the complementary property
between spatial and temporal information and the robustness to noise.
",1,0,0,0,0,0
11636,11637,Connectedness of the Balmer spectra of right bounded derived categories,"  By virtue of Balmer's celebrated theorem, the classification of thick tensor
ideals of a tensor triangulated category $\T$ is equivalent to the topological
structure of its Balmer spectrum $\spc \T$. Motivated by this theorem, we
discuss connectedness and noetherianity of the Balmer spectrum of a right
bounded derived category of finitely generated modules over a commutative ring.
",0,0,1,0,0,0
19513,19514,On a cross-diffusion system arising in image denosing,"  We study a generalization of a cross-diffusion problem deduced from a
nonlinear complex-variable diffusion model for signal and image denoising.
We prove the existence of weak solutions of the time-independent problem with
fidelity terms under mild conditions on the data problem. Then, we show that
this translates on the well-posedness of a quasi-steady state approximation of
the evolution problem, and also prove the existence of weak solutions of the
latter under more restrictive hypothesis.
We finally perform some numerical simulations for image denoising, comparing
the performance of the cross-diffusion model and its corresponding scalar
Perona-Malik equation.
",0,0,1,0,0,0
11713,11714,Calibrated Filtered Reduced Order Modeling,"  We propose a calibrated filtered reduced order model (CF-ROM) framework for
the numerical simulation of general nonlinear PDEs that are amenable to reduced
order modeling. The novel CF-ROM framework consists of two steps: (i) In the
first step, we use explicit ROM spatial filtering of the nonlinear PDE to
construct a filtered ROM. This filtered ROM is low-dimensional, but is not
closed (because of the nonlinearity in the given PDE). (ii) In the second step,
we use a calibration procedure to close the filtered ROM, i.e., to model the
interaction between the resolved and unresolved modes. To this end, we use a
linear or quadratic ansatz to model this interaction and close the filtered
ROM. To find the new coefficients in the closed filtered ROM, we solve an
optimization problem that minimizes the difference between the full order model
data and our ansatz. Although we use a fluid dynamics setting to illustrate how
to construct and use the CF-ROM framework, we emphasize that it is built on
general ideas of spatial filtering and optimization and is independent of
(restrictive) phenomenological arguments. Thus, the CF-ROM framework can be
applied to a wide variety of PDEs.
",0,1,1,0,0,0
12578,12579,A Constrained Conditional Likelihood Approach for Estimating the Means of Selected Populations,"  Given p independent normal populations, we consider the problem of estimating
the mean of those populations, that based on the observed data, give the
strongest signals. We explicitly condition on the ranking of the sample means,
and consider a constrained conditional maximum likelihood (CCMLE) approach,
avoiding the use of any priors and of any sparsity requirement between the
population means. Our results show that if the observed means are too close
together, we should in fact use the grand mean to estimate the mean of the
population with the larger sample mean. If they are separated by more than a
certain threshold, we should shrink the observed means towards each other. As
intuition suggests, it is only if the observed means are far apart that we
should conclude that the magnitude of separation and consequent ranking are not
due to chance. Unlike other methods, our approach does not need to pre-specify
the number of selected populations and the proposed CCMLE is able to perform
simultaneous inference. Our method, which is conceptually straightforward, can
be easily adapted to incorporate other selection criteria.
Selected populations, Maximum likelihood, Constrained MLE, Post-selection
inference
",0,0,0,1,0,0
16344,16345,Idempotents in Intersection of the Kernel and the Image of Locally Finite Derivations and $\mathcal E$-derivations,"  Let $K$ be a field of characteristic zero, $\mathcal A$ a $K$-algebra and
$\delta$ a $K$-derivation of $\mathcal A$ or $K$-$\mathcal E$-derivation of
$\mathcal A$ (i.e., $\delta=\operatorname{Id}_A-\phi$ for some $K$-algebra
endomorphism $\phi$ of $\mathcal A$). Motivated by the Idempotent conjecture
proposed in [Z4], we first show that for every idempotent $e$ lying in both the
kernel ${\mathcal A}^\delta$ and the image $\operatorname{Im}\delta \!:=\delta
({\mathcal A})$ of $\delta$, the principal ideal $(e)\subseteq
\operatorname{Im} \delta$ if $\delta$ is a locally finite $K$-derivation or a
locally nilpotent $K$-$\mathcal E$-derivation of $\mathcal A$; and $e{\mathcal
A}, {\mathcal A}e \subseteq \operatorname{Im} \delta$ if $\delta$ is a locally
finite $K$-$\mathcal E$-derivation of $\mathcal A$. Consequently, the
Idempotent conjecture holds for all locally finite $K$-derivations and all
locally nilpotent $K$-$\mathcal E$-derivations of $\mathcal A$. We then show
that $1_{\mathcal A} \in \operatorname{Im} \delta$, (if and) only if $\delta$
is surjective, which generalizes the same result [GN, W] for locally nilpotent
$K$-derivations of commutative $K$-algebras to locally finite $K$-derivations
and $K$-$\mathcal E$-derivations $\delta$ of all $K$-algebras $\mathcal A$.
",0,0,1,0,0,0
11530,11531,Computational Flows in Arithmetic,"  A computational flow is a pair consisting of a sequence of computational
problems of a certain sort and a sequence of computational reductions among
them. In this paper we will develop a theory for these computational flows and
we will use it to make a sound and complete interpretation for bounded theories
of arithmetic. This property helps us to decompose a first order arithmetical
proof to a sequence of computational reductions by which we can extract the
computational content of low complexity statements in some bounded theories of
arithmetic such as $I\Delta_0$, $T^k_n$, $I\Delta_0+EXP$ and $PRA$. In the last
section, by generalizing term-length flows to ordinal-length flows, we will
extend our investigation from bounded theories to strong unbounded ones such as
$I\Sigma_n$ and $PA+TI(\alpha)$ and we will capture their total $NP$ search
problems as a consequence.
",1,0,1,0,0,0
5856,5857,Effects of the Mach number on the evolution of vortex-surface fields in compressible Taylor--Green flows,"  We investigate the evolution of vortex-surface fields (VSFs) in compressible
Taylor--Green flows at Mach numbers ($Ma$) ranging from 0.5 to 2.0 using direct
numerical simulation. The formulation of VSFs in incompressible flows is
extended to compressible flows, and a mass-based renormalization of VSFs is
used to facilitate characterizing the evolution of a particular vortex surface.
The effects of the Mach number on the VSF evolution are different in three
stages. In the early stage, the jumps of the compressive velocity component
near shocklets generate sinks to contract surrounding vortex surfaces, which
shrink vortex volume and distort vortex surfaces. The subsequent reconnection
of vortex surfaces, quantified by the minimal distance between approaching
vortex surfaces and the exchange of vorticity fluxes, occurs earlier and has a
higher reconnection degree for larger $Ma$ owing to the dilatational
dissipation and shocklet-induced reconnection of vortex lines. In the late
stage, the positive dissipation rate and negative pressure work accelerate the
loss of kinetic energy and suppress vortex twisting with increasing $Ma$.
",0,1,0,0,0,0
11984,11985,A Game of Random Variables,"  This paper analyzes a simple game with $n$ players. We fix a mean, $\mu$, in
the interval $[0, 1]$ and let each player choose any random variable
distributed on that interval with the given mean. The winner of the zero-sum
game is the player whose random variable has the highest realization. We show
that the position of the mean within the interval is paramount. Remarkably, if
the given mean is above a crucial threshold then the unique equilibrium must
contain a point mass on $1$. The cutoff is strictly decreasing in the number of
players, $n$; and for fixed $\mu$, as the number of players is increased, each
player places more weight on $1$ at equilibrium. We characterize the
equilibrium as the number of players goes to infinity.
",1,0,0,0,0,0
13891,13892,Semidefinite tests for latent causal structures,"  Testing whether a probability distribution is compatible with a given
Bayesian network is a fundamental task in the field of causal inference, where
Bayesian networks model causal relations. Here we consider the class of causal
structures where all correlations between observed quantities are solely due to
the influence from latent variables. We show that each model of this type
imposes a certain signature on the observable covariance matrix in terms of a
particular decomposition into positive semidefinite components. This signature,
and thus the underlying hypothetical latent structure, can be tested in a
computationally efficient manner via semidefinite programming. This stands in
stark contrast with the algebraic geometric tools required if the full
observable probability distribution is taken into account. The semidefinite
test is compared with tests based on entropic inequalities.
",0,0,1,1,0,0
3170,3171,On Stein's Identity and Near-Optimal Estimation in High-dimensional Index Models,"  We consider estimating the parametric components of semi-parametric multiple
index models in a high-dimensional and non-Gaussian setting. Such models form a
rich class of non-linear models with applications to signal processing, machine
learning and statistics. Our estimators leverage the score function based first
and second-order Stein's identities and do not require the covariates to
satisfy Gaussian or elliptical symmetry assumptions common in the literature.
Moreover, to handle score functions and responses that are heavy-tailed, our
estimators are constructed via carefully thresholding their empirical
counterparts. We show that our estimator achieves near-optimal statistical rate
of convergence in several settings. We supplement our theoretical results via
simulation experiments that confirm the theory.
",0,0,1,1,0,0
16525,16526,Synthesis versus analysis in patch-based image priors,"  In global models/priors (for example, using wavelet frames), there is a well
known analysis vs synthesis dichotomy in the way signal/image priors are
formulated. In patch-based image models/priors, this dichotomy is also present
in the choice of how each patch is modeled. This paper shows that there is
another analysis vs synthesis dichotomy, in terms of how the whole image is
related to the patches, and that all existing patch-based formulations that
provide a global image prior belong to the analysis category. We then propose a
synthesis formulation, where the image is explicitly modeled as being
synthesized by additively combining a collection of independent patches. We
formally establish that these analysis and synthesis formulations are not
equivalent in general and that both formulations are compatible with analysis
and synthesis formulations at the patch level. Finally, we present an instance
of the alternating direction method of multipliers (ADMM) that can be used to
perform image denoising under the proposed synthesis formulation, showing its
computational feasibility. Rather than showing the superiority of the synthesis
or analysis formulations, the contributions of this paper is to establish the
existence of both alternatives, thus closing the corresponding gap in the field
of patch-based image processing.
",1,0,0,0,0,0
13446,13447,"General-purpose Tagging of Freesound Audio with AudioSet Labels: Task Description, Dataset, and Baseline","  This paper describes Task 2 of the DCASE 2018 Challenge, titled
""General-purpose audio tagging of Freesound content with AudioSet labels"". This
task was hosted on the Kaggle platform as ""Freesound General-Purpose Audio
Tagging Challenge"". The goal of the task is to build an audio tagging system
that can recognize the category of an audio clip from a subset of 41 diverse
categories drawn from the AudioSet Ontology. We present the task, the dataset
prepared for the competition, and a baseline system.
",1,0,0,1,0,0
14781,14782,Casimir-Polder force fluctuations as spatial probes of dissipation in metals,"  We study the spatial fluctuations of the Casimir-Polder force experienced by
an atom or a small sphere moved above a metallic plate at fixed separation
distance. We demonstrate that unlike the mean force, the magnitude of these
fluctuations crucially relies on the relaxation of conduction electron in the
metallic bulk, and even achieves values that differ by orders of magnitude
depending on the amount of dissipation. We also discover that fluctuations
suffer a spectacular decrease at large distances in the case of nonzero
temperature.
",0,1,0,0,0,0
7427,7428,A Study of Energy Trading in a Low-Voltage Network: Centralised and Distributed Approaches,"  Over the past years, distributed energy resources (DER) have been the object
of many studies, which recognise and establish their emerging role in the
future of power systems. However, the implementation of many scenarios and
mechanism are still challenging. This paper provides an overview of a local
energy market and explores the approaches in which consumers and prosumers take
part in this market. Therefore, the purpose of this paper is to review the
benefits of local markets for users. This study assesses the performance of
distributed and centralised trading mechanisms, comparing scenarios where the
objective of the exchange may be based on individual or social welfare.
Simulation results show the advantages of local markets and demonstrate the
importance of advancing the understanding of local markets.
",1,0,0,0,0,0
5752,5753,Uniformly Bounded Sets in Quasiperiodically Forced Dynamical Systems,"  This paper addresses structures of state space in quasiperiodically forced
dynamical systems. We develop a theory of ergodic partition of state space in a
class of measure-preserving and dissipative flows, which is a natural extension
of the existing theory for measure-preserving maps. The ergodic partition
result is based on eigenspace at eigenvalue 0 of the associated Koopman
operator, which is realized via time-averages of observables, and provides a
constructive way to visualize a low-dimensional slice through a
high-dimensional invariant set. We apply the result to the systems with a
finite number of attractors and show that the time-average of a continuous
observable is well-defined and reveals the invariant sets, namely, a finite
number of basins of attraction. We provide a characterization of invariant sets
in the quasiperiodically forced systems. A theorem on uniform boundedness of
the invariant sets is proved. The series of analytical results enables
numerical analysis of invariant sets in the quasiperiodically forced systems
based on the ergodic partition and time-averages. Using this, we analyze a
nonlinear model of complex power grids that represents the short-term swing
instability, named the coherent swing instability. We show that our analytical
results can be used to understand stability regions in such complex systems.
",1,0,0,0,0,0
12020,12021,Uniformization and Steinness,"  It is shown that the unit ball in ${\mathbb C}^n$ is the only complex
manifold that can universally cover both Stein and non-Stein strictly
pseudoconvex domains.
",0,0,1,0,0,0
10570,10571,Unoriented Cobordism Maps on Link Floer Homology,"  We study the problem of defining maps on link Floer homology induced by
unoriented link cobordisms. We provide a natural notion of link cobordism,
disoriented link cobordism, which tracks the motion of index zero and index
three critical points. Then we construct a map on unoriented link Floer
homology associated to a disoriented link cobordism. Furthermore, we give a
comparison with Oszváth-Stipsicz-Szabó's and Manolescu's constructions of
link cobordism maps for an unoriented band move.
",0,0,1,0,0,0
6992,6993,The exit time finite state projection scheme: bounding exit distributions and occupation measures of continuous-time Markov chains,"  We introduce the exit time finite state projection (ETFSP) scheme, a
truncation-based method that yields approximations to the exit distribution and
occupation measure associated with the time of exit from a domain (i.e., the
time of first passage to the complement of the domain) of time-homogeneous
continuous-time Markov chains. We prove that: (i) the computed approximations
bound the measures from below; (ii) the total variation distances between the
approximations and the measures decrease monotonically as states are added to
the truncation; and (iii) the scheme converges, in the sense that, as the
truncation tends to the entire state space, the total variation distances tend
to zero. Furthermore, we give a computable bound on the total variation
distance between the exit distribution and its approximation, and we delineate
the cases in which the bound is sharp. We also revisit the related finite state
projection scheme and give a comprehensive account of its theoretical
properties. We demonstrate the use of the ETFSP scheme by applying it to two
biological examples: the computation of the first passage time associated with
the expression of a gene, and the fixation times of competing species subject
to demographic noise.
",0,0,0,0,1,0
2825,2826,World Literature According to Wikipedia: Introduction to a DBpedia-Based Framework,"  Among the manifold takes on world literature, it is our goal to contribute to
the discussion from a digital point of view by analyzing the representation of
world literature in Wikipedia with its millions of articles in hundreds of
languages. As a preliminary, we introduce and compare three different
approaches to identify writers on Wikipedia using data from DBpedia, a
community project with the goal of extracting and providing structured
information from Wikipedia. Equipped with our basic set of writers, we analyze
how they are represented throughout the 15 biggest Wikipedia language versions.
We combine intrinsic measures (mostly examining the connectedness of articles)
with extrinsic ones (analyzing how often articles are frequented by readers)
and develop methods to evaluate our results. The better part of our findings
seems to convey a rather conservative, old-fashioned version of world
literature, but a version derived from reproducible facts revealing an implicit
literary canon based on the editing and reading behavior of millions of people.
While still having to solve some known issues, the introduced methods will help
us build an observatory of world literature to further investigate its
representativeness and biases.
",1,0,0,0,0,0
4123,4124,"Shannon's entropy and its Generalizations towards Statistics, Reliability and Information Science during 1948-2018","  Starting from the pioneering works of Shannon and Weiner in 1948, a plethora
of works have been reported on entropy in different directions. Entropy-related
review work in the direction of statistics, reliability and information
science, to the best of our knowledge, has not been reported so far. Here we
have tried to collect all possible works in this direction during the period
1948-2018 so that people interested in entropy, specially the new researchers,
get benefited.
",0,0,0,1,0,0
9448,9449,Derivatives pricing using signature payoffs,"  We introduce signature payoffs, a family of path-dependent derivatives that
are given in terms of the signature of the price path of the underlying asset.
We show that these derivatives are dense in the space of continuous payoffs, a
result that is exploited to quickly price arbitrary continuous payoffs. This
approach to pricing derivatives is then tested with European options, American
options, Asian options, lookback options and variance swaps. As we show,
signature payoffs can be used to price these derivatives with very high
accuracy.
",0,0,0,0,0,1
5949,5950,"Disordered statistical physics in low dimensions: extremes, glass transition, and localization","  This thesis presents original results in two domains of disordered
statistical physics: logarithmic correlated Random Energy Models (logREMs), and
localization transitions in long-range random matrices.
In the first part devoted to logREMs, we show how to characterise their
common properties and model--specific data. Then we develop their replica
symmetry breaking treatment, which leads to the freezing scenario of their free
energy distribution and the general description of their minima process, in
terms of decorated Poisson point process. We also report a series of new
applications of the Jack polynomials in the exact predictions of some
observables in the circular model and its variants. Finally, we present the
recent progress on the exact connection between logREMs and the Liouville
conformal field theory.
The goal of the second part is to introduce and study a new class of banded
random matrices, the broadly distributed class, which is characterid an
effective sparseness. We will first study a specific model of the class, the
Beta Banded random matrices, inspired by an exact mapping to a recently studied
statistical model of long--range first--passage percolation/epidemics dynamics.
Using analytical arguments based on the mapping and numerics, we show the
existence of localization transitions with mobility edges in the
""stretch--exponential"" parameter--regime of the statistical models. Then, using
a block--diagonalization renormalization approach, we argue that such
localization transitions occur generically in the broadly distributed class.
",0,1,0,0,0,0
16057,16058,The Continuity of the Gauge Fixing Condition $n\cdot\partial n\cdot A=0$ for $SU(2)$ Gauge Theory,"  The continuity of the gauge fixing condition $n\cdot\partial n\cdot A=0$ for
$SU(2)$ gauge theory on the manifold $R\bigotimes S^{1}\bigotimes
S^{1}\bigotimes S^{1}$ is studied here, where $n^{\mu}$ stands for directional
vector along $x_{i}$-axis($i=1,2,3$). It is proved that the gauge fixing
condition is continuous given that gauge potentials are differentiable with
continuous derivatives on the manifold $R\bigotimes S^{1}\bigotimes
S^{1}\bigotimes S^{1}$ which is compact.
",0,1,0,0,0,0
15261,15262,Estimation of the infinitesimal generator by square-root approximation,"  For the analysis of molecular processes, the estimation of time-scales, i.e.,
transition rates, is very important. Estimating the transition rates between
molecular conformations is -- from a mathematical point of view -- an invariant
subspace projection problem. A certain infinitesimal generator acting on
function space is projected to a low-dimensional rate matrix. This projection
can be performed in two steps. First, the infinitesimal generator is
discretized, then the invariant subspace is approxi-mated and used for the
subspace projection. In our approach, the discretization will be based on a
Voronoi tessellation of the conformational space. We will show that the
discretized infinitesimal generator can simply be approximated by the geometric
average of the Boltzmann weights of the Voronoi cells. Thus, there is a direct
correla-tion between the potential energy surface of molecular structures and
the transition rates of conformational changes. We present results for a
2d-diffusion process and Alanine dipeptide.
",0,1,0,0,0,0
8392,8393,Techniques for visualizing LSTMs applied to electrocardiograms,"  This paper explores four different visualization techniques for long
short-term memory (LSTM) networks applied to continuous-valued time series. On
the datasets analysed, we find that the best visualization technique is to
learn an input deletion mask that optimally reduces the true class score. With
a specific focus on single-lead electrocardiograms from the MIT-BIH arrhythmia
dataset, we show that salient input features for the LSTM classifier align well
with medical theory.
",1,0,0,1,0,0
19972,19973,$R$-triviality of some exceptional groups,"  The main aim of this paper is to prove $R$-triviality for simple, simply
connected algebraic groups with Tits index $E_{8,2}^{78}$ or $E_{7,1}^{78}$,
defined over a field $k$ of arbitrary characteristic. Let $G$ be such a group.
We prove that there exists a quadratic extension $K$ of $k$ such that $G$ is
$R$-trivial over $K$, i.e., for any extension $F$ of $K$, $G(F)/R=\{1\}$, where
$G(F)/R$ denotes the group of $R$-equivalence classes in $G(F)$, in the sense
of Manin (see \cite{M}). As a consequence, it follows that the variety $G$ is
retract $K$-rational and that the Kneser-Tits conjecture holds for these groups
over $K$. Moreover, $G(L)$ is projectively simple as an abstract group for any
field extension $L$ of $K$. In their monograph (\cite{TW}) J. Tits and Richard
Weiss conjectured that for an Albert division algebra $A$ over a field $k$, its
structure group $Str(A)$ is generated by scalar homotheties and its
$U$-operators. This is known to be equivalent to the Kneser-Tits conjecture for
groups with Tits index $E_{8,2}^{78}$. We settle this conjecture for Albert
division algebras which are first constructions, in affirmative. These results
are obtained as corollaries to the main result, which shows that if $A$ is an
Albert division algebra which is a first construction and $\Gamma$ its
structure group, i.e., the algebraic group of the norm similarities of $A$,
then $\Gamma(F)/R=\{1\}$ for any field extension $F$ of $k$, i.e., $\Gamma$ is
$R$-trivial.
",0,0,1,0,0,0
13803,13804,"Search for Food of Birds, Fish and Insects","  This book chapter introduces to the problem to which extent search strategies
of foraging biological organisms can be identified by statistical data analysis
and mathematical modeling. A famous paradigm in this field is the Levy Flight
Hypothesis: It states that under certain mathematical conditions Levy flights,
which are a key concept in the theory of anomalous stochastic processes,
provide an optimal search strategy. This hypothesis may be understood
biologically as the claim that Levy flights represent an evolutionary adaptive
optimal search strategy for foraging organisms. Another interpretation,
however, is that Levy flights emerge from the interaction between a forager and
a given (scale-free) distribution of food sources. These hypotheses are
discussed controversially in the current literature. We give examples and
counterexamples of experimental data and their analyses supporting and
challenging them.
",0,0,0,0,1,0
20387,20388,Improved TDNNs using Deep Kernels and Frequency Dependent Grid-RNNs,"  Time delay neural networks (TDNNs) are an effective acoustic model for large
vocabulary speech recognition. The strength of the model can be attributed to
its ability to effectively model long temporal contexts. However, current TDNN
models are relatively shallow, which limits the modelling capability. This
paper proposes a method of increasing the network depth by deepening the kernel
used in the TDNN temporal convolutions. The best performing kernel consists of
three fully connected layers with a residual (ResNet) connection from the
output of the first to the output of the third. The addition of
spectro-temporal processing as the input to the TDNN in the form of a
convolutional neural network (CNN) and a newly designed Grid-RNN was
investigated. The Grid-RNN strongly outperforms a CNN if different sets of
parameters for different frequency bands are used and can be further enhanced
by using a bi-directional Grid-RNN. Experiments using the multi-genre broadcast
(MGB3) English data (275h) show that deep kernel TDNNs reduces the word error
rate (WER) by 6% relative and when combined with the frequency dependent
Grid-RNN gives a relative WER reduction of 9%.
",0,0,0,1,0,0
14482,14483,SAND: An automated VLBI imaging and analysing pipeline - I. Stripping component trajectories,"  We present our implementation of an automated VLBI data reduction pipeline
dedicated to interferometric data imaging and analysis. The pipeline can handle
massive VLBI data efficiently which makes it an appropriate tool to investigate
multi-epoch multiband VLBI data. Compared to traditional manual data reduction,
our pipeline provides more objective results since less human interference is
involved. Source extraction is done in the image plane, while deconvolution and
model fitting are done in both the image plane and the uv plane for parallel
comparison. The output from the pipeline includes catalogues of CLEANed images
and reconstructed models, polarisation maps, proper motion estimates, core
light curves and multi-band spectra. We have developed a regression strip
algorithm to automatically detect linear or non-linear patterns in the jet
component trajectories. This algorithm offers an objective method to match jet
components at different epochs and determine their proper motions.
",0,1,0,0,0,0
4141,4142,Face Detection and Face Recognition In the Wild Using Off-the-Shelf Freely Available Components,"  This paper presents an easy and efficient face detection and face recognition
approach using free software components from the internet. Face detection and
face recognition problems have wide applications in home and office security.
Therefore this work will helpful for those searching for a free face
off-the-shelf face detection system. Using this system, faces can be detected
in uncontrolled environments. In the detection phase, every individual face is
detected and in the recognition phase the detected faces are compared with the
faces in a given data set and recognized.
",1,0,0,0,0,0
20003,20004,"Wild ramification and K(pi, 1) spaces","  We prove that every connected affine scheme of positive characteristic is a
K(pi, 1) space for the etale topology. The main ingredient is the special case
of the affine space over a field k. This is dealt with by induction on n, using
a key ""Bertini-type""' statement regarding the wild ramification of l-adic local
systems on affine spaces, which might be of independent interest. Its proof
uses in an essential way recent advances in higher ramification theory due to
T. Saito. We also give rigid analytic and mixed characteristic versions of the
main result.
",0,0,1,0,0,0
13861,13862,Probabilistic Forwarding of Coded Packets on Networks,"  We consider a scenario of broadcasting information over a network of nodes
connected by noiseless communication links. A source node in the network has
$k$ data packets to broadcast, and it suffices that a large fraction of the
network nodes receives the broadcast. The source encodes the $k$ data packets
into $n \ge k$ coded packets using a maximum distance separable (MDS) code, and
transmits them to its one-hop neighbours. Every other node in the network
follows a probabilistic forwarding protocol, in which it forwards a previously
unreceived packet to all its neighbours with a certain probability $p$. A
""near-broadcast"" is when the expected fraction of nodes that receive at least
$k$ of the $n$ coded packets is close to $1$. The forwarding probability $p$ is
chosen so as to minimize the expected total number of transmissions needed for
a near-broadcast. In this paper, we analyze the probabilistic forwarding of
coded packets on two specific network topologies: binary trees and square
grids. For trees, our analysis shows that for fixed $k$, the expected total
number of transmissions increases with $n$. On the other hand, on grids, we use
ideas from percolation theory to show that a judicious choice of $n$ will
significantly reduce the expected total number of transmissions needed for a
near-broadcast.
",1,0,0,0,0,0
1701,1702,Linearization of the box-ball system: an elementary approach,"  Kuniba, Okado, Takagi and Yamada have found that the time-evolution of the
Takahashi-Satsuma box-ball system can be linearized by considering rigged
configurations associated with states of the box-ball system. We introduce a
simple way to understand the rigged configuration of $\mathfrak{sl}_2$-type,
and give an elementary proof of the linearization property. Our approach can be
applied to a box-ball system with finite carrier, which is related to a
discrete modified KdV equation, and also to the combinatorial $R$-matrix of
$A_1^{(1)}$-type. We also discuss combinatorial statistics and related
fermionic formulas associated with the states of the box-ball systems. A
fermionic-type formula we obtain for the finite carrier case seems to be new.
",0,1,0,0,0,0
10085,10086,Spin wave propagation and spin polarized electron transport in single crystal iron films,"  The technique of propagating spin wave spectroscopy is applied to a 20 nm
thick Fe/MgO (001) film. The magnetic parameters extracted from the position of
the resonance peaks are very close to those tabulated for bulk iron. From the
propagating waveforms, a group velocity of 4 km/s and an attenuation length of
about 6 micrometers are extracted for 1.6 micrometers-wavelength spin-wave at
18 GHz. From the measured current-induced spin-wave Doppler shift, we also
extract a surprisingly high degree of spin-polarization of the current of 83%.
This set of results makes single-crystalline iron a promising candidate for
building devices utilizing high frequency spin-waves and spin-polarized
currents.
",0,1,0,0,0,0
17774,17775,Mendelian randomization with fine-mapped genetic data: choosing from large numbers of correlated instrumental variables,"  Mendelian randomization uses genetic variants to make causal inferences about
the effect of a risk factor on an outcome. With fine-mapped genetic data, there
may be hundreds of genetic variants in a single gene region any of which could
be used to assess this causal relationship. However, using too many genetic
variants in the analysis can lead to spurious estimates and inflated Type 1
error rates. But if only a few genetic variants are used, then the majority of
the data is ignored and estimates are highly sensitive to the particular choice
of variants. We propose an approach based on summarized data only (genetic
association and correlation estimates) that uses principal components analysis
to form instruments. This approach has desirable theoretical properties: it
takes the totality of data into account and does not suffer from numerical
instabilities. It also has good properties in simulation studies: it is not
particularly sensitive to varying the genetic variants included in the analysis
or the genetic correlation matrix, and it does not have greatly inflated Type 1
error rates. Overall, the method gives estimates that are not so precise as
those from variable selection approaches (such as using a conditional analysis
or pruning approach to select variants), but are more robust to seemingly
arbitrary choices in the variable selection step. Methods are illustrated by an
example using genetic associations with testosterone for 320 genetic variants
to assess the effect of sex hormone-related pathways on coronary artery disease
risk, in which variable selection approaches give inconsistent inferences.
",0,0,0,1,0,0
12060,12061,DeepTingle,"  DeepTingle is a text prediction and classification system trained on the
collected works of the renowned fantastic gay erotica author Chuck Tingle.
Whereas the writing assistance tools you use everyday (in the form of
predictive text, translation, grammar checking and so on) are trained on
generic, purportedly ""neutral"" datasets, DeepTingle is trained on a very
specific, internally consistent but externally arguably eccentric dataset. This
allows us to foreground and confront the norms embedded in data-driven
creativity and productivity assistance tools. As such tools effectively
function as extensions of our cognition into technology, it is important to
identify the norms they embed within themselves and, by extension, us.
DeepTingle is realized as a web application based on LSTM networks and the
GloVe word embedding, implemented in JavaScript with Keras-JS.
",1,0,0,0,0,0
20108,20109,An Information Theoretic Approach to Sample Acquisition and Perception in Planetary Robotics,"  An important and emerging component of planetary exploration is sample
retrieval and return to Earth. Obtaining and analyzing rock samples can provide
unprecedented insight into the geology, geo-history and prospects for finding
past life and water. Current methods of exploration rely on mission scientists
to identify objects of interests and this presents major operational
challenges. Finding objects of interests will require systematic and efficient
methods to quickly and correctly evaluate the importance of hundreds if not
thousands of samples so that the most interesting are saved for further
analysis by the mission scientists. In this paper, we propose an automated
information theoretic approach to identify shapes of interests using a library
of predefined interesting shapes. These predefined shapes maybe human input or
samples that are then extrapolated by the shape matching system using the
Superformula to judge the importance of newly obtained objects. Shape samples
are matched to a library of shapes using the eigenfaces approach enabling
categorization and prioritization of the sample. The approach shows robustness
to simulated sensor noise of up to 20%. The effect of shape parameters and
rotational angle on shape matching accuracy has been analyzed. The approach
shows significant promise and efforts are underway in testing the algorithm
with real rock samples.
",1,1,0,0,0,0
14573,14574,Generative Adversarial Residual Pairwise Networks for One Shot Learning,"  Deep neural networks achieve unprecedented performance levels over many tasks
and scale well with large quantities of data, but performance in the low-data
regime and tasks like one shot learning still lags behind. While recent work
suggests many hypotheses from better optimization to more complicated network
structures, in this work we hypothesize that having a learnable and more
expressive similarity objective is an essential missing component. Towards
overcoming that, we propose a network design inspired by deep residual networks
that allows the efficient computation of this more expressive pairwise
similarity objective. Further, we argue that regularization is key in learning
with small amounts of data, and propose an additional generator network based
on the Generative Adversarial Networks where the discriminator is our residual
pairwise network. This provides a strong regularizer by leveraging the
generated data samples. The proposed model can generate plausible variations of
exemplars over unseen classes and outperforms strong discriminative baselines
for few shot classification tasks. Notably, our residual pairwise network
design outperforms previous state-of-theart on the challenging mini-Imagenet
dataset for one shot learning by getting over 55% accuracy for the 5-way
classification task over unseen classes.
",1,0,0,0,0,0
3163,3164,An extensible cluster-graph taxonomy for open set sound scene analysis,"  We present a new extensible and divisible taxonomy for open set sound scene
analysis. This new model allows complex scene analysis with tangible
descriptors and perception labels. Its novel structure is a cluster graph such
that each cluster (or subset) can stand alone for targeted analyses such as
office sound event detection, whilst maintaining integrity over the whole graph
(superset) of labels. The key design benefit is its extensibility as new labels
are needed during new data capture. Furthermore, datasets which use the same
taxonomy are easily augmented, saving future data collection effort. We balance
the details needed for complex scene analysis with avoiding 'the taxonomy of
everything' with our framework to ensure no duplicity in the superset of labels
and demonstrate this with DCASE challenge classifications.
",1,0,0,0,0,0
13162,13163,Seven Lessons from Manyfield Inflation in Random Potentials,"  We study inflation in models with many interacting fields subject to randomly
generated scalar potentials. We use methods from non-equilibrium random matrix
theory to construct the potentials and an adaption of the 'transport method' to
evolve the two-point correlators during inflation. This construction allows,
for the first time, for an explicit study of models with up to 100 interacting
fields supporting a period of 'approximately saddle-point' inflation. We
determine the statistical predictions for observables by generating over 30,000
models with 2-100 fields supporting at least 60 efolds of inflation. These
studies lead us to seven lessons: i) Manyfield inflation is not single-field
inflation, ii) The larger the number of fields, the simpler and sharper the
predictions, iii) Planck compatibility is not rare, but future experiments may
rule out this class of models, iv) The smoother the potentials, the sharper the
predictions, v) Hyperparameters can transition from stiff to sloppy, vi)
Despite tachyons, isocurvature can decay, vii) Eigenvalue repulsion drives the
predictions. We conclude that many of the 'generic predictions' of single-field
inflation can be emergent features of complex inflation models.
",0,1,0,0,0,0
6216,6217,The Impedance of Flat Metallic Plates with Small Corrugations,"  Summarizes recent work on the wakefields and impedances of flat, metallic
plates with small corrugations
",0,1,0,0,0,0
20417,20418,Numerical Gaussian Processes for Time-dependent and Non-linear Partial Differential Equations,"  We introduce the concept of numerical Gaussian processes, which we define as
Gaussian processes with covariance functions resulting from temporal
discretization of time-dependent partial differential equations. Numerical
Gaussian processes, by construction, are designed to deal with cases where: (1)
all we observe are noisy data on black-box initial conditions, and (2) we are
interested in quantifying the uncertainty associated with such noisy data in
our solutions to time-dependent partial differential equations. Our method
circumvents the need for spatial discretization of the differential operators
by proper placement of Gaussian process priors. This is an attempt to construct
structured and data-efficient learning machines, which are explicitly informed
by the underlying physics that possibly generated the observed data. The
effectiveness of the proposed approach is demonstrated through several
benchmark problems involving linear and nonlinear time-dependent operators. In
all examples, we are able to recover accurate approximations of the latent
solutions, and consistently propagate uncertainty, even in cases involving very
long time integration.
",1,0,1,1,0,0
16877,16878,Aggregated Momentum: Stability Through Passive Damping,"  Momentum is a simple and widely used trick which allows gradient-based
optimizers to pick up speed along low curvature directions. Its performance
depends crucially on a damping coefficient $\beta$. Large $\beta$ values can
potentially deliver much larger speedups, but are prone to oscillations and
instability; hence one typically resorts to small values such as 0.5 or 0.9. We
propose Aggregated Momentum (AggMo), a variant of momentum which combines
multiple velocity vectors with different $\beta$ parameters. AggMo is trivial
to implement, but significantly dampens oscillations, enabling it to remain
stable even for aggressive $\beta$ values such as 0.999. We reinterpret
Nesterov's accelerated gradient descent as a special case of AggMo and analyze
rates of convergence for quadratic objectives. Empirically, we find that AggMo
is a suitable drop-in replacement for other momentum methods, and frequently
delivers faster convergence.
",0,0,0,1,0,0
12573,12574,Deep Learning to Improve Breast Cancer Early Detection on Screening Mammography,"  The rapid development of deep learning, a family of machine learning
techniques, has spurred much interest in its application to medical imaging
problems. Here, we develop a deep learning algorithm that can accurately detect
breast cancer on screening mammograms using an ""end-to-end"" training approach
that efficiently leverages training datasets with either complete clinical
annotation or only the cancer status (label) of the whole image. In this
approach, lesion annotations are required only in the initial training stage,
and subsequent stages require only image-level labels, eliminating the reliance
on rarely available lesion annotations. Our all convolutional network method
for classifying screening mammograms attained excellent performance in
comparison with previous methods. On an independent test set of digitized film
mammograms from Digital Database for Screening Mammography (DDSM), the best
single model achieved a per-image AUC of 0.88, and four-model averaging
improved the AUC to 0.91 (sensitivity: 86.1%, specificity: 80.1%). On a
validation set of full-field digital mammography (FFDM) images from the
INbreast database, the best single model achieved a per-image AUC of 0.95, and
four-model averaging improved the AUC to 0.98 (sensitivity: 86.7%, specificity:
96.1%). We also demonstrate that a whole image classifier trained using our
end-to-end approach on the DDSM digitized film mammograms can be transferred to
INbreast FFDM images using only a subset of the INbreast data for fine-tuning
and without further reliance on the availability of lesion annotations. These
findings show that automatic deep learning methods can be readily trained to
attain high accuracy on heterogeneous mammography platforms, and hold
tremendous promise for improving clinical tools to reduce false positive and
false negative screening mammography results.
",1,0,0,1,0,0
14613,14614,Discovering Playing Patterns: Time Series Clustering of Free-To-Play Game Data,"  The classification of time series data is a challenge common to all
data-driven fields. However, there is no agreement about which are the most
efficient techniques to group unlabeled time-ordered data. This is because a
successful classification of time series patterns depends on the goal and the
domain of interest, i.e. it is application-dependent.
In this article, we study free-to-play game data. In this domain, clustering
similar time series information is increasingly important due to the large
amount of data collected by current mobile and web applications. We evaluate
which methods cluster accurately time series of mobile games, focusing on
player behavior data. We identify and validate several aspects of the
clustering: the similarity measures and the representation techniques to reduce
the high dimensionality of time series. As a robustness test, we compare
various temporal datasets of player activity from two free-to-play video-games.
With these techniques we extract temporal patterns of player behavior
relevant for the evaluation of game events and game-business diagnosis. Our
experiments provide intuitive visualizations to validate the results of the
clustering and to determine the optimal number of clusters. Additionally, we
assess the common characteristics of the players belonging to the same group.
This study allows us to improve the understanding of player dynamics and churn
behavior.
",1,0,0,1,0,0
18186,18187,Lattice Boltzmann study of chemically-driven self-propelled droplets,"  We numerically study the behavior of self-propelled liquid droplets whose
motion is triggered by a Marangoni-like flow. This latter is generated by
variations of surfactant concentration which affect the droplet surface tension
promoting its motion. In the present paper a model for droplets with a third
amphiphilic component is adopted. The dynamics is described by Navier-Stokes
and convection-diffusion equations, solved by lattice Boltzmann method coupled
with finite-difference schemes. We focus on two cases. First the study of
self-propulsion of an isolated droplet is carried on and, then, the interaction
of two self-propelled droplets is investigated. In both cases, when the
surfactant migrates towards the interface, a quadrupolar vortex of the velocity
field forms inside the droplet and causes the motion. A weaker dipolar field
emerges instead when the surfactant is mainly diluted in the bulk. The dynamics
of two interacting droplets is more complex and strongly depends on their
reciprocal distance. If, in a head-on collision, droplets are close enough, the
velocity field initially attracts them until a motionless steady state is
achieved. If the droplets are vertically shifted, the hydrodynamic field leads
to an initial reciprocal attraction followed by a scattering along opposite
directions. This hydrodynamic interaction acts on a separation of some droplet
radii otherwise it becomes negligible and droplets motion is only driven by
Marangoni effect. Finally, if one of the droplets is passive, this latter is
generally advected by the fluid flow generated by the active one.
",0,1,0,0,0,0
1004,1005,"Laplace Beltrami operator in the Baran metric and pluripotential equilibrium measure: the ball, the simplex and the sphere","  The Baran metric $\delta_E$ is a Finsler metric on the interior of $E\subset
\R^n$ arising from Pluripotential Theory. We consider the few instances, namely
$E$ being the ball, the simplex, or the sphere, where $\delta_E$ is known to be
Riemaniann and we prove that the eigenfunctions of the associated Laplace
Beltrami operator (with no boundary conditions) are the orthogonal polynomials
with respect to the pluripotential equilibrium measure $\mu_E$ of $E.$ We
conjecture that this may hold in a wider generality.
The considered differential operators have been already introduced in the
framework of orthogonal polynomials and studied in connection with certain
symmetry groups. In this work instead we highlight the relationships between
orthogonal polynomials with respect to $\mu_E$ and the Riemaniann structure
naturally arising from Pluripotential Theory
",0,0,1,0,0,0
8452,8453,Parkinson's Disease Digital Biomarker Discovery with Optimized Transitions and Inferred Markov Emissions,"  We search for digital biomarkers from Parkinson's Disease by observing
approximate repetitive patterns matching hypothesized step and stride periodic
cycles. These observations were modeled as a cycle of hidden states with
randomness allowing deviation from a canonical pattern of transitions and
emissions, under the hypothesis that the averaged features of hidden states
would serve to informatively characterize classes of patients/controls. We
propose a Hidden Semi-Markov Model (HSMM), a latent-state model, emitting
3D-acceleration vectors. Transitions and emissions are inferred from data. We
fit separate models per unique device and training label. Hidden Markov Models
(HMM) force geometric distributions of the duration spent at each state before
transition to a new state. Instead, our HSMM allows us to specify the
distribution of state duration. This modified version is more effective because
we are interested more in each state's duration than the sequence of distinct
states, allowing inclusion of these durations the feature vector.
",1,0,0,1,0,0
19116,19117,On the Optimality of Secret Key Agreement via Omniscience,"  For the multiterminal secret key agreement problem under a private source
model, it is known that the maximum key rate, i.e., the secrecy capacity, can
be achieved through communication for omniscience, but the omniscience strategy
can be strictly suboptimal in terms of minimizing the public discussion rate.
While a single-letter characterization is not known for the minimum discussion
rate needed for achieving the secrecy capacity, we derive single-letter lower
and upper bounds that yield some simple conditions for omniscience to be
discussion-rate optimal. These conditions turn out to be enough to deduce the
optimality of omniscience for a large class of sources including the
hypergraphical sources. Through conjectures and examples, we explore other
source models to which our methods do not easily extend.
",1,0,0,0,0,0
13735,13736,Real-time public transport service-level monitoring using passive WiFi: a spectral clustering approach for train timetable estimation,"  A new area in which passive WiFi analytics have promise for delivering value
is the real-time monitoring of public transport systems. One example is
determining the true (as opposed to the published) timetable of a public
transport system in real-time. In most cases, there are no other
publicly-available sources for this information. Yet, it is indispensable for
the real-time monitoring of public transport service levels. Furthermore, this
information, if accurate and temporally fine-grained, can be used for very
low-latency incident detection. In this work, we propose using spectral
clustering based on trajectories derived from passive WiFi traces of users of a
public transport system to infer the true timetable and two key performance
indicators of the transport service, namely public transport vehicle headway
and in-station dwell time. By detecting anomalous dwell times or headways, we
demonstrate that a fast and accurate real-time incident-detection procedure can
be obtained. The method we introduce makes use of the advantages of the
high-frequency WiFi data, which provides very low-latency,
universally-accessible information, while minimizing the impact of the noise in
the data.
",1,0,0,0,0,0
18497,18498,The connectivity of graphs of graphs with self-loops and a given degree sequence,"  `Double edge swaps' transform one graph into another while preserving the
graph's degree sequence, and have thus been used in a number of popular Markov
chain Monte Carlo (MCMC) sampling techniques. However, while double edge-swaps
can transform, for any fixed degree sequence, any two graphs inside the classes
of simple graphs, multigraphs, and pseudographs, this is not true for graphs
which allow self-loops but not multiedges (loopy graphs). Indeed, we exactly
characterize the degree sequences where double edge swaps cannot reach every
valid loopy graph and develop an efficient algorithm to determine such degree
sequences. The same classification scheme to characterize degree sequences can
be used to prove that, for all degree sequences, loopy graphs are connected by
a combination of double and triple edge swaps. Thus, we contribute the first
MCMC sampler that uniformly samples loopy graphs with any given sequence.
",1,1,1,0,0,0
16245,16246,Meta-Learning for Resampling Recommendation Systems,"  One possible approach to tackle the class imbalance in classification tasks
is to resample a training dataset, i.e., to drop some of its elements or to
synthesize new ones. There exist several widely-used resampling methods. Recent
research showed that the choice of resampling method significantly affects the
quality of classification, which raises resampling selection problem.
Exhaustive search for optimal resampling is time-consuming and hence it is of
limited use. In this paper, we describe an alternative approach to the
resampling selection. We follow the meta-learning concept to build resampling
recommendation systems, i.e., algorithms recommending resampling for datasets
on the basis of their properties.
",1,0,0,1,0,0
19779,19780,Self-Regulated Transport in Photonic Crystals with Phase-Changing Defects,"  Phase changing materials (PCM) are widely used for optical data recording,
sensing, all-optical switching, and optical limiting. Our focus here is on the
case when the change in the transmission characteristics of the optical
material is caused by the input light itself. Specifically, the light-induced
heating triggers the phase transition in the PCM. In this paper, using a
numerical example, we demonstrate that incorporating the PCM in a photonic
structure can lead to a dramatic modification of the effects of light-induced
phase transition, as compared to a stand-alone sample of the same PCM. Our
focus is on short pulses. We discuss some possible applications of such
phase-changing photonic structures for optical sensing and limiting.
",0,1,0,0,0,0
16320,16321,Cooperative Multi-Sender Index Coding,"  In this paper, we propose a new coding scheme and establish new bounds on the
capacity region for the multi-sender unicast index-coding problem. We revisit
existing partitioned Distributed Composite Coding (DCC) proposed by Sadeghi et
al. and identify its limitations in the implementation of multi-sender
composite coding and in the strategy of sender partitioning. We then propose
two new coding components to overcome these limitations and develop a
multi-sender Cooperative Composite Coding (CCC). We show that CCC can strictly
improve upon partitioned DCC, and is the key to achieve optimality for a number
of index-coding instances. The usefulness of CCC and its special cases is
illuminated via non-trivial examples, and the capacity region is established
for each example. Comparisons between CCC and other non-cooperative schemes in
recent works are also provided to further demonstrate the advantage of CCC.
",1,0,1,0,0,0
13026,13027,Stochastic Maximum Likelihood Optimization via Hypernetworks,"  This work explores maximum likelihood optimization of neural networks through
hypernetworks. A hypernetwork initializes the weights of another network, which
in turn can be employed for typical functional tasks such as regression and
classification. We optimize hypernetworks to directly maximize the conditional
likelihood of target variables given input. Using this approach we obtain
competitive empirical results on regression and classification benchmarks.
",1,0,0,1,0,0
667,668,The Diverse Club: The Integrative Core of Complex Networks,"  A complex system can be represented and analyzed as a network, where nodes
represent the units of the network and edges represent connections between
those units. For example, a brain network represents neurons as nodes and axons
between neurons as edges. In many networks, some nodes have a
disproportionately high number of edges. These nodes also have many edges
between each other, and are referred to as the rich club. In many different
networks, the nodes of this club are assumed to support global network
integration. However, another set of nodes potentially exhibits a connectivity
structure that is more advantageous to global network integration. Here, in a
myriad of different biological and man-made networks, we discover the diverse
club--a set of nodes that have edges diversely distributed across the network.
The diverse club exhibits, to a greater extent than the rich club, properties
consistent with an integrative network function--these nodes are more highly
interconnected and their edges are more critical for efficient global
integration. Moreover, we present a generative evolutionary network model that
produces networks with a diverse club but not a rich club, thus demonstrating
that these two clubs potentially evolved via distinct selection pressures.
Given the variety of different networks that we analyzed--the c. elegans, the
macaque brain, the human brain, the United States power grid, and global air
traffic--the diverse club appears to be ubiquitous in complex networks. These
results warrant the distinction and analysis of two critical clubs of nodes in
all complex systems.
",0,1,0,0,0,0
14893,14894,$b$-symbol distance distribution of repeated-root cyclic codes,"  Symbol-pair codes, introduced by Cassuto and Blaum [1], have been raised for
symbol-pair read channels. This new idea is motivated by the limitation of the
reading process in high-density data storage technologies. Yaakobi et al. [8]
introduced codes for $b$-symbol read channels, where the read operation is
performed as a consecutive sequence of $b>2$ symbols. In this paper, we come up
with a method to compute the $b$-symbol-pair distance of two $n$-tuples, where
$n$ is a positive integer. Also, we deal with the $b$-symbol-pair distances of
some kind of cyclic codes of length $p^e$ over $\mathbb{F}_{p^m}$.
",1,0,0,0,0,0
13568,13569,The usefulness of Poynting's theorem in magnetic turbulence,"  We rewrite Poynting's theorem, already used in a previous publication
(Treumann & Baumjohann 2017) to derive relations between the turbulent magnetic
and electric power spectral densities, to make explicit where the mechanical
contributions enter. We then make explicit use of the relativistic
transformation of the turbulent electric fluctuations to obtain expressions
which depend only on the magnetic and velocity fluctuations. Any electric
fluctuations play just an intermediate role. Equations are constructed for the
turbulent conductivity spectrum in Alfvénic and non-Alfvénic turbulence in
extension of the results in the above citation. An observation-based discussion
of their use in application to solar wind turbulence is given. The inertial
range solar wind turbulence exhibits signs of chaos and self-organisation.
",0,1,0,0,0,0
15618,15619,State Distribution-aware Sampling for Deep Q-learning,"  A critical and challenging problem in reinforcement learning is how to learn
the state-action value function from the experience replay buffer and
simultaneously keep sample efficiency and faster convergence to a high quality
solution. In prior works, transitions are uniformly sampled at random from the
replay buffer or sampled based on their priority measured by
temporal-difference (TD) error. However, these approaches do not fully take
into consideration the intrinsic characteristics of transition distribution in
the state space and could result in redundant and unnecessary TD updates,
slowing down the convergence of the learning procedure. To overcome this
problem, we propose a novel state distribution-aware sampling method to balance
the replay times for transitions with skew distribution, which takes into
account both the occurrence frequencies of transitions and the uncertainty of
state-action values. Consequently, our approach could reduce the unnecessary TD
updates and increase the TD updates for state-action value with more
uncertainty, making the experience replay more effective and efficient.
Extensive experiments are conducted on both classic control tasks and Atari
2600 games based on OpenAI gym platform and the experimental results
demonstrate the effectiveness of our approach in comparison with the standard
DQN approach.
",0,0,0,1,0,0
17575,17576,Effective difference elimination and Nullstellensatz,"  We prove effective Nullstellensatz and elimination theorems for difference
equations in sequence rings. More precisely, we compute an explicit function of
geometric quantities associated to a system of difference equations (and these
geometric quantities may themselves be bounded by a function of the number of
variables, the order of the equations, and the degrees of the equations) so
that for any system of difference equations in variables $\mathbf{x} = (x_1,
\ldots, x_m)$ and $\mathbf{u} = (u_1, \ldots, u_r)$, if these equations have
any nontrivial consequences in the $\mathbf{x}$ variables, then such a
consequence may be seen algebraically considering transforms up to the order of
our bound. Specializing to the case of $m = 0$, we obtain an effective method
to test whether a given system of difference equations is consistent.
",0,0,1,0,0,0
18933,18934,Neurally Plausible Model of Robot Reaching Inspired by Infant Motor Babbling,"  In this paper we present a neurally plausible model of robot reaching
inspired by human infant reaching that is based on embodied artificial
intelligence, which emphasizes the importance of the sensory-motor interaction
of an agent and the world. This model encompasses both learning sensory-motor
correlations through motor babbling and also arm motion planning using
spreading activation. This model is organized in three layers of neural maps
with parallel structures representing the same sensory-motor space. The motor
babbling period shapes the structure of the three neural maps as well as the
connections within and between them. We describe an implementation of this
model and an investigation of this implementation using a simple reaching task
on a humanoid robot. The robot has learned successfully to plan reaching
motions from a test set with high accuracy and smoothness.
",1,0,0,0,0,0
13433,13434,NIP formulas and Baire 1 definability,"  In this short note, using results of Bourgain, Fremlin, and Talagrand
\cite{BFT}, we show that for a countable structure $M$, a saturated elementary
extension $M^*$ of $M$ and a formula $\phi(x,y)$ the following are equivalent:
(i) $\phi(x,y)$ is NIP on $M$ (in the sense of Definition 2.1).
(ii) Whenever $p(x)\in S_\phi(M^*)$ is finitely satisfiable in $M$ then it is
Baire 1 definable over $M$ (in sense of Definition 2.5).
",0,0,1,0,0,0
19406,19407,Radiomics strategies for risk assessment of tumour failure in head-and-neck cancer,"  Quantitative extraction of high-dimensional mineable data from medical images
is a process known as radiomics. Radiomics is foreseen as an essential
prognostic tool for cancer risk assessment and the quantification of
intratumoural heterogeneity. In this work, 1615 radiomic features (quantifying
tumour image intensity, shape, texture) extracted from pre-treatment FDG-PET
and CT images of 300 patients from four different cohorts were analyzed for the
risk assessment of locoregional recurrences (LR) and distant metastases (DM) in
head-and-neck cancer. Prediction models combining radiomic and clinical
variables were constructed via random forests and imbalance-adjustment
strategies using two of the four cohorts. Independent validation of the
prediction and prognostic performance of the models was carried out on the
other two cohorts (LR: AUC = 0.69 and CI = 0.67; DM: AUC = 0.86 and CI = 0.88).
Furthermore, the results obtained via Kaplan-Meier analysis demonstrated the
potential of radiomics for assessing the risk of specific tumour outcomes using
multiple stratification groups. This could have important clinical impact,
notably by allowing for a better personalization of chemo-radiation treatments
for head-and-neck cancer patients from different risk groups.
",1,0,0,0,0,0
16435,16436,Cycle Consistent Adversarial Denoising Network for Multiphase Coronary CT Angiography,"  In coronary CT angiography, a series of CT images are taken at different
levels of radiation dose during the examination. Although this reduces the
total radiation dose, the image quality during the low-dose phases is
significantly degraded. To address this problem, here we propose a novel
semi-supervised learning technique that can remove the noises of the CT images
obtained in the low-dose phases by learning from the CT images in the routine
dose phases. Although a supervised learning approach is not possible due to the
differences in the underlying heart structure in two phases, the images in the
two phases are closely related so that we propose a cycle-consistent
adversarial denoising network to learn the non-degenerate mapping between the
low and high dose cardiac phases. Experimental results showed that the proposed
method effectively reduces the noise in the low-dose CT image while the
preserving detailed texture and edge information. Moreover, thanks to the
cyclic consistency and identity loss, the proposed network does not create any
artificial features that are not present in the input images. Visual grading
and quality evaluation also confirm that the proposed method provides
significant improvement in diagnostic quality.
",0,0,0,1,0,0
340,341,Untangling Planar Curves,"  Any generic closed curve in the plane can be transformed into a simple closed
curve by a finite sequence of local transformations called homotopy moves. We
prove that simplifying a planar closed curve with $n$ self-crossings requires
$\Theta(n^{3/2})$ homotopy moves in the worst case. Our algorithm improves the
best previous upper bound $O(n^2)$, which is already implicit in the classical
work of Steinitz; the matching lower bound follows from the construction of
closed curves with large defect, a topological invariant of generic closed
curves introduced by Aicardi and Arnold. Our lower bound also implies that
$\Omega(n^{3/2})$ facial electrical transformations are required to reduce any
plane graph with treewidth $\Omega(\sqrt{n})$ to a single vertex, matching
known upper bounds for rectangular and cylindrical grid graphs. More generally,
we prove that transforming one immersion of $k$ circles with at most $n$
self-crossings into another requires $\Theta(n^{3/2} + nk + k^2)$ homotopy
moves in the worst case. Finally, we prove that transforming one
noncontractible closed curve to another on any orientable surface requires
$\Omega(n^2)$ homotopy moves in the worst case; this lower bound is tight if
the curve is homotopic to a simple closed curve.
",1,0,1,0,0,0
15154,15155,Smart grid modeling and simulation - Comparing GridLAB-D and RAPSim via two Case studies,"  One of the most important tools for the development of the smart grid is
simulation. Therefore, analyzing, designing, modeling, and simulating the smart
grid will allow to explore future scenarios and support decision making for the
grid's development. In this paper, we compare two open source simulation tools
for the smart grid, GridLAB-Distribution (GridLAB-D) and Renewable Alternative
Power systems Simulation (RAPSim). The comparison is based on the
implementation of two case studies related to a power flow problem and the
integration of renewable energy resources to the grid. Results show that even
for very simple case studies, specific properties such as weather simulation or
load modeling are influencing the results in a way that they are not
reproducible with a different simulator.
",1,0,0,0,0,0
11936,11937,CANDELS Sheds Light on the Environmental Quenching of Low-mass Galaxies,"  We investigate the environmental quenching of galaxies, especially those with
stellar masses (M*)$<10^{9.5} M_\odot$, beyond the local universe. Essentially
all local low-mass quenched galaxies (QGs) are believed to live close to
massive central galaxies, which is a demonstration of environmental quenching.
We use CANDELS data to test {\it whether or not} such a dwarf QG--massive
central galaxy connection exists beyond the local universe. To this purpose, we
only need a statistically representative, rather than a complete, sample of
low-mass galaxies, which enables our study to $z\gtrsim1.5$. For each low-mass
galaxy, we measure the projected distance ($d_{proj}$) to its nearest massive
neighbor (M*$>10^{10.5} M_\odot$) within a redshift range. At a given redshift
and M*, the environmental quenching effect is considered to be observed if the
$d_{proj}$ distribution of QGs ($d_{proj}^Q$) is significantly skewed toward
lower values than that of star-forming galaxies ($d_{proj}^{SF}$). For galaxies
with $10^{8} M_\odot < M* < 10^{10} M_\odot$, such a difference between
$d_{proj}^Q$ and $d_{proj}^{SF}$ is detected up to $z\sim1$. Also, about 10\%
of the quenched galaxies in our sample are located between two and four virial
radii ($R_{Vir}$) of the massive halos. The median projected distance from
low-mass QGs to their massive neighbors, $d_{proj}^Q / R_{Vir}$, decreases with
satellite M* at $M* \lesssim 10^{9.5} M_\odot$, but increases with satellite M*
at $M* \gtrsim 10^{9.5} M_\odot$. This trend suggests a smooth, if any,
transition of the quenching timescale around $M* \sim 10^{9.5} M_\odot$ at
$0.5<z<1.0$.
",0,1,0,0,0,0
8826,8827,Global aspects of polarization optics and coset space geometry,"  We use group theoretic ideas and coset space methods to deal with problems in
polarization optics of a global nature. These include the possibility of a
globally smooth phase convention for electric fields for all points on the
Poincaré sphere, and a similar possibility of real or complex bases of
transverse electric vectors for all possible propagation directions. It is
shown that these methods help in understanding some known results in an
effective manner, and in answering new questions as well. We find that apart
from the groups $SU(2)$ and $SO(3)$ which occur naturally in these problems,
the group $SU(3)$ also plays an important role.
",0,1,0,0,0,0
3856,3857,Subextensions for co-induced modules,"  Using cohomological methods, we prove a criterion for the embedding of a
group extension with abelian kernel into the split extension of a co-induced
module. This generalises some earlier similar results. We also prove an
assertion about the conjugacy of complements in split extensions of co-induced
modules. Both results follow from a relation between homomorphisms of certain
cohomology groups.
",0,0,1,0,0,0
952,953,Likelihood ratio test for variance components in nonlinear mixed effects models,"  Mixed effects models are widely used to describe heterogeneity in a
population. A crucial issue when adjusting such a model to data consists in
identifying fixed and random effects. From a statistical point of view, it
remains to test the nullity of the variances of a given subset of random
effects. Some authors have proposed to use the likelihood ratio test and have
established its asymptotic distribution in some particular cases. Nevertheless,
to the best of our knowledge, no general variance components testing procedure
has been fully investigated yet. In this paper, we study the likelihood ratio
test properties to test that the variances of a general subset of the random
effects are equal to zero in both linear and nonlinear mixed effects model,
extending the existing results. We prove that the asymptotic distribution of
the test is a chi-bar-square distribution, that is to say a mixture of
chi-square distributions, and we identify the corresponding weights. We
highlight in particular that the limiting distribution depends on the presence
of correlations between the random effects but not on the linear or nonlinear
structure of the mixed effects model. We illustrate the finite sample size
properties of the test procedure through simulation studies and apply the test
procedure to two real datasets of dental growth and of coucal growth.
",0,0,0,1,0,0
2681,2682,A simultaneous generalization of the theorems of Chevalley-Warning and Morlaye,"  Inspired by recent work of I. Baoulina, we give a simultaneous generalization
of the theorems of Chevalley-Warning and Morlaye.
",0,0,1,0,0,0
10869,10870,Mining Application-aware Community Organization with Expanded Feature Subspaces from Concerned Attributes in Social Networks,"  Social networks are typical attributed networks with node attributes.
Different from traditional attribute community detection problem aiming at
obtaining the whole set of communities in the network, we study an
application-oriented problem of mining an application-aware community
organization with respect to specific concerned attributes. The concerned
attributes are designated based on the requirements of any application by a
user in advance. The application-aware community organization w.r.t. concerned
attributes consists of the communities with feature subspaces containing these
concerned attributes. Besides concerned attributes, feature subspace of each
required community may contain some other relevant attributes. All relevant
attributes of a feature subspace jointly describe and determine the community
embedded in such subspace. Thus the problem includes two subproblems, i.e., how
to expand the set of concerned attributes to complete feature subspaces and how
to mine the communities embedded in the expanded subspaces. Two subproblems are
jointly solved by optimizing a quality function called subspace fitness. An
algorithm called ACM is proposed. In order to locate the communities
potentially belonging to the application-aware community organization, cohesive
parts of a network backbone composed of nodes with similar concerned attributes
are detected and set as the community seeds. The set of concerned attributes is
set as the initial subspace for all community seeds. Then each community seed
and its attribute subspace are adjusted iteratively to optimize the subspace
fitness. Extensive experiments on synthetic datasets demonstrate the
effectiveness and efficiency of our method and applications on real-world
networks show its application values.
",1,1,0,0,0,0
16834,16835,Handling Incomplete Heterogeneous Data using VAEs,"  Variational autoencoders (VAEs), as well as other generative models, have
been shown to be efficient and accurate to capture the latent structure of vast
amounts of complex high-dimensional data. However, existing VAEs can still not
directly handle data that are heterogenous (mixed continuous and discrete) or
incomplete (with missing data at random), which is indeed common in real-world
applications.
In this paper, we propose a general framework to design VAEs, suitable for
fitting incomplete heterogenous data. The proposed HI-VAE includes likelihood
models for real-valued, positive real valued, interval, categorical, ordinal
and count data, and allows to estimate (and potentially impute) missing data
accurately. Furthermore, HI-VAE presents competitive predictive performance in
supervised tasks, outperforming super- vised models when trained on incomplete
data
",0,0,0,1,0,0
16231,16232,Random data wave equations,"  Nowadays we have many methods allowing to exploit the regularising properties
of the linear part of a nonlinear dispersive equation (such as the KdV
equation, the nonlinear wave or the nonlinear Schroedinger equations) in order
to prove well-posedness in low regularity Sobolev spaces. By well-posedness in
low regularity Sobolev spaces we mean that less regularity than the one imposed
by the energy methods is required (the energy methods do not exploit the
dispersive properties of the linear part of the equation). In many cases these
methods to prove well-posedness in low regularity Sobolev spaces lead to
optimal results in terms of the regularity of the initial data. By optimal we
mean that if one requires slightly less regularity then the corresponding
Cauchy problem becomes ill-posed in the Hadamard sense. We call the Sobolev
spaces in which these ill-posedness results hold spaces of supercritical
regularity.
More recently, methods to prove probabilistic well-posedness in Sobolev
spaces of supercritical regularity were developed. More precisely, by
probabilistic well-posedness we mean that one endows the corresponding Sobolev
space of supercritical regularity with a non degenerate probability measure and
then one shows that almost surely with respect to this measure one can define a
(unique) global flow. However, in most of the cases when the methods to prove
probabilistic well-posedness apply, there is no information about the measure
transported by the flow. Very recently, a method to prove that the transported
measure is absolutely continuous with respect to the initial measure was
developed. In such a situation, we have a measure which is quasi-invariant
under the corresponding flow.
The aim of these lectures is to present all of the above described
developments in the context of the nonlinear wave equation.
",0,0,1,0,0,0
5339,5340,Near-perfect spin filtering and negative differential resistance in an Fe(II)S complex,"  Density functional theory and nonequilibrium Green's function calculations
have been used to explore spin-resolved transport through the high-spin state
of an iron(II)sulfur single molecular magnet. Our results show that this
molecule exhibits near-perfect spin filtering, where the spin-filtering
efficiency is above 99%, as well as significant negative differential
resistance centered at a low bias voltage. The rise in the spin-up conductivity
up to the bias voltage of 0.4 V is dominated by a conductive lowest unoccupied
molecular orbital, and this is accompanied by a slight increase in the magnetic
moment of the Fe atom. The subsequent drop in the spin-up conductivity is
because the conductive channel moves to the highest occupied molecular orbital
which has a lower conductance contribution. This is accompanied by a drop in
the magnetic moment of the Fe atom. These two exceptional properties, and the
fact that the onset of negative differential resistance occurs at low bias
voltage, suggests the potential of the molecule in nanoelectronic and
nanospintronic applications.
",0,1,0,0,0,0
6690,6691,Two-component domain decomposition scheme with overlapping subdomains for parabolic equations,"  An iteration-free method of domain decomposition is considered for
approximate solving a boundary value problem for a second-order parabolic
equation. A standard approach to constructing domain decomposition schemes is
based on a partition of unity for the domain under the consideration. Here a
new general approach is proposed for constructing domain decomposition schemes
with overlapping subdomains based on indicator functions of subdomains. The
basic peculiarity of this method is connected with a representation of the
problem operator as the sum of two operators, which are constructed for two
separate subdomains with the subtraction of the operator that is associated
with the intersection of the subdomains. There is developed a two-component
factorized scheme, which can be treated as a generalization of the standard
Alternating Direction Implicit (ADI) schemes to the case of a special
three-component splitting. There are obtained conditions for the unconditional
stability of regionally additive schemes constructed using indicator functions
of subdomains. Numerical results are presented for a model two-dimensional
problem.
",1,0,0,0,0,0
6070,6071,Statistical mechanics of low-rank tensor decomposition,"  Often, large, high dimensional datasets collected across multiple modalities
can be organized as a higher order tensor. Low-rank tensor decomposition then
arises as a powerful and widely used tool to discover simple low dimensional
structures underlying such data. However, we currently lack a theoretical
understanding of the algorithmic behavior of low-rank tensor decompositions. We
derive Bayesian approximate message passing (AMP) algorithms for recovering
arbitrarily shaped low-rank tensors buried within noise, and we employ dynamic
mean field theory to precisely characterize their performance. Our theory
reveals the existence of phase transitions between easy, hard and impossible
inference regimes, and displays an excellent match with simulations. Moreover,
it reveals several qualitative surprises compared to the behavior of symmetric,
cubic tensor decomposition. Finally, we compare our AMP algorithm to the most
commonly used algorithm, alternating least squares (ALS), and demonstrate that
AMP significantly outperforms ALS in the presence of noise.
",0,0,0,0,1,0
6451,6452,"Chow Rings of Mp_{0,2}(N,d) and Mbar_{0,2}(P^{N-1},d) and Gromov-Witten Invariants of Projective Hypersurfaces of Degree 1 and 2","  In this paper, we prove formulas that represent two-pointed Gromov-Witten
invariant <O_{h^a}O_{h^b}>_{0,d} of projective hypersurfaces with d=1,2 in
terms of Chow ring of Mbar_{0,2}(P^{N-1},d), the moduli spaces of stable maps
from genus 0 stable curves to projective space P^{N-1}. Our formulas are based
on representation of the intersection number w(O_{h^a}O_{h^b})_{0,d}, which was
introduced by Jinzenji, in terms of Chow ring of Mp_{0,2}(N,d), the moduli
space of quasi maps from P^1 to P^{N-1} with two marked points. In order to
prove our formulas, we use the results on Chow ring of Mbar_{0,2}(P^{N-1},d),
that were derived by A. Mustata and M. Mustata. We also present explicit toric
data of Mp_{0,2}(N,d) and prove relations of Chow ring of Mp_{0,2}(N,d).
",0,0,1,0,0,0
12698,12699,Predicting the Quality of Short Narratives from Social Media,"  An important and difficult challenge in building computational models for
narratives is the automatic evaluation of narrative quality. Quality evaluation
connects narrative understanding and generation as generation systems need to
evaluate their own products. To circumvent difficulties in acquiring
annotations, we employ upvotes in social media as an approximate measure for
story quality. We collected 54,484 answers from a crowd-powered
question-and-answer website, Quora, and then used active learning to build a
classifier that labeled 28,320 answers as stories. To predict the number of
upvotes without the use of social network features, we create neural networks
that model textual regions and the interdependence among regions, which serve
as strong benchmarks for future research. To our best knowledge, this is the
first large-scale study for automatic evaluation of narrative quality.
",1,0,0,0,0,0
3724,3725,IVOA Recommendation: HiPS - Hierarchical Progressive Survey,"  This document presents HiPS, a hierarchical scheme for the description,
storage and access of sky survey data. The system is based on hierarchical
tiling of sky regions at finer and finer spatial resolution which facilitates a
progressive view of a survey, and supports multi-resolution zooming and
panning. HiPS uses the HEALPix tessellation of the sky as the basis for the
scheme and is implemented as a simple file structure with a direct indexing
scheme that leads to practical implementations.
",0,1,0,0,0,0
11626,11627,Constructing confidence sets for the matrix completion problem,"  In the present note we consider the problem of constructing honest and
adaptive confidence sets for the matrix completion problem. For the Bernoulli
model with known variance of the noise we provide a realizable method for
constructing confidence sets that adapt to the unknown rank of the true matrix.
",0,0,1,1,0,0
20831,20832,Testing the validity of the local and global GKLS master equations on an exactly solvable model,"  When deriving a master equation for a multipartite weakly-interacting open
quantum systems, dissipation is often addressed \textit{locally} on each
component, i.e. ignoring the coherent couplings, which are later added `by
hand'. Although simple, the resulting local master equation (LME) is known to
be thermodynamically inconsistent. Otherwise, one may always obtain a
consistent \textit{global} master equation (GME) by working on the energy basis
of the full interacting Hamiltonian. Here, we consider a two-node `quantum
wire' connected to two heat baths. The stationary solution of the LME and GME
are obtained and benchmarked against the exact result. Importantly, in our
model, the validity of the GME is constrained by the underlying secular
approximation. Whenever this breaks down (for resonant weakly-coupled nodes),
we observe that the LME, in spite of being thermodynamically flawed: (a)
predicts the correct steady state, (b) yields the exact asymptotic heat
currents, and (c) reliably reflects the correlations between the nodes. In
contrast, the GME fails at all three tasks. Nonetheless, as the inter-node
coupling grows, the LME breaks down whilst the GME becomes correct. Hence, the
global and local approach may be viewed as \textit{complementary} tools, best
suited to different parameter regimes.
",0,1,0,0,0,0
8556,8557,Pattern representation and recognition with accelerated analog neuromorphic systems,"  Despite being originally inspired by the central nervous system, artificial
neural networks have diverged from their biological archetypes as they have
been remodeled to fit particular tasks. In this paper, we review several
possibilites to reverse map these architectures to biologically more realistic
spiking networks with the aim of emulating them on fast, low-power neuromorphic
hardware. Since many of these devices employ analog components, which cannot be
perfectly controlled, finding ways to compensate for the resulting effects
represents a key challenge. Here, we discuss three different strategies to
address this problem: the addition of auxiliary network components for
stabilizing activity, the utilization of inherently robust architectures and a
training method for hardware-emulated networks that functions without perfect
knowledge of the system's dynamics and parameters. For all three scenarios, we
corroborate our theoretical considerations with experimental results on
accelerated analog neuromorphic platforms.
",1,0,0,1,0,0
6986,6987,The first global-scale 30 m resolution mangrove canopy height map using Shuttle Radar Topography Mission data,"  No high-resolution canopy height map exists for global mangroves. Here we
present the first global mangrove height map at a consistent 30 m pixel
resolution derived from digital elevation model data collected through shuttle
radar topography mission. Additionally, we refined the current global mangrove
area maps by discarding the non-mangrove areas that are included in current
mangrove maps.
",0,1,0,0,0,0
1375,1376,Adversarial Examples: Opportunities and Challenges,"  With the advent of the era of artificial intelligence(AI), deep neural
networks (DNNs) have shown huge superiority over human in image recognition,
speech processing, autonomous vehicles and medical diagnosis. However, recent
studies indicate that DNNs are vulnerable to adversarial examples (AEs) which
are designed by attackers to fool deep learning models. Different from real
examples, AEs can hardly be distinguished from human eyes, but mislead the
model to predict incorrect outputs and therefore threaten security critical
deep-learning applications. In recent years, the generation and defense of AEs
have become a research hotspot in the field of AI security. This article
reviews the latest research progress of AEs. First, we introduce the concept,
cause, characteristic and evaluation metrics of AEs, then give a survey on the
state-of-the-art AE generation methods with the discussion of advantages and
disadvantages. After that we review the existing defenses and discuss their
limitations. Finally, the future research opportunities and challenges of AEs
are prospected.
",0,0,0,1,0,0
11159,11160,"The Uranie platform: an Open-source software for optimisation, meta-modelling and uncertainty analysis","  The high-performance computing resources and the constant improvement of both
numerical simulation accuracy and the experimental measurements with which they
are confronted, bring a new compulsory step to strengthen the credence given to
the simulation results: uncertainty quantification. This can have different
meanings, according to the requested goals (rank uncertainty sources, reduce
them, estimate precisely a critical threshold or an optimal working point) and
it could request mathematical methods with greater or lesser complexity. This
paper introduces the Uranie platform, an Open-source framework which is
currently developed at the Alternative Energies and Atomic Energy Commission
(CEA), in the nuclear energy division, in order to deal with uncertainty
propagation, surrogate models, optimisation issues, code calibration... This
platform benefits from both its dependencies, but also from personal
developments, to offer an efficient data handling model, a C++ and Python
interpreter, advanced graphical tools, several parallelisation solutions...
These methods are very generic and can then be applied to many kinds of code
(as Uranie considers them as black boxes) so to many fields of physics as well.
In this paper, the example of thermal exchange between a plate-sheet and a
fluid is introduced to show how Uranie can be used to perform a large range of
analysis. The code used to produce the figures of this paper can be found in
this https URL along with the sources of the
platform.
",0,0,0,1,0,0
7373,7374,EMRIs and the relativistic loss-cone: The curious case of the fortunate coincidence,"  Extreme mass ratio inspiral (EMRI) events are vulnerable to perturbations by
the stellar background, which can abort them prematurely by deflecting EMRI
orbits to plunging ones that fall directly into the massive black hole (MBH),
or to less eccentric ones that no longer interact strongly with the MBH. A
coincidental hierarchy between the collective resonant Newtonian torques due to
the stellar background, and the relative magnitudes of the leading-order
post-Newtonian precessional and radiative terms of the general relativistic
2-body problem, allows EMRIs to decouple from the background and produce
semi-periodic gravitational wave signals. I review the recent theoretical
developments that confirm this conjectured fortunate coincidence, and briefly
discuss the implications for EMRI rates, and show how these dynamical effects
can be probed locally by stars near the Galactic MBH.
",0,1,0,0,0,0
11273,11274,How AD Can Help Solve Differential-Algebraic Equations,"  A characteristic feature of differential-algebraic equations is that one
needs to find derivatives of some of their equations with respect to time, as
part of so called index reduction or regularisation, to prepare them for
numerical solution. This is often done with the help of a computer algebra
system. We show in two significant cases that it can be done efficiently by
pure algorithmic differentiation. The first is the Dummy Derivatives method,
here we give a mainly theoretical description, with tutorial examples. The
second is the solution of a mechanical system directly from its Lagrangian
formulation. Here we outline the theory and show several non-trivial examples
of using the ""Lagrangian facility"" of the Nedialkov-Pryce initial-value solver
DAETS, namely: a spring-mass-multipendulum system, a prescribed-trajectory
control problem, and long-time integration of a model of the outer planets of
the solar system, taken from the DETEST testing package for ODE solvers.
",0,0,1,0,0,0
6971,6972,Nonequilibrium Work and its Hamiltonian Connection for a Microstate in Nonequilibrium Statistical Thermodynamics: A Case of Mistaken Identity,"  Nonequilibrium work-Hamiltonian connection for a microstate plays a central
role in diverse branches of statistical thermodynamics (fluctuation theorems,
quantum thermodynamics, stochastic thermodynamics, etc.). We show that the
change in the Hamiltonian for a microstate should be identified with the work
done by it, and not the work done on it. This contradicts the current practice
in the field. The difference represents a contribution whose average gives the
work that is dissipated due to irreversibility. As the latter has been
overlooked, the current identification does not properly account for
irreversibilty. As an example, we show that the corrected version of
Jarzynski's relation can be applied to free expansion, where the original
relation fails. Thus, the correction has far-reaching consequences and requires
reassessment of current applications.
",0,1,0,0,0,0
14949,14950,Graphical virtual links and a polynomial of signed cyclic graphs,"  For a signed cyclic graph G, we can construct a unique virtual link L by
taking the medial construction and convert 4-valent vertices of the medial
graph to crossings according to the signs. If a virtual link can occur in this
way then we say that the virtual link is graphical. In the article we shall
prove that a virtual link L is graphical if and only if it is checkerboard
colorable. On the other hand, we introduce a polynomial F[G] for signed cyclic
graphs, which is defined via a deletion-marking recursion. We shall establish
the relationship between F[G] of a signed cyclic graph G and the bracket
polynomial of one of the virtual link diagrams associated with G. Finally we
give a spanning subgraph expansion for F[G].
",0,0,1,0,0,0
216,217,The challenge of realistic music generation: modelling raw audio at scale,"  Realistic music generation is a challenging task. When building generative
models of music that are learnt from data, typically high-level representations
such as scores or MIDI are used that abstract away the idiosyncrasies of a
particular performance. But these nuances are very important for our perception
of musicality and realism, so in this work we embark on modelling music in the
raw audio domain. It has been shown that autoregressive models excel at
generating raw audio waveforms of speech, but when applied to music, we find
them biased towards capturing local signal structure at the expense of
modelling long-range correlations. This is problematic because music exhibits
structure at many different timescales. In this work, we explore autoregressive
discrete autoencoders (ADAs) as a means to enable autoregressive models to
capture long-range correlations in waveforms. We find that they allow us to
unconditionally generate piano music directly in the raw audio domain, which
shows stylistic consistency across tens of seconds.
",0,0,0,1,0,0
11650,11651,Seemless Utilization of Heterogeneous XSede Resources to Accelerate Processing of a High Value Functional Neuroimaging Dataset,"  We describe the technical effort used to process a voluminous high value
human neuroimaging dataset on the Open Science Grid with opportunistic use of
idle HPC resources to boost computing capacity more than 5-fold. With minimal
software development effort and no discernable competitive interference with
other HPC users, this effort delivered 15,000,000 core hours over 7 months.
",0,0,0,0,1,0
7724,7725,Asymptotic Distribution and Simultaneous Confidence Bands for Ratios of Quantile Functions,"  Ratio of medians or other suitable quantiles of two distributions is widely
used in medical research to compare treatment and control groups or in
economics to compare various economic variables when repeated cross-sectional
data are available. Inspired by the so-called growth incidence curves
introduced in poverty research, we argue that the ratio of quantile functions
is a more appropriate and informative tool to compare two distributions. We
present an estimator for the ratio of quantile functions and develop
corresponding simultaneous confidence bands, which allow to assess significance
of certain features of the quantile functions ratio. Derived simultaneous
confidence bands rely on the asymptotic distribution of the quantile functions
ratio and do not require re-sampling techniques. The performance of the
simultaneous confidence bands is demonstrated in simulations. Analysis of the
expenditure data from Uganda in years 1999, 2002 and 2005 illustrates the
relevance of our approach.
",0,0,0,1,0,0
16451,16452,Homology of torus knots,"  Using the method of Elias-Hogancamp and combinatorics of toric braids we give
an explicit formula for the triply graded Khovanov-Rozansky homology of an
arbitrary torus knot, thereby proving some of the conjectures of
Aganagic-Shakirov, Cherednik, Gorsky-Negut and Oblomkov-Rasmussen-Shende.
",0,0,1,0,0,0
3356,3357,On $p$-degree of elliptic curves,"  In this note we investigate the $p$-degree function of elliptic curves over
the field $\mathbb{Q}_p$ of $p$-adic numbers. The $p$-degree measures the least
complexity of a non-zero $p$-torsion point on an elliptic curve. We prove some
properties of this function and compute it explicitly in some special cases.
",0,0,1,0,0,0
10984,10985,Removal of Narrowband Interference (PLI in ECG Signal) Using Ramanujan Periodic Transform (RPT),"  Suppression of interference from narrowband frequency signals play vital role
in many signal processing and communication applications. A transform based
method for suppression of narrow band interference in a biomedical signal is
proposed. As a specific example Electrocardiogram (ECG) is considered for the
analysis. ECG is one of the widely used biomedical signal. ECG signal is often
contaminated with baseline wander noise, powerline interference (PLI) and
artifacts (bioelectric signals), which complicates the processing of raw ECG
signal. This work proposes an approach using Ramanujan periodic transform for
reducing PLI and is tested on a subject data from MIT-BIH Arrhythmia database.
A sum ($E$) of Euclidean error per block ($e_i$) is used as measure to quantify
the suppression capability of RPT and notch filter based methods. The
transformation is performed for different lengths ($N$), namely $36$, $72$,
$108$, $144$, $180$. Every doubling of $N$-points results in $50{\%}$ reduction
in error ($E$).
",0,0,1,1,0,0
17020,17021,Is Smaller Better: A Proposal To Consider Bacteria For Biologically Inspired Modeling,"  Bacteria are easily characterizable model organisms with an impressively
complicated set of capabilities. Among their capabilities is quorum sensing, a
detailed cell-cell signaling system that may have a common origin with
eukaryotic cell-cell signaling. Not only are the two phenomena similar, but
quorum sensing, as is the case with any bacterial phenomenon when compared to
eukaryotes, is also easier to study in depth than eukaryotic cell-cell
signaling. This ease of study is a contrast to the only partially understood
cellular dynamics of neurons. Here we review the literature on the strikingly
neuron-like qualities of bacterial colonies and biofilms, including ion-based
and hormonal signaling, and action potential-like behavior. This allows them to
feasibly act as an analog for neurons that could produce more detailed and more
accurate biologically-based computational models. Using bacteria as the basis
for biologically feasible computational models may allow models to better
harness the tremendous ability of biological organisms to make decisions and
process information. Additionally, principles gleaned from bacterial function
have the potential to influence computational efforts divorced from biology,
just as neuronal function has in the abstract influenced countless machine
learning efforts.
",1,0,0,0,0,0
16827,16828,A finite temperature study of ideal quantum gases in the presence of one dimensional quasi-periodic potential,"  We study the thermodynamics of ideal Bose gas as well as the transport
properties of non interacting bosons and fermions in a one dimensional
quasi-periodic potential, namely Aubry-André (AA) model at finite
temperature. For bosons in finite size systems, the effect of quasi-periodic
potential on the crossover phenomena corresponding to Bose-Einstein
condensation (BEC), superfluidity and localization phenomena at finite
temperatures are investigated. From the ground state number fluctuation we
calculate the crossover temperature of BEC which exhibits a non monotonic
behavior with the strength of AA potential and vanishes at the self-dual
critical point following power law. Appropriate rescaling of the crossover
temperatures reveals universal behavior which is studied for different
quasi-periodicity of the AA model. Finally, we study the temperature and flux
dependence of the persistent current of fermions in presence of a
quasi-periodic potential to identify the localization at the Fermi energy from
the decay of the current.
",0,1,0,0,0,0
12995,12996,Vecchia approximations of Gaussian-process predictions,"  Gaussian processes (GPs) are highly flexible function estimators used for
geospatial analysis, nonparametric regression, and machine learning, but they
are computationally infeasible for large datasets. Vecchia approximations of
GPs have been used to enable fast evaluation of the likelihood for parameter
inference. Here, we study Vecchia approximations of spatial predictions at
observed and unobserved locations, including obtaining joint predictive
distributions at large sets of locations. We propose a general Vecchia
framework for GP predictions, which contains some novel and some existing
special cases. We study the accuracy and computational properties of these
approaches theoretically and numerically. We show that our new approaches
exhibit linear computational complexity in the total number of spatial
locations. We also apply our methods to a satellite dataset of chlorophyll
fluorescence.
",0,0,0,1,0,0
15972,15973,A Correction Method of a Binary Classifier Applied to Multi-label Pairwise Models,"  In this work, we addressed the issue of applying a stochastic classifier and
a local, fuzzy confusion matrix under the framework of multi-label
classification. We proposed a novel solution to the problem of correcting label
pairwise ensembles. The main step of the correction procedure is to compute
classifier- specific competence and cross-competence measures, which estimates
error pattern of the underlying classifier. We considered two improvements of
the method of obtaining confusion matrices. The first one is aimed to deal with
imbalanced labels. The other utilizes double labelled instances which are
usually removed during the pairwise transformation. The proposed methods were
evaluated using 29 benchmark datasets. In order to assess the efficiency of the
introduced models, they were compared against 1 state-of-the-art approach and
the correction scheme based on the original method of confusion matrix
estimation. The comparison was performed using four different multi-label
evaluation measures: macro and micro-averaged F1 loss, zero-one loss and
Hamming loss. Additionally, we investigated relations between classification
quality, which is expressed in terms of different quality criteria, and
characteristics of multi-label datasets such as average imbalance ratio or
label density. The experimental study reveals that the correction approaches
significantly outperforms the reference method only in terms of zero-one loss.
",1,0,0,1,0,0
4051,4052,On orthogonality and learning recurrent networks with long term dependencies,"  It is well known that it is challenging to train deep neural networks and
recurrent neural networks for tasks that exhibit long term dependencies. The
vanishing or exploding gradient problem is a well known issue associated with
these challenges. One approach to addressing vanishing and exploding gradients
is to use either soft or hard constraints on weight matrices so as to encourage
or enforce orthogonality. Orthogonal matrices preserve gradient norm during
backpropagation and may therefore be a desirable property. This paper explores
issues with optimization convergence, speed and gradient stability when
encouraging or enforcing orthogonality. To perform this analysis, we propose a
weight matrix factorization and parameterization strategy through which we can
bound matrix norms and therein control the degree of expansivity induced during
backpropagation. We find that hard constraints on orthogonality can negatively
affect the speed of convergence and model performance.
",1,0,0,0,0,0
20433,20434,Design and characterization of the Large-Aperture Experiment to Detect the Dark Age (LEDA) radiometer systems,"  The Large-Aperture Experiment to Detect the Dark Age (LEDA) was designed to
detect the predicted O(100)mK sky-averaged absorption of the Cosmic Microwave
Background by Hydrogen in the neutral pre- and intergalactic medium just after
the cosmological Dark Age. The spectral signature would be associated with
emergence of a diffuse Ly$\alpha$ background from starlight during 'Cosmic
Dawn'. Recently, Bowman et al. (2018) have reported detection of this predicted
absorption feature, with an unexpectedly large amplitude of 530 mK, centered at
78 MHz. Verification of this result by an independent experiment, such as LEDA,
is pressing. In this paper, we detail design and characterization of the LEDA
radiometer systems, and a first-generation pipeline that instantiates a signal
path model. Sited at the Owens Valley Radio Observatory Long Wavelength Array,
LEDA systems include the station correlator, five well-separated redundant dual
polarization radiometers and backend electronics. The radiometers deliver a
30-85MHz band (16<z<34) and operate as part of the larger interferometric
array, for purposes ultimately of in situ calibration. Here, we report on the
LEDA system design, calibration approach, and progress in characterization as
of January 2016. The LEDA systems are currently being modified to improve
performance near 78 MHz in order to verify the purported absorption feature.
",0,1,0,0,0,0
6260,6261,A Distributed Scheduling Algorithm to Provide Quality-of-Service in Multihop Wireless Networks,"  Control of multihop Wireless networks in a distributed manner while providing
end-to-end delay requirements for different flows, is a challenging problem.
Using the notions of Draining Time and Discrete Review from the theory of fluid
limits of queues, an algorithm that meets delay requirements to various flows
in a network is constructed. The algorithm involves an optimization which is
implemented in a cyclic distributed manner across nodes by using the technique
of iterative gradient ascent, with minimal information exchange between nodes.
The algorithm uses time varying weights to give priority to flows. The
performance of the algorithm is studied in a network with interference modelled
by independent sets.
",1,0,0,0,0,0
11289,11290,A Robot Localization Framework Using CNNs for Object Detection and Pose Estimation,"  External localization is an essential part for the indoor operation of small
or cost-efficient robots, as they are used, for example, in swarm robotics. We
introduce a two-stage localization and instance identification framework for
arbitrary robots based on convolutional neural networks. Object detection is
performed on an external camera image of the operation zone providing robot
bounding boxes for an identification and orientation estimation convolutional
neural network. Additionally, we propose a process to generate the necessary
training data. The framework was evaluated with 3 different robot types and
various identification patterns. We have analyzed the main framework
hyperparameters providing recommendations for the framework operation settings.
We achieved up to 98% mAP@IOU0.5 and only 1.6° orientation error, running
with a frame rate of 50 Hz on a GPU.
",1,0,0,0,0,0
14182,14183,Application of Surface Coil for Nuclear Magnetic Resonance Studies of Semi-conducting Thin Films,"  We conduct a comprehensive set of tests of performance of surface coils used
for nuclear magnetic resonance (NMR) study of quasi 2-dimensional samples. We
report ${^{115} \rm{In}}$ and ${^{31} \rm{P}}$ NMR measurements on InP,
semi-conducting thin substrate samples. Surface coils of both zig-zag
meander-line and concentric spiral geometries were used. We compare reception
sensitivity and signal-to-noise ratio (SNR) of NMR signal obtained by using
surface-type coils to that obtained by standard solenoid-type coils. As
expected, we find that surface-type coils provide better sensitivity for NMR
study of thin films samples. Moreover, we compare the reception sensitivity of
different types of the surface coils. We identify the optimal geometry of the
surface coils for a given application and/or direction of the applied magnetic
field.
",0,1,0,0,0,0
11291,11292,Cardinal Virtues: Extracting Relation Cardinalities from Text,"  Information extraction (IE) from text has largely focused on relations
between individual entities, such as who has won which award. However, some
facts are never fully mentioned, and no IE method has perfect recall. Thus, it
is beneficial to also tap contents about the cardinalities of these relations,
for example, how many awards someone has won. We introduce this novel problem
of extracting cardinalities and discusses the specific challenges that set it
apart from standard IE. We present a distant supervision method using
conditional random fields. A preliminary evaluation results in precision
between 3% and 55%, depending on the difficulty of relations.
",1,0,0,0,0,0
17688,17689,Ranking Causal Influence of Financial Markets via Directed Information Graphs,"  A non-parametric method for ranking stock indices according to their mutual
causal influences is presented. Under the assumption that indices reflect the
underlying economy of a country, such a ranking indicates which countries exert
the most economic influence in an examined subset of the global economy. The
proposed method represents the indices as nodes in a directed graph, where the
edges' weights are estimates of the pair-wise causal influences, quantified
using the directed information functional. This method facilitates using a
relatively small number of samples from each index. The indices are then ranked
according to their net-flow in the estimated graph (sum of the incoming weights
subtracted from the sum of outgoing weights). Daily and minute-by-minute data
from nine indices (three from Asia, three from Europe and three from the US)
were analyzed. The analysis of daily data indicates that the US indices are the
most influential, which is consistent with intuition that the indices
representing larger economies usually exert more influence. Yet, it is also
shown that an index representing a small economy can strongly influence an
index representing a large economy if the smaller economy is indicative of a
larger phenomenon. Finally, it is shown that while inter-region interactions
can be captured using daily data, intra-region interactions require more
frequent samples.
",0,0,0,0,0,1
20766,20767,Deep Neural Generative Model of Functional MRI Images for Psychiatric Disorder Diagnosis,"  Accurate diagnosis of psychiatric disorders plays a critical role in
improving quality of life for patients and potentially supports the development
of new treatments. Many studies have been conducted on machine learning
techniques that seek brain imaging data for specific biomarkers of disorders.
These studies have encountered the following dilemma: An end-to-end
classification overfits to a small number of high-dimensional samples but
unsupervised feature-extraction has the risk of extracting a signal of no
interest. In addition, such studies often provided only diagnoses for patients
without presenting the reasons for these diagnoses. This study proposed a deep
neural generative model of resting-state functional magnetic resonance imaging
(fMRI) data. The proposed model is conditioned by the assumption of the
subject's state and estimates the posterior probability of the subject's state
given the imaging data, using Bayes' rule. This study applied the proposed
model to diagnose schizophrenia and bipolar disorders. Diagnosis accuracy was
improved by a large margin over competitive approaches, namely a support vector
machine, logistic regression, and multilayer perceptron with or without
unsupervised feature-extractors in addition to a Gaussian mixture model. The
proposed model visualizes brain regions largely related to the disorders, thus
motivating further biological investigation.
",1,0,0,1,0,0
14567,14568,Design of $n$- and $p$-type oxide thermoelectrics in LaNiO$_3$/SrTiO$_3(001)$ superlattices exploiting interface polarity,"  We investigate the structural, electronic, transport, and thermoelectric
properties of LaNiO$_3$/SrTiO$_3(001)$ superlattices containing either
exclusively $n$- or $p$-type interfaces or coupled interfaces of opposite
polarity by using density functional theory calculations with an on-site
Coulomb repulsion term. The results show that significant octahedral tilts are
induced in the SrTiO$_3$ part of the superlattice. Moreover, the La-Sr
distances and Ni-O out-of-plane bond lengths at the interfaces exhibit a
distinct variation by about $7\,\%$ with the sign of the electrostatic doping.
In contrast to the much studied LaAlO$_3$/SrTiO$_3$ system, the charge mismatch
at the interfaces is exclusively accommodated within the LaNiO$_3$ layers,
whereas the interface polarity leads to a band offset and to the formation of
an electric field within the coupled superlattice. Features of the electronic
structure indicate an orbital-selective quantization of quantum well states.
The potential- and confinement-induced multiband splitting results in complex
cylindrical Fermi surfaces with a tendency towards nesting that depends on the
interface polarity. The analysis of the thermoelectric response reveals a
particularly large positive Seebeck coefficient ($135~\mu$V/K) and a high
figure of merit ($0.35$) for room-temperature cross-plane transport in the
$p$-type superlattice that is attributed to the participation of the SrTiO$_3$
valence band. Superlattices with either $n$- or $p$-type interfaces show
cross-plane Seebeck coefficients of opposite sign and thus emerge as a platform
to construct an oxide-based thermoelectric generator with structurally and
electronically compatible $n$- and $p$-type oxide thermoelectrics.
",0,1,0,0,0,0
10904,10905,Randomly cross-linked polymer models,"  Polymer models are used to describe chromatin, which can be folded at
different spatial scales by binding molecules. By folding, chromatin generates
loops of various sizes. We present here a randomly cross-linked (RCL) polymer
model, where monomer pairs are connected randomly. We obtain asymptotic
formulas for the steady-state variance, encounter probability, the radius of
gyration, instantaneous displacement and the mean first encounter time between
any two monomers. The analytical results are confirmed by Brownian simulations.
Finally, the present results can be used to extract the minimum number of
cross-links in a chromatin region from {conformation capture} data.
",0,1,0,0,0,0
15623,15624,Information Retrieval and Criticality in Parity-Time-Symmetric Systems,"  By investigating information flow between a general parity-time (PT)
-symmetric non-Hermitian system and an environment, we find that the complete
information retrieval from the environment can be achieved in the PT-unbroken
phase, whereas no information can be retrieved in the PT-broken phase. The
PT-transition point thus marks the reversible-irreversible criticality of
information flow, around which many physical quantities such as the recurrence
time and the distinguishability between quantum states exhibit power-law
behavior. Moreover, by embedding a PT-symmetric system into a larger Hilbert
space so that the entire system obeys unitary dynamics, we reveal that behind
the information retrieval lies a hidden entangled partner protected by PT
symmetry. Possible experimental situations are also discussed.
",0,1,0,0,0,0
19620,19621,Rotation Averaging and Strong Duality,"  In this paper we explore the role of duality principles within the problem of
rotation averaging, a fundamental task in a wide range of computer vision
applications. In its conventional form, rotation averaging is stated as a
minimization over multiple rotation constraints. As these constraints are
non-convex, this problem is generally considered challenging to solve globally.
We show how to circumvent this difficulty through the use of Lagrangian
duality. While such an approach is well-known it is normally not guaranteed to
provide a tight relaxation. Based on spectral graph theory, we analytically
prove that in many cases there is no duality gap unless the noise levels are
severe. This allows us to obtain certifiably global solutions to a class of
important non-convex problems in polynomial time.
We also propose an efficient, scalable algorithm that out-performs general
purpose numerical solvers and is able to handle the large problem instances
commonly occurring in structure from motion settings. The potential of this
proposed method is demonstrated on a number of different problems, consisting
of both synthetic and real-world data.
",1,0,0,0,0,0
8824,8825,Cellular function given parametric variation: excitability in the Hodgkin-Huxley model,"  How is reliable physiological function maintained in cells despite
considerable variability in the values of key parameters of multiple
interacting processes that govern that function? Here we use the classic
Hodgkin-Huxley formulation of the squid giant axon action potential to propose
a possible approach to this problem. Although the full Hodgkin-Huxley model is
very sensitive to fluctuations that independently occur in its many parameters,
the outcome is in fact determined by simple combinations of these parameters
along two physiological dimensions: Structural and Kinetic (denoted $S$ and
$K$). Structural parameters describe the properties of the cell, including its
capacitance and the densities of its ion channels. Kinetic parameters are those
that describe the opening and closing of the voltage-dependent conductances.
The impacts of parametric fluctuations on the dynamics of the system, seemingly
complex in the high dimensional representation of the Hodgkin-Huxley model, are
tractable when examined within the $S-K$ plane. We demonstrate that slow
inactivation, a ubiquitous activity-dependent feature of ionic channels, is a
powerful local homeostatic control mechanism that stabilizes excitability amid
changes in structural and kinetic parameters.
",0,0,0,0,1,0
5641,5642,Latent Hinge-Minimax Risk Minimization for Inference from a Small Number of Training Samples,"  Deep Learning (DL) methods show very good performance when trained on large,
balanced data sets. However, many practical problems involve imbalanced data
sets, or/and classes with a small number of training samples. The performance
of DL methods as well as more traditional classifiers drops significantly in
such settings. Most of the existing solutions for imbalanced problems focus on
customizing the data for training. A more principled solution is to use mixed
Hinge-Minimax risk [19] specifically designed to solve binary problems with
imbalanced training sets. Here we propose a Latent Hinge Minimax (LHM) risk and
a training algorithm that generalizes this paradigm to an ensemble of
hyperplanes that can form arbitrary complex, piecewise linear boundaries. To
extract good features, we combine LHM model with CNN via transfer learning. To
solve multi-class problem we map pre-trained category-specific LHM classifiers
to a multi-class neural network and adjust the weights with very fast tuning.
LHM classifier enables the use of unlabeled data in its training and the
mapping allows for multi-class inference, resulting in a classifier that
performs better than alternatives when trained on a small number of training
samples.
",1,0,0,0,0,0
20475,20476,Learning in the Repeated Secretary Problem,"  In the classical secretary problem, one attempts to find the maximum of an
unknown and unlearnable distribution through sequential search. In many
real-world searches, however, distributions are not entirely unknown and can be
learned through experience. To investigate learning in such a repeated
secretary problem we conduct a large-scale behavioral experiment in which
people search repeatedly from fixed distributions. In contrast to prior
investigations that find no evidence for learning in the classical scenario, in
the repeated setting we observe substantial learning resulting in near-optimal
stopping behavior. We conduct a Bayesian comparison of multiple behavioral
models which shows that participants' behavior is best described by a class of
threshold-based models that contains the theoretically optimal strategy.
Fitting such a threshold-based model to data reveals players' estimated
thresholds to be surprisingly close to the optimal thresholds after only a
small number of games.
",1,0,0,0,0,0
8271,8272,Human-in-the-Loop SLAM,"  Building large-scale, globally consistent maps is a challenging problem, made
more difficult in environments with limited access, sparse features, or when
using data collected by novice users. For such scenarios, where
state-of-the-art mapping algorithms produce globally inconsistent maps, we
introduce a systematic approach to incorporating sparse human corrections,
which we term Human-in-the-Loop Simultaneous Localization and Mapping
(HitL-SLAM). Given an initial factor graph for pose graph SLAM, HitL-SLAM
accepts approximate, potentially erroneous, and rank-deficient human input,
infers the intended correction via expectation maximization (EM),
back-propagates the extracted corrections over the pose graph, and finally
jointly optimizes the factor graph including the human inputs as human
correction factor terms, to yield globally consistent large-scale maps. We thus
contribute an EM formulation for inferring potentially rank-deficient human
corrections to mapping, and human correction factor extensions to the factor
graphs for pose graph SLAM that result in a principled approach to joint
optimization of the pose graph while simultaneously accounting for multiple
forms of human correction. We present empirical results showing the
effectiveness of HitL-SLAM at generating globally accurate and consistent maps
even when given poor initial estimates of the map.
",1,0,0,0,0,0
17991,17992,"A study of cyber security in hospitality industry- threats and countermeasures: case study in Reno, Nevada","  The purpose of this study is to analyze cyber security and security practices
of electronic information and network system, network threats, and techniques
to prevent the cyber attacks in hotels. Helping the information technology
directors and chief information officers (CIO) is the aim of this study to
advance policy for security of electronic information in hotels and suggesting
some techniques and tools to secure the computer networks. This research is
completely qualitative while the case study and interviews have done in 5
random hotels in Reno, Nevada, United States of America. The interview has done
with 50 hotel guests, 10 front desk employees, 3 IT manager and 2 assistant of
General manager. The results show that hotels' cyber security is very low and
hotels are very vulnerable in this regard and at the end, the implications and
contribution of the study is mentioned.
",1,0,0,0,0,0
13862,13863,Quantifiers on languages and codensity monads,"  This paper contributes to the techniques of topo-algebraic recognition for
languages beyond the regular setting as they relate to logic on words. In
particular, we provide a general construction on recognisers corresponding to
adding one layer of various kinds of quantifiers and prove a related
Reutenauer-type theorem. Our main tools are codensity monads and duality
theory. Our construction hinges, in particular, on a measure-theoretic
characterisation of the profinite monad of the free S-semimodule monad for
finite and commutative semirings S, which generalises our earlier insight that
the Vietoris monad on Boolean spaces is the codensity monad of the finite
powerset functor.
",1,0,1,0,0,0
17837,17838,Asymptotics for high-dimensional covariance matrices and quadratic forms with applications to the trace functional and shrinkage,"  We establish large sample approximations for an arbitray number of bilinear
forms of the sample variance-covariance matrix of a high-dimensional vector
time series using $ \ell_1$-bounded and small $\ell_2$-bounded weighting
vectors. Estimation of the asymptotic covariance structure is also discussed.
The results hold true without any constraint on the dimension, the number of
forms and the sample size or their ratios. Concrete and potential applications
are widespread and cover high-dimensional data science problems such as tests
for large numbers of covariances, sparse portfolio optimization and projections
onto sparse principal components or more general spanning sets as frequently
considered, e.g. in classification and dictionary learning. As two specific
applications of our results, we study in greater detail the asymptotics of the
trace functional and shrinkage estimation of covariance matrices. In shrinkage
estimation, it turns out that the asymptotics differs for weighting vectors
bounded away from orthogonaliy and nearly orthogonal ones in the sense that
their inner product converges to 0.
",0,0,1,1,0,0
5973,5974,Pressure-induced Superconductivity in the Three-component Fermion Topological Semimetal Molybdenum Phosphide,"  Topological semimetal, a novel state of quantum matter hosting exotic
emergent quantum phenomena dictated by the non-trivial band topology, has
emerged as a new frontier in condensed-matter physics. Very recently, a
coexistence of triply degenerate points of band crossing and Weyl points near
the Fermi level was theoretically predicted and immediately experimentally
verified in single crystalline molybdenum phosphide (MoP). Here we show in this
material the high-pressure electronic transport and synchrotron X-ray
diffraction (XRD) measurements, combined with density functional theory (DFT)
calculations. We report the emergence of pressure-induced superconductivity in
MoP with a critical temperature Tc of about 2 K at 27.6 GPa, rising to 3.7 K at
the highest pressure of 95.0 GPa studied. No structural phase transitions is
detected up to 60.6 GPa from the XRD. Meanwhile, the Weyl points and triply
degenerate points topologically protected by the crystal symmetry are retained
at high pressure as revealed by our DFT calculations. The coexistence of
three-component fermion and superconductivity in heavily pressurized MoP offers
an excellent platform to study the interplay between topological phase of
matter and superconductivity.
",0,1,0,0,0,0
4086,4087,Integrating Runtime Values with Source Code to Facilitate Program Comprehension,"  An inherently abstract nature of source code makes programs difficult to
understand. In our research, we designed three techniques utilizing concrete
values of variables and other expressions during program execution.
RuntimeSearch is a debugger extension searching for a given string in all
expressions at runtime. DynamiDoc generates documentation sentences containing
examples of arguments, return values and state changes. RuntimeSamp augments
source code lines in the IDE (integrated development environment) with sample
variable values. In this post-doctoral article, we briefly describe these three
approaches and related motivational studies, surveys and evaluations. We also
reflect on the PhD study, providing advice for current students. Finally,
short-term and long-term future work is described.
",1,0,0,0,0,0
16279,16280,Superluminal transmission of phase modulation information by a long macroscopic pulse propagating through interstellar space,"  A method of transmitting information in interstellar space at superluminal,
or $> c$, speeds is proposed. The information is encoded as phase modulation of
an electromagnetic wave of constant intensity, i.e. fluctuations in the rate of
energy transport plays no role in the communication, and no energy is
transported at speed $>$ c. Of course, such a constant wave can ultimately last
only the duration of its enveloping wave packet. However, as a unique feature
of this paper, we assume the source is sufficiently steady to be capable of
emitting wave packets, or pulses, of size much larger than the separation
between sender and receiver. Therefore, if a pre-existing and enduring wave
envelope already connects the two sides, the subluminal nature of the
envelope's group velocity will no longer slow down the communication, which is
now limited by the speed at which information encoded as phase modulation
propagates through the plasma, i.e. the phase velocity $v_p > c$. The method
involves no sharp structure in either time or frequency. As a working example,
we considered two spaceships separated by 1 lt-s in the local hot bubble.
Provided the bandwidth of the extra Fourier modes generated by the phase
modulation is much smaller than the carrier wave frequency, the radio
communication of a message, encoded as a specific alignment between the carrier
wave phase and the anomalous (modulated) phase, can take place at a speed in
excess of light by a few parts in 10$^{11}$ at $\nu\approx 1$~GHz, and higher
at smaller $\nu$.
",0,1,0,0,0,0
16295,16296,Orbits of monomials and factorization into products of linear forms,"  This paper is devoted to the factorization of multivariate polynomials into
products of linear forms, a problem which has applications to differential
algebra, to the resolution of systems of polynomial equations and to Waring
decomposition (i.e., decomposition in sums of d-th powers of linear forms; this
problem is also known as symmetric tensor decomposition). We provide three
black box algorithms for this problem. Our main contribution is an algorithm
motivated by the application to Waring decomposition. This algorithm reduces
the corresponding factorization problem to simultaenous matrix diagonalization,
a standard task in linear algebra. The algorithm relies on ideas from invariant
theory, and more specifically on Lie algebras. Our second algorithm
reconstructs a factorization from several bi-variate projections. Our third
algorithm reconstructs it from the determination of the zero set of the input
polynomial, which is a union of hyperplanes.
",1,0,0,0,0,0
12909,12910,A game theoretic approach to a network cloud storage problem,"  The use of game theory in the design and control of large scale networked
systems is becoming increasingly more important. In this paper, we follow this
approach to efficiently solve a network allocation problem motivated by
peer-to- peer cloud storage models as alternatives to classical centralized
cloud storage services. To this aim, we propose an allocation algorithm that
allows the units to use their neighbors to store a back up of their data. We
prove convergence, characterize the final allocation, and corroborate our
analysis with extensive numerical simulation that shows the good performance of
the algorithm in terms of scalability, complexity and structure of the
solution.
",1,0,1,0,0,0
11431,11432,A Koszul sign map,"  We define a Koszul sign map encoding the Koszul sign convention. A
cohomological interpretation is given.
",0,0,1,0,0,0
12710,12711,"Fabrication of quencher-free liquid scintillator-based, high-activity $^{222}$Rn calibration sources for the Borexino detector","  A reliable and consistently reproducible technique to fabricate
$^{222}$Rn-loaded radioactive sources ($\sim$0.5-1 kBq just after fabrication)
based on liquid scintillator (LS), with negligible amounts of LS quencher
contaminants, was implemented. This work demonstrates the process that will be
used during the Borexino detector's upcoming calibration campaign, with one or
several $\sim$100 Bq such sources will be deployed at different positions in
its fiducial volume, currently showing unprecedented levels of radiopurity.
These sources need to fulfill stringent requirements of $^{222}$Rn activity,
transparency to the radiations of interest and complete removability from the
detector to ensure their impact on Borexino's radiopurity is negligible.
Moreover, the need for a clean, undistorted spectral signal for the
calibrations imposes a tight requirement to minimize quenching agents
(""quenchers"") to null or extremely low levels.
",0,1,0,0,0,0
2306,2307,Developmental tendencies in the Academic Field of Intellectual Property through the Identification of Invisible Colleges,"  The emergence of intellectual property as an academic issue opens a big gate
to a cross-disciplinary field. Different disciplines start a dialogue in the
framework of the international multilateral treaties in the early 90's. As
global economy demands new knowledge on intellectual property, Science grows at
its own pace. However, the degree of consolidation of cross-disciplinary
academic communities is not clear. In order to know how closely related are
these communities, this paper proposes a mixed methodology to find invisible
colleges in the production about intellectual property. The articles examined
in this paper were extracted from Web of Science. The analyzed period was from
1994 to 2016, taking into account the signature of the agreement on
Trade-Related Aspects of Intellectual Property Rights in the early 90's. A
total amount of 1580 papers were processed through co-citation network
analysis. An especial technique, which combine algorithms of community
detection and defining population of articles through thresholds of shared
references, was applied. In order to contrast the invisible colleges that
emerged with the existence of formal institutional relations, it was made a
qualitative tracking of the authors with respect to their institutional
affiliation, lines of research and meeting places. Both methods show that the
subjects of interest can be grouped into 13 different issues related to
intellectual property field. Even though most of them are related to Laws and
Economics, there are weak linkages between disciplines which could indicate the
construction of a cross-disciplinary field.
",1,1,0,0,0,0
15052,15053,Selective probing of hidden spin-polarized states in inversion-symmetric bulk MoS2,"  Spin- and angle-resolved photoemission spectroscopy is used to reveal that a
large spin polarization is observable in the bulk centrosymmetric transition
metal dichalcogenide MoS2. It is found that the measured spin polarization can
be reversed by changing the handedness of incident circularly-polarized light.
Calculations based on a three-step model of photoemission show that the valley
and layer-locked spin-polarized electronic states can be selectively addressed
by circularly-polarized light, therefore providing a novel route to probe these
hidden spin-polarized states in inversion-symmetric systems as predicted by
Zhang et al. [Nature Physics 10, 387 (2014)].
",0,1,0,0,0,0
7740,7741,Improved Absolute Frequency Measurement of the 171Yb Optical Lattice Clock at KRISS Relative to the SI Second,"  We measured the absolute frequency of the $^1S_0$ - $^3P_0$ transition of
$^{171}$Yb atoms confined in a one-dimensional optical lattice relative to the
SI second. The determined frequency was 518 295 836 590 863.38(57) Hz. The
uncertainty was reduced by a factor of 14 compared with our previously reported
value in 2013 due to the significant improvements in decreasing the systematic
uncertainties. This result is expected to contribute to the determination of a
new recommended value for the secondary representations of the second.
",0,1,0,0,0,0
11781,11782,Total energy of radial mappings,"  The main aim of this paper is to extend one of the main results of Iwaniec
and Onninen (Arch. Ration. Mech. Anal., 194: 927-986, 2009). We prove that, the
so called total energy functional defined on the class of radial streachings
between annuli attains its minimum on a total energy diffeomorphism between
annuli. This involves a subtle analysis of some special ODE.
",0,0,1,0,0,0
13931,13932,Value Directed Exploration in Multi-Armed Bandits with Structured Priors,"  Multi-armed bandits are a quintessential machine learning problem requiring
the balancing of exploration and exploitation. While there has been progress in
developing algorithms with strong theoretical guarantees, there has been less
focus on practical near-optimal finite-time performance. In this paper, we
propose an algorithm for Bayesian multi-armed bandits that utilizes
value-function-driven online planning techniques. Building on previous work on
UCB and Gittins index, we introduce linearly-separable value functions that
take both the expected return and the benefit of exploration into consideration
to perform n-step lookahead. The algorithm enjoys a sub-linear performance
guarantee and we present simulation results that confirm its strength in
problems with structured priors. The simplicity and generality of our approach
makes it a strong candidate for analyzing more complex multi-armed bandit
problems.
",1,0,0,1,0,0
7226,7227,Envy-free Matchings with Lower Quotas,"  While every instance of the Hospitals/Residents problem admits a stable
matching, the problem with lower quotas (HR-LQ) has instances with no stable
matching. For such an instance, we expect the existence of an envy-free
matching, which is a relaxation of a stable matching preserving a kind of
fairness property. In this paper, we investigate the existence of an envy-free
matching in several settings, in which hospitals have lower quotas and not all
doctor-hospital pairs are acceptable. We first show that, for an HR-LQ
instance, we can efficiently decide the existence of an envy-free matching.
Then, we consider envy-freeness in the Classified Stable Matching model due to
Huang (2010), i.e., each hospital has lower and upper quotas on subsets of
doctors. We show that, for this model, deciding the existence of an envy-free
matching is NP-hard in general, but solvable in polynomial time if quotas are
paramodular.
",1,0,0,0,0,0
12370,12371,Annealing stability of magnetic tunnel junctions based on dual MgO free layers and [Co/Ni] based thin synthetic antiferromagnet fixed system,"  We study the annealing stability of bottom-pinned perpendicularly magnetized
magnetic tunnel junctions based on dual MgO free layers and thin fixed systems
comprising a hard [Co/Ni] multilayer antiferromagnetically coupled to thin a Co
reference layer and a FeCoB polarizing layer. Using conventional magnetometry
and advanced broadband ferromagnetic resonance, we identify the properties of
each sub-unit of the magnetic tunnel junction and demonstrate that this
material option can ensure a satisfactory resilience to the 400$^\circ$C
thermal annealing needed in solid-state magnetic memory applications. The dual
MgO free layer possesses an anneal-robust 0.4 T effective anisotropy and
suffers only a minor increase of its Gilbert damping from 0.007 to 0.010 for
the toughest annealing conditions. Within the fixed system, the ferro-coupler
and texture-breaking TaFeCoB layer keeps an interlayer exchange above 0.8
mJ/m$^2$, while the Ru antiferrocoupler layer within the synthetic
antiferromagnet maintains a coupling above -0.5 mJ/m$^2$. These two strong
couplings maintain the overall functionality of the tunnel junction upon the
toughest annealing despite the gradual degradation of the thin Co layer
anisotropy that may reduce the operation margin in spin torque memory
applications. Based on these findings, we propose further optimization routes
for the next generation magnetic tunnel junctions.
",0,1,0,0,0,0
2865,2866,A Survey of Bandwidth and Latency Enhancement Approaches for Mobile Cloud Game Multicasting,"  Among mobile cloud applications, mobile cloud gaming has gained a significant
popularity in the recent years. In mobile cloud games, textures, game objects,
and game events are typically streamed from a server to the mobile client.
One of the challenges in cloud mobile gaming is how to efficiently multicast
gaming contents and updates in Massively Multi-player Online Games (MMOGs).
This report surveys the state of art techniques introduced for game
synchronization and multicasting mechanisms to decrease latency and bandwidth
consumption, and discuss several schemes that have been proposed in this area
that can be applied to any networked gaming context. From our point of view,
gaming applications demand high interactivity. Therefore, concentrating on
gaming applications will eventually cover a wide range of applications without
violating the limited scope of this survey.
",1,0,0,0,0,0
20624,20625,Towards a Science of Mind,"  The ancient mind/body problem continues to be one of deepest mysteries of
science and of the human spirit. Despite major advances in many fields, there
is still no plausible link between subjective experience (qualia) and its
realization in the body. This paper outlines some of the elements of a rigorous
science of mind (SoM) - key ideas include scientific realism of mind, agnostic
mysterianism, careful attention to language, and a focus on concrete
(touchstone) questions and results.
",1,0,0,0,1,0
15484,15485,On-line Building Energy Optimization using Deep Reinforcement Learning,"  Unprecedented high volumes of data are becoming available with the growth of
the advanced metering infrastructure. These are expected to benefit planning
and operation of the future power system, and to help the customers transition
from a passive to an active role. In this paper, we explore for the first time
in the smart grid context the benefits of using Deep Reinforcement Learning, a
hybrid type of methods that combines Reinforcement Learning with Deep Learning,
to perform on-line optimization of schedules for building energy management
systems. The learning procedure was explored using two methods, Deep Q-learning
and Deep Policy Gradient, both of them being extended to perform multiple
actions simultaneously. The proposed approach was validated on the large-scale
Pecan Street Inc. database. This highly-dimensional database includes
information about photovoltaic power generation, electric vehicles as well as
buildings appliances. Moreover, these on-line energy scheduling strategies
could be used to provide real-time feedback to consumers to encourage more
efficient use of electricity.
",1,0,1,0,0,0
520,521,Towards a Service-oriented Platform for Intelligent Apps in Intermediate Cities,"  Smart cities are a growing trend in many cities in Argentina. In particular,
the so-called intermediate cities present a context and requirements different
from those of large cities with respect to smart cities. One aspect of
relevance is to encourage the development of applications (generally for mobile
devices) that enable citizens to take advantage of data and services normally
associated with the city, for example, in the urban mobility domain. In this
work, a platform is proposed for intermediate cities that provide ""high level""
services and that allow the construction of software applications that consume
those services. Our platform-centric strategy focused aims to integrate systems
and heterogeneous data sources, and provide ""intelligent"" services to different
applications. Examples of these services include: construction of user
profiles, recommending local events, and collaborative sensing based on data
mining techniques, among others. In this work, the design of this platform
(currently in progress) is described, and experiences of applications for urban
mobility are discussed, which are being migrated in the form of reusable
services provided by the platform
",1,0,0,0,0,0
12335,12336,Non-Gaussian Autoregressive Processes with Tukey g-and-h Transformations,"  When performing a time series analysis of continuous data, for example from
climate or environmental problems, the assumption that the process is Gaussian
is often violated. Therefore, we introduce two non-Gaussian autoregressive time
series models that are able to fit skewed and heavy-tailed time series data.
Our two models are based on the Tukey g-and-h transformation. We discuss
parameter estimation, order selection, and forecasting procedures for our
models and examine their performances in a simulation study. We demonstrate the
usefulness of our models by applying them to two sets of wind speed data.
",0,0,0,1,0,0
15216,15217,Face Deidentification with Generative Deep Neural Networks,"  Face deidentification is an active topic amongst privacy and security
researchers. Early deidentification methods relying on image blurring or
pixelization were replaced in recent years with techniques based on formal
anonymity models that provide privacy guaranties and at the same time aim at
retaining certain characteristics of the data even after deidentification. The
latter aspect is particularly important, as it allows to exploit the
deidentified data in applications for which identity information is irrelevant.
In this work we present a novel face deidentification pipeline, which ensures
anonymity by synthesizing artificial surrogate faces using generative neural
networks (GNNs). The generated faces are used to deidentify subjects in images
or video, while preserving non-identity-related aspects of the data and
consequently enabling data utilization. Since generative networks are very
adaptive and can utilize a diverse set of parameters (pertaining to the
appearance of the generated output in terms of facial expressions, gender,
race, etc.), they represent a natural choice for the problem of face
deidentification. To demonstrate the feasibility of our approach, we perform
experiments using automated recognition tools and human annotators. Our results
show that the recognition performance on deidentified images is close to
chance, suggesting that the deidentification process based on GNNs is highly
effective.
",1,0,0,0,0,0
16555,16556,Privacy Preserving Identification Using Sparse Approximation with Ambiguization,"  In this paper, we consider a privacy preserving encoding framework for
identification applications covering biometrics, physical object security and
the Internet of Things (IoT). The proposed framework is based on a sparsifying
transform, which consists of a trained linear map, an element-wise
nonlinearity, and privacy amplification. The sparsifying transform and privacy
amplification are not symmetric for the data owner and data user. We
demonstrate that the proposed approach is closely related to sparse ternary
codes (STC), a recent information-theoretic concept proposed for fast
approximate nearest neighbor (ANN) search in high dimensional feature spaces
that being machine learning in nature also offers significant benefits in
comparison to sparse approximation and binary embedding approaches. We
demonstrate that the privacy of the database outsourced to a server as well as
the privacy of the data user are preserved at a low computational cost, storage
and communication burdens.
",1,0,0,1,0,0
15359,15360,"Note on ""Average resistance of toroidal graphs"" by Rossi, Frasca and Fagnani","  In our recent paper W.S. Rossi, P. Frasca and F. Fagnani, ""Average resistance
of toroidal graphs"", SIAM Journal on Control and Optimization,
53(4):2541--2557, 2015, we studied how the average resistances of
$d$-dimensional toroidal grids depend on the graph topology and on the
dimension of the graph. Our results were based on the connection between
resistance and Laplacian eigenvalues. In this note, we contextualize our work
in the body of literature about random walks on graphs. Indeed, the average
effective resistance of the $d$-dimensional toroidal grid is proportional to
the mean hitting time of the simple random walk on that grid. If $d\geq3 $,
then the average resistance can be bounded uniformly in the number of nodes and
its value is of order $1/d$ for large $d$.
",1,0,1,0,0,0
8652,8653,Bayesian Detection of Abnormal ADS in Mutant Caenorhabditis elegans Embryos,"  Cell division timing is critical for cell fate specification and
morphogenesis during embryogenesis. How division timings are regulated among
cells during development is poorly understood. Here we focus on the comparison
of asynchrony of division between sister cells (ADS) between wild-type and
mutant individuals of Caenorhabditis elegans. Since the replicate number of
mutant individuals of each mutated gene, usually one, is far smaller than that
of wild-type, direct comparison of two distributions of ADS between wild-type
and mutant type, such as Kolmogorov- Smirnov test, is not feasible. On the
other hand, we find that sometimes ADS is correlated with the life span of
corresponding mother cell in wild-type. Hence, we apply a semiparametric
Bayesian quantile regression method to estimate the 95% confidence interval
curve of ADS with respect to life span of mother cell of wild-type individuals.
Then, mutant-type ADSs outside the corresponding confidence interval are
selected out as abnormal one with a significance level of 0.05. Simulation
study demonstrates the accuracy of our method and Gene Enrichment Analysis
validates the results of real data sets.
",0,0,0,1,1,0
13810,13811,"Normalizing the Taylor expansion of non-deterministic λ-terms, via parallel reduction of resource vectors","  It has been known since Ehrhard and Regnier's seminal work on the Taylor
expansion of {\lambda}-terms that this operation commutes with normalization:
the expansion of a {\lambda}-term is always normalizable and its normal form is
the expansion of the Böhm tree of the term. We generalize this result to the
non-uniform setting of the algebraic {\lambda}-calculus, i.e.
{\lambda}-calculus extended with linear combinations of terms. This requires us
to tackle two difficulties: foremost is the fact that Ehrhard and Regnier's
techniques rely heavily on the uniform, deterministic nature of the ordinary
{\lambda}-calculus, and thus cannot be adapted; second is the absence of any
satisfactory generic extension of the notion of Böhm tree in presence of
quantitative non-determinism, which is reflected by the fact that the Taylor
expansion of an algebraic {\lambda}-term is not always normalizable. Our
solution is to provide a fine grained study of the dynamics of
{\beta}-reduction under Taylor expansion, by introducing a notion of reduction
on resource vectors, i.e. infinite linear combinations of resource
{\lambda}-terms. The latter form the multilinear fragment of the differential
{\lambda}-calculus, and resource vectors are the target of the Taylor expansion
of {\lambda}-terms. We show the reduction of resource vectors contains the
image of any {\beta}-reduction step, from which we deduce that Taylor expansion
and normalization commute on the nose. We moreover identify a class of
algebraic {\lambda}-terms, encompassing both normalizable algebraic
{\lambda}-terms and arbitrary ordinary {\lambda}-terms: the expansion of these
is always normalizable, which guides the definition of a generalization of
Böhm trees to this setting.
",1,0,0,0,0,0
5393,5394,Collaborative similarity analysis of multilayer developer-project bipartite network,"  To understand the multiple relations between developers and projects on
GitHub as a whole, we model them as a multilayer bipartite network and analyze
the degree distributions, the nearest neighbors' degree distributions and their
correlations with degree, and the collaborative similarity distributions and
their correlations with degree. Our results show that all degree distributions
have a power-law form, especially, the degree distribution of projects in
watching layer has double power-law form. Negative correlations between nearest
neighbors' degree and degree for both developers and projects are observed in
both layers, exhibiting a disassortative mixing pattern. The collaborative
similarity of both developers and projects negatively correlates with degree in
watching layer, while a positive correlations is observed for developers in
forking layer and no obvious correlation is observed for projects in forking
layer.
",1,1,0,0,0,0
3885,3886,The Externalities of Exploration and How Data Diversity Helps Exploitation,"  Online learning algorithms, widely used to power search and content
optimization on the web, must balance exploration and exploitation, potentially
sacrificing the experience of current users for information that will lead to
better decisions in the future. Recently, concerns have been raised about
whether the process of exploration could be viewed as unfair, placing too much
burden on certain individuals or groups. Motivated by these concerns, we
initiate the study of the externalities of exploration - the undesirable side
effects that the presence of one party may impose on another - under the linear
contextual bandits model. We introduce the notion of a group externality,
measuring the extent to which the presence of one population of users impacts
the rewards of another. We show that this impact can in some cases be negative,
and that, in a certain sense, no algorithm can avoid it. We then study
externalities at the individual level, interpreting the act of exploration as
an externality imposed on the current user of a system by future users. This
drives us to ask under what conditions inherent diversity in the data makes
explicit exploration unnecessary. We build on a recent line of work on the
smoothed analysis of the greedy algorithm that always chooses the action that
currently looks optimal, improving on prior results to show that a greedy
approach almost matches the best possible Bayesian regret rate of any other
algorithm on the same problem instance whenever the diversity conditions hold,
and that this regret is at most $\tilde{O}(T^{1/3})$. Returning to group-level
effects, we show that under the same conditions, negative group externalities
essentially vanish under the greedy algorithm. Together, our results uncover a
sharp contrast between the high externalities that exist in the worst case, and
the ability to remove all externalities if the data is sufficiently diverse.
",0,0,0,1,0,0
16326,16327,A short note on the computation of the generalised Jacobsthal function for paired progressions,"  Jacobsthal's function was recently generalised for the case of paired
progressions. It was proven that a specific bound of this function is
sufficient for the truth of Goldbach's conjecture and of the prime pairs
conjecture as well. We extended and adapted algorithms described for the
computation of the common Jacobsthal function, and computed respective function
values of the paired Jacobsthal function for primorial numbers for primes up to
73. All these values fulfil the conjectured specific bound. In addition to this
note, we provide a detailed review of the algorithmic approaches and the
complete computational results in ancillary files.
",0,0,1,0,0,0
5248,5249,Focused time-lapse inversion of radio and audio magnetotelluric data,"  Geoelectrical techniques are widely used to monitor groundwater processes,
while surprisingly few studies have considered audio (AMT) and radio (RMT)
magnetotellurics for such purposes. In this numerical investigation, we analyze
to what extent inversion results based on AMT and RMT monitoring data can be
improved by (1) time-lapse difference inversion; (2) incorporation of
statistical information about the expected model update (i.e., the model
regularization is based on a geostatistical model); (3) using alternative model
norms to quantify temporal changes (i.e., approximations of l1 and Cauchy norms
using iteratively reweighted least-squares), (4) constraining model updates to
predefined ranges (i.e., using Lagrange Multipliers to only allow either
increases or decreases of electrical resistivity with respect to background
conditions). To do so, we consider a simple illustrative model and a more
realistic test case related to seawater intrusion. The results are encouraging
and show significant improvements when using time-lapse difference inversion
with non l2 model norms. Artifacts that may arise when imposing compactness of
regions with temporal changes can be suppressed through inequality constraints
to yield models without oscillations outside the true region of temporal
changes. Based on these results, we recommend approximate l1-norm solutions as
they can resolve both sharp and smooth interfaces within the same model.
",0,1,0,0,0,0
9201,9202,A new computational method for a model of C. elegans biomechanics: Insights into elasticity and locomotion performance,"  An organism's ability to move freely is a fundamental behaviour in the animal
kingdom. To understand animal locomotion requires a characterisation of the
material properties, as well as the biomechanics and physiology. We present a
biomechanical model of C. elegans locomotion together with a novel finite
element method. We formulate our model as a nonlinear initial-boundary value
problem which allows the study of the dynamics of arbitrary body shapes,
undulation gaits and the link between the animal's material properties and its
performance across a range of environments. Our model replicates behaviours
across a wide range of environments. It makes strong predictions on the viable
range of the worm's Young's modulus and suggests that animals can control speed
via the known mechanism of gait modulation that is observed across different
media.
",0,1,0,0,0,0
2488,2489,Learning in anonymous nonatomic games with applications to first-order mean field games,"  We introduce a model of anonymous games with the player dependent action
sets. We propose several learning procedures based on the well-known Fictitious
Play and Online Mirror Descent and prove their convergence to equilibrium under
the classical monotonicity condition. Typical examples are first-order mean
field games.
",0,0,1,0,0,0
8831,8832,Research Opportunities and Visions for Smart and Pervasive Health,"  Improving the health of the nation's population and increasing the
capabilities of the US healthcare system to support diagnosis, treatment, and
prevention of disease is a critical national and societal priority. In the past
decade, tremendous advances in expanding computing capabilities--sensors, data
analytics, networks, advanced imaging, and cyber-physical systems--have, and
will continue to, enhance healthcare and health research, with resulting
improvements in health and wellness. However, the cost and complexity of
healthcare continues to rise alongside the impact of poor health on
productivity and quality of life. What is lacking are transformative
capabilities that address significant health and healthcare trends: the growing
demands and costs of chronic disease, the greater responsibility placed on
patients and informal caregivers, and the increasing complexity of health
challenges in the US, including mental health, that are deeply rooted in a
person's social and environmental context.
",1,0,0,0,0,0
10598,10599,Second Order Statistics Analysis and Comparison between Arithmetic and Geometric Average Fusion,"  Two fundamental approaches to information averaging are based on linear and
logarithmic combination, yielding the arithmetic average (AA) and geometric
average (GA) of the fusing initials, respectively. In the context of target
tracking, the two most common formats of data to be fused are random variables
and probability density functions, namely $v$-fusion and $f$-fusion,
respectively. In this work, we analyze and compare the second order statistics
(including variance and mean square error) of AA and GA in terms of both
$v$-fusion and $f$-fusion. The case of weighted Gaussian mixtures representing
multitarget densities in the presence of false alarms and misdetection (whose
weight sums are not necessarily unit) is also considered, the result of which
appears significantly different from that for a single target. In addition to
exact derivation, exemplifying analysis and illustrations are provided.
",1,0,1,1,0,0
8868,8869,"On synthetic data with predetermined subject partitioning and cluster profiling, and pre-specified categorical variable marginal dependence structure","  A standard approach for assessing the performance of partition or mixture
models is to create synthetic data sets with a pre-specified clustering
structure, and assess how well the model reveals this structure. A common
format is that subjects are assigned to different clusters, with variable
observations simulated so that subjects within the same cluster have similar
profiles, allowing for some variability. In this manuscript, we consider
observations from nominal, ordinal and interval categorical variables.
Theoretical and empirical results are utilized to explore the dependence
structure between the variables, in relation to the clustering structure for
the subjects. A novel approach is proposed that allows to control the marginal
association or correlation structure of the variables, and to specify exact
correlation values. Practical examples are shown and additional theoretical
results are derived for interval data, commonly observed in cohort studies,
including observations that emulate Single Nucleotide Polymorphisms. We compare
a synthetic dataset to a real one, to demonstrate similarities and differences.
",0,0,0,1,0,0
16424,16425,The Tensor Memory Hypothesis,"  We discuss memory models which are based on tensor decompositions using
latent representations of entities and events. We show how episodic memory and
semantic memory can be realized and discuss how new memory traces can be
generated from sensory input: Existing memories are the basis for perception
and new memories are generated via perception. We relate our mathematical
approach to the hippocampal memory indexing theory. We describe the first
detailed mathematical models for the complete processing pipeline from sensory
input and its semantic decoding, i.e., perception, to the formation of episodic
and semantic memories and their declarative semantic decodings. Our main
hypothesis is that perception includes an active semantic decoding process,
which relies on latent representations of entities and predicates, and that
episodic and semantic memories depend on the same decoding process. We
contribute to the debate between the leading memory consolidation theories,
i.e., the standard consolidation theory (SCT) and the multiple trace theory
(MTT). The latter is closely related to the complementary learning systems
(CLS) framework. In particular, we show explicitly how episodic memory can
teach the neocortex to form a semantic memory, which is a core issue in MTT and
CLS.
",1,0,0,1,0,0
15604,15605,SySeVR: A Framework for Using Deep Learning to Detect Software Vulnerabilities,"  The detection of software vulnerabilities (or vulnerabilities for short) is
an important problem that has yet to be tackled, as manifested by many
vulnerabilities reported on a daily basis. This calls for machine learning
methods to automate vulnerability detection. Deep learning is attractive for
this purpose because it does not require human experts to manually define
features. Despite the tremendous success of deep learning in other domains, its
applicability to vulnerability detection is not systematically understood. In
order to fill this void, we propose the first systematic framework for using
deep learning to detect vulnerabilities. The framework, dubbed Syntax-based,
Semantics-based, and Vector Representations (SySeVR), focuses on obtaining
program representations that can accommodate syntax and semantic information
pertinent to vulnerabilities. Our experiments with 4 software products
demonstrate the usefulness of the framework: we detect 15 vulnerabilities that
are not reported in the National Vulnerability Database. Among these 15
vulnerabilities, 7 are unknown and have been reported to the vendors, and the
other 8 have been ""silently"" patched by the vendors when releasing newer
versions of the products.
",0,0,0,1,0,0
12023,12024,Conformally invariant elliptic Liouville equation and its symmetry preserving discretization,"  The symmetry algebra of the real elliptic Liouville equation is an
infinite-dimensional loop algebra with the simple Lie algebra $o(3,1)$ as its
maximal finite-dimensional subalgebra. The entire algebra generates the
conformal group of the Euclidean plane $E_2$. This infinite-dimensional algebra
distinguishes the elliptic Liouville equation from the hyperbolic one with its
symmetry algebra that is the direct sum of two Virasoro algebras. Following a
discretisation procedure developed earlier, we present a difference scheme that
is invariant under the group $O(3,1)$ and has the elliptic Liouville equation
in polar coordinates as its continuous limit. The lattice is a solution of an
equation invariant under $O(3,1)$ and is itself invariant under a subgroup of
$O(3,1)$, namely the $O(2)$ rotations of the Euclidean plane.
",0,1,1,0,0,0
9451,9452,Synergies between Asteroseismology and Three-dimensional Simulations of Stellar Turbulence,"  Turbulent mixing of chemical elements by convection has fundamental effects
on the evolution of stars. The standard algorithm at present, mixing-length
theory (MLT), is intrinsically local, and must be supplemented by extensions
with adjustable parameters. As a step toward reducing this arbitrariness, we
compare asteroseismically inferred internal structures of two Kepler slowly
pulsating B stars (SPB's; $M\sim 3.25 M_\odot$) to predictions of 321D
turbulence theory, based upon well-resolved, truly turbulent three-dimensional
simulations (Arnett , et al. 2015, Christini, et al. 2016) which include
boundary physics absent from MLT. We find promising agreement between the
steepness and shapes of the theoretically-predicted composition profile outside
the convective region in 3D simulations and in asteroseismically constrained
composition profiles in the best 1D models of the two SPBs. The structure and
motion of the boundary layer, and the generation of waves, are discussed.
",0,1,0,0,0,0
1741,1742,The Sizes and Depletions of the Dust and Gas Cavities in the Transitional Disk J160421.7-213028,"  We report ALMA Cycle 2 observations of 230 GHz (1.3 mm) dust continuum
emission, and $^{12}$CO, $^{13}$CO, and C$^{18}$O J = 2-1 line emission, from
the Upper Scorpius transitional disk [PZ99] J160421.7-213028, with an angular
resolution of ~0"".25 (35 AU). Armed with these data and existing H-band
scattered light observations, we measure the size and depth of the disk's
central cavity, and the sharpness of its outer edge, in three components:
sub-$\mu$m-sized ""small"" dust traced by scattered light, millimeter-sized ""big""
dust traced by the millimeter continuum, and gas traced by line emission. Both
dust populations feature a cavity of radius $\sim$70 AU that is depleted by
factors of at least 1000 relative to the dust density just outside. The
millimeter continuum data are well explained by a cavity with a sharp edge.
Scattered light observations can be fitted with a cavity in small dust that has
either a sharp edge at 60 AU, or an edge that transitions smoothly over an
annular width of 10 AU near 60 AU. In gas, the data are consistent with a
cavity that is smaller, about 15 AU in radius, and whose surface density at 15
AU is $10^{3\pm1}$ times smaller than the surface density at 70 AU; the gas
density grades smoothly between these two radii. The CO isotopologue
observations rule out a sharp drop in gas surface density at 30 AU or a
double-drop model as found by previous modeling. Future observations are needed
to assess the nature of these gas and dust cavities, e.g., whether they are
opened by multiple as-yet-unseen planets or photoevaporation.
",0,1,0,0,0,0
8468,8469,Liouville integrability of conservative peakons for a modified CH equation,"  The modified Camassa-Holm equation (also called FORQ) is one of numerous
$cousins$ of the Camassa-Holm equation possessing non-smoth solitons
($peakons$) as special solutions. The peakon sector of solutions is not
uniquely defined: in one peakon sector (dissapative) the Sobolev $H^1$ norm is
not preserved, in the other sector (conservative), introduced in [2], the time
evolution of peakons leaves the $H^1$ norm invariant. In this Letter, it is
shown that the conservative peakon equations of the modified Camassa-Holm can
be given an appropriate Poisson structure relative to which the equations are
Hamiltonian and, in fact, Liouville integrable. The latter is proved directly
by exploiting the inverse spectral techniques, especially asymptotic analysis
of solutions, developed elsewhere (in [3]).
",0,1,1,0,0,0
6142,6143,Approximation and Convergence Properties of Generative Adversarial Learning,"  Generative adversarial networks (GAN) approximate a target data distribution
by jointly optimizing an objective function through a ""two-player game"" between
a generator and a discriminator. Despite their empirical success, however, two
very basic questions on how well they can approximate the target distribution
remain unanswered. First, it is not known how restricting the discriminator
family affects the approximation quality. Second, while a number of different
objective functions have been proposed, we do not understand when convergence
to the global minima of the objective function leads to convergence to the
target distribution under various notions of distributional convergence.
In this paper, we address these questions in a broad and unified setting by
defining a notion of adversarial divergences that includes a number of recently
proposed objective functions. We show that if the objective function is an
adversarial divergence with some additional conditions, then using a restricted
discriminator family has a moment-matching effect. Additionally, we show that
for objective functions that are strict adversarial divergences, convergence in
the objective function implies weak convergence, thus generalizing previous
results.
",1,0,0,1,0,0
20886,20887,Trace norm regularization and faster inference for embedded speech recognition RNNs,"  We propose and evaluate new techniques for compressing and speeding up dense
matrix multiplications as found in the fully connected and recurrent layers of
neural networks for embedded large vocabulary continuous speech recognition
(LVCSR). For compression, we introduce and study a trace norm regularization
technique for training low rank factored versions of matrix multiplications.
Compared to standard low rank training, we show that our method leads to good
accuracy versus number of parameter trade-offs and can be used to speed up
training of large models. For speedup, we enable faster inference on ARM
processors through new open sourced kernels optimized for small batch sizes,
resulting in 3x to 7x speed ups over the widely used gemmlowp library. Beyond
LVCSR, we expect our techniques and kernels to be more generally applicable to
embedded neural networks with large fully connected or recurrent layers.
",1,0,0,1,0,0
3857,3858,Specht Polytopes and Specht Matroids,"  The generators of the classical Specht module satisfy intricate relations. We
introduce the Specht matroid, which keeps track of these relations, and the
Specht polytope, which also keeps track of convexity relations. We establish
basic facts about the Specht polytope, for example, that the symmetric group
acts transitively on its vertices and irreducibly on its ambient real vector
space. A similar construction builds a matroid and polytope for a tensor
product of Specht modules, giving ""Kronecker matroids"" and ""Kronecker
polytopes"" instead of the usual Kronecker coefficients. We dub this process of
upgrading numbers to matroids and polytopes ""matroidification,"" giving two more
examples. In the course of describing these objects, we also give an elementary
account of the construction of Specht modules different from the standard one.
Finally, we provide code to compute with Specht matroids and their Chow rings.
",0,0,1,0,0,0
12936,12937,Community Interaction and Conflict on the Web,"  Users organize themselves into communities on web platforms. These
communities can interact with one another, often leading to conflicts and toxic
interactions. However, little is known about the mechanisms of interactions
between communities and how they impact users.
Here we study intercommunity interactions across 36,000 communities on
Reddit, examining cases where users of one community are mobilized by negative
sentiment to comment in another community. We show that such conflicts tend to
be initiated by a handful of communities---less than 1% of communities start
74% of conflicts. While conflicts tend to be initiated by highly active
community members, they are carried out by significantly less active members.
We find that conflicts are marked by formation of echo chambers, where users
primarily talk to other users from their own community. In the long-term,
conflicts have adverse effects and reduce the overall activity of users in the
targeted communities.
Our analysis of user interactions also suggests strategies for mitigating the
negative impact of conflicts---such as increasing direct engagement between
attackers and defenders. Further, we accurately predict whether a conflict will
occur by creating a novel LSTM model that combines graph embeddings, user,
community, and text features. This model can be used toreate early-warning
systems for community moderators to prevent conflicts. Altogether, this work
presents a data-driven view of community interactions and conflict, and paves
the way towards healthier online communities.
",1,0,0,0,0,0
9182,9183,Accurate fast computation of steady two-dimensional surface gravity waves in arbitrary depth,"  This paper describes an efficient algorithm for computing steady
two-dimensional surface gravity wave in irrotational motion. The algorithm
complexity is O(N log N), N being the number of Fourier modes. The algorithm
allows the arbitrary precision computation of waves in arbitrary depth, i.e.,
it works efficiently for Stokes, cnoidal and solitary waves, even for quite
large steepnesses. The method is based on conformal mapping, Babenko equation
rewritten in a suitable way, pseudo-spectral method and Petviashvili's
iterations. The efficiency of the algorithm is illustrated via some relevant
numerical examples. The code is open source, so interested readers can easily
check the claims, use and modify the algorithm.
",0,1,1,0,0,0
3953,3954,Long Short-Term Memory (LSTM) networks with jet constituents for boosted top tagging at the LHC,"  Multivariate techniques based on engineered features have found wide adoption
in the identification of jets resulting from hadronic top decays at the Large
Hadron Collider (LHC). Recent Deep Learning developments in this area include
the treatment of the calorimeter activation as an image or supplying a list of
jet constituent momenta to a fully connected network. This latter approach
lends itself well to the use of Recurrent Neural Networks. In this work the
applicability of architectures incorporating Long Short-Term Memory (LSTM)
networks is explored. Several network architectures, methods of ordering of jet
constituents, and input pre-processing are studied. The best performing LSTM
network achieves a background rejection of 100 for 50% signal efficiency. This
represents more than a factor of two improvement over a fully connected Deep
Neural Network (DNN) trained on similar types of inputs.
",1,0,0,1,0,0
19073,19074,Boltzmann Transport in Nanostructures as a Friction Effect,"  Surface scattering is the key limiting factor to thermal transport in
dielectric crystals as the length scales are reduced or when temperature is
lowered. To explain this phenomenon, it is commonly assumed that the mean free
paths of heat carriers are bound by the crystal size and that thermal
conductivity is reduced in a manner proportional to such mean free paths. We
show here that these conclusions rely on simplifying assumptions and
approximated transport models. Instead, starting from the linearized Boltzmann
transport equation in the relaxon basis, we show how the problem can be reduced
to a set of decoupled linear differential equations. Then, the heat flow can be
interpreted as a hydrodynamic phenomenon, with the relaxon gas being slowed
down in proximity of a surface by friction effects, similar to the flux of a
viscous fluid in a pipe. As an example, we study a ribbon and a trench of
monolayer molybdenum disulphide, describing the procedure to reconstruct the
temperature and thermal conductivity profile in the sample interior and showing
how to estimate the effect of nanostructuring. The approach is general and
could be extended to other transport carriers, such as electrons, or extended
to materials of higher dimensionality and to different geometries, such as thin
films.
",0,1,0,0,0,0
19414,19415,"Waring-Goldbach Problem: One Square, Four Cubes and Higher Powers","  Let $\mathcal{P}_r$ denote an almost-prime with at most $r$ prime factors,
counted according to multiplicity. In this paper, it is proved that, for
$12\leqslant b\leqslant 35$ and for every sufficiently large odd integer $N$,
the equation \begin{equation*}
N=x^2+p_1^3+p_2^3+p_3^3+p_4^3+p_5^4+p_6^b \end{equation*} is solvable with
$x$ being an almost-prime $\mathcal{P}_{r(b)}$ and the other variables primes,
where $r(b)$ is defined in the Theorem. This result constitutes an improvement
upon that of Lü and Mu.
",0,0,1,0,0,0
7417,7418,Haptic Assembly and Prototyping: An Expository Review,"  An important application of haptic technology to digital product development
is in virtual prototyping (VP), part of which deals with interactive planning,
simulation, and verification of assembly-related activities, collectively
called virtual assembly (VA). In spite of numerous research and development
efforts over the last two decades, the industrial adoption of haptic-assisted
VP/VA has been slower than expected. Putting hardware limitations aside, the
main roadblocks faced in software development can be traced to the lack of
effective and efficient computational models of haptic feedback. Such models
must 1) accommodate the inherent geometric complexities faced when assembling
objects of arbitrary shape; and 2) conform to the computation time limitation
imposed by the notorious frame rate requirements---namely, 1 kHz for haptic
feedback compared to the more manageable 30-60 Hz for graphic rendering. The
simultaneous fulfillment of these competing objectives is far from trivial.
This survey presents some of the conceptual and computational challenges and
opportunities as well as promising future directions in haptic-assisted VP/VA,
with a focus on haptic assembly from a geometric modeling and spatial reasoning
perspective. The main focus is on revisiting definitions and classifications of
different methods used to handle the constrained multibody simulation in
real-time, ranging from physics-based and geometry-based to hybrid and unified
approaches using a variety of auxiliary computational devices to specify,
impose, and solve assembly constraints. Particular attention is given to the
newly developed 'analytic methods' inherited from motion planning and protein
docking that have shown great promise as an alternative paradigm to the more
popular combinatorial methods.
",1,0,0,0,0,0
14809,14810,Uniform cohomological expansion of uniformly quasiregular mappings,"  Let $f\colon M \to M$ be a uniformly quasiregular self-mapping of a compact,
connected, and oriented Riemannian $n$-manifold $M$ without boundary, $n\ge 2$.
We show that, for $k \in \{0,\ldots, n\}$, the induced homomorphism $f^* \colon
H^k(M;\mathbb{R}) \to H^k(M;\mathbb{R})$, where $H^k(M;\mathbb{R})$ is the
$k$:th singular cohomology of $M$, is complex diagonalizable and the
eigenvalues of $f^*$ have modulus $(\mathrm{deg}\ f)^{k/n}$. As an application,
we obtain a degree restriction for uniformly quasiregular self-mappings of
closed manifolds. In the proof of the main theorem, we use a Sobolev--de Rham
cohomology based on conformally invariant differential forms and an induced
push-forward operator.
",0,0,1,0,0,0
2317,2318,Multi-proton bunch driven hollow plasma wakefield acceleration in the nonlinear regime,"  Proton-driven plasma wakefield acceleration has been demonstrated in
simulations to be capable of accelerating particles to the energy frontier in a
single stage, but its potential is hindered by the fact that currently
available proton bunches are orders of magnitude longer than the plasma
wavelength. Fortunately, proton micro-bunching allows driving plasma waves
resonantly. In this paper, we propose using a hollow plasma channel for
multiple proton bunch driven plasma wakefield acceleration and demonstrate that
it enables the operation in the nonlinear regime and resonant excitation of
strong plasma waves. This new regime also involves beneficial features of
hollow channels for the accelerated beam (such as emittance preservation and
uniform accelerating field) and long buckets of stable deceleration for the
drive beam. The regime is attained at a proper ratio among plasma skin depth,
driver radius, hollow channel radius, and micro-bunch period.
",0,1,0,0,0,0
3904,3905,Convergence of Stochastic Approximation Monte Carlo and modified Wang-Landau algorithms: Tests for the Ising model,"  We investigate the behavior of the deviation of the estimator for the density
of states (DOS) with respect to the exact solution in the course of Wang-Landau
and Stochastic Approximation Monte Carlo (SAMC) simulations of the
two-dimensional Ising model. We find that the deviation saturates in the
Wang-Landau case. This can be cured by adjusting the refinement scheme. To this
end, the 1/t-modification of the Wang-Landau algorithm has been suggested. A
similar choice of refinement scheme is employed in the SAMC algorithm. The
convergence behavior of all three algorithms is examined. It turns out that the
convergence of the SAMC algorithm is very sensitive to the onset of the
refinement. Finally, the internal energy and specific heat of the Ising model
are calculated from the SAMC DOS and compared to exact values.
",0,1,0,0,0,0
8723,8724,Vector-valued extensions of operators through multilinear limited range extrapolation,"  We give an extension of Rubio de Francia's extrapolation theorem for
functions taking values in UMD Banach function spaces to the multilinear
limited range setting. In particular we show how boundedness of an
$m$-(sub)linear operator \[T:L^{p_1}(w_1^{p_1})\times\cdots\times
L^{p_m}(w_m^{p_m})\to L^p(w^p) \] for a certain class of Muckenhoupt weights
yields an extension of the operator to Bochner spaces $L^{p}(w^p;X)$ for a wide
class of Banach function spaces $X$, which includes certain Lebesgue, Lorentz
and Orlicz spaces.
We apply the extrapolation result to various operators, which yields new
vector-valued bounds. Our examples include the bilinear Hilbert transform,
certain Fourier multipliers and various operators satisfying sparse domination
results.
",0,0,1,0,0,0
5622,5623,Determination of hysteresis in finite-state random walks using Bayesian cross validation,"  Consider the problem of modeling hysteresis for finite-state random walks
using higher-order Markov chains. This Letter introduces a Bayesian framework
to determine, from data, the number of prior states of recent history upon
which a trajectory is statistically dependent. The general recommendation is to
use leave-one-out cross validation, using an easily-computable formula that is
provided in closed form. Importantly, Bayes factors using flat model priors are
biased in favor of too-complex a model (more hysteresis) when a large amount of
data is present and the Akaike information criterion (AIC) is biased in favor
of too-sparse a model (less hysteresis) when few data are present.
",0,1,0,1,0,0
7337,7338,The Mean and Median Criterion for Automatic Kernel Bandwidth Selection for Support Vector Data Description,"  Support vector data description (SVDD) is a popular technique for detecting
anomalies. The SVDD classifier partitions the whole space into an inlier
region, which consists of the region near the training data, and an outlier
region, which consists of points away from the training data. The computation
of the SVDD classifier requires a kernel function, and the Gaussian kernel is a
common choice for the kernel function. The Gaussian kernel has a bandwidth
parameter, whose value is important for good results. A small bandwidth leads
to overfitting, and the resulting SVDD classifier overestimates the number of
anomalies. A large bandwidth leads to underfitting, and the classifier fails to
detect many anomalies. In this paper we present a new automatic, unsupervised
method for selecting the Gaussian kernel bandwidth. The selected value can be
computed quickly, and it is competitive with existing bandwidth selection
methods.
",1,0,0,1,0,0
5650,5651,Diffusion along chains of normally hyperbolic cylinders,"  The present paper is part of a series of articles dedicated to the existence
of Arnold diffusion for cusp-residual perturbations of Tonelli Hamiltonians on
$\mathbb{A}^3$. Our goal here is to construct an abstract geometric framework
that can be used to prove the existence of diffusing orbits in the so-called a
priori stable setting, once the preliminary geometric reductions are preformed.
Our framework also applies, rather directly, to the a priori unstable setting.
The main geometric objects of interest are $3$-dimensional normally
hyperbolic invariant cylinders with boundary, which in particular admit
well-defined stable and unstable manifolds. These enable us to define, in our
setting, chains of cylinders, i.e., finite, ordered families of cylinders in
which each cylinder admits homoclinic connections, and any two consecutive
elements in the family admit heteroclinic connections.
Our main result is the existence of diffusing orbits drifting along such
chains, under precise conditions on the dynamics on the cylinders, and on their
homoclinic and heteroclinic structure.
",0,1,1,0,0,0
14013,14014,On the complexity of non-orientable Seifert fibre spaces,"  In this paper we deal with Seifert fibre spaces, which are compact
3-manifolds admitting a foliation by circles. We give a combinatorial
description for these manifolds in all the possible cases: orientable,
non-orientable, closed, with boundary. Moreover, we compute a potentially sharp
upper bound for their complexity in terms of the invariants of the
combinatorial description, extending to the non-orientable case results by
Fominykh and Wiest for the orientable case with boundary and by Martelli and
Petronio for the closed orientable case.
",0,0,1,0,0,0
4582,4583,Experimental Determination of the Structural Coefficient of Restitution of a Bouncing Asteroid Lander,"  The structural coefficient of restitution describes the kinetic energy
dissipation upon low-velocity (~0.1 m/s) impact of a small asteroid lander,
MASCOT, against a hard, ideally elastic plane surface. It is a crucial
worst-case input for mission analysis for landing MACOT on a 1km asteroid in
2018. We conducted pendulum tests and describe their analysis and the results.
",0,1,0,0,0,0
19914,19915,Synthesis and In Situ Modification of Hierarchical SAPO-34 by PEG with Different Molecular Weights; Application in MTO Process,"  Modified structures of SAPO-34 were prepared using polyethylene glycol as the
mesopores generating agent. The synthesized catalysts were applied in
methanol-to-olefins (MTO) process. All modified synthesized catalysts were
characterized via XRD, XRF, FESEM, FTIR, N2 adsorption-desorption techniques,
and temperature-programmed NH3 desorption and they were compared with
conventional microporous SAPO-34. Introduction of non-ionic PEG capping agent
affected the degree of homogeneity and integrity of the synthesis media and
thus reduced the number of nuclei and order of coordination structures
resulting in larger and less crystalline particles compared with the
conventional sample. During the calcination process, decomposition of absorbed
PEG moieties among the piled up SAPO patches formed a great portion of tuned
mesopores into the microporous matrix. These tailored mesopores were served as
auxiliary diffusion pathways in MTO reaction. The effects of molecular weight
of PEG and PEG/Al molar ratio on the properties of the synthesized materials
were investigated in order to optimize their MTO reaction performance. It was
revealed that both of these two parameters can significantly change the
structural composition and physicochemical properties of resultant products.
Using PEG with MW of 6000 has led to the formation of RHO and CHA structural
frameworks i.e. DNL-6 and SAPO-34, simultaneously, while addition of PEG with
MW of 4000 resulted the formation of pure SAPO-34 phase. Altering the PEG/Al
molar ratio in the precursor significantly influenced the porosity and acidity
of the synthesized silicoaluminophosphate products. SAPO-34 impregnated with
PEG molecular weight of 4000 and PEG/Al molar ratio of 0.0125 showed superior
catalytic stability in MTO reaction because of the tuned bi-modal porosity and
tailored acidity pattern.
",0,1,0,0,0,0
9088,9089,On the correlation between a level of structure order and properties of composites. In Memory of Yu.L. Klimontovich,"  Proposed the computerized method for calculating the relative level of order
composites. Correlation between a level of structure order and properties of
solids is shown. Discussed the possibility of clarifying the terminology used
in describing the structure.
",1,0,0,0,0,0
3022,3023,Phonon-assisted oscillatory exciton dynamics in monolayer MoSe2,"  In monolayer semiconductor transition metal dichalcogenides, the
exciton-phonon interaction is expected to strongly affect the photocarrier
dynamics. Here, we report on an unusual oscillatory enhancement of the neutral
exciton photoluminescence with the excitation laser frequency in monolayer
MoSe2. The frequency of oscillation matches that of the M-point longitudinal
acoustic phonon, LA(M). Oscillatory behavior is also observed in the
steady-state emission linewidth and in timeresolved photoluminescence
excitation data, which reveals variation with excitation energy in the exciton
lifetime. These results clearly expose the key role played by phonons in the
exciton formation and relaxation dynamics of two-dimensional van der Waals
semiconductors.
",0,1,0,0,0,0
17887,17888,Sub-harmonic Injection Locking in Metronomes,"  In this paper, we demonstrate sub-harmonic injection locking (SHIL) in
mechanical metronomes. To do so, we first formulate metronome's physical
compact model, focusing on its nonlinear terms for friction and the escapement
mechanism. Then we analyze metronomes using phase-macromodel-based techniques
and show that the phase of their oscillation is in fact very immune to periodic
perturbation at twice its natural frequency, making SHIL difficult. Guided by
the phase-macromodel-based analysis, we are able to modify the escapement
mechanism of metronomes such that SHIL can happen more easily. Then we verify
the occurrence of SHIL in experiments. To our knowledge, this is the first
demonstration of SHIL in metronomes; As such, it provides many valuable
insights into the modelling, simulation, analysis and design of nonlinear
oscillators. The demonstration is also suitable to use for teaching the subject
of injection locking and SHIL.
",0,1,0,0,0,0
5437,5438,Optimal segmentation of directed graph and the minimum number of feedback arcs,"  The minimum feedback arc set problem asks to delete a minimum number of arcs
(directed edges) from a digraph (directed graph) to make it free of any
directed cycles. In this work we approach this fundamental cycle-constrained
optimization problem by considering a generalized task of dividing the digraph
into D layers of equal size. We solve the D-segmentation problem by the
replica-symmetric mean field theory and belief-propagation heuristic
algorithms. The minimum feedback arc density of a given random digraph ensemble
is then obtained by extrapolating the theoretical results to the limit of large
D. A divide-and-conquer algorithm (nested-BPR) is devised to solve the minimum
feedback arc set problem with very good performance and high efficiency.
",1,1,0,0,0,0
1473,1474,"Ad-blocking: A Study on Performance, Privacy and Counter-measures","  Many internet ventures rely on advertising for their revenue. However, users
feel discontent by the presence of ads on the websites they visit, as the
data-size of ads is often comparable to that of the actual content. This has an
impact not only on the loading time of webpages, but also on the internet bill
of the user in some cases. In absence of a mutually-agreed procedure for opting
out of advertisements, many users resort to ad-blocking browser-extensions. In
this work, we study the performance of popular ad-blockers on a large set of
news websites. Moreover, we investigate the benefits of ad-blockers on user
privacy as well as the mechanisms used by websites to counter them. Finally, we
explore the traffic overhead due to the ad-blockers themselves.
",1,0,0,0,0,0
14060,14061,A Universal Ordinary Differential Equation,"  An astonishing fact was established by Lee A. Rubel (1981): there exists a
fixed non-trivial fourth-order polynomial differential algebraic equation (DAE)
such that for any positive continuous function $\varphi$ on the reals, and for
any positive continuous function $\epsilon(t)$, it has a $\mathcal{C}^\infty$
solution with $| y(t) - \varphi(t) | < \epsilon(t)$ for all $t$. Lee A. Rubel
provided an explicit example of such a polynomial DAE. Other examples of
universal DAE have later been proposed by other authors. However, Rubel's DAE
\emph{never} has a unique solution, even with a finite number of conditions of
the form $y^{(k_i)}(a_i)=b_i$.
The question whether one can require the solution that approximates $\varphi$
to be the unique solution for a given initial data is a well known open problem
[Rubel 1981, page 2], [Boshernitzan 1986, Conjecture 6.2]. In this article, we
solve it and show that Rubel's statement holds for polynomial ordinary
differential equations (ODEs), and since polynomial ODEs have a unique solution
given an initial data, this positively answers Rubel's open problem. More
precisely, we show that there exists a \textbf{fixed} polynomial ODE such that
for any $\varphi$ and $\epsilon(t)$ there exists some initial condition that
yields a solution that is $\epsilon$-close to $\varphi$ at all times.
In particular, the solution to the ODE is necessarily analytic, and we show
that the initial condition is computable from the target function and error
function.
",1,0,1,0,0,0
6345,6346,Estimation under group actions: recovering orbits from invariants,"  Motivated by geometric problems in signal processing, computer vision, and
structural biology, we study a class of orbit recovery problems where we
observe very noisy copies of an unknown signal, each acted upon by a random
element of some group (such as Z/p or SO(3)). The goal is to recover the orbit
of the signal under the group action in the high-noise regime. This generalizes
problems of interest such as multi-reference alignment (MRA) and the
reconstruction problem in cryo-electron microscopy (cryo-EM). We obtain
matching lower and upper bounds on the sample complexity of these problems in
high generality, showing that the statistical difficulty is intricately
determined by the invariant theory of the underlying symmetry group.
In particular, we determine that for cryo-EM with noise variance $\sigma^2$
and uniform viewing directions, the number of samples required scales as
$\sigma^6$. We match this bound with a novel algorithm for ab initio
reconstruction in cryo-EM, based on invariant features of degree at most 3. We
further discuss how to recover multiple molecular structures from heterogeneous
cryo-EM samples.
",1,0,1,0,0,0
11263,11264,Chemical-disorder-caused Medium Range Order in Covalent Glass,"  How atoms in covalent solids rearrange over a medium-range length-scale
during amorphization is a long pursued question whose answer could profoundly
shape our understanding on amorphous (a-) networks. Based on ab-intio
calculations and reverse Monte Carlo simulations of experiments, we
surprisingly find that even though the severe chemical disorder in a-GeTe
undermined the prevailing medium range order (MRO) picture, it is responsible
for the experimentally observed MRO. That this thing could happen depends on a
novel atomic packing scheme. And this scheme results in a kind of homopolar
bond chain-like polyhedral clusters. Within this scheme, the formation of
homopolar bonds can be well explained by an electron-counting model and further
validated by quantitative bond energy analysis based. Our study suggests that
the underlying physics for chemical disorder in a-GeTe is intrinsic and
universal to all severely chemically disordered covalent glasses.
",0,1,0,0,0,0
14641,14642,Optimizing deep video representation to match brain activity,"  The comparison of observed brain activity with the statistics generated by
artificial intelligence systems is useful to probe brain functional
organization under ecological conditions. Here we study fMRI activity in ten
subjects watching color natural movies and compute deep representations of
these movies with an architecture that relies on optical flow and image
content. The association of activity in visual areas with the different layers
of the deep architecture displays complexity-related contrasts across visual
areas and reveals a striking foveal/peripheral dichotomy.
",0,0,0,0,1,0
4076,4077,General analytical solution for the electromagnetic grating diffraction problem,"  Implementing the modal method in the electromagnetic grating diffraction
problem delivered by the curvilinear coordinate transformation yields a general
analytical solution to the 1D grating diffraction problem in a form of a
T-matrix. Simultaneously it is shown that the validity of the Rayleigh
expansion is defined by the validity of the modal expansion in a transformed
medium delivered by the coordinate transformation.
",0,1,1,0,0,0
7383,7384,Stabilization Bounds for Linear Finite Dynamical Systems,"  A common problem to all applications of linear finite dynamical systems is
analyzing the dynamics without enumerating every possible state transition. Of
particular interest is the long term dynamical behaviour. In this paper, we
study the number of iterations needed for a system to settle on a fixed set of
elements. As our main result, we present two upper bounds on iterations needed,
and each one may be readily applied to a fixed point system test. The bounds
are based on submodule properties of iterated images and reduced systems modulo
a prime. We also provide examples where our bounds are optimal.
",0,0,1,0,0,0
4590,4591,Sampling-based vs. Design-based Uncertainty in Regression Analysis,"  Consider a researcher estimating the parameters of a regression function
based on data for all 50 states in the United States or on data for all visits
to a website. What is the interpretation of the estimated parameters and the
standard errors? In practice, researchers typically assume that the sample is
randomly drawn from a large population of interest and report standard errors
that are designed to capture sampling variation. This is common practice, even
in applications where it is difficult to articulate what that population of
interest is, and how it differs from the sample. In this article, we explore an
alternative approach to inference, which is partly design-based. In a
design-based setting, the values of some of the regressors can be manipulated,
perhaps through a policy intervention. Design-based uncertainty emanates from
lack of knowledge about the values that the regression outcome would have taken
under alternative interventions. We derive standard errors that account for
design-based uncertainty instead of, or in addition to, sampling-based
uncertainty. We show that our standard errors in general are smaller than the
infinite-population sampling-based standard errors and provide conditions under
which they coincide.
",0,0,1,1,0,0
17389,17390,Pseudogap and Fermi surface in the presence of spin-vortex checkerboard for 1/8-doped lanthanum cuprates,"  Lanthanum family of high-temperature cuprate superconductors is known to
exhibit both spin and charge electronic modulations around doping level 1/8. We
assume that these modulations have the character of two-dimensional spin-vortex
checkerboard and investigate whether this assumption is consistent with the
Fermi surface and the pseudogap measured by angle-resolved photo-emission
spectroscopy. We also explore the possibility of observing quantum oscillations
of transport coefficients in such a background. These investigations are based
on a model of non-interacting spin-1/2 fermions hopping on a square lattice and
coupled through spins to a magnetic field imitating spin-vortex checkerboard.
The main results of this article include (i) calculation of Fermi surface
containing Fermi arcs at the positions in the Brillouin zone largely consistent
with experiments; (ii) identification of factors complicating the observations
of quantum oscillations in the presence of spin modulations; and (iii)
investigation of the symmetries of the resulting electronic energy bands,
which, in particular, indicates that each band is double-degenerate and, in
addition, has at least one conical point, where it touches another
double-degenerate band. We discuss possible implications these cones may have
for the transport properties and the pseudogap.
",0,1,0,0,0,0
1886,1887,Information Extraction in Illicit Domains,"  Extracting useful entities and attribute values from illicit domains such as
human trafficking is a challenging problem with the potential for widespread
social impact. Such domains employ atypical language models, have `long tails'
and suffer from the problem of concept drift. In this paper, we propose a
lightweight, feature-agnostic Information Extraction (IE) paradigm specifically
designed for such domains. Our approach uses raw, unlabeled text from an
initial corpus, and a few (12-120) seed annotations per domain-specific
attribute, to learn robust IE models for unobserved pages and websites.
Empirically, we demonstrate that our approach can outperform feature-centric
Conditional Random Field baselines by over 18\% F-Measure on five annotated
sets of real-world human trafficking datasets in both low-supervision and
high-supervision settings. We also show that our approach is demonstrably
robust to concept drift, and can be efficiently bootstrapped even in a serial
computing environment.
",1,0,0,0,0,0
4272,4273,Quasi-Oracle Estimation of Heterogeneous Treatment Effects,"  Flexible estimation of heterogeneous treatment effects lies at the heart of
many statistical challenges, such as personalized medicine and optimal resource
allocation. In this paper, we develop a general class of two-step algorithms
for heterogeneous treatment effect estimation in observational studies. We
first estimate marginal effects and treatment propensities in order to form an
objective function that isolates the causal component of the signal. Then, we
optimize this data-adaptive objective function. Our approach has several
advantages over existing methods. From a practical perspective, our method is
flexible and easy to use: In both steps, we can use any loss-minimization
method, e.g., penalized regression, deep neutral networks, or boosting;
moreover, these methods can be fine-tuned by cross validation. Meanwhile, in
the case of penalized kernel regression, we show that our method has a
quasi-oracle property: Even if the pilot estimates for marginal effects and
treatment propensities are not particularly accurate, we achieve the same error
bounds as an oracle who has a priori knowledge of these two nuisance
components. We implement variants of our approach based on both penalized
regression and boosting in a variety of simulation setups, and find promising
performance relative to existing baselines.
",0,0,1,1,0,0
2712,2713,A giant with feet of clay: on the validity of the data that feed machine learning in medicine,"  This paper considers the use of Machine Learning (ML) in medicine by focusing
on the main problem that this computational approach has been aimed at solving
or at least minimizing: uncertainty. To this aim, we point out how uncertainty
is so ingrained in medicine that it biases also the representation of clinical
phenomena, that is the very input of ML models, thus undermining the clinical
significance of their output. Recognizing this can motivate both medical
doctors, in taking more responsibility in the development and use of these
decision aids, and the researchers, in pursuing different ways to assess the
value of these systems. In so doing, both designers and users could take this
intrinsic characteristic of medicine more seriously and consider alternative
approaches that do not ""sweep uncertainty under the rug"" within an objectivist
fiction, which everyone can come up by believing as true.
",1,0,0,1,0,0
19480,19481,On 2d-4d motivic wall-crossing formulas,"  In this paper we propose definitions and examples of categorical enhancements
of the data involved in the $2d$-$4d$ wall-crossing formulas which generalize
both Cecotti-Vafa and Kontsevich-Soibelman motivic wall-crossing formulas.
",0,0,1,0,0,0
4972,4973,Some results on the annihilators and attached primes of local cohomology modules,"  Let $(R, \frak m)$ be a local ring and $M$ a finitely generated $R$-module.
It is shown that if $M$ is relative Cohen-Macaulay with respect to an ideal
$\frak a$ of $R$, then $\text{Ann}_R(H_{\mathfrak{a}}^{\text{cd}(\mathfrak{a},
M)}(M))=\text{Ann}_RM/L=\text{Ann}_RM$ and
$\text{Ass}_R(R/\text{Ann}_RM)\subseteq \{\mathfrak{p} \in \text{Ass}_R
M|\,{\rm cd}(\mathfrak{a}, R/\mathfrak{p})=\text{cd}(\mathfrak{a}, M)\},$ where
$L$ is the largest submodule of $M$ such that ${\rm cd}(\mathfrak{a}, L)< {\rm
cd}(\mathfrak{a}, M)$. We also show that if $H^{\dim M}_{\mathfrak{a}}(M)=0$,
then $\text{Att}_R(H^{\dim M-1}_{\mathfrak{a}}(M))= \{\mathfrak{p} \in
\text{Supp} (M)|\,{\rm cd}(\mathfrak{a}, R/\mathfrak{p})=\dim M-1\},$ and so
the attached primes of $H^{\dim M-1}_{\mathfrak{a}}(M)$ depends only on
$\text{Supp} (M)$. Finally, we prove that if $M$ is an arbitrary module (not
necessarily finitely generated) over a Noetherian ring $R$ with ${\rm
cd}(\mathfrak{a}, M)={\rm cd}(\mathfrak{a}, R/\text{Ann}_RM)$, then
$\text{Att}_R(H^{{\rm cd}(\mathfrak{a},
M)}_{\mathfrak{a}}(M))\subseteq\{\mathfrak{p} \in V(\text{Ann}_RM)|\,{\rm
cd}(\mathfrak{a}, R/\mathfrak{p})={\rm cd}(\mathfrak{a}, M)\}.$
As a consequence of this it is shown that if $\dim M=\dim R$, then
$\text{Att}_R(H^{\dim M}_{\mathfrak{a}}(M))\subseteq\{\mathfrak{p} \in
\text{Ass}_R M|\,{\rm cd}(\mathfrak{a}, R/\mathfrak{p})=\dim M\}.$
",0,0,1,0,0,0
4240,4241,Time-dynamic inference for non-Markov transition probabilities under independent right-censoring,"  In this article, weak convergence of the general non-Markov state transition
probability estimator by Titman (2015) is established which, up to now, has not
been verified yet for other general non-Markov estimators. A similar theorem is
shown for the bootstrap, yielding resampling-based inference methods for
statistical functionals. Formulas of the involved covariance functions are
presented in detail. Particular applications include the conditional expected
length of stay in a specific state, given occupation of another state in the
past, as well as the construction of time-simultaneous confidence bands for the
transition probabilities. The expected lengths of stay in the two-sample liver
cirrhosis data-set by Andersen et al. (1993) are compared and confidence
intervals for their difference are constructed. With borderline significance
and in comparison to the placebo group, the treatment group has an elevated
expected length of stay in the healthy state given an earlier disease state
occupation. In contrast, the Aalen-Johansen estimator-based confidence
interval, which relies on a Markov assumption, leads to a drastically different
conclusion. Also, graphical illustrations of confidence bands for the
transition probabilities demonstrate the biasedness of the Aalen-Johansen
estimator in this data example. The reliability of these results is assessed in
a simulation study.
",0,0,1,1,0,0
9585,9586,Almost Buchsbaumness of some rings arising from complexes with isolated singularities,"  We study properties of the Stanley-Reisner rings of simplicial complexes with
isolated singularities modulo two generic linear forms. Miller, Novik, and
Swartz proved that if a complex has homologically isolated singularities, then
its Stanley-Reisner ring modulo one generic linear form is Buchsbaum. Here we
examine the case of non-homologically isolated singularities, providing many
examples in which the Stanley-Reisner ring modulo two generic linear forms is a
quasi-Buchsbaum but not Buchsbaum ring.
",0,0,1,0,0,0
7703,7704,Offloading Execution from Edge to Cloud: a Dynamic Node-RED Based Approach,"  Fog computing enables use cases where data produced in end devices are
stored, processed, and acted on directly at the edges of the network, yet
computation can be offloaded to more powerful instances through the edge to
cloud continuum. Such offloading mechanism is especially needed in case of
modern multi-purpose IoT gateways, where both demand and operation conditions
can vary largely between deployments. To facilitate the development and
operations of gateways, we implement offloading directly as part of the IoT
rapid prototyping process embedded in the software stack, based on Node-RED. We
evaluate the implemented method using an image processing example, and compare
various offloading strategies based on resource consumption and other system
metrics, highlighting the differences in handling demand and service levels
reached.
",1,0,0,0,0,0
12241,12242,Sharp estimates for oscillatory integral operators via polynomial partitioning,"  The sharp range of $L^p$-estimates for the class of Hörmander-type
oscillatory integral operators is established in all dimensions under a
positive-definite assumption on the phase. This is achieved by generalising a
recent approach of the first author for studying the Fourier extension
operator, which utilises polynomial partitioning arguments.
",0,0,1,0,0,0
5495,5496,Dynamic coupling of ferromagnets via spin Hall magnetoresistance,"  The synchronized magnetization dynamics in ferromagnets on a nonmagnetic
heavy metal caused by the spin Hall effect is investigated theoretically. The
direct and inverse spin Hall effects near the ferromagnetic/nonmagnetic
interface generate longitudinal and transverse electric currents. The
phenomenon is known as the spin Hall magnetoresistance effect, whose magnitude
depends on the magnetization direction in the ferromagnet due to the spin
transfer effect. When another ferromagnet is placed onto the same nonmagnet,
these currents are again converted to the spin current by the spin Hall effect
and excite the spin torque to this additional ferromagnet, resulting in the
excitation of the coupled motions of the magnetizations. The in-phase or
antiphase synchronization of the magnetization oscillations, depending on the
value of the Gilbert damping constant and the field-like torque strength, is
found in the transverse geometry by solving the Landau-Lifshitz-Gilbert
equation numerically. On the other hand, in addition to these synchronizations,
the synchronization having a phase difference of a quarter of a period is also
found in the longitudinal geometry. The analytical theory clarifying the
relation among the current, frequency, and phase difference is also developed,
where it is shown that the phase differences observed in the numerical
simulations correspond to that giving the fixed points of the energy supplied
by the coupling torque.
",0,1,0,0,0,0
15016,15017,Stream Graphs and Link Streams for the Modeling of Interactions over Time,"  Graph theory provides a language for studying the structure of relations, and
it is often used to study interactions over time too. However, it poorly
captures the both temporal and structural nature of interactions, that calls
for a dedicated formalism. In this paper, we generalize graph concepts in order
to cope with both aspects in a consistent way. We start with elementary
concepts like density, clusters, or paths, and derive from them more advanced
concepts like cliques, degrees, clustering coefficients, or connected
components. We obtain a language to directly deal with interactions over time,
similar to the language provided by graphs to deal with relations. This
formalism is self-consistent: usual relations between different concepts are
preserved. It is also consistent with graph theory: graph concepts are special
cases of the ones we introduce. This makes it easy to generalize higher-level
objects such as quotient graphs, line graphs, k-cores, and centralities. This
paper also considers discrete versus continuous time assumptions, instantaneous
links, and extensions to more complex cases.
",1,0,0,1,0,0
7364,7365,Experimental and Theoretical Study of Magnetohydrodynamic Ship Models,"  Magnetohydrodynamic (MHD) ships represent a clear demonstration of the
Lorentz force in fluids, which explains the number of students practicals or
exercises described on the web. However, the related literature is rather
specific and no complete comparison between theory and typical small scale
experiments is currently available. This work provides, in a self-consistent
framework, a detailed presentation of the relevant theoretical equations for
small MHD ships and experimental measurements for future benchmarks.
Theoretical results of the literature are adapted to these simple
battery/magnets powered ships moving on salt water. Comparison between theory
and experiments are performed to validate each theoretical step such as the
Tafel and the Kohlrausch laws, or the predicted ship speed. A successful
agreement is obtained without any adjustable parameter. Finally, based on these
results, an optimal design is then deduced from the theory. Therefore this work
provides a solid theoretical and experimental ground for small scale MHD ships,
by presenting in detail several approximations and how they affect the boat
efficiency. Moreover, the theory is general enough to be adapted to other
contexts, such as large scale ships or industrial flow measurement techniques.
",0,1,0,0,0,0
15834,15835,Electronic characteristics of ultrathin SrRuO$_3$ films and their relationship with the metal$-$insulator transition,"  SrRuO$_3$ (SRO) films are known to exhibit insulating behavior as their
thickness approaches four unit cells. We employ electron energy$-$loss (EEL)
spectroscopy to probe the spatially resolved electronic structures of both
insulating and conducting SRO to correlate them with the metal$-$insulator
transition (MIT). Importantly, the central layer of the ultrathin insulating
film exhibits distinct features from the metallic SRO. Moreover, EEL near edge
spectra adjacent to the SrTiO$_3$ (STO) substrate or to the capping layer are
remarkably similar to those of STO. The site$-$projected density of states
based on density functional theory (DFT) partially reflects the characteristics
of the spectra of these layers. These results may provide important information
on the possible influence of STO on the electronic states of ultrathin SRO.
",0,1,0,0,0,0
17850,17851,Identifying exogenous and endogenous activity in social media,"  The occurrence of new events in a system is typically driven by external
causes and by previous events taking place inside the system. This is a general
statement, applying to a range of situations including, more recently, to the
activity of users in Online social networks (OSNs). Here we develop a method
for extracting from a series of posting times the relative contributions of
exogenous, e.g. news media, and endogenous, e.g. information cascade. The
method is based on the fitting of a generalized linear model (GLM) equipped
with a self-excitation mechanism. We test the method with synthetic data
generated by a nonlinear Hawkes process, and apply it to a real time series of
tweets with a given hashtag. In the empirical dataset, the estimated
contributions of exogenous and endogenous volumes are close to the amounts of
original tweets and retweets respectively. We conclude by discussing the
possible applications of the method, for instance in online marketing.
",1,0,0,0,0,0
15964,15965,Sharp Minima Can Generalize For Deep Nets,"  Despite their overwhelming capacity to overfit, deep learning architectures
tend to generalize relatively well to unseen data, allowing them to be deployed
in practice. However, explaining why this is the case is still an open area of
research. One standing hypothesis that is gaining popularity, e.g. Hochreiter &
Schmidhuber (1997); Keskar et al. (2017), is that the flatness of minima of the
loss function found by stochastic gradient based methods results in good
generalization. This paper argues that most notions of flatness are problematic
for deep models and can not be directly applied to explain generalization.
Specifically, when focusing on deep networks with rectifier units, we can
exploit the particular geometry of parameter space induced by the inherent
symmetries that these architectures exhibit to build equivalent models
corresponding to arbitrarily sharper minima. Furthermore, if we allow to
reparametrize a function, the geometry of its parameters can change drastically
without affecting its generalization properties.
",1,0,0,0,0,0
18889,18890,Capacitive Mechanism of Oxygen Functional Groups on Carbon Surface in Supercapacitors,"  Oxygen functional groups are one of the most important subjects in the study
of electrochemical properties of carbon materials which can change the
wettability, conductivity and pore size distributions of carbon materials, and
can occur redox reactions. In the electrode materials of carbon-based
supercapacitors, the oxygen functional groups have widely been used to improve
the capacitive performance. In this paper, we not only analyzed the reasons for
the increase of the capacity that promoted by oxygen functional groups in the
charge-discharge cycling tests, but also analyzed the mechanism how the
pseudocapacitance was provided by the oxygen functional groups in the
acid/alkaline aqueous electrolyte. Moreover, we also discussed the effect of
the oxygen functional groups in electrochemical impedance spectroscopy.
",0,1,0,0,0,0
13882,13883,Analytic evaluation of some three- and four- electron atomic integrals involving s STO's and exponential correlation with unlinked $r_{ij}$'s,"  The method of evaluation outlined in a previous work has been utilized here
to evaluate certain other three- electron and four- electron atomic integrals
involving s Slater-type orbitals and exponential correlation with unlinked
$r_{ij}$'s. Limiting expressions for various such integrals have been derived,
which has not been done earlier. Closed-form expressions for $<r_{12} r_{13} /
r_{14}>$, $<r_{12}r_{34}/r_{23}>$, $<r_{12}r_{23}/r_{34}>$,
$<r_{12}r_{13}/r_{34}>$ and $<r_{12}r_{34}/r_{13}>$ have been obtained.
",0,1,0,0,0,0
9184,9185,Fate of Weyl semimetals in the presence of incommensurate potentials,"  We investigate the effect of the incommensurate potential on Weyl semimetal,
which is proposed to be realized in ultracold atomic systems trapped in
three-dimensional optical lattices. For the system without the Fermi arc, we
find that the Weyl points are robust against the incommensurate potential and
the system enters into a metallic phase only when the incommensurate potential
strength exceeds a critical value. We unveil the trastition by analysing the
properties of wave functions and the density of states as a function of the
incommensurate potential strength. We also study the system with Fermi arcs and
find the Fermi arcs are sensitive against the incommensurate potential and can
be destoryed by a weak incommensurate potential.
",0,1,0,0,0,0
2696,2697,Bonsai: Synthesis-Based Reasoning for Type Systems,"  We describe algorithms for symbolic reasoning about executable models of type
systems, supporting three queries intended for designers of type systems.
First, we check for type soundness bugs and synthesize a counterexample program
if such a bug is found. Second, we compare two versions of a type system,
synthesizing a program accepted by one but rejected by the other. Third, we
minimize the size of synthesized counterexample programs.
These algorithms symbolically evaluate typecheckers and interpreters,
producing formulas that characterize the set of programs that fail or succeed
in the typechecker and the interpreter. However, symbolically evaluating
interpreters poses efficiency challenges, which are caused by having to merge
execution paths of the various possible input programs. Our main contribution
is the Bonsai tree, a novel symbolic representation of programs and program
states which addresses these challenges. Bonsai trees encode complex syntactic
information in terms of logical constraints, enabling more efficient merging.
We implement these algorithms in the Bonsai tool, an assistant for type
system designers. We perform case studies on how Bonsai helps test and explore
a variety of type systems. Bonsai efficiently synthesizes counterexamples for
soundness bugs that have been inaccessible to automatic tools, and is the first
automated tool to find a counterexample for the recently discovered Scala
soundness bug SI-9633.
",1,0,0,0,0,0
7790,7791,Top-k Overlapping Densest Subgraphs: Approximation and Complexity,"  A central problem in graph mining is finding dense subgraphs, with several
applications in different fields, a notable example being identifying
communities. While a lot of effort has been put on the problem of finding a
single dense subgraph, only recently the focus has been shifted to the problem
of finding a set of dens- est subgraphs. Some approaches aim at finding
disjoint subgraphs, while in many real-world networks communities are often
overlapping. An approach introduced to find possible overlapping subgraphs is
the Top-k Overlapping Densest Subgraphs problem. For a given integer k >= 1,
the goal of this problem is to find a set of k densest subgraphs that may share
some vertices. The objective function to be maximized takes into account both
the density of the subgraphs and the distance between subgraphs in the
solution.
The Top-k Overlapping Densest Subgraphs problem has been shown to admit a
1/10-factor approximation algorithm. Furthermore, the computational complexity
of the problem has been left open. In this paper, we present contributions
concerning the approximability and the computational complexity of the problem.
For the approximability, we present approximation algorithms that improves the
approximation factor to 1/2 , when k is bounded by the vertex set, and to 2/3
when k is a constant. For the computational complexity, we show that the
problem is NP-hard even when k = 3.
",1,0,0,0,0,0
3527,3528,Cross-Entropy Loss and Low-Rank Features Have Responsibility for Adversarial Examples,"  State-of-the-art neural networks are vulnerable to adversarial examples; they
can easily misclassify inputs that are imperceptibly different than their
training and test data. In this work, we establish that the use of
cross-entropy loss function and the low-rank features of the training data have
responsibility for the existence of these inputs. Based on this observation, we
suggest that addressing adversarial examples requires rethinking the use of
cross-entropy loss function and looking for an alternative that is more suited
for minimization with low-rank features. In this direction, we present a
training scheme called differential training, which uses a loss function
defined on the differences between the features of points from opposite
classes. We show that differential training can ensure a large margin between
the decision boundary of the neural network and the points in the training
dataset. This larger margin increases the amount of perturbation needed to flip
the prediction of the classifier and makes it harder to find an adversarial
example with small perturbations. We test differential training on a binary
classification task with CIFAR-10 dataset and demonstrate that it radically
reduces the ratio of images for which an adversarial example could be found --
not only in the training dataset, but in the test dataset as well.
",1,0,0,1,0,0
10331,10332,A note on MLE of covariance matrix,"  For a multivariate normal set up, it is well known that the maximum
likelihood estimator of covariance matrix is neither admissible nor minimax
under the Stein loss function. For the past six decades, a bunch of researches
have followed along this line for Stein's phenomenon in the literature. In this
note, the results are two folds: Firstly, with respect to Stein type loss
function we use the full Iwasawa decomposition to enhance the unpleasant
phenomenon that the minimum risks of maximum likelihood estimators for the
different coordinate systems (Cholesky decomposition and full Iwasawa
decomposition) are different. Secondly, we introduce a new class of loss
functions to show that the minimum risks of maximum likelihood estimators for
the different coordinate systems, the Cholesky decomposition and the full
Iwasawa decomposition, are of the same, and hence the Stein's paradox
disappears.
",0,0,1,1,0,0
16028,16029,Structured Optimal Transport,"  Optimal Transport has recently gained interest in machine learning for
applications ranging from domain adaptation, sentence similarities to deep
learning. Yet, its ability to capture frequently occurring structure beyond the
""ground metric"" is limited. In this work, we develop a nonlinear generalization
of (discrete) optimal transport that is able to reflect much additional
structure. We demonstrate how to leverage the geometry of this new model for
fast algorithms, and explore connections and properties. Illustrative
experiments highlight the benefit of the induced structured couplings for tasks
in domain adaptation and natural language processing.
",1,0,0,1,0,0
13558,13559,Hilbert $C^*$-modules over $Σ^*$-algebras II: $Σ^*$-Morita equivalence,"  In previous work, we defined and studied $\Sigma^*$-modules, a class of
Hilbert $C^*$-modules over $\Sigma^*$-algebras (the latter are $C^*$-algebras
that are sequentially closed in the weak operator topology). The present work
continues this study by developing the appropriate $\Sigma^*$-algebraic
analogue of the notion of strong Morita equivalence for $C^*$-algebras. We
define strong $\Sigma^*$-Morita equivalence, prove a few characterizations,
look at the relationship with equivalence of categories of a certain type of
Hilbert space representation, study $\Sigma^*$-versions of the interior and
exterior tensor products, and prove a $\Sigma^*$-version of the
Brown-Green-Rieffel stable isomorphism theorem.
",0,0,1,0,0,0
1529,1530,New Determinant Expressions of the Multi-indexed Orthogonal Polynomials in Discrete Quantum Mechanics,"  The multi-indexed orthogonal polynomials (the Meixner, little $q$-Jacobi
(Laguerre), ($q$-)Racah, Wilson, Askey-Wilson types) satisfying second order
difference equations were constructed in discrete quantum mechanics. They are
polynomials in the sinusoidal coordinates $\eta(x)$ ($x$ is the coordinate of
quantum system) and expressed in terms of the Casorati determinants whose
matrix elements are functions of $x$ at various points. By using shape
invariance properties, we derive various equivalent determinant expressions,
especially those whose matrix elements are functions of the same point $x$.
Except for the ($q$-)Racah case, they can be expressed in terms of $\eta$ only,
without explicit $x$-dependence.
",0,1,1,0,0,0
4130,4131,Truncation in Hahn Fields is Undecidable and Wild,"  We show that in any nontrivial Hahn field with truncation as a primitive
operation we can interpret the monadic second-order logic of the additive
monoid of natural numbers and are thus undecidable. We also specify a definable
binary relation on such a structure that has $\SOP$ and $\TP$.
",0,0,1,0,0,0
11793,11794,Pore lifetimes in cell electroporation: Complex dark pores?,"  We review some of the basic concepts and the possible pore structures
associated with electroporation (EP) for times after electrical pulsing. We
purposefully give only a short description of pore creation and subsequent
evolution of pore populations, as these are adequately discussed in both
reviews and original research reports. In contrast, post-pulse pore concepts
have changed dramatically. For perspective we note that pores are not directly
observed. Instead understanding of pores is based on inference from experiments
and, increasingly, molecular dynamics (MD) simulations. In the past decade
concepts for post-pulse pores have changed significantly: The idea of pure
lipidic transient pores (TPs) that exist for milliseconds or longer post-pulse
has become inconsistent with MD results, which support TP lifetimes of only
$\sim$100 ns. A typical large TP number during cell EP pulsing is of order
$10^6$. In twenty MD-based TP lifetimes (2 us total), the TP number plummets to
$\sim$0.001. In short, TPs vanish 2 us after a pulse ends, and cannot account
for post-pulse behavior such as large and relatively non-specific ionic and
molecular transport. Instead, an early conjecture of complex pores (CPs) with
both lipidic and other molecule should be taken seriously. Indeed, in the past
decade several experiments provide partial support for complex pores (CPs).
Presently, CPs are ""dark"", in the sense that while some CP functions are known,
little is known about their structure(s). There may be a wide range of
lifetimes and permeabilities, not yet revealed by experiments. Like cosmology's
dark matter, these unseen pores present us with an outstanding problem.
",0,1,0,0,0,0
5159,5160,Comparing Neural and Attractiveness-based Visual Features for Artwork Recommendation,"  Advances in image processing and computer vision in the latest years have
brought about the use of visual features in artwork recommendation. Recent
works have shown that visual features obtained from pre-trained deep neural
networks (DNNs) perform very well for recommending digital art. Other recent
works have shown that explicit visual features (EVF) based on attractiveness
can perform well in preference prediction tasks, but no previous work has
compared DNN features versus specific attractiveness-based visual features
(e.g. brightness, texture) in terms of recommendation performance. In this
work, we study and compare the performance of DNN and EVF features for the
purpose of physical artwork recommendation using transactional data from
UGallery, an online store of physical paintings. In addition, we perform an
exploratory analysis to understand if DNN embedded features have some relation
with certain EVF. Our results show that DNN features outperform EVF, that
certain EVF features are more suited for physical artwork recommendation and,
finally, we show evidence that certain neurons in the DNN might be partially
encoding visual features such as brightness, providing an opportunity for
explaining recommendations based on visual neural models.
",1,0,0,0,0,0
19267,19268,On the union complexity of families of axis-parallel rectangles with a low packing number,"  Let R be a family of n axis-parallel rectangles with packing number p-1,
meaning that among any p of the rectangles, there are two with a non-empty
intersection. We show that the union complexity of R is at most O(n+p^2), and
that the (<=k)-level complexity of R is at most O(kn+k^2p^2). Both upper bounds
are tight.
",1,0,1,0,0,0
6901,6902,New conformal map for the Sinc approximation for exponentially decaying functions over the semi-infinite interval,"  The Sinc approximation has shown high efficiency for numerical methods in
many fields. Conformal maps play an important role in the success, i.e.,
appropriate conformal map must be employed to elicit high performance of the
Sinc approximation. Appropriate conformal maps have been proposed for typical
cases; however, such maps may not be optimal. Thus, the performance of the Sinc
approximation may be improved by using another conformal map rather than an
existing map. In this paper, we propose a new conformal map for the case where
functions are defined over the semi-infinite interval and decay exponentially.
Then, we demonstrate in both theoretical and numerical ways that the
convergence rate is improved by replacing the existing conformal map with the
proposed map.
",1,0,0,0,0,0
9774,9775,Direct measurement of superdiffusive and subdiffusive energy transport in disordered granular chains,"  The study of energy transport properties in heterogeneous materials has
attracted scientific interest for more than a century, and it continues to
offer fundamental and rich questions. One of the unanswered challenges is to
extend Anderson theory for uncorrelated and fully disordered lattices in
condensed-matter systems to physical settings in which additional effects
compete with disorder. Specifically, the effect of strong nonlinearity has been
largely unexplored experimentally, partly due to the paucity of testbeds that
can combine the effect of disorder and nonlinearity in a controllable manner.
Here we present the first systematic experimental study of energy transport and
localization properties in simultaneously disordered and nonlinear granular
crystals. We demonstrate experimentally that disorder and nonlinearity ---
which are known from decades of studies to individually favor energy
localization --- can in some sense ""cancel each other out"", resulting in the
destruction of wave localization. We also report that the combined effect of
disorder and nonlinearity can enable the manipulation of energy transport speed
in granular crystals from subdiffusive to superdiffusive ranges.
",0,1,0,0,0,0
6936,6937,Neural correlates of episodic memory in the Memento cohort,"  IntroductionThe free and cued selective reminding test is used to identify
memory deficits in mild cognitive impairment and demented patients. It allows
assessing three processes: encoding, storage, and recollection of verbal
episodic memory.MethodsWe investigated the neural correlates of these three
memory processes in a large cohort study. The Memento cohort enrolled 2323
outpatients presenting either with subjective cognitive decline or mild
cognitive impairment who underwent cognitive, structural MRI and, for a subset,
fluorodeoxyglucose--positron emission tomography evaluations.ResultsEncoding
was associated with a network including parietal and temporal cortices; storage
was mainly associated with entorhinal and parahippocampal regions, bilaterally;
retrieval was associated with a widespread network encompassing frontal
regions.DiscussionThe neural correlates of episodic memory processes can be
assessed in large and standardized cohorts of patients at risk for Alzheimer's
disease. Their relation to pathophysiological markers of Alzheimer's disease
remains to be studied.
",0,0,0,0,1,0
5157,5158,Stochastic graph Voronoi tessellation reveals community structure,"  Given a network, the statistical ensemble of its graph-Voronoi diagrams with
randomly chosen cell centers exhibits properties convertible into information
on the network's large scale structures. We define a node-pair level measure
called {\it Voronoi cohesion} which describes the probability for sharing the
same Voronoi cell, when randomly choosing $g$ centers in the network. This
measure provides information based on the global context (the network in its
entirety) a type of information that is not carried by other similarity
measures. We explore the mathematical background of this phenomenon and several
of its potential applications. A special focus is laid on the possibilities and
limitations pertaining to the exploitation of the phenomenon for community
detection purposes.
",1,1,0,0,0,0
9280,9281,Testing Docker Performance for HPC Applications,"  The main goal for this article is to compare performance penalties when using
KVM virtualization and Docker containers for creating isolated environments for
HPC applications. The article provides both data obtained using commonly
accepted synthetic tests (High Performance Linpack) and real life applications
(OpenFOAM). The article highlights the influence on resulting application
performance of major infrastructure configuration options: CPU type presented
to VM, networking connection type used.
",1,0,0,0,0,0
8074,8075,Collisions in shape memory alloys,"  We present here a model for instantaneous collisions in a solid made of shape
memory alloys (SMA) by means of a predictive theory which is based on the
introduction not only of macroscopic velocities and temperature, but also of
microscopic velocities responsible of the austenite-martensites phase changes.
Assuming time discontinuities for velocities, volume fractions and temperature,
and applying the principles of thermodynamics for non-smooth evolutions
together with constitutive laws typical of SMA, we end up with a system of
nonlinearly coupled elliptic equations for which we prove an existence and
uniqueness result in the 2 and 3 D cases. Finally, we also present numerical
results for a SMA 2D solid subject to an external percussion by an hammer
stroke.
",0,0,1,0,0,0
718,719,Database Learning: Toward a Database that Becomes Smarter Every Time,"  In today's databases, previous query answers rarely benefit answering future
queries. For the first time, to the best of our knowledge, we change this
paradigm in an approximate query processing (AQP) context. We make the
following observation: the answer to each query reveals some degree of
knowledge about the answer to another query because their answers stem from the
same underlying distribution that has produced the entire dataset. Exploiting
and refining this knowledge should allow us to answer queries more
analytically, rather than by reading enormous amounts of raw data. Also,
processing more queries should continuously enhance our knowledge of the
underlying distribution, and hence lead to increasingly faster response times
for future queries.
We call this novel idea---learning from past query answers---Database
Learning. We exploit the principle of maximum entropy to produce answers, which
are in expectation guaranteed to be more accurate than existing sample-based
approximations. Empowered by this idea, we build a query engine on top of Spark
SQL, called Verdict. We conduct extensive experiments on real-world query
traces from a large customer of a major database vendor. Our results
demonstrate that Verdict supports 73.7% of these queries, speeding them up by
up to 23.0x for the same accuracy level compared to existing AQP systems.
",1,0,0,0,0,0
5814,5815,Promising Accurate Prefix Boosting for sequence-to-sequence ASR,"  In this paper, we present promising accurate prefix boosting (PAPB), a
discriminative training technique for attention based sequence-to-sequence
(seq2seq) ASR. PAPB is devised to unify the training and testing scheme in an
effective manner. The training procedure involves maximizing the score of each
partial correct sequence obtained during beam search compared to other
hypotheses. The training objective also includes minimization of token
(character) error rate. PAPB shows its efficacy by achieving 10.8\% and 3.8\%
WER with and without RNNLM respectively on Wall Street Journal dataset.
",1,0,0,0,0,0
14763,14764,A unified treatment of multiple testing with prior knowledge using the p-filter,"  There is a significant literature on methods for incorporating knowledge into
multiple testing procedures so as to improve their power and precision. Some
common forms of prior knowledge include (a) beliefs about which hypotheses are
null, modeled by non-uniform prior weights; (b) differing importances of
hypotheses, modeled by differing penalties for false discoveries; (c) multiple
arbitrary partitions of the hypotheses into (possibly overlapping) groups; and
(d) knowledge of independence, positive or arbitrary dependence between
hypotheses or groups, suggesting the use of more aggressive or conservative
procedures. We present a unified algorithmic framework called p-filter for
global null testing and false discovery rate (FDR) control that allows the
scientist to incorporate all four types of prior knowledge (a)-(d)
simultaneously, recovering a variety of known algorithms as special cases.
",0,0,1,1,0,0
6411,6412,Personalizing Path-Specific Effects,"  Unlike classical causal inference, which often has an average causal effect
of a treatment within a population as a target, in settings such as
personalized medicine, the goal is to map a given unit's characteristics to a
treatment tailored to maximize the expected outcome for that unit. Obtaining
high-quality mappings of this type is the goal of the dynamic regime literature
(Chakraborty and Moodie 2013), with connections to reinforcement learning and
experimental design. Aside from the average treatment effects, mechanisms
behind causal relationships are also of interest. A well-studied approach to
mechanism analysis is establishing average effects along with a particular set
of causal pathways, in the simplest case the direct and indirect effects.
Estimating such effects is the subject of the mediation analysis literature
(Robins and Greenland 1992; Pearl 2001).
In this paper, we consider how unit characteristics may be used to tailor a
treatment assignment strategy that maximizes a particular path-specific effect.
In healthcare applications, finding such a policy is of interest if, for
instance, we are interested in maximizing the chemical effect of a drug on an
outcome (corresponding to the direct effect), while assuming drug adherence
(corresponding to the indirect effect) is set to some reference level. To solve
our problem, we define counterfactuals associated with path-specific effects of
a policy, give a general identification algorithm for these counterfactuals,
give a proof of completeness, and show how classification algorithms in machine
learning (Chen, Zeng, and Kosorok 2016) may be used to find a high-quality
policy. We validate our approach via a simulation study.
",0,0,0,1,0,0
13453,13454,Maximum-order Complexity and Correlation Measures,"  We estimate the maximum-order complexity of a binary sequence in terms of its
correlation measures. Roughly speaking, we show that any sequence with small
correlation measure up to a sufficiently large order $k$ cannot have very small
maximum-order complexity.
",0,0,1,0,0,0
681,682,Bias Reduction in Instrumental Variable Estimation through First-Stage Shrinkage,"  The two-stage least-squares (2SLS) estimator is known to be biased when its
first-stage fit is poor. I show that better first-stage prediction can
alleviate this bias. In a two-stage linear regression model with Normal noise,
I consider shrinkage in the estimation of the first-stage instrumental variable
coefficients. For at least four instrumental variables and a single endogenous
regressor, I establish that the standard 2SLS estimator is dominated with
respect to bias. The dominating IV estimator applies James-Stein type shrinkage
in a first-stage high-dimensional Normal-means problem followed by a
control-function approach in the second stage. It preserves invariances of the
structural instrumental variable equations.
",0,0,1,1,0,0
11349,11350,Attacking the Madry Defense Model with $L_1$-based Adversarial Examples,"  The Madry Lab recently hosted a competition designed to test the robustness
of their adversarially trained MNIST model. Attacks were constrained to perturb
each pixel of the input image by a scaled maximal $L_\infty$ distortion
$\epsilon$ = 0.3. This discourages the use of attacks which are not optimized
on the $L_\infty$ distortion metric. Our experimental results demonstrate that
by relaxing the $L_\infty$ constraint of the competition, the elastic-net
attack to deep neural networks (EAD) can generate transferable adversarial
examples which, despite their high average $L_\infty$ distortion, have minimal
visual distortion. These results call into question the use of $L_\infty$ as a
sole measure for visual distortion, and further demonstrate the power of EAD at
generating robust adversarial examples.
",1,0,0,1,0,0
6526,6527,The ALMA View of the OMC1 Explosion in Orion,"  Most massive stars form in dense clusters where gravitational interactions
with other stars may be common. The two nearest forming massive stars, the BN
object and Source I, located behind the Orion Nebula, were ejected with
velocities of $\sim$29 and $\sim$13 km s$^{-1}$ about 500 years ago by such
interactions. This event generated an explosion in the gas. New ALMA
observations show in unprecedented detail, a roughly spherically symmetric
distribution of over a hundred $^{12}$CO J=2$-$1 streamers with velocities
extending from V$_{LSR}$ =$-$150 to +145 km s$^{-1}$. The streamer radial
velocities increase (or decrease) linearly with projected distance from the
explosion center, forming a `Hubble Flow' confined to within 50 arcseconds of
the explosion center. They point toward the high proper-motion, shock-excited
H$_2$ and [Fe ii ] `fingertips' and lower-velocity CO in the H$_2$ wakes
comprising Orion's `fingers'. In some directions, the H$_2$ `fingers' extend
more than a factor of two farther from the ejection center than the CO
streamers. Such deviations from spherical symmetry may be caused by ejecta
running into dense gas or the dynamics of the N-body interaction that ejected
the stars and produced the explosion. This $\sim$10$^{48}$ erg event may have
been powered by the release of gravitational potential energy associated with
the formation of a compact binary or a protostellar merger. Orion may be the
prototype for a new class of stellar explosion responsible for luminous
infrared transients in nearby galaxies.
",0,1,0,0,0,0
7723,7724,Thermal memristor and neuromorphic networks for manipulating heat flow,"  A memristor is one of four fundamental two-terminal solid elements in
electronics. In addition with the resistor, the capacitor and the inductor,
this passive element relates the electric charges to current in solid state
elements. Here we report the existence of a thermal analog for this element
made with metal-insulator transition materials. We demonstrate that these
memristive systems can be used to create thermal neurons opening so the way to
neuromophic networks for smart thermal management and information treatment.
",0,1,0,0,0,0
11315,11316,The perfect spin injection in silicene FS/NS junction,"  We theoretically investigate the spin injection from a ferromagnetic silicene
to a normal silicene (FS/NS), where the magnetization in the FS is assumed from
the magnetic proximity effect. Based on a silicene lattice model, we
demonstrated that the pure spin injection could be obtained by tuning the Fermi
energy of two spin species, where one is in the spin orbit coupling gap and the
other one is outside the gap. Moreover, the valley polarity of the spin species
can be controlled by a perpendicular electric field in the FS region. Our
findings may shed light on making silicene-based spin and valley devices in the
spintronics and valleytronics field.
",0,1,0,0,0,0
9086,9087,Adiponitrile-LiTFSI solution as alkylcarbonate free electrolyte for LTO/NMC Li-ion batteries,"  Recently, dinitriles (NC(CH2)nCN) and especially adiponitrile (ADN, n=4) have
attracted the attention as secure electrolyte solvents due to their chemical
stability, high boiling points, high flash points and low vapor pressure. The
good solvating properties of ADN toward lithium salts and its high
electrochemical stability (~ 6V vs. Li/Li+) make it suitable for safer Li-ions
cells without performances loss. In this study, ADN is used as a single
electrolyte solvent with lithium bis(trimethylsulfonyl)imide (LiTFSI). This
electrolyte allows the use of aluminum collectors as almost no corrosion occurs
at voltages up to 4.2 V. Physico-chemical properties of ADN-LiTFSI electrolyte
such as salt dissolution, conductivity and viscosity were determined. The
cycling performances of batteries using Li4Ti5O12 (LTO) as anode and
LiNi1/3Co1/3Mn1/3O2 (NMC) as cathode were determined. The results indicate that
LTO/NMC batteries exhibit excellent rate capabilities with a columbic
efficiency close to 100%. As an example, cells were able to reach a capacity of
165 mAh.g-1 at 0.1C and a capacity retention of more than 98% after 200 cycles
at 0.5C. In addition, electrodes analyses by SEM, XPS and electrochemical
impedance spectroscopy after cycling confirming minimal surface changes of the
electrodes in the studied battery system
",0,1,0,0,0,0
2210,2211,Overview of Recent Studies and Design Changes for the FNAL Magnetron Ion Source,"  This paper will cover several studies and design changes that will eventually
be implemented to the Fermi National Accelerator Laboratory (FNAL) magnetron
ion source. The topics include tungsten cathode insert, solenoid gas valves,
current controlled arc pulser, cesium boiler redesign, gas mixtures of hydrogen
and nitrogen, and duty factor reduction. The studies were performed on the FNAL
test stand, with the aim to improve source lifetime, stability, and reducing
the amount of tuning needed.
",0,1,0,0,0,0
16918,16919,A $q$-generalization of the para-Racah polynomials,"  New bispectral orthogonal polynomials are obtained from an unconventional
truncation of the Askey-Wilson polynomials. In the limit $q \to 1$, they reduce
to the para-Racah polynomials which are orthogonal with respect to a quadratic
bi-lattice. The three term recurrence relation and q-difference equation are
obtained through limits of those of the Askey-Wilson polynomials. An explicit
expression in terms of hypergeometric series and the orthogonality relation are
provided. A $q$-generalization of the para-Krawtchouk polynomials is obtained
as a special case. Connections with the $q$-Racah and dual-Hahn polynomials are
also presented.
",0,0,1,0,0,0
9794,9795,DSVO: Direct Stereo Visual Odometry,"  This paper proposes a novel approach to stereo visual odometry without stereo
matching. It is particularly robust in scenes of repetitive high-frequency
textures. Referred to as DSVO (Direct Stereo Visual Odometry), it operates
directly on pixel intensities, without any explicit feature matching, and is
thus efficient and more accurate than the state-of-the-art
stereo-matching-based methods. It applies a semi-direct monocular visual
odometry running on one camera of the stereo pair, tracking the camera pose and
mapping the environment simultaneously; the other camera is used to optimize
the scale of monocular visual odometry. We evaluate DSVO in a number of
challenging scenes to evaluate its performance and present comparisons with the
state-of-the-art stereo visual odometry algorithms.
",1,0,0,0,0,0
9753,9754,Remark on a theorem of H. Hauser on textile maps,"  We give a counter example to the new theorem that appeared in the survey
\cite{H} on Artin approximation. We then provide a correct statement and a
proof of it.
",0,0,1,0,0,0
11933,11934,Multidimensional upwind hydrodynamics on unstructured meshes using Graphics Processing Units I. Two-dimensional uniform meshes,"  We present a new method for numerical hydrodynamics which uses a
multidimensional generalisation of the Roe solver and operates on an
unstructured triangular mesh. The main advantage over traditional methods based
on Riemann solvers, which commonly use one-dimensional flux estimates as
building blocks for a multidimensional integration, is its inherently
multidimensional nature, and as a consequence its ability to recognise
multidimensional stationary states that are not hydrostatic. A second novelty
is the focus on Graphics Processing Units (GPUs). By tailoring the algorithms
specifically to GPUs we are able to get speedups of 100-250 compared to a
desktop machine. We compare the multidimensional upwind scheme to a
traditional, dimensionally split implementation of the Roe solver on several
test problems, and we find that the new method significantly outperforms the
Roe solver in almost all cases. This comes with increased computational costs
per time step, which makes the new method approximately a factor of 2 slower
than a dimensionally split scheme acting on a structured grid.
",0,1,0,0,0,0
3956,3957,On Hoffman's conjectural identity,"  In this paper, we shall prove the equality \[
\zeta(3,\{2\}^{n},1,2)=\zeta(\{2\}^{n+3})+2\zeta(3,3,\{2\}^{n}) \] conjectured
by Hoffman using certain identities among iterated integrals on
$\mathbb{P}^{1}\setminus\{0,1,\infty,z\}$.
",0,0,1,0,0,0
11747,11748,Partial Knowledge In Embeddings,"  Representing domain knowledge is crucial for any task. There has been a wide
range of techniques developed to represent this knowledge, from older logic
based approaches to the more recent deep learning based techniques (i.e.
embeddings). In this paper, we discuss some of these methods, focusing on the
representational expressiveness tradeoffs that are often made. In particular,
we focus on the the ability of various techniques to encode `partial knowledge'
- a key component of successful knowledge systems. We introduce and describe
the concepts of `ensembles of embeddings' and `aggregate embeddings' and
demonstrate how they allow for partial knowledge.
",1,0,0,0,0,0
18818,18819,Robust Tracking with Model Mismatch for Fast and Safe Planning: an SOS Optimization Approach,"  In the pursuit of real-time motion planning, a commonly adopted practice is
to compute a trajectory by running a planning algorithm on a simplified,
low-dimensional dynamical model, and then employ a feedback tracking controller
that tracks such a trajectory by accounting for the full, high-dimensional
system dynamics. While this strategy of planning with model mismatch generally
yields fast computation times, there are no guarantees of dynamic feasibility,
which hampers application to safety-critical systems. Building upon recent work
that addressed this problem through the lens of Hamilton-Jacobi (HJ)
reachability, we devise an algorithmic framework whereby one computes, offline,
for a pair of ""planner"" (i.e., low-dimensional) and ""tracking"" (i.e.,
high-dimensional) models, a feedback tracking controller and associated
tracking bound. This bound is then used as a safety margin when generating
motion plans via the low-dimensional model. Specifically, we harness the
computational tool of sum-of-squares (SOS) programming to design a bilinear
optimization algorithm for the computation of the feedback tracking controller
and associated tracking bound. The algorithm is demonstrated via numerical
experiments, with an emphasis on investigating the trade-off between the
increased computational scalability afforded by SOS and its intrinsic
conservativeness. Collectively, our results enable scaling the appealing
strategy of planning with model mismatch to systems that are beyond the reach
of HJ analysis, while maintaining safety guarantees.
",1,0,0,0,0,0
17971,17972,Laser opacity in underdense preplasma of solid targets due to quantum electrodynamics effects,"  We investigate how next-generation laser pulses at 10 PW $-$ 200 PW interact
with a solid target in the presence of a relativistically underdense preplasma
produced by amplified spontaneous emission (ASE). Laser hole boring and
relativistic transparency are strongly restrained due to the generation of
electron-positron pairs and $\gamma$-ray photons via quantum electrodynamics
(QED) processes. A pair plasma with a density above the initial preplasma
density is formed, counteracting the electron-free channel produced by the hole
boring. This pair-dominated plasma can block the laser transport and trigger an
avalanche-like QED cascade, efficiently transfering the laser energy to
photons. This renders a 1-$\rm\mu m$-scalelength, underdense preplasma
completely opaque to laser pulses at this power level. The QED-induced opacity
therefore sets much higher contrast requirements for such pulse in solid-target
experiments than expected by classical plasma physics. Our simulations show for
example, that proton acceleration from the rear of a solid with a preplasma
would be strongly impaired.
",0,1,0,0,0,0
15814,15815,Case studies in network community detection,"  Community structure describes the organization of a network into subgraphs
that contain a prevalence of edges within each subgraph and relatively few
edges across boundaries between subgraphs. The development of
community-detection methods has occurred across disciplines, with numerous and
varied algorithms proposed to find communities. As we present in this Chapter
via several case studies, community detection is not just an ""end game"" unto
itself, but rather a step in the analysis of network data which is then useful
for furthering research in the disciplinary domain of interest. These
case-study examples arise from diverse applications, ranging from social and
political science to neuroscience and genetics, and we have chosen them to
demonstrate key aspects of community detection and to highlight that community
detection, in practice, should be directed by the application at hand.
",1,1,0,0,0,0
2406,2407,Convergence rate bounds for a proximal ADMM with over-relaxation stepsize parameter for solving nonconvex linearly constrained problems,"  This paper establishes convergence rate bounds for a variant of the proximal
alternating direction method of multipliers (ADMM) for solving nonconvex
linearly constrained optimization problems. The variant of the proximal ADMM
allows the inclusion of an over-relaxation stepsize parameter belonging to the
interval $(0,2)$. To the best of our knowledge, all related papers in the
literature only consider the case where the over-relaxation parameter lies in
the interval $(0,(1+\sqrt{5})/2)$.
",0,0,1,0,0,0
19775,19776,Malware distributions and graph structure of the Web,"  Knowledge about the graph structure of the Web is important for understanding
this complex socio-technical system and for devising proper policies supporting
its future development. Knowledge about the differences between clean and
malicious parts of the Web is important for understanding potential treats to
its users and for devising protection mechanisms. In this study, we conduct
data science methods on a large crawl of surface and deep Web pages with the
aim to increase such knowledge. To accomplish this, we answer the following
questions. Which theoretical distributions explain important local
characteristics and network properties of websites? How are these
characteristics and properties different between clean and malicious
(malware-affected) websites? What is the prediction power of local
characteristics and network properties to classify malware websites? To the
best of our knowledge, this is the first large-scale study describing the
differences in global properties between malicious and clean parts of the Web.
In other words, our work is building on and bridging the gap between
\textit{Web science} that tackles large-scale graph representations and
\textit{Web cyber security} that is concerned with malicious activities on the
Web. The results presented herein can also help antivirus vendors in devising
approaches to improve their detection algorithms.
",1,0,0,0,0,0
1273,1274,Optimization of distributions differences for classification,"  In this paper we introduce a new classification algorithm called Optimization
of Distributions Differences (ODD). The algorithm aims to find a transformation
from the feature space to a new space where the instances in the same class are
as close as possible to one another while the gravity centers of these classes
are as far as possible from one another. This aim is formulated as a
multiobjective optimization problem that is solved by a hybrid of an
evolutionary strategy and the Quasi-Newton method. The choice of the
transformation function is flexible and could be any continuous space function.
We experiment with a linear and a non-linear transformation in this paper. We
show that the algorithm can outperform 6 other state-of-the-art classification
methods, namely naive Bayes, support vector machines, linear discriminant
analysis, multi-layer perceptrons, decision trees, and k-nearest neighbors, in
12 standard classification datasets. Our results show that the method is less
sensitive to the imbalanced number of instances comparing to these methods. We
also show that ODD maintains its performance better than other classification
methods in these datasets, hence, offers a better generalization ability.
",1,0,0,1,0,0
16702,16703,Deep Models Under the GAN: Information Leakage from Collaborative Deep Learning,"  Deep Learning has recently become hugely popular in machine learning,
providing significant improvements in classification accuracy in the presence
of highly-structured and large databases.
Researchers have also considered privacy implications of deep learning.
Models are typically trained in a centralized manner with all the data being
processed by the same training algorithm. If the data is a collection of users'
private data, including habits, personal pictures, geographical positions,
interests, and more, the centralized server will have access to sensitive
information that could potentially be mishandled. To tackle this problem,
collaborative deep learning models have recently been proposed where parties
locally train their deep learning structures and only share a subset of the
parameters in the attempt to keep their respective training sets private.
Parameters can also be obfuscated via differential privacy (DP) to make
information extraction even more challenging, as proposed by Shokri and
Shmatikov at CCS'15.
Unfortunately, we show that any privacy-preserving collaborative deep
learning is susceptible to a powerful attack that we devise in this paper. In
particular, we show that a distributed, federated, or decentralized deep
learning approach is fundamentally broken and does not protect the training
sets of honest participants. The attack we developed exploits the real-time
nature of the learning process that allows the adversary to train a Generative
Adversarial Network (GAN) that generates prototypical samples of the targeted
training set that was meant to be private (the samples generated by the GAN are
intended to come from the same distribution as the training data).
Interestingly, we show that record-level DP applied to the shared parameters of
the model, as suggested in previous work, is ineffective (i.e., record-level DP
is not designed to address our attack).
",1,0,0,1,0,0
16284,16285,Statistical Physics of the Symmetric Group,"  Ordered chains (such as chains of amino acids) are ubiquitous in biological
cells, and these chains perform specific functions contingent on the sequence
of their components. Using the existence and general properties of such
sequences as a theoretical motivation, we study the statistical physics of
systems whose state space is defined by the possible permutations of an ordered
list, i.e., the symmetric group, and whose energy is a function of how certain
permutations deviate from some chosen correct ordering. Such a non-factorizable
state space is quite different from the state spaces typically considered in
statistical physics systems and consequently has novel behavior in systems with
interacting and even non-interacting Hamiltonians. Various parameter choices of
a mean-field model reveal the system to contain five different physical regimes
defined by two transition temperatures, a triple point, and a quadruple point.
Finally, we conclude by discussing how the general analysis can be extended to
state spaces with more complex combinatorial properties and to other standard
questions of statistical mechanics models.
",0,1,0,0,0,0
17685,17686,Theory of Disorder-Induced Half-Integer Thermal Hall Conductance,"  Electrons that are confined to a single Landau level in a two dimensional
electron gas realize the effects of strong electron-electron repulsion in its
purest form. The kinetic energy of individual electrons is completely quenched
and all physical properties are dictated solely by many-body effects. A
remarkable consequence is the emergence of new quasiparticles with fractional
charge and exotic quantum statistics of which the most exciting ones are
non-Abelian quasiparticles. A non-integer quantized thermal Hall conductance
$\kappa_{xy}$ (in units of temperature times the universal constant $\pi^2
k_B^2 /3 h$; $h$ is the Planck constant and $k_B$ the Boltzmann constant)
necessitates the existence of such quasiparticles. It has been predicted, and
verified numerically, that such states are realized in the clean half-filled
first Landau level of electrons with Coulomb repulsion, with $\kappa_{xy}$
being either $3/2$ or $7/2$. Excitingly, a recent experiment has indeed
observed a half-integer value, which was measured, however, to be
$\kappa_{xy}=5/2$. We resolve this contradiction within a picture where smooth
disorder results in the formation of mesoscopic puddles with locally
$\kappa_{xy}=3/2$ or $7/2$. Interactions between these puddles generate a
coherent macroscopic state, which is reflected in an extended plateau with
quantized $\kappa_{xy}=5/2$. The topological properties of quasiparticles at
large distances are determined by the macroscopic phase, and not by the
microscopic puddle where they reside. In principle, the same mechanism might
also allow non-Abelian quasiparticles to emerge from a system comprised of
microscopic Abelian puddles.
",0,1,0,0,0,0
10206,10207,Adaptive Neural Networks for Efficient Inference,"  We present an approach to adaptively utilize deep neural networks in order to
reduce the evaluation time on new examples without loss of accuracy. Rather
than attempting to redesign or approximate existing networks, we propose two
schemes that adaptively utilize networks. We first pose an adaptive network
evaluation scheme, where we learn a system to adaptively choose the components
of a deep network to be evaluated for each example. By allowing examples
correctly classified using early layers of the system to exit, we avoid the
computational time associated with full evaluation of the network. We extend
this to learn a network selection system that adaptively selects the network to
be evaluated for each example. We show that computational time can be
dramatically reduced by exploiting the fact that many examples can be correctly
classified using relatively efficient networks and that complex,
computationally costly networks are only necessary for a small fraction of
examples. We pose a global objective for learning an adaptive early exit or
network selection policy and solve it by reducing the policy learning problem
to a layer-by-layer weighted binary classification problem. Empirically, these
approaches yield dramatic reductions in computational cost, with up to a 2.8x
speedup on state-of-the-art networks from the ImageNet image recognition
challenge with minimal (<1%) loss of top5 accuracy.
",1,0,0,1,0,0
13992,13993,Sampling and Reconstruction of Graph Signals via Weak Submodularity and Semidefinite Relaxation,"  We study the problem of sampling a bandlimited graph signal in the presence
of noise, where the objective is to select a node subset of prescribed
cardinality that minimizes the signal reconstruction mean squared error (MSE).
To that end, we formulate the task at hand as the minimization of MSE subject
to binary constraints, and approximate the resulting NP-hard problem via
semidefinite programming (SDP) relaxation. Moreover, we provide an alternative
formulation based on maximizing a monotone weak submodular function and propose
a randomized-greedy algorithm to find a sub-optimal subset. We then derive a
worst-case performance guarantee on the MSE returned by the randomized greedy
algorithm for general non-stationary graph signals. The efficacy of the
proposed methods is illustrated through numerical simulations on synthetic and
real-world graphs. Notably, the randomized greedy algorithm yields an
order-of-magnitude speedup over state-of-the-art greedy sampling schemes, while
incurring only a marginal MSE performance loss.
",1,0,0,1,0,0
352,353,Qualification Conditions in Semi-algebraic Programming,"  For an arbitrary finite family of semi-algebraic/definable functions, we
consider the corresponding inequality constraint set and we study qualification
conditions for perturbations of this set. In particular we prove that all
positive diagonal perturbations, save perhaps a finite number of them, ensure
that any point within the feasible set satisfies Mangasarian-Fromovitz
constraint qualification. Using the Milnor-Thom theorem, we provide a bound for
the number of singular perturbations when the constraints are polynomial
functions. Examples show that the order of magnitude of our exponential bound
is relevant. Our perturbation approach provides a simple protocol to build
sequences of ""regular"" problems approximating an arbitrary
semi-algebraic/definable problem. Applications to sequential quadratic
programming methods and sum of squares relaxation are provided.
",0,0,1,0,0,0
6843,6844,Micro-sized cold atmospheric plasma source for brain and breast cancer treatment,"  Micro-sized cold atmospheric plasma (uCAP) has been developed to expand the
applications of CAP in cancer therapy. In this paper, uCAP devices with
different nozzle lengths were applied to investigate effects on both brain
(glioblastoma U87) and breast (MDA-MB-231) cancer cells. Various diagnostic
techniques were employed to evaluate the parameters of uCAP devices with
different lengths such as potential distribution, electron density, and optical
emission spectroscopy. The generation of short- and long-lived species (such as
hydroxyl radical (.OH), superoxide (O2-), hydrogen peroxide (H2O2), nitrite
(NO2-), et al) were studied. These data revealed that uCAP treatment with a 20
mm length tube has a stronger effect than that of the 60 mm tube due to the
synergetic effects of reactive species and free radicals. Reactive species
generated by uCAP enhanced tumor cell death in a dose-dependent fashion and was
not specific with regards to tumor cell type.
",0,0,0,0,1,0
5473,5474,Algebraic cycles on some special hyperkähler varieties,"  This note contains some examples of hyperkähler varieties $X$ having a
group $G$ of non-symplectic automorphisms, and such that the action of $G$ on
certain Chow groups of $X$ is as predicted by Bloch's conjecture. The examples
range in dimension from $6$ to $132$. For each example, the quotient $Y=X/G$ is
a Calabi-Yau variety which has interesting Chow-theoretic properties; in
particular, the variety $Y$ satisfies (part of) a strong version of the
Beauville-Voisin conjecture.
",0,0,1,0,0,0
10105,10106,Suppression of material transfer at contacting surfaces: The effect of adsorbates on Al/TiN and Cu/diamond interfaces from first-principles calculations,"  The effect of monolayers of oxygen (O) and hydrogen (H) on the possibility of
material transfer at aluminium/titanium nitride (Al/TiN) and copper/diamond
(Cu/C$_{\text{dia}}$) interfaces, respectively, were investigated within the
framework of density functional theory (DFT). To this end the approach,
contact, and subsequent separation of two atomically flat surfaces consisting
of the aforementioned pairs of materials were simulated. These calculations
were performed for the clean as well as oxygenated and hydrogenated Al and
C$_{\text{dia}}$ surfaces, respectively. Various contact configurations were
considered by studying several lateral arrangements of the involved surfaces at
the interface. Material transfer is typically possible at interfaces between
the investigated clean surfaces; however, the addition of O to the Al and H to
the C$_{\text{dia}}$ surfaces was found to hinder material transfer. This
passivation occurs because of a significant reduction of the adhesion energy at
the examined interfaces, which can be explained by the distinct bonding
situations.
",0,1,0,0,0,0
18689,18690,Scale-free networks are rare,"  A central claim in modern network science is that real-world networks are
typically ""scale free,"" meaning that the fraction of nodes with degree $k$
follows a power law, decaying like $k^{-\alpha}$, often with $2 < \alpha < 3$.
However, empirical evidence for this belief derives from a relatively small
number of real-world networks. We test the universality of scale-free structure
by applying state-of-the-art statistical tools to a large corpus of nearly 1000
network data sets drawn from social, biological, technological, and
informational sources. We fit the power-law model to each degree distribution,
test its statistical plausibility, and compare it via a likelihood ratio test
to alternative, non-scale-free models, e.g., the log-normal. Across domains, we
find that scale-free networks are rare, with only 4% exhibiting the
strongest-possible evidence of scale-free structure and 52% exhibiting the
weakest-possible evidence. Furthermore, evidence of scale-free structure is not
uniformly distributed across sources: social networks are at best weakly scale
free, while a handful of technological and biological networks can be called
strongly scale free. These results undermine the universality of scale-free
networks and reveal that real-world networks exhibit a rich structural
diversity that will likely require new ideas and mechanisms to explain.
",1,0,0,1,1,0
5013,5014,The Topology of Statistical Verifiability,"  Topological models of empirical and formal inquiry are increasingly
prevalent. They have emerged in such diverse fields as domain theory [1, 16],
formal learning theory [18], epistemology and philosophy of science [10, 15, 8,
9, 2], statistics [6, 7] and modal logic [17, 4]. In those applications, open
sets are typically interpreted as hypotheses deductively verifiable by true
propositional information that rules out relevant possibilities. However, in
statistical data analysis, one routinely receives random samples logically
compatible with every statistical hypothesis. We bridge the gap between
propositional and statistical data by solving for the unique topology on
probability measures in which the open sets are exactly the statistically
verifiable hypotheses. Furthermore, we extend that result to a topological
characterization of learnability in the limit from statistical data.
",1,0,0,0,0,0
1033,1034,Khintchine's Theorem with random fractions,"  We prove versions of Khintchine's Theorem (1924) for approximations by
rational numbers whose numerators lie in randomly chosen sets of integers, and
we explore the extent to which the monotonicity assumption can be removed.
Roughly speaking, we show that if the number of available fractions for each
denominator grows too fast, then the monotonicity assumption cannot be removed.
There are questions in this random setting which may be seen as cognates of the
Duffin-Schaeffer Conjecture (1941), and are likely to be more accessible. We
point out that the direct random analogue of the Duffin-Schaeffer Conjecture,
like the Duffin-Schaeffer Conjecture itself, implies Catlin's Conjecture
(1976). It is not obvious whether the Duffin-Schaeffer Conjecture and its
random version imply one another, and it is not known whether Catlin's
Conjecture implies either of them. The question of whether Catlin implies
Duffin-Schaeffer has been unsettled for decades.
",0,0,1,0,0,0
